[
  {
    "url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nResource Manager\nDocumentation\nGuides\nThe Organization Policy Service gives you centralized and programmatic control over your\norganization's cloud resources. As theorganization policy administrator,\nyou can configure constraints across your entireresource hierarchy.\nCentralize control to configure restrictions on how your organization's\nresources can be used.\nDefine and establish guardrails for your development teams to stay within\ncompliance boundaries.\nHelp project owners and their teams move quickly without worry of breaking\ncompliance.\nOrganization policies allow you to do the following:\nLimit resource sharing based on domain.\nLimit the usage of Identity and Access Management (IAM) service accounts.\nRestrict the physical location of newly created resources.\nThere are many more constraints that give you fine-grained control of your\norganization's resources. For more information, see thelist of all Organization Policy Service constraints.\nIdentity and Access Managementfocuses onwho, and lets the administratorauthorizewho can take action on\nspecific resources based on permissions.\nOrganization Policy focuses onwhat, and lets the administrator set\nrestrictions on specific resources to determine how they can be configured.\nAn organization policy configures a singleconstraintthat\nrestricts one or more Google Cloud services. The organization policy is\nset on an organization, folder, or project resource to enforce the constraint on\nthat resource and any child resources.\nAn organization policy contains one or morerulesthat specify how, and\nwhether, to enforce the constraint. For example, an organization policy could\ncontain one rule that enforces the constraint only on resources taggedenvironment=development, and another rule that prevents the constraint from\nbeing enforced on other resources.\nDescendants of the resource to which the organization policy is attachedinheritthe organization policy. By applying an organization\npolicy to the organization resource, the organization policy administrator can\ncontrol enforcement of that organization policy and configuration of\nrestrictions across your organization.\nA constraint is a particular type of restriction against aGoogle Cloud serviceor a list\nof Google Cloud services. Think of the constraint as a blueprint that\ndefines what behaviors are controlled. For example, you can restrict project\nresources from accessing Compute Engine storage resources using thecompute.storageResourceUseRestrictionsconstraint.\nThis blueprint is then set on a resource in yourresource hierarchyas an organization policy, which applies the rules defined in the constraint.\nThe Google Cloud service mapped to that constraint and associated with\nthat resource enforces the restrictions configured within the organization\npolicy.\nAn organization policy is defined in a YAML or JSON file by the constraint it\nenforces, and optionally by the conditions under which the constraint are\nenforced. Each organization policy enforces exactly one constraint in active\nmode, dry-run mode, or both.\nPredefined constraints have aconstraint typeof list or boolean, which\ndetermines the values that can be used for checking enforcement. The enforcing\nGoogle Cloud service will evaluate the constraint type and value to\ndetermine the restriction that is enforced.\nCustom constraints are functionally similar to boolean constraints, and are\neither enforced or not enforced.\nManaged constraints have list or boolean parameters. The available parameters\nare determined by the enforcing Google Cloud service.\nA list constraint is a predefined constraint that allows or disallows a list of\nvalues that is defined in an organization policy. This list of values is\nexpressed as a hierarchy subtree string. The subtree string specifies the type\nof resource it applies to. For example, the list constraintconstraints/compute.trustedImageProjectstakes a list of project IDs in the\nform ofprojects/PROJECT_ID.\nValues can be given a prefix in the formprefix:valuefor constraints that\nsupport them, which gives the value additional meaning:\nis:- applies a comparison against the exact value. This is the same\nbehavior as not having a prefix, and is required when the value includes a\ncolon.\nis:- applies a comparison against the exact value. This is the same\nbehavior as not having a prefix, and is required when the value includes a\ncolon.\nunder:- applies a comparison to the value and all of its child values. If\na resource is allowed or denied with this prefix, its child resources are also\nallowed or denied. The value provided must be the ID of an organization,\nfolder, or project resource.\nunder:- applies a comparison to the value and all of its child values. If\na resource is allowed or denied with this prefix, its child resources are also\nallowed or denied. The value provided must be the ID of an organization,\nfolder, or project resource.\nin:- applies a comparison to all resources that include this value. For\nexample, you can addin:us-locationsto the denied list of theconstraints/gcp.resourceLocationsconstraint to block all locations that are\nincluded in theusregion.\nin:- applies a comparison to all resources that include this value. For\nexample, you can addin:us-locationsto the denied list of theconstraints/gcp.resourceLocationsconstraint to block all locations that are\nincluded in theusregion.\nIf no list of values is provided, or the organization policy is set to the\nGoogle-managed default, then the default behavior of the constraint takes\neffect, which either allows all values or denies all values.\nThe following organization policy enforces a list constraint that allows\nthe Compute Engine VM instancesvm-1andvm-2inorganizations/1234567890123to access external IP addresses:\nA boolean constraint is a predefined constraint that is either enforced or not\nenforced. For example, the predefined constraintconstraints/compute.disableSerialPortAccesshas two possible states:\nEnforced - the constraint is enforced, and serial port access is not\nallowed.\nNot enforced - thedisableSerialPortAccessconstraint is not enforced or\nchecked, so serial port access is allowed.\nIf the organization policy is set to the Google-managed default, then the\ndefault behavior for the constraint takes effect.\nThe following organization policy enforces a predefined constraint that disables\nthe creation of external service accounts inorganizations/1234567890123:\nManaged constraints are constraints that have been built on thecustom organization policyplatform. The custom\norganization policy platform allows for organization policies to be designed\nwith more flexibility, and with greater insight fromPolicy Intelligence tools.\nManaged constraints are designed to replace equivalent predefined constraints.\nIf the equivalent predefined constraint has a constraint type of boolean, the\nmanaged constraint can either be enforced or not in the same way. For example,\nthe following organization policy enforcesiam.managed.disableServiceAccountCreation, which is the equivalent constraint\ntoiam.disableServiceAccountCreation:\nIf the equivalent predefined constraint has a constraint type of list, the\nmanaged constraint supports defining parameters that define the resources and\nbehaviors that are restricted by the constraint. For example, the following\norganization policy enforces a managed constraint that only allows theexample.comandaltostrat.comdomains to be added to\nEssential Contacts fororganizations/1234567890123:\nTo learn more about using managed constraints, seeUsing constraints.\nCustom constraints allow or restrict resource creation and updates in the same\nway that boolean constraints do, but allow administrators to configure\nconditions based on request parameters and other metadata. You can usePolicy Intelligence toolsto test and analyze\nyour custom organization policies.\nFor a list of service resources that support custom constraints, seeCustom constraint supported services.\nTo learn more about using custom organization policies, seeCreating and managing custom organization policies.\nAn organization policy in dry-run mode is created and enforced similarly to\nother organization policies, and violations of the policy are audit-logged, but\nthe violating actions aren't denied.\nYou can use organization policies in dry-run mode to monitor how policy changes\nwould impact your workflows before it is enforced. For more information, seeCreate an organization policy in dry-run mode.\nTags provide a way to conditionally enforce constraints based on whether a\nresource has a specific tag. You can use tags and conditional enforcement of\nconstraints to provide centralized control of the resources in your hierarchy.\nFor more information about tags, seeTags overview. To learn how to set\na conditional organization policy using tags, seeSetting an organization policy with tags.\nWhen an organization policy is set on a resource, all descendants of that\nresource inherit the organization policy by default. If you set an\norganization policy on the organization resource, then the configuration of\nrestrictions defined by that policy will be passed down through all descendant\nfolders, projects, and service resources.\nYou can set an organization policy on a descendant resource that either\noverwrites the inheritance, or inherits the organization policy of the parent\nresource. In the latter case, the two organization policies are merged based on\nthe rules of hierarchy evaluation. This provides precise control for how your\norganization policies apply throughout your organization, and where you want\nexceptions made.\nTo learn more, seeUnderstanding hierarchy evaluation.\nA violation is when a Google Cloud service acts or is in a state that is\ncounter to the organization policy restriction configuration within the scope of\nits resource hierarchy. Google Cloud services will enforce constraints to\nprevent violations, but the application of new organization policies is usually\nnot retroactive. If an organization policy constraint is retroactively\nenforced, it will be labeled as such on theorganization policy constraintspage.\nIf a new organization policy sets a restriction on an action or state that a\nservice is already in, the policy is considered to be in violation, but the\nservice won't stop its original behavior. You will need to address this\nviolation manually. This prevents the risk of a new organization policy\ncompletely shutting down your business continuity.\nPolicy Intelligence is a suite of tools designed to help you manage\nsecurity policies. These tools can help you understand resource usage,\nunderstand and improve existing security policies, and prevent policy\nmisconfigurations.\nSome Policy Intelligence tools are specifically designed to help\ntest and analyze Organization Policy Service policies. We recommend that you test and\ndry-run all changes to your organization policies. With\nPolicy Intelligence, you can do tasks like the following:\nTest changesto\norganization policies and constraints and identify resources that are\nnon-compliant under the proposed\npolicy (Preview))\nCreate a dry-run organization policyto monitor how a policy change would affect your workflows\nAnalyze existing organization policiesto understand which Google Cloud resources are covered by which\norganization policy\nTo learn more about these tools and other Policy Intelligence\ntools, seePolicy Intelligence overview.\nRead theCreating and managing organization resourcespage to learn how to acquire an organization resource.\nLearnhow to define organization policies.\nExplore thesolutions you can accomplishwith organization policy constraints.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-01 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/training-overview",
    "title": "Train and use your own modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nAutoML: Create and train models with minimal technical knowledge\nand effort. To learn more about AutoML, seeAutoML beginner's guide.\nVertex AI custom training: Create and train models at scale using any ML framework.\nTo learn more about custom training on Vertex AI, seeCustom training overview.\nRay on Vertex AI: Use open source Ray code to write programs and\ndevelop applications on Vertex AI with minimal changes.\nFor help on deciding which of these methods to use, seeChoose a training method.\nAutoML on Vertex AI lets you build a code-free ML model based\non the training data that you provide. AutoML can automate tasks like\ndata preparation, model selection, hyperparameter tuning, and deployment for\nvarious data types and prediction tasks, which can make ML more accessible for\na wide range of users.\nThe types of models you can build depend on the type of data that you have.\nVertex AI offers AutoML solutions for the following data types and\nmodel objectives:\nTo learn more about AutoML, seeAutoML training overview.\nIf none of the AutoML solutions address your needs, you can also create\nyour own training application and use it to train custom models on\nVertex AI. You can use any ML framework that you want and configure the\ncompute resources to use for training, including the following:\nType and number of VMs.\nGraphics processing units (GPUs).\nTensor processing units (TPUs).\nType and size of boot disk.\nTo learn more about custom training on Vertex AI, seeCustom training overview.\nRay on Vertex AI is a service that lets you use the open-source\nRay framework for scaling AI and Python applications directly within the\nVertex AI platform. Ray is designed to provide the\ninfrastructure for distributed computing and parallel processing for your\nML workflow.\nRay on Vertex AI provides a managed environment for running\ndistributed applications using the Ray framework, offering scalability and\nintegration with Google Cloud services.\nTo learn more about Ray on Vertex AI seeRay on Vertex AI overview.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-12 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nDocumentation\nAuthentication\nThis document helps you understand some key authentication methods and concepts,\nand where to get help with implementing or troubleshooting authentication.\n\nThe primary focus of the authentication documentation is for Google Cloud\nservices, but the list ofauthentication use casesand the\nintroductory material on this page includes use cases for other Google products\nas well.IntroductionAuthentication is the process by which your identity is confirmed\nthrough the use of some kind ofcredential. Authentication is\nabout proving that you are who you say you are.Google provides many APIs and services, which require\nauthentication to access. Google also provides a number of\nservices that host applications written by our customers; these applications\nalso need to determine the identity of their users.How to get help with authenticationI want to...InformationAuthenticate to Vertex AI in express mode\n      (Preview).Use the API key created for you during the sign-on process to\n      authenticate to Vertex AI. For more information, seeVertex AI in express mode overview.Authenticate to a Google Cloud service from my application using\n      a high-level programming language.Set up Application Default Credentials,\n      and then use one of theCloud Client Libraries.Authenticate to an application that requires an ID token.Get an OpenID Connect (OIDC) ID tokenand provide it with your request.Implement user authentication for an application that accesses Google\n      or Google Cloud services and resources.SeeAuthenticate application usersfor a comparison of options.Try out somegcloudcommands in my local development\n      environment.Initialize the gcloud CLI.Try out some Google Cloud REST API\n      requests in my local development environment.Use a command-line tool such ascurltocall the REST API.Try out a code snippet included in my product documentation.Set up ADC for a local development environment,\n      and install your product's client library in your local environment. The\n      client libraryfinds your credentials automatically.Get help with another authentication use case.See theAuthentication use casespage.See a list of the products Google provides in the identity and access\n      management space.See theGoogle identity and access management productspage.Choose the right authentication method for your use caseWhen you access Google Cloud services by using the Google Cloud CLI, Cloud Client Libraries,\n  tools that supportApplication Default Credentials (ADC)like Terraform, or REST requests, use the following diagram to help you choose an authentication\n  method:This diagram guides you through the following questions:Are you running code in a single-user development environment, such as your own workstation,\n    Cloud Shell, or a virtual desktop interface?If yes, proceed to question 4.If no, proceed to question 2.Are you running code in Google Cloud?If yes, proceed to question 3.If no, proceed to question 5.Are you running containers in Google Kubernetes Engine?If yes, useWorkload Identity Federation for GKEto attach service accounts to Kubernetes pods.If no,attach a service accountto the resource.Does your use case require a service account?For example, you want to configure authentication and authorization consistently for your\n      application across all environments.If no,authenticate with user credentials.If yes,impersonate a service account with user credentials.Does your workload authenticate with an external identity provider that supportsworkload identity federation?If yes,configure Workload Identity Federationto let applications running on-premises or on other cloud providers use a service account.If no,create a service account key.OAuth 2.0Google APIs implement and extend theOAuth 2.0 framework.\nSee the documentation for your environment and use case for details.Authorization methods for Google Cloud servicesGoogle Cloud services useIdentity and Access Management (IAM)for authentication. IAM offers granular control, by principal\nand by resource. When you authenticate to Google Cloud services, you\ngenerally use a scope that includes all Google Cloud services\n(https://www.googleapis.com/auth/cloud-platform).OAuth 2.0 scopes can provide a second layer of protection, which is useful\nif your code is running in an environment where token security is a concern,\nsuch as a mobile app. In this scenario, you can use\nfiner-grained scopes to reduce risk in the event of a compromised token. You\ncan find the list of scopes accepted by an API method in its API reference\npages in the product documentation.Application Default CredentialsApplication Default Credentials (ADC)is a strategy used by the authentication libraries\nto automatically find credentials based on the application environment. The authentication libraries\nmake those credentials available toCloud Client Libraries and Google API Client Libraries.\nWhen you use ADC, your code can run in either a development or production environment without\nchanging how your application authenticates to Google Cloud services and APIs.Using ADC can simplify your development process, because it lets you use the\nsame authentication code in a variety of environments. If you're using a service\nin express mode, however, you don't need to use ADC.Before you can use ADC,you must provide your credentials to ADC,\nbased on where you want your code to run. ADCautomatically locates credentialsand gets a token in the background,\nenabling your authentication code to run in different environments without\nmodification. For example, the same version of your code could authenticate with\nGoogle Cloud APIs when running on a development workstation or on\nCompute Engine.Your gcloud credentials are not the same as the credentials you provide to ADC using the\ngcloud CLI. For more information, seegcloud CLI authentication configuration and ADC configuration.TerminologyThe following terms are important to understand when discussing authentication\nand authorization.AuthenticationAuthentication is the process of determining the identity of the principal\nattempting to access a resource.AuthorizationAuthorization is the process of determining whether the principal or application\nattempting to access a resource has been authorized for that level of access.CredentialsWhen this document uses the termuser account, it refers to a Google Account,\n  or a user account managed by your identity provider and federated withWorkforce Identity Federation.For authentication, credentials are a digital object that provide proof of\nidentity. Passwords, PINs, and biometric data can all be used as credentials,\ndepending on the application requirements. For example, when you log into your\nuser account, you provide your password and satisfy any two-factor\nauthentication requirement as proof that the account in fact belongs to you, and\nyou are not being spoofed by a bad actor.Tokensare not credentials. They are a digital object that proves that\nthe caller provided proper credentials.The type of credential you need to provide depends on what you are\nauthenticating to.The following types of credentials can be created in the\nGoogle Cloud console:API keysYou can use API keys with APIs that accept them to access the API. API\n      keys that are not bound to a service account provide a project, which is\n      used for billing and quota purposes. If the API key is bound to a service\n      account, the API key also provides the identity and authorization of the\n      service account (Preview).For more information about API\n      keys, seeAPI keys. For more\n      information about API keys that are bound to a service account, see theGoogle Cloud express mode FAQ.OAuth Client IDsOAuth Client IDs are used to identify an application to\n      Google Cloud. This is necessary when you want to access resources\n      owned by your end users, also called three-legged OAuth (3LO). For more\n      information about how to get and use an OAuth Client ID, seeSetting up OAuth 2.0.Service account keysService account keys identify a principal (the service account) and the\n      project associated with the service account.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the\nsecurity of the private key and for other operations described byBest practices for managing service account keys.\nIf you are prevented from creating a service account key, service account key creation might\nbe disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.You can also create credentials by using the gcloud CLI. These\ncredentials include the following types:Local ADC filesCredential configurations used byWorkload Identity FederationCredential configurations used byWorkforce Identity FederationNote:If you are accepting credential configurations (JSON, files, or streams)\ncreated by an external organization, you must validate the credential\nconfiguration before you use it. For more information, seeSecurity\nrequirements when using credential configurations from an external source.PrincipalA principal is an identity that can be granted access\nto a resource. For authentication, Google APIs support two types of principals:user accountsandservice accounts.Whether you use a user account or a service account to authenticate depends on\nyour use case. You might use both, each at different stages of your project or\nin different development environments.User accountsUser accounts represent a developer, administrator, or any other person who\ninteracts with Google APIs and services.User accounts are managed asGoogle Accounts,\neither withGoogle WorkspaceorCloud Identity. They can also be user accounts that are managed\nby a third-party identity provider and federated withWorkforce Identity Federation.With a user account, you can authenticate to Google APIs and services in the\nfollowing ways:Use the gcloud CLI toset up Application Default Credentials (ADC).Use your user credentials tosign in to the Google Cloud CLI, and then use the tool to\naccess Google Cloud services.Use your user credentials toimpersonate a service account.Use your user credentials tosign in to the Google Cloud CLI, and then use the tool togenerate access tokens.For an overview of ways to configure identities for users in Google Cloud,\nseeIdentities for users.Service accountsService accountsare accounts that do not\nrepresent a human user. They provide a way to manage authentication and\nauthorization when a human is not directly involved, such as when an application\nneeds to access Google Cloud resources. Service accounts are managed by\nIAM.The following list provides some methods for using a service account to\nauthenticate to Google APIs and services, in order from most secure to least\nsecure. For more information, seeChoose the right authentication method for your use caseon this page.Attach a user-managed service account to the resourceanduse ADC to authenticate.This is the recommended way to authenticate production code running on\nGoogle Cloud.Use a service account to impersonate another service account.Service account impersonation lets you temporarily grant more privileges to\na service account. Granting extra privileges on a temporary basis enables\nthat service account to perform the required access without having to\npermanently acquire more privilege.UseWorkload Identity Federationto authenticate workloads that run\non-premises or on a different cloud provider.Use thedefault service account.Using the default service account is not recommended, because by\ndefault the default service account is highly privileged, which violates theprinciple of least privilege.Use a service account key.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the\nsecurity of the private key and for other operations described byBest practices for managing service account keys.\nIf you are prevented from creating a service account key, service account key creation might\nbe disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.For an overview of ways to configure workload identities, including service\naccounts, for Google Cloud,\nseeIdentities for workloads. For best practices,\nseeBest practices for using service accounts.TokenFor authentication and authorization, a token is a digital object that shows\nthat a caller provided proper credentials that were exchanged for that token.\nThe token contains information about the identity of the principal making the\nrequest and what kind of access they are authorized to make.Tokens can be thought of as being like hotel keys. When you check in to a hotel\nand present the proper documentation to the hotel registration desk, you receive\na key that gives you access to specific hotel resources. For example, the key\nmight give you access to your room and the guest elevator, but would not give\nyou access to any other room or the service elevator.With the exception of API keys, Google APIs do not support credentials directly.\nYour application must acquire or generate a token and provide it to the API.\nThere are several different types of tokens. For more information, seeToken types.Workload and workforceGoogle Cloud identity and access products enable access to\nGoogle Cloud services and resources for both programmatic access and human\nusers. Google Cloud uses the termsworkloadfor programmatic access andworkforcefor user access.Workload Identity Federationlets you provide access to\non-premises or multi-cloud workloads without having to create and manage\nservice account keys.Workforce Identity Federationlets you use an external identity provider\nto authenticate and authorize a workforce—a group of users, such as employees,\npartners, and contractors—using IAM, so that the users can access\nGoogle Cloud services.What's nextLearn more about how Google Cloud servicesuse IAM to control access to Google Cloud resources.Understandhow Application Default Credentials works, andhow you can set it up for a variety of development environments.\nAuthentication is the process by which your identity is confirmed\nthrough the use of some kind ofcredential. Authentication is\nabout proving that you are who you say you are.\nGoogle provides many APIs and services, which require\nauthentication to access. Google also provides a number of\nservices that host applications written by our customers; these applications\nalso need to determine the identity of their users.\nWhen you access Google Cloud services by using the Google Cloud CLI, Cloud Client Libraries,\n  tools that supportApplication Default Credentials (ADC)like Terraform, or REST requests, use the following diagram to help you choose an authentication\n  method:\nThis diagram guides you through the following questions:\nAre you running code in a single-user development environment, such as your own workstation,\n    Cloud Shell, or a virtual desktop interface?If yes, proceed to question 4.If no, proceed to question 2.\nIf yes, proceed to question 4.\nIf no, proceed to question 2.\nAre you running code in Google Cloud?If yes, proceed to question 3.If no, proceed to question 5.\nIf yes, proceed to question 3.\nIf no, proceed to question 5.\nAre you running containers in Google Kubernetes Engine?If yes, useWorkload Identity Federation for GKEto attach service accounts to Kubernetes pods.If no,attach a service accountto the resource.\nIf yes, useWorkload Identity Federation for GKEto attach service accounts to Kubernetes pods.\nIf no,attach a service accountto the resource.\nDoes your use case require a service account?For example, you want to configure authentication and authorization consistently for your\n      application across all environments.If no,authenticate with user credentials.If yes,impersonate a service account with user credentials.\nDoes your use case require a service account?\nFor example, you want to configure authentication and authorization consistently for your\n      application across all environments.\nIf no,authenticate with user credentials.\nIf yes,impersonate a service account with user credentials.\nDoes your workload authenticate with an external identity provider that supportsworkload identity federation?If yes,configure Workload Identity Federationto let applications running on-premises or on other cloud providers use a service account.If no,create a service account key.\nIf yes,configure Workload Identity Federationto let applications running on-premises or on other cloud providers use a service account.\nIf no,create a service account key.\nGoogle APIs implement and extend theOAuth 2.0 framework.\nSee the documentation for your environment and use case for details.\nGoogle Cloud services useIdentity and Access Management (IAM)for authentication. IAM offers granular control, by principal\nand by resource. When you authenticate to Google Cloud services, you\ngenerally use a scope that includes all Google Cloud services\n(https://www.googleapis.com/auth/cloud-platform).\nOAuth 2.0 scopes can provide a second layer of protection, which is useful\nif your code is running in an environment where token security is a concern,\nsuch as a mobile app. In this scenario, you can use\nfiner-grained scopes to reduce risk in the event of a compromised token. You\ncan find the list of scopes accepted by an API method in its API reference\npages in the product documentation.\nApplication Default Credentials (ADC)is a strategy used by the authentication libraries\nto automatically find credentials based on the application environment. The authentication libraries\nmake those credentials available toCloud Client Libraries and Google API Client Libraries.\nWhen you use ADC, your code can run in either a development or production environment without\nchanging how your application authenticates to Google Cloud services and APIs.\nUsing ADC can simplify your development process, because it lets you use the\nsame authentication code in a variety of environments. If you're using a service\nin express mode, however, you don't need to use ADC.\nBefore you can use ADC,you must provide your credentials to ADC,\nbased on where you want your code to run. ADCautomatically locates credentialsand gets a token in the background,\nenabling your authentication code to run in different environments without\nmodification. For example, the same version of your code could authenticate with\nGoogle Cloud APIs when running on a development workstation or on\nCompute Engine.\nYour gcloud credentials are not the same as the credentials you provide to ADC using the\ngcloud CLI. For more information, seegcloud CLI authentication configuration and ADC configuration.\nThe following terms are important to understand when discussing authentication\nand authorization.\nAuthentication is the process of determining the identity of the principal\nattempting to access a resource.\nAuthorization is the process of determining whether the principal or application\nattempting to access a resource has been authorized for that level of access.\nWhen this document uses the termuser account, it refers to a Google Account,\n  or a user account managed by your identity provider and federated withWorkforce Identity Federation.\nFor authentication, credentials are a digital object that provide proof of\nidentity. Passwords, PINs, and biometric data can all be used as credentials,\ndepending on the application requirements. For example, when you log into your\nuser account, you provide your password and satisfy any two-factor\nauthentication requirement as proof that the account in fact belongs to you, and\nyou are not being spoofed by a bad actor.\nTokensare not credentials. They are a digital object that proves that\nthe caller provided proper credentials.\nThe type of credential you need to provide depends on what you are\nauthenticating to.\nThe following types of credentials can be created in the\nGoogle Cloud console:\nAPI keysYou can use API keys with APIs that accept them to access the API. API\n      keys that are not bound to a service account provide a project, which is\n      used for billing and quota purposes. If the API key is bound to a service\n      account, the API key also provides the identity and authorization of the\n      service account (Preview).For more information about API\n      keys, seeAPI keys. For more\n      information about API keys that are bound to a service account, see theGoogle Cloud express mode FAQ.\nAPI keys\nYou can use API keys with APIs that accept them to access the API. API\n      keys that are not bound to a service account provide a project, which is\n      used for billing and quota purposes. If the API key is bound to a service\n      account, the API key also provides the identity and authorization of the\n      service account (Preview).\nFor more information about API\n      keys, seeAPI keys. For more\n      information about API keys that are bound to a service account, see theGoogle Cloud express mode FAQ.\nOAuth Client IDsOAuth Client IDs are used to identify an application to\n      Google Cloud. This is necessary when you want to access resources\n      owned by your end users, also called three-legged OAuth (3LO). For more\n      information about how to get and use an OAuth Client ID, seeSetting up OAuth 2.0.\nOAuth Client IDs\nOAuth Client IDs are used to identify an application to\n      Google Cloud. This is necessary when you want to access resources\n      owned by your end users, also called three-legged OAuth (3LO). For more\n      information about how to get and use an OAuth Client ID, seeSetting up OAuth 2.0.\nService account keysService account keys identify a principal (the service account) and the\n      project associated with the service account.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the\nsecurity of the private key and for other operations described byBest practices for managing service account keys.\nIf you are prevented from creating a service account key, service account key creation might\nbe disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.\nService account keys\nService account keys identify a principal (the service account) and the\n      project associated with the service account.\nIf you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.\nYou can also create credentials by using the gcloud CLI. These\ncredentials include the following types:\nLocal ADC files\nCredential configurations used byWorkload Identity Federation\nCredential configurations used byWorkforce Identity Federation\nA principal is an identity that can be granted access\nto a resource. For authentication, Google APIs support two types of principals:user accountsandservice accounts.\nWhether you use a user account or a service account to authenticate depends on\nyour use case. You might use both, each at different stages of your project or\nin different development environments.\nUser accounts represent a developer, administrator, or any other person who\ninteracts with Google APIs and services.\nUser accounts are managed asGoogle Accounts,\neither withGoogle WorkspaceorCloud Identity. They can also be user accounts that are managed\nby a third-party identity provider and federated withWorkforce Identity Federation.With a user account, you can authenticate to Google APIs and services in the\nfollowing ways:Use the gcloud CLI toset up Application Default Credentials (ADC).Use your user credentials tosign in to the Google Cloud CLI, and then use the tool to\naccess Google Cloud services.Use your user credentials toimpersonate a service account.Use your user credentials tosign in to the Google Cloud CLI, and then use the tool togenerate access tokens.For an overview of ways to configure identities for users in Google Cloud,\nseeIdentities for users.Service accountsService accountsare accounts that do not\nrepresent a human user. They provide a way to manage authentication and\nauthorization when a human is not directly involved, such as when an application\nneeds to access Google Cloud resources. Service accounts are managed by\nIAM.The following list provides some methods for using a service account to\nauthenticate to Google APIs and services, in order from most secure to least\nsecure. For more information, seeChoose the right authentication method for your use caseon this page.Attach a user-managed service account to the resourceanduse ADC to authenticate.This is the recommended way to authenticate production code running on\nGoogle Cloud.Use a service account to impersonate another service account.Service account impersonation lets you temporarily grant more privileges to\na service account. Granting extra privileges on a temporary basis enables\nthat service account to perform the required access without having to\npermanently acquire more privilege.UseWorkload Identity Federationto authenticate workloads that run\non-premises or on a different cloud provider.Use thedefault service account.Using the default service account is not recommended, because by\ndefault the default service account is highly privileged, which violates theprinciple of least privilege.Use a service account key.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the\nsecurity of the private key and for other operations described byBest practices for managing service account keys.\nIf you are prevented from creating a service account key, service account key creation might\nbe disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.For an overview of ways to configure workload identities, including service\naccounts, for Google Cloud,\nseeIdentities for workloads. For best practices,\nseeBest practices for using service accounts.TokenFor authentication and authorization, a token is a digital object that shows\nthat a caller provided proper credentials that were exchanged for that token.\nThe token contains information about the identity of the principal making the\nrequest and what kind of access they are authorized to make.Tokens can be thought of as being like hotel keys. When you check in to a hotel\nand present the proper documentation to the hotel registration desk, you receive\na key that gives you access to specific hotel resources. For example, the key\nmight give you access to your room and the guest elevator, but would not give\nyou access to any other room or the service elevator.With the exception of API keys, Google APIs do not support credentials directly.\nYour application must acquire or generate a token and provide it to the API.\nThere are several different types of tokens. For more information, seeToken types.Workload and workforceGoogle Cloud identity and access products enable access to\nGoogle Cloud services and resources for both programmatic access and human\nusers. Google Cloud uses the termsworkloadfor programmatic access andworkforcefor user access.Workload Identity Federationlets you provide access to\non-premises or multi-cloud workloads without having to create and manage\nservice account keys.Workforce Identity Federationlets you use an external identity provider\nto authenticate and authorize a workforce—a group of users, such as employees,\npartners, and contractors—using IAM, so that the users can access\nGoogle Cloud services.What's nextLearn more about how Google Cloud servicesuse IAM to control access to Google Cloud resources.Understandhow Application Default Credentials works, andhow you can set it up for a variety of development environments.\nWith a user account, you can authenticate to Google APIs and services in the\nfollowing ways:\nUse the gcloud CLI toset up Application Default Credentials (ADC).\nUse your user credentials tosign in to the Google Cloud CLI, and then use the tool to\naccess Google Cloud services.\nUse your user credentials toimpersonate a service account.\nUse your user credentials tosign in to the Google Cloud CLI, and then use the tool togenerate access tokens.\nFor an overview of ways to configure identities for users in Google Cloud,\nseeIdentities for users.\nService accountsare accounts that do not\nrepresent a human user. They provide a way to manage authentication and\nauthorization when a human is not directly involved, such as when an application\nneeds to access Google Cloud resources. Service accounts are managed by\nIAM.\nThe following list provides some methods for using a service account to\nauthenticate to Google APIs and services, in order from most secure to least\nsecure. For more information, seeChoose the right authentication method for your use caseon this page.\nAttach a user-managed service account to the resourceanduse ADC to authenticate.This is the recommended way to authenticate production code running on\nGoogle Cloud.\nAttach a user-managed service account to the resourceanduse ADC to authenticate.\nThis is the recommended way to authenticate production code running on\nGoogle Cloud.\nUse a service account to impersonate another service account.Service account impersonation lets you temporarily grant more privileges to\na service account. Granting extra privileges on a temporary basis enables\nthat service account to perform the required access without having to\npermanently acquire more privilege.\nUse a service account to impersonate another service account.\nService account impersonation lets you temporarily grant more privileges to\na service account. Granting extra privileges on a temporary basis enables\nthat service account to perform the required access without having to\npermanently acquire more privilege.\nUseWorkload Identity Federationto authenticate workloads that run\non-premises or on a different cloud provider.\nUseWorkload Identity Federationto authenticate workloads that run\non-premises or on a different cloud provider.\nUse thedefault service account.Using the default service account is not recommended, because by\ndefault the default service account is highly privileged, which violates theprinciple of least privilege.\nUse thedefault service account.\nUsing the default service account is not recommended, because by\ndefault the default service account is highly privileged, which violates theprinciple of least privilege.\nUse a service account key.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the\nsecurity of the private key and for other operations described byBest practices for managing service account keys.\nIf you are prevented from creating a service account key, service account key creation might\nbe disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.\nUse a service account key.\nIf you acquired the service account key from an external source, you must validate it before use.\nFor more information, seeSecurity requirements for externally sourced credentials.\nFor an overview of ways to configure workload identities, including service\naccounts, for Google Cloud,\nseeIdentities for workloads. For best practices,\nseeBest practices for using service accounts.\nFor authentication and authorization, a token is a digital object that shows\nthat a caller provided proper credentials that were exchanged for that token.\nThe token contains information about the identity of the principal making the\nrequest and what kind of access they are authorized to make.\nTokens can be thought of as being like hotel keys. When you check in to a hotel\nand present the proper documentation to the hotel registration desk, you receive\na key that gives you access to specific hotel resources. For example, the key\nmight give you access to your room and the guest elevator, but would not give\nyou access to any other room or the service elevator.\nWith the exception of API keys, Google APIs do not support credentials directly.\nYour application must acquire or generate a token and provide it to the API.\nThere are several different types of tokens. For more information, seeToken types.\nGoogle Cloud identity and access products enable access to\nGoogle Cloud services and resources for both programmatic access and human\nusers. Google Cloud uses the termsworkloadfor programmatic access andworkforcefor user access.\nWorkload Identity Federationlets you provide access to\non-premises or multi-cloud workloads without having to create and manage\nservice account keys.\nWorkforce Identity Federationlets you use an external identity provider\nto authenticate and authorize a workforce—a group of users, such as employees,\npartners, and contractors—using IAM, so that the users can access\nGoogle Cloud services.\nLearn more about how Google Cloud servicesuse IAM to control access to Google Cloud resources.\nUnderstandhow Application Default Credentials works, andhow you can set it up for a variety of development environments.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-13 UTC."
  },
  {
    "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch",
    "title": "Google Distributed Cloud air-gapped documentationStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGoogle Distributed Cloud air-gapped\nThe Google Distributed Cloud (GDC) air-gapped\n                      option doesn't require connectivity to Google Cloud at any time to manage\n                      infrastructure, services, APIs or tools, and uses a\n                      local control plane for operations.\nDistributed Cloud runs sensitive workloads and supports public-sector customers\n                      and commercial entities to address data residency and\n                      strict security and privacy requirements.\nDistributed Cloud provides you with a safe and secure way to modernize an on-premises\n                      deployment, whether you do it yourself or choose to host\n                      through a designated, trusted partner.\nView Distributed Cloudgeneral availability announcement,product page,overview,release notes,FAQ, andapplication programming interfaces."
  },
  {
    "url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGoogle Cloud Free Program\nDocumentation\nThe Google Cloud Free Program comprises the following:\n90-day, $300\nFree Trial:\nNew Google Cloud and\nGoogle Maps Platform users can take advantage of a\n90-day trial period that includes $300 in\nfree Cloud Billing credits to explore and evaluate Google Cloud\nand Google Maps Platform products and services. You can use these\ncredits toward one or a combination of products.\nFree Tier:\nAll Google Cloud\ncustomers can use select Google Cloud products—like\nCompute Engine, Cloud Storage, and\nBigQuery—free of charge, within specified monthly usage limits.\nWhen you stay withinthe Free Tier limits,\nthese resources are not charged against your Free Trial credits\nor to your Cloud Billing account's payment method after your\ntrial ends.\nGoogle Maps Platform free usage caps:\nGoogle Maps Platform offers key pricing incentives, including free usage\ncaps and volume discounts. For more information,see the billing and pricing overview.\nFor more information about Google Maps Platform pay-as-you-go pricing,\nseethe Google Maps Platform pricing page,\nwhich includes a monthly cost calculator for Maps, Routes, Places, and\nEnvironment.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nThe Free Trial provides you with free Cloud Billing\ncredits to pay for resources used while you learn about Google Cloud.\nYou're eligible for the Free Trial if you meet the\n        following conditions:\nYou've never been a paying customer of Google Cloud,\n          Google Maps Platform, or Firebase.\nYou haven't previously signed up for the Free Trial.\nIf you're in India, you must have an INR-based Cloud Billing\n          account before creating Firebase billing accounts to sign up for the\n          Free Trial.\nTo complete your Free Trial signup, you must\n          provide acredit card or other payment methodwhich is used to set up a\n          Free Trial Cloud Billing account and to verify your\n          identity. Be assured, when a Cloud Billing account has thestatus of aFree trial account, Google won't charge\n          you.\nWhen you complete theStart freesignup, the\n        90-day, $300\n        Free Trial period starts automatically. TheFree Trial periodcan't be paused or extended.\nAfter you initiate the Free Trial, Google uses the\n        information you provided at signup to create a limited Google Cloud\n        account called aFree trial account. This account has the\n        following features:\nA newGoogle Cloud projectnamed \"My First Project\":Use this project to create and manageGoogle Cloud resources and services.Thisproject is linked to the new Cloud Billing account,\n          described in the following information.To control project access, the username provided during the\n          free trial signup is granted thelegacy basic Owner roleon the project, which provides admin-level\n          permissions. Learn more aboutIAM permissions.\nUse this project to create and manageGoogle Cloud resources and services.\nThisproject is linked to the new Cloud Billing account,\n          described in the following information.\nTo control project access, the username provided during the\n          free trial signup is granted thelegacy basic Owner roleon the project, which provides admin-level\n          permissions. Learn more aboutIAM permissions.\nA newCloud Billing accountnamed \"My Billing Account\":The billing account accrues Google Cloud usage costs that\n           are generated in the Google Cloud project, and the available\n           Free Trial credits pay for those usage costs.ThisCloud Billing account is linked to the Google payments profile, described in the following information.To control billing account access, the username provided during\n          the free trial signup is granted theBilling Account Administratorrole on the Cloud Billing\n          account.Until youactivate a full, paid account, thebillable status of the accountisFree trial account,\n           subject to theprogram coverage limitationsand theFree Trial Terms and Conditions.\nThe billing account accrues Google Cloud usage costs that\n           are generated in the Google Cloud project, and the available\n           Free Trial credits pay for those usage costs.\nThisCloud Billing account is linked to the Google payments profile, described in the following information.\nTo control billing account access, the username provided during\n          the free trial signup is granted theBilling Account Administratorrole on the Cloud Billing\n          account.\nUntil youactivate a full, paid account, thebillable status of the accountisFree trial account,\n           subject to theprogram coverage limitationsand theFree Trial Terms and Conditions.\nA newGoogle payments profileconfigured with the information\n          provided in the signup process:The payments profile stores information like name,\n           address, and tax ID (when required legally) of who is responsible for\n           the profile.Theform of paymentprovided during Free Trial signup\n           is stored and managed in the Google payments profile.To control access to the payments profile, the\n           username provided during the free trial signup is designated\n           theAdmin and Primary Contactfor this payments\n          profile.\nThe payments profile stores information like name,\n           address, and tax ID (when required legally) of who is responsible for\n           the profile.\nTheform of paymentprovided during Free Trial signup\n           is stored and managed in the Google payments profile.\nTo control access to the payments profile, the\n           username provided during the free trial signup is designated\n           theAdmin and Primary Contactfor this payments\n          profile.\nYour Free Trial credits apply to all Google Cloud\n        resources, includingGoogle Maps Platformusage, but with the following exceptions:\nYou can't add GPUs to your VM instances.\nYou can't request a quota increase. For an overview of\n          Compute Engine quotas, seeResource quotas.\nYou can't access or use Free Trial credits for generative AI partner models offered as managed APIs (also known asmodel as a service).\nYou can't create VM instances that are based on Windows Serverimages.\nYou can't create Google Cloud VMware Engine resources.\nTo perform any of the actions in the preceding list, you mustactivate a full Cloud Billing account.\nIn addition to resource constraints, theFree Trial Terms and Conditionsdescribe use\n          cases that are prohibited during the Free Trial. For\n          example, you may not use Google Cloud services to mine\n          cryptocurrency during your Free Trial.\nYourFree Trial endswhen one of the following occurs:\nYou've spent the $300 in credits.\n90 days have elapsed since you signed up.\nThroughout your Free Trial period, yourremaining credits and daysare displayed on the Billing Account\n        Overview page in the Google Cloud console.\nYou are not billed during your Free Trial. When the\n        Free Trial ends, all resources you created during the\n        trial are stopped, and you won't be charged, unless youactivate a full, paid Cloud Billing account.\nYou must accept theFree Trial Terms and Conditionsand theGoogle Cloud Terms of Service.\nService level agreements don't apply during the\n        Free Trial. The Free Trial is intended\n        to help you explore and evaluate Google Cloud. We don't recommend\n        running production applications on Google Cloud during the\n        Free Trial.\nDuring your Free Trial period, when you use resources\n        covered by theFree Tier, the Free Tier usage is\n        not charged against your Free Trial credits.\nWhen you sign up for the Free Trial, Google requires a credit card\nor other payment method. Google uses this payment information for the following\npurposes:\nTo verify your identity.\nTo distinguish actual people from robots.\nTo create your Google payments profile.\nTo create your Free Trial Cloud Billing account.\nFor more information about payment methods, including which types of credit\ncards are accepted, seeavailable payment methods.\nIf your payment method expires or otherwise becomes invalid during the\nFree Trial, your Cloud Billing account may be suspended.\nAfter you submit your payment information, Google submits a one-time transaction\nfor verification purposes only.No charges are made after this verification\nprocess, unless youactivate a full, paid Cloud Billing account.\nThe transaction has the following attributes:\nThe transaction is an authorization request to validate your\nCloud Billing account. It is not a permanent charge.\nThe transaction is an authorization request to validate your\nCloud Billing account. It is not a permanent charge.\nThe transaction appears on your statement as being from Google.\nThe transaction appears on your statement as being from Google.\nThe transaction is between $0.00 and $1.00 USD. Your bank might convert this\namount to a local currency.\nThe transaction is between $0.00 and $1.00 USD. Your bank might convert this\namount to a local currency.\nIf you provide bank account information, the transaction might take up to 3\ndays to appear on your statement.\nIf you provide bank account information, the transaction might take up to 3\ndays to appear on your statement.\nIf you provide credit card information, this transaction might appear on\nyour statement for up to one month before being automatically reversed.\nIf you provide credit card information, this transaction might appear on\nyour statement for up to one month before being automatically reversed.\nDepending on your country, you might need to verify your bank account to\ncomplete the signup process. For information about verifying bank accounts,see verify your bank account.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nYou can access Cloud Billing reports in the Google Cloud console\nto monitor and analyze your usage costs and credits.\nDuring the Free Trial offer, you'll typically see bills with a net\nzero balance, but you can view the details of the charges and credits so you can\nbetter understand the costs of using Google Cloud after yourFree Trial ends.\nUse thebilling reportto view and analyze your Google Cloud usage costs using many selectablesettings and filters.\nOpen Cloud Billing Reports\nTo view the individual services that contribute to your usage costs and credits,group your costs by SKU.\nIn the report, your usage costs calculated at the on-demand rate appear in theCostcolumn, Free Trial credits appear in thePromotions and otherscolumn, and any credits for usage covered byFree Tier limitsappear in theDiscountscolumn.\nExample when viewing your Cloud Billing report grouped by SKU:SKUCostDiscountsPromotions and othersSubtotalN1 Predefined Instance Core running in Americas$33.75$0.00-$33.75$0.00Regional Kubernetes Clusters$55.80$0.00-$55.80$0.00Zonal Kubernetes Clusters$120.40-$74.40-$26.00$0.00\nLearn about other reports available to view and analyze your Google Cloud usage costs.\nThe Free Trial ends when you use all of your credit, or after\n90 days, whichever happens first. At that time, the\nfollowing conditions apply:\nTo continue using Google Cloud, you mustactivate a full, paid Cloud Billing account.If you choose not to activate a full, paid account, when the\nFree Trial ends, billing becomes disabled on your projects.\nTo continue using Google Cloud, you mustactivate a full, paid Cloud Billing account.\nIf you choose not to activate a full, paid account, when the\nFree Trial ends, billing becomes disabled on your projects.\nAll resources you created during the trial are stopped.\nAll resources you created during the trial are stopped.\nAny data you stored in Compute Engine is marked for deletion and might\nbe lost.Learn more about data deletion on Google Cloud.\nAny data you stored in Compute Engine is marked for deletion and might\nbe lost.Learn more about data deletion on Google Cloud.\nYour Cloud Billing account enters a\n30-day grace period, during which you canactivate a full, paid Cloud Billing accountto recover resources and data you stored in any Google Cloud services\nduring the trial period.\nYour Cloud Billing account enters a\n30-day grace period, during which you canactivate a full, paid Cloud Billing accountto recover resources and data you stored in any Google Cloud services\nduring the trial period.\nYou might receive a message stating that your Cloud Billing\naccount has been canceled, which indicates that your account has been\nsuspended to prevent charges.\nYou might receive a message stating that your Cloud Billing\naccount has been canceled, which indicates that your account has been\nsuspended to prevent charges.\nThere is no action you need to take to cancel your Free Trial,\nunless you have alreadyactivated a full, paid Cloud Billing account.\nIf your Cloud Billing account is limited to aFree Trial status,\nyou aren't billed during the Free Trial period. When theFree Trial ends,\nall resources you created during the trial are automatically stopped, your\nFree Trial billing account is suspended, and you aren't charged.\nIf youactivateda full, paid account, you can cancel your paid Cloud Billing account\nbyclosing the Cloud Billing account.\nClosing a billing account stops all billable services and prevents your\nCloud Billing account fromincurring charges.\nLearn about additional methods tominimize or stop charges to your paid Cloud Billing account.\nYou can activate a full, paid Cloud Billing account at any time after\nstarting the Free Trial, to ensure that your resources keep running\nuninterrupted after the trial ends. You might also want to activate a full, paid\naccount if you want to usefeatures that aren't included in the Free Trial,\nsuch as GPUs and Windows servers.\nWhen you activate a full, paid account, the following conditions apply:\nIf you activate a full, paid account before the trial is over: Any\nremaining, unexpired Free Trial credits remain in your\nCloud Billing account. You can continue to use the resources you\ncreated during the Free Trial without interruption.For resources you use in excess of what's covered by any remaining credit,\nyour form of payment on file is charged (credit card or bank account).\nIf you activate a full, paid account before the trial is over: Any\nremaining, unexpired Free Trial credits remain in your\nCloud Billing account. You can continue to use the resources you\ncreated during the Free Trial without interruption.\nFor resources you use in excess of what's covered by any remaining credit,\nyour form of payment on file is charged (credit card or bank account).\nIf you activate a full, paid account within 30 days\nafter the end of the trial: Your resources are marked for deletion, but\nyou might be able to recover them.Learn more about data deletion on Google Cloud.\nIf you activate a full, paid account within 30 days\nafter the end of the trial: Your resources are marked for deletion, but\nyou might be able to recover them.Learn more about data deletion on Google Cloud.\nIf you activate a full, paid account more than\n30 days after the end of the trial, your\nFree Trial resources are lost.\nIf you activate a full, paid account more than\n30 days after the end of the trial, your\nFree Trial resources are lost.\nIn the Google Cloud console, you can convert your Free Trial billing\naccount to a full, paid Cloud Billing account using theActivateoption. You must be aBilling Account Administratoron the Cloud Billing account to make this change.\nTo activate your full, paid Cloud Billing account, complete the\nfollowing steps:\nSign in to the Google Cloud console.Sign in to the Google Cloud console\nSign in to the Google Cloud console.\nSign in to the Google Cloud console\nLook for theFree trial statusbanner at the top of the page.\nLook for theFree trial statusbanner at the top of the page.\nClickActivate.If theFree trial bannerwith theActivatebutton is not visible, on\nthe Google Cloud console menu bar, click theFree trial statusicon to open the banner and access theActivatebutton.\nClickActivate.\nIf theFree trial bannerwith theActivatebutton is not visible, on\nthe Google Cloud console menu bar, click theFree trial statusicon to open the banner and access theActivatebutton.\nIf you don't see anActivatebutton, the reasons could include the\nfollowing:\nYou don't have the permissions needed to convert this\nFree Trial Cloud Billing account to a full, paid\naccount. You must be aBilling Account Administratoron the Cloud Billing account to activate to a full, paid account.\nThis Cloud Billing account is already activated as a full, paid\naccount. You canconfirm the billing statusof your Cloud Billing account by looking at the BillingOverviewpage.\nIn the Cloud Billing console, you can confirm the free or\npaid (billable) status of your Cloud Billing account and the status\nof your free trial credits.\nIn the Google Cloud console, go to your Cloud Billing account.Go to your Cloud Billing account\nIn the Google Cloud console, go to your Cloud Billing account.\nGo to your Cloud Billing account\nIf you have more than one billing account, at the prompt, choose the\nCloud Billing account you want to view. The BillingOverviewpage opens for the selected billing account.\nIf you have more than one billing account, at the prompt, choose the\nCloud Billing account you want to view. The BillingOverviewpage opens for the selected billing account.\nOn the BillingOverviewpage, your billing account status is displayed\nat the top of the page:Free trial account: The Cloud Billing account isn't billable\nand is subject to theprogram coverage limitationsand theFree Trial Terms and ConditionsThe account might be suspended if it's more than\n30 days after yourFree Trial ends.Paid account: The Cloud Billing account is billable. If you\nupgraded to a paid Cloud Billing account before theFree Trial ends, you can use\nany remaining, unexpired Free Trial credits. Your\nCloud Billing account is charged for resources you use in excess\nof what's covered by any remaining credit.\nOn the BillingOverviewpage, your billing account status is displayed\nat the top of the page:\nFree trial account: The Cloud Billing account isn't billable\nand is subject to theprogram coverage limitationsand theFree Trial Terms and ConditionsThe account might be suspended if it's more than\n30 days after yourFree Trial ends.\nPaid account: The Cloud Billing account is billable. If you\nupgraded to a paid Cloud Billing account before theFree Trial ends, you can use\nany remaining, unexpired Free Trial credits. Your\nCloud Billing account is charged for resources you use in excess\nof what's covered by any remaining credit.\nTo learn the status of any remaining free trial credits, on theBilling Account Overviewtab, look at theCreditinfo card.If the Cloud Billing account is still limited to aFree TrialCloud Billing account, you\nwill see aFree trial creditinfo card.This card displays the status of any remaining free trial\ncredits, and provides anActivatebutton. If you want to\nconvert your limited Free Trial billing account to a full,\npaid Cloud Billing account, clickActivate.If the Cloud Billing account is a paid account, you will see aCreditsinfo card.This card displays the status of any remaining free trial\ncredits. To view the details of the free trial, clickCredit details.\nTo learn the status of any remaining free trial credits, on theBilling Account Overviewtab, look at theCreditinfo card.\nIf the Cloud Billing account is still limited to aFree TrialCloud Billing account, you\nwill see aFree trial creditinfo card.This card displays the status of any remaining free trial\ncredits, and provides anActivatebutton. If you want to\nconvert your limited Free Trial billing account to a full,\npaid Cloud Billing account, clickActivate.\nIf the Cloud Billing account is still limited to aFree TrialCloud Billing account, you\nwill see aFree trial creditinfo card.\nThis card displays the status of any remaining free trial\ncredits, and provides anActivatebutton. If you want to\nconvert your limited Free Trial billing account to a full,\npaid Cloud Billing account, clickActivate.\nIf the Cloud Billing account is a paid account, you will see aCreditsinfo card.This card displays the status of any remaining free trial\ncredits. To view the details of the free trial, clickCredit details.\nIf the Cloud Billing account is a paid account, you will see aCreditsinfo card.\nThis card displays the status of any remaining free trial\ncredits. To view the details of the free trial, clickCredit details.\nWhen youactivate a full, paid Cloud Billing account,\nyou'll incur charges after theend of the Free Trial.\nYou'll also incur chargesduring the free trial periodif you useservices that aren't covered by the Free Trial.\nGoogle Cloud and Google Maps Platform services charge you only for\nresources you use. Each service has its own pricing model, which you can find in\nthepricing documentation for each individual service.\nCloud Billing offers several tools to help you monitor and optimize\nyour Google Cloud and Google Maps Platform usage costs.\nReports- AccessCloud Billing reportsto monitor and analyze your usage costs and credits.\nBudgets- CreateCloud Billing budgetsto help you stay informed about how your actual Google Cloud spend is\ntracking against your planned spend.\nCommitted use discounts- When you have workloads with predictable\nresource needs, optimize your costs by purchasingcommitted use discounts (CUDs).\nCUDs provide discounted prices in exchange for your commitment to use a\nminimum level of resources for a specified term.\nSeveral options are available to help you estimate your Google Cloud\nand Google Maps Platform usage costs.\nForGoogle Cloud, you can estimate the cost of using\nGoogle Cloud services by using thepricing calculatoror by consulting thepricing page.Also, for some Google Cloud services, you can use theCost estimation toolto estimate your monthly costs for simulated workloads.\nForGoogle Cloud, you can estimate the cost of using\nGoogle Cloud services by using thepricing calculatoror by consulting thepricing page.\nAlso, for some Google Cloud services, you can use theCost estimation toolto estimate your monthly costs for simulated workloads.\nForGoogle Maps Platform, you can estimate your monthly bill by\nconsulting thepricing page.To understand your Google Maps Platform monthly bill, consultGoogle Maps Platform Billing.\nForGoogle Maps Platform, you can estimate your monthly bill by\nconsulting thepricing page.\nTo understand your Google Maps Platform monthly bill, consultGoogle Maps Platform Billing.\nAfter you have activated your full, paid Cloud Billing account,sign up for Customer Care.\nChoose the support service that best fits your organization's technical support\nneeds.\nTo minimize costs or prevent your Cloud Billing account from incurring\ncharges, you can take the following actions:\nLimit your usage of each service to theFree Tier limits.\nShut down services in your projects. Follow theguidance in the Google Cloud docsfor each of your services.\nDisable billingon the associatedprojects linked to your Cloud Billing account.\nClose your Cloud Billing accountto stop all billable services and prevent your Cloud Billing\naccount from incurring charges.\nThe Free Tier provides limited access to many common\nGoogle Cloud products and services at no cost. Unlike the\nFree Trial, the Free Tier is available to all\nGoogle Cloud users.\nFree Tier resources are provided at intervals, usually monthly.\nFree Tier resources are not credits; they don't accumulate or\nroll over from one interval to the next.\nFree Tier limits are calculated per billing account.\nYou're eligible for the Free Tier program if you meet\n        the following criteria:\nYou don't have a negotiated pricing contract or a custom rate card\n          with Google, except as described for certain services listed in theFree Tier usage limitstable.\nYou are actively in theFree Trialperiod or you have activated your\n          full, paid Cloud Billing account.\nYour Cloud Billing account is active and in good standing.\nIf you are deemed ineligible, you are charged at the normal rates\n        for the resources you use. If you think you are being incorrectly\n        charged for Free Tier usage, contactCloud Billing Supportfor assistance.\nNo special action is required by you before you can begin using\n        resources that offer Free Tier usage (withinFree Tier limits).\nFree Tier coveragevaries by service. Not all Google Cloud services offer\n        resources as part of the Free Tier.\nThe Free Tier has no end date, but Google\n        reserves the right to change the offering, including changing or\n        eliminating usage limits, subject to 30 days advance notice.\nYou must accept theGoogle Cloud Terms of Service.\nWhen you use resources covered by Free Tier during your\n        Free Trial period, those resources are not charged against\n        your Free Trial credits.\nFree Tier resources are available for the Google Cloud\nservices listed in the following table, subject to the listed limitations. For\ninformation about Google Maps Platform, see thepricing page.\n28 hours per day ofF1 instances\n9 hours per day ofB1 instances\n1 GB of outbound data transfer per day\nThe Google Cloud Free Tier is available only for the Standard Environment.\nLearn more\n0.5 GB storage per month\nLearn more\n5000 units of prediction per month\nLearn more\n6 node hours for training and prediction\nLearn more\n500,000 translated characters per month\nLearn more\n1 TB of querying per month\n10 GB of storage\nLearn more\n2,500 build-minutes per month formachine typee2-standard-2\nLearn more\nFirst active delivery pipeline (per billing account)\nLearn more\n100 free active key versions per month\n10,000 free cryptographic operations per month\nThe monthly free usage only applies to key versions created using Cloud KMS Autokey.\nLearn more\nFree monthly logging allotment\nFree monthly metrics allotment\nLearn more\n5,000 units per month\nLearn more\n2 million requests per month\n360,000 GB-seconds of memory, 180,000 vCPU-seconds of compute time\n1 GB of outbound data transfer from North America per month\nLearn more\n2 million invocations per month (includes both background and HTTP invocations)\n400,000 GB-seconds, 200,000 GHz-seconds of compute time\n5 GB of outbound data transfer per month\nLearn more\nFree access to Cloud Shell, including 5 GB of persistent disk storage\nLearn more\nUp to 5 users\n50 GB of storage\n50 GB of outbound data transfer\nLearn more\n5 GB-months of regional storage (US regions only) per month\n5,000 Class A Operations per month\n50,000 Class B Operations per month\n100 GB of outbound data transfer from North America to all region\n          destinations (excluding China and Australia) per month\nFree Tier is only available inus-east1,us-west1, andus-central1regions. Usage calculations are\n        combined across those regions.\nLearn more\n1,000 units per month\nLearn more\n1 non-preemptiblee2-microVM instance per month in one\n                of the following US regions:Oregon:us-west1Iowa:us-central1South Carolina:us-east1\nOregon:us-west1\nIowa:us-central1\nSouth Carolina:us-east1\n30 GB-months standard persistent disk\n1 GB of outbound data transfer from North America to all region\n                destinations (excluding China and Australia) per month\nYour Free Tiere2-microinstance limit is by time, not by instance. Each month, eligible use of all of youre2-microinstances is free until you have used a number of hours equal to the total hours in the current month. Usage calculations are combined across the supportedregions.\nCompute Engine free tier does not charge for an external IP address.\nGPUs and TPUs are not included in the Free Tier offer. You\n            are always charged for GPUs and TPUs that you add to VM instances.Learn more\n1 GB storage per project\n50,000 reads, 20,000 writes, 20,000 deletes per day, per project\nLearn more\nNo cluster management fee for one Autopilot or Zonal cluster per\n          Cloud Billing account. For clusters created in Autopilot mode, pods are billed per second for vCPU, memory and disk resource requests.\n          For clusters created in Standard mode, each user node is charged at\n          standard Compute Engine pricing.\nNo fee for GKE Enterprise pay-as-you-go SKUs for\n          90 days, for a one-time trial period on any project linked to your\n          Cloud Billing account.Note: During the 90-day period,\n          GKE Enterprise is not meant for production workloads,\n          and we don't offer an SLA. Only the fee for the\n          GKE Enterprise pay-as-you-go SKU is waived. You are\n          still charged for all usage related to existing services and services\n          deployed using GKE Enterprise.\nNo fee for GKE Enterprise pay-as-you-go SKUs for\n          90 days, for a one-time trial period on any project linked to your\n          Cloud Billing account.\nLearn more\nFor more information, see thePricing page.\n10 GB of messages per month\nLearn more\n10,000assessments.createorsiteverifycalls per month for SKU ID F553-86ED-15F4.\n1 millionassessments.createorsiteverifycalls per month for SKU IDs A1ED-4B1C-A692 and 922B-4FA3-9210.\nLearn more\n6 active secret versions per month\n10,000 access operations per month\n3 secret rotation notifications per month\nLearn more\n60 minutes per month\nLearn more\n1,000 units per month\nLearn more\n100,000uris.searchcalls per month\n100uris.submitcalls per month\nThis Free Tier usage is also available if you have a negotiated\n          pricing contract for Web Risk.\nLearn more\n5,000 internal steps per month\n2,000 external HTTP calls per month\nLearn more\nAny usage that exceeds Free Tier usage limits is automatically\nbilled at standard rates.\nYou can help monitor and control costs by setting upbudgets and alertsthrough the Google Cloud console.\nYou incur normal expenses for anyGoogle Cloud Marketplaceproducts andpremium OS licensesyou use, even if your Compute Engine use is covered by the\nFree Tier.\nBilling supportis included with all Cloud Billing\n          accounts. You must be aBilling Account Administratorto\n          interact with billing support.\nUpgrade your support planto access technical\n          support for Google Cloud services.\nGoogle Cloud documentationdescribes how\n          to use Google Cloud services.\nGoogle Maps Platform documentationdescribes how\n          to use Google Maps Platform APIs and industry solutions.\nCloud Billing documentationdescribes how\n          to use Cloud Billing, a collection of tools that help you track\n          and understand your Google Cloud spending, pay your bill, and\n          optimize your costs.\nGoogle Cloud console Helpanswers common support questions.\nGoogle Cloud Free Trial Troubleshooterhelps you find answers to questions about the Free Trial.\nFind answers, ask questions, and connect with experts in theGoogle Cloud Community forums.\nJoin theInnovators program,\n          which gives you the latest updates, access to technologies and\n          expertise, and exclusive benefits to build your skills on\n          Google Cloud.\nJoin acommunity meetupof knowledgeable Google Cloud enthusiasts.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-20 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nThe predictive AI evaluation service lets you evaluate model performance across specific use cases. You might also refer to evaluation as observability into a model's performance. The model evaluation provided by Vertex AI can fit in the typical machine learning workflow in several ways:\nAfter you train your model, review model evaluation metrics before you deploy your model. You can compare evaluation metrics across multiple models to help you decide which model you should deploy.\nAfter you train your model, review model evaluation metrics before you deploy your model. You can compare evaluation metrics across multiple models to help you decide which model you should deploy.\nAfter your model is deployed to production, periodically evaluate your model with new incoming data. If the evaluation metrics show that your model performance is degrading, consider re-training your model. This process is calledcontinuous evaluation.\nAfter your model is deployed to production, periodically evaluate your model with new incoming data. If the evaluation metrics show that your model performance is degrading, consider re-training your model. This process is calledcontinuous evaluation.\nHow you interpret and use those metrics depends on your business need and the problem your model is trained to solve. For example, you might have a lower tolerance for false positives than for false negatives, or the other way around. These kinds of questions affect which metrics you would focus on as youiterate\non your model.\nSome key metrics provided by the predictive AI model evaluation service include the following:\nPrecision\nRecall\nAuPRC\nConfusion matrix\nTo evaluate a model with Vertex AI, you should have a\ntrained model, a batch prediction output, and a ground truth dataset. The\nfollowing is a typical model evaluation workflow using Vertex AI:\nTrain a model. You can do this in Vertex AI using\nAutoML or custom training.\nTrain a model. You can do this in Vertex AI using\nAutoML or custom training.\nRun a batch prediction job on the model to generate prediction results.\nRun a batch prediction job on the model to generate prediction results.\nPrepare theground truthdata, which is the \"correctly labeled\" data as\ndetermined by humans. The ground truth is usually in the form of the test\ndataset you used during the model training process.\nPrepare theground truthdata, which is the \"correctly labeled\" data as\ndetermined by humans. The ground truth is usually in the form of the test\ndataset you used during the model training process.\nRun an evaluation job on the model, which evaluates the accuracy of the batch\nprediction results compared to the ground truth data.\nRun an evaluation job on the model, which evaluates the accuracy of the batch\nprediction results compared to the ground truth data.\nAnalyze the metrics that result from the evaluation job.\nAnalyze the metrics that result from the evaluation job.\nIterate on your model to see you if you can improve your model's accuracy.\nYou can run multiple evaluation jobs, and compare the results of multiple jobs\nacross models or model versions.\nIterate on your model to see you if you can improve your model's accuracy.\nYou can run multiple evaluation jobs, and compare the results of multiple jobs\nacross models or model versions.\nYou can run model evaluation in Vertex AI in several ways:\nCreate evaluations through the Vertex AI Model Registry in the\nGoogle Cloud console.\nCreate evaluations through the Vertex AI Model Registry in the\nGoogle Cloud console.\nUse model evaluations from Vertex AI as apipeline componentwith Vertex AI Pipelines. You\ncan create pipeline runs and templates that include model evaluations as a\npart of your automated MLOps workflow.You can run themodel evaluation component by itself, or with\nother pipeline components such as thebatch prediction\ncomponent.\nUse model evaluations from Vertex AI as apipeline componentwith Vertex AI Pipelines. You\ncan create pipeline runs and templates that include model evaluations as a\npart of your automated MLOps workflow.\nYou can run themodel evaluation component by itself, or with\nother pipeline components such as thebatch prediction\ncomponent.\nVertex AI supports evaluation of the following model types:\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nAuPRC: Thearea\nunder the precision-recall (PR) curve, also referred to as average\nprecision. This value ranges from zero to one, where a higher value indicates\na higher-quality model.\nLog loss: The cross-entropy between the model predictions and the target\nvalues. This ranges from zero to infinity, where a lower value indicates a\nhigher-quality model.\nConfidence threshold: A confidence score that determines which\npredictions to return. A model returns predictions that are at this value or\nhigher. A higher confidence threshold increases precision but lowers recall.\nVertex AI returns confidence metrics at different threshold values\nto show how the threshold affectsprecisionandrecall.\nRecall: The fraction of predictions with this class that the model\ncorrectly predicted. Also calledtrue positive rate.\nPrecision: The fraction of classification predictions produced by the\nmodel that were correct.\nConfusion matrix: Aconfusion\nmatrixshows how often a model correctly predicted a result. For incorrectly\npredicted results, the matrix shows what the model predicted instead. The\nconfusion matrix helps you understand where your model is \"confusing\" two\nresults.\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nAuPRC: Thearea\nunder the precision-recall (PR) curve, also referred to as average\nprecision. This value ranges from zero to one, where a higher value indicates\na higher-quality model.\nAuROC: Thearea\nunder receiver operating characteristic curve. This ranges from zero to one,\nwhere a higher value indicates a higher-quality model.\nLog loss: The cross-entropy between the model predictions and the target\nvalues. This ranges from zero to infinity, where a lower value indicates a\nhigher-quality model.\nConfidence threshold: A confidence score that determines which\npredictions to return. A model returns predictions that are at this value or\nhigher. A higher confidence threshold increases precision but lowers recall.\nVertex AI returns confidence metrics at different threshold values\nto show how the threshold affectsprecisionandrecall.\nRecall: The fraction of predictions with this class that the model\ncorrectly predicted. Also calledtrue positive rate.\nRecall at 1: The recall (true positive rate) when only considering the\nlabel that has the highest prediction score and not below the confidence\nthreshold for each example.\nPrecision: The fraction of classification predictions produced by the\nmodel that were correct.\nPrecision at 1: The precision when only considering the label that has\nthe highest prediction score and not below the confidence threshold for each\nexample.\nF1 score: The harmonic mean of precision and recall. F1 is a useful\nmetric if you're looking for a balance between precision and recall and there's\nan uneven class distribution.\nF1 score at 1: The harmonic mean of recall at 1 and precision at 1.\nConfusion matrix: Aconfusion\nmatrixshows how often a model correctly predicted a result. For incorrectly\npredicted results, the matrix shows what the model predicted instead. The\nconfusion matrix helps you understand where your model is \"confusing\" two\nresults.\nTrue negative count: The number of times a model correctly predicted\na negative class.\nTrue positive count: The number of times a model correctly predicted a\npositive class.\nFalse negative count: The number of times a model mistakenly predicted\na negative class.\nFalse positive count: The number of times a model mistakenly predicted\na positive class.\nFalse positive rate: The fraction of incorrectly predicted results out of\nall predicted results.\nFalse positive rate at 1: The false positive rate when only considering\nthe label that has the highest prediction score and not below the confidence\nthreshold for each example.\nModel feature attributions:Vertex AI shows you how much each feature impacts a model. The values are provided as a\npercentage for each feature: the higher the percentage, the more impact the feature had on\nmodel training. Review this information to ensure that all of the most important\nfeatures make sense for your data and business problem.\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nMAE: The mean absolute error (MAE) is the average absolute difference\nbetween the target values and the predicted values. This metric ranges from zero\nto infinity; a lower value indicates a higher quality model.\nRMSE: The root-mean-squared error is the square root of the average\nsquared difference between the target and predicted values. RMSE is more\nsensitive to outliers than MAE,so if you're concerned about large errors, then\nRMSE can be a more useful metric to evaluate. Similar to MAE, a smaller value\nindicates a higher quality model (0 represents a perfect predictor).\nRMSLE: The root-mean-squared logarithmic error metric is similar to RMSE,\nexcept that it uses the natural logarithm of the predicted and actual values\nplus 1. RMSLE penalizes under-prediction more heavily than over-prediction. It\ncan also be a good metric when you don't want to penalize differences for large\nprediction values more heavily than for small prediction values. This metric\nranges from zero to infinity; a lower value indicates a higher quality model.\nThe RMSLE evaluation metric is returned only if all label and predicted values\nare non-negative.\nr^2: r squared (r^2) is the square of the Pearson correlation\ncoefficient between the labels and predicted values. This metric ranges between\nzero and one. A higher value indicates a closer fit to the regression line.\nMAPE: Mean absolute percentage error (MAPE) is the average absolute\npercentage difference between the labels and the predicted values. This metric\nranges between zero and infinity; a lower value indicates a higher quality\nmodel.MAPE is not shown if the target column contains any 0 values. In this case,\nMAPE is undefined.\nModel feature attributions:Vertex AI shows you how much each feature impacts a model. The values are provided as a\npercentage for each feature: the higher the percentage, the more impact the feature had on\nmodel training. Review this information to ensure that all of the most important\nfeatures make sense for your data and business problem.\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nMAE: The mean absolute error (MAE) is the average absolute difference\nbetween the target values and the predicted values. This metric ranges from zero\nto infinity; a lower value indicates a higher quality model.\nRMSE: The root-mean-squared error is the square root of the average\nsquared difference between the target and predicted values. RMSE is more\nsensitive to outliers than MAE,so if you're concerned about large errors, then\nRMSE can be a more useful metric to evaluate. Similar to MAE, a smaller value\nindicates a higher quality model (0 represents a perfect predictor).\nRMSLE: The root-mean-squared logarithmic error metric is similar to RMSE,\nexcept that it uses the natural logarithm of the predicted and actual values\nplus 1. RMSLE penalizes under-prediction more heavily than over-prediction. It\ncan also be a good metric when you don't want to penalize differences for large\nprediction values more heavily than for small prediction values. This metric\nranges from zero to infinity; a lower value indicates a higher quality model.\nThe RMSLE evaluation metric is returned only if all label and predicted values\nare non-negative.\nr^2: r squared (r^2) is the square of the Pearson correlation\ncoefficient between the labels and predicted values. This metric ranges between\nzero and one. A higher value indicates a closer fit to the regression line.\nMAPE: Mean absolute percentage error (MAPE) is the average absolute\npercentage difference between the labels and the predicted values. This metric\nranges between zero and infinity; a lower value indicates a higher quality\nmodel.MAPE is not shown if the target column contains any 0 values. In this case,\nMAPE is undefined.\nWAPE: Weighted absolute percentage error (WAPE) is the overall difference\nbetween the value predicted by a model and the values observed over the values\nobserved. Compared to RMSE, WAPE is weighted towards the overall differences\nrather than individual differences, which can be highly influenced by low\nor intermittent values. A lower value indicates a higher quality model.\nRMSPE: Root mean squared percentage error (RMPSE) shows RMSE as a\npercentage of the actual values instead of an absolute number. A lower value\nindicates a higher quality model.\nQuantile: The percent quantile, which indicates the probability that an\nobserved value will be below the predicted value. For example, at the 0.5\nquantile, the observed values are expected to be lower than the predicted values\n50% of the time.\nObserved quantile: Shows the percentage of true values that were less\nthan the predicted value for a given quantile.\nScaled pinball loss: The scaled pinball loss at a particular quantile.\nA lower value indicates a higher quality model at the given quantile.\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nAuPRC: Thearea\nunder the precision-recall (PR) curve, also referred to as average\nprecision. This value ranges from zero to one, where a higher value indicates\na higher-quality model.\nLog loss: The cross-entropy between the model predictions and the target\nvalues. This ranges from zero to infinity, where a lower value indicates a\nhigher-quality model.\nConfidence threshold: A confidence score that determines which\npredictions to return. A model returns predictions that are at this value or\nhigher. A higher confidence threshold increases precision but lowers recall.\nVertex AI returns confidence metrics at different threshold values\nto show how the threshold affectsprecisionandrecall.\nRecall: The fraction of predictions with this class that the model\ncorrectly predicted. Also calledtrue positive rate.\nRecall at 1: The recall (true positive rate) when only considering the\nlabel that has the highest prediction score and not below the confidence\nthreshold for each example.\nPrecision: The fraction of classification predictions produced by the\nmodel that were correct.\nPrecision at 1: The precision when only considering the label that has\nthe highest prediction score and not below the confidence threshold for each\nexample.\nConfusion matrix: Aconfusion\nmatrixshows how often a model correctly predicted a result. For incorrectly\npredicted results, the matrix shows what the model predicted instead. The\nconfusion matrix helps you understand where your model is \"confusing\" two\nresults.\nF1 score: The harmonic mean of precision and recall. F1 is a useful\nmetric if you're looking for a balance between precision and recall and there's\nan uneven class distribution.\nF1 score at 1: The harmonic mean of recall at 1 and precision at 1.\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nYou can view and download schema files from the following Cloud Storage\nlocation:gs://google-cloud-aiplatform/schema/modelevaluation/\nAuPRC: Thearea\nunder the precision-recall (PR) curve, also referred to as average\nprecision. This value ranges from zero to one, where a higher value indicates\na higher-quality model.\nConfidence threshold: A confidence score that determines which\npredictions to return. A model returns predictions that are at this value or\nhigher. A higher confidence threshold increases precision but lowers recall.\nVertex AI returns confidence metrics at different threshold values\nto show how the threshold affectsprecisionandrecall.\nRecall: The fraction of predictions with this class that the model\ncorrectly predicted. Also calledtrue positive rate.\nPrecision: The fraction of classification predictions produced by the\nmodel that were correct.\nConfusion matrix: Aconfusion\nmatrixshows how often a model correctly predicted a result. For incorrectly\npredicted results, the matrix shows what the model predicted instead. The\nconfusion matrix helps you understand where your model is \"confusing\" two\nresults.\nF1 score: The harmonic mean of precision and recall. F1 is a useful\nmetric if you're looking for a balance between precision and recall and there's\nan uneven class distribution.\nTo learn more,\n      run the following Jupyter notebooks in the environment of your choice:\n\"Vertex AI: Evaluating batch prediction results from an AutoML Tabular classification model\":Open\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\n\"Vertex AI: Evaluating batch prediction results from an AutoML Tabular classification model\":\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\n\"Vertex AI Pipelines: Evaluating batch prediction results from AutoML Tabular regression model\":Open\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\n\"Vertex AI Pipelines: Evaluating batch prediction results from AutoML Tabular regression model\":\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo learn more,\n      run the \"Vertex AI Pipelines: AutoML text classification pipelines using google-cloud-pipeline-components\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo learn more,\n      run the \"Vertex AI Pipelines: Evaluating batch prediction results from AutoML Video classification model\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo learn more,\n      run the following Jupyter notebooks in the environment of your choice:\n\"Vertex AI Pipelines: Evaluating BatchPrediction results from a Custom Tabular classification model\":Open\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\n\"Vertex AI Pipelines: Evaluating BatchPrediction results from a Custom Tabular classification model\":\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\n\"Vertex AI Pipelines: Evaluating batch prediction results from custom tabular regression model\":Open\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\n\"Vertex AI Pipelines: Evaluating batch prediction results from custom tabular regression model\":\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo learn more,\n      run the \"Get started with importing a custom model evaluation to the Vertex AI Model Registry\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nLearn how toperform model evaluation using Vertex AI.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/network-connectivity/docs/router",
    "title": "Cloud Router documentation",
    "content": "Documentation\nCloud Router enables you to dynamically exchange routes between your\n  Virtual Private Cloud (VPC) and peer network by usingBorder Gateway Protocol (BGP).\nFor example, if you use a Cloud VPN tunnel to connect your networks, you can\n  use Cloud Router to establish a BGP session with\n  a router in your peer network over a Cloud VPN tunnel. The peer network\n  can be an on-premises network, multicloud network, or another VPC network.\n  Cloud Router automatically learns new subnet IP address ranges in your VPC\n  network and can announce them to your peer network.\nSetting the dynamic routing mode\nSetting the dynamic routing mode\nQuickstart: Create a Cloud Router to connect a VPC network to a peer network\nQuickstart: Create a Cloud Router to connect a VPC network to a peer network\nEstablishing BGP sessions\nEstablishing BGP sessions\nAdvertised routes\nAdvertised routes\nViewing Cloud Router details\nViewing Cloud Router details\nViewing logs and metrics\nViewing logs and metrics\nCloud Router overview\nCloud Router overview\nKey terms\nKey terms\nBest practices\nBest practices\nTroubleshoot BGP sessions\nTroubleshoot BGP sessions\nTroubleshoot BGP peering\nTroubleshoot BGP peering\nBGP routes and route selection\nBGP routes and route selection\nTroubleshoot Cloud Router log messages\nTroubleshoot Cloud Router log messages\nRelease notes\nRelease notes\nGetting support\nGetting support\nBilling questions\nBilling questions\nPricing\nPricing\nQuotas\nQuotas\nAPIs\nAPIs\nCreating an HA VPN gateway to a Peer VPN gateway\nCreate a highly available VPN gateway that connects to a peer VPN gateway.VPNNetwork Connectivity\nMigration to Google Cloud: Transferring your large datasets\nExplore the process of getting your data into Google Cloud, from planning a data transfer to using best practices in implementing a plan.MigrationCloud InterconnectNetwork Connectivity\nTCP optimization for network performance in Google Cloud and hybrid scenarios\nLearn about ways to improve connection latency between processes within Google Cloud, including how to compute correct settings for decreasing the latency of TCP connections.Network Connectivity\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/application-hosting",
    "title": "Application hosting",
    "content": "Home\nDocumentation\nRun and manage applications on a secure platform.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nDevelop and deploy highly scalable applications and functions on a fully managed serverless platform.\nProvision, deploy, scale, and manage containerized applications.\nSimplify managing multi-cluster deployments.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI Feature Store is a managed, cloud-native feature store service\nthat's integral to Vertex AI. It streamlines your ML feature management\nand online serving processes by letting you manage your feature data in a\nBigQuery table or view. You can then serve features online directly from the\nBigQuery data source.\nVertex AI Feature Store provisions resources that let you set up\nonline serving by specifying your feature data sources. It then acts as a metadata\nlayer interfacing with the BigQuery data sources and serves\nthe latest feature values directly from BigQuery for online\npredictions at low latencies.\nIn Vertex AI Feature Store, the BigQuery tables or\nviews containing the feature data collectively form theoffline store. You can\nmaintain feature values, including historical feature data, in the offline store.\nBecause all the feature data is maintained in BigQuery,\nVertex AI Feature Store doesn't need to provision a separate\noffline store within Vertex AI. Moreover, if you want to use the\ndata in the offline store to train ML models, you can use the APIs and\ncapabilities in BigQuery to export or fetch the data.\nThe workflow to set up and start online serving using Vertex AI Feature Store\ncan be summarized as follows:\nPrepare your data source in BigQuery.\nPrepare your data source in BigQuery.\nOptional: Register your data sources by creating feature groups and features.\nOptional: Register your data sources by creating feature groups and features.\nSet up online store and feature view resources to connect the feature data sources\nwith online serving clusters.\nSet up online store and feature view resources to connect the feature data sources\nwith online serving clusters.\nServe the latest feature values online from a feature view.\nServe the latest feature values online from a feature view.\nThis section explains the data models and resources associated with the\nfollowing aspects of Vertex AI Feature Store:\nData source preparation in BigQuery\nData source preparation in BigQuery\nFeature Registry setup\nFeature Registry setup\nOnline serving setup\nOnline serving setup\nOnline serving\nOnline serving\nDuring online serving, Vertex AI Feature Store uses feature data\nfrom BigQuery data sources. Before you set up Feature Registry\nor online serving resources, you must store your feature data in one or more\nBigQuery tables or views.\nWithin a BigQuery table or view, each column represents a feature.\nEach row contains feature values corresponding to a unique ID. For more\ninformation about how to prepare the feature data in BigQuery,\nseePrepare data source.\nFor example, in figure 1, the BigQuery table includes the\nfollowing columns:\nf1andf2: Feature columns.\nf1andf2: Feature columns.\nentity_id: An ID column containing the unique IDs to identify each feature record.\nentity_id: An ID column containing the unique IDs to identify each feature record.\nfeature_timestamp:A timestamp column.\nfeature_timestamp:A timestamp column.\nBecause you prepare the data source in BigQuery and not in Vertex AI,\nyou don't need to create any Vertex AI resources at this stage.\nAfter you've prepared your data sources in BigQuery, you can\nregister those data sources, including specific feature columns, in\nthe Feature Registry.\nRegistering your features is optional. You can serve features online even if you\ndon't add your BigQuery data sources to the Feature Registry. However,\nregistering your features is advantageous in the following scenarios:\nYour data contains multiple instances of the same entity ID and you need\nto prepare your data in a time-series format with a timestamp column. When you\nregister your features, Vertex AI Feature Store looks up the\ntimestamp and serves only the latest feature values.\nYour data contains multiple instances of the same entity ID and you need\nto prepare your data in a time-series format with a timestamp column. When you\nregister your features, Vertex AI Feature Store looks up the\ntimestamp and serves only the latest feature values.\nYou want to register specific feature columns from a data source.\nYou want to register specific feature columns from a data source.\nYou want to aggregate specific columns from multiple data sources to define\na feature view instance.\nYou want to aggregate specific columns from multiple data sources to define\na feature view instance.\nYou want to monitor the feature statistics and detect feature drift.\nYou want to monitor the feature statistics and detect feature drift.\nThere are two types of Vertex AI Feature Store resources in the\nFeature Registry:\nFeature Registry resources for feature data\nFeature Registry resources for feature data\nFeature Registry resources for feature monitoring\nFeature Registry resources for feature monitoring\nTo register your feature data in the Feature Registry, you need to create the\nfollowing Vertex AI Feature Store resources:\nFeature group(FeatureGroup):\nAFeatureGroupresource is associated with a specific BigQuery source table or\nview. It represents a logical grouping of feature columns, which are represented byFeatureresources.\nA feature group also contains one or multiple entity ID columns to identify\nthe feature records. If the feature data is in a time-series format, the feature\ngroup must also contain a timestamp column. For information about how to create\na feature group, seeCreate a feature group.\nFeature group(FeatureGroup):\nAFeatureGroupresource is associated with a specific BigQuery source table or\nview. It represents a logical grouping of feature columns, which are represented byFeatureresources.\nA feature group also contains one or multiple entity ID columns to identify\nthe feature records. If the feature data is in a time-series format, the feature\ngroup must also contain a timestamp column. For information about how to create\na feature group, seeCreate a feature group.\nFeature(Feature):\nAFeatureresource represents a specific column containing feature values from the feature\ndata source associated with its parentFeatureGroupresource. For information\nabout how to create features within a feature group, seeCreate a feature.\nFeature(Feature):\nAFeatureresource represents a specific column containing feature values from the feature\ndata source associated with its parentFeatureGroupresource. For information\nabout how to create features within a feature group, seeCreate a feature.\nFor example, figure 2 illustrates a feature group including feature columnsf1andf2, sourced from a BigQuery table associated with the\nfeature group. The BigQuery data source contains four feature\ncolumns—two columns are aggregated to form the feature group. The feature\ngroup also contains an entity ID column and a feature timestamp column.\nFeature monitoring resources let you monitor the feature data registered usingFeatureGroupandFeatureresources. You can create the following resources\nrelated to feature monitoring:\nFeature monitor(FeatureMonitor):\nAFeatureMonitorresource is associated with aFeatureGroupresource and one or more features\nwithin that feature group. It specifies the monitoring schedule. You can create\nmultiple feature monitor resources to set up different monitoring schedules\nfor the same a set of features within a feature group. For example, if the featuresf1andf2are updated every hour, but the featuresf3andf4are\nupdated every day, you can create two feature monitor resources to efficiently\nmonitor these features:Feature monitorfm1that runs a monitoring job every hour on the featuresf1andf2.Feature monitorfm2that runs a monitoring job every day on the featuresf3andf4.\nFeature monitor(FeatureMonitor):\nAFeatureMonitorresource is associated with aFeatureGroupresource and one or more features\nwithin that feature group. It specifies the monitoring schedule. You can create\nmultiple feature monitor resources to set up different monitoring schedules\nfor the same a set of features within a feature group. For example, if the featuresf1andf2are updated every hour, but the featuresf3andf4are\nupdated every day, you can create two feature monitor resources to efficiently\nmonitor these features:\nFeature monitorfm1that runs a monitoring job every hour on the featuresf1andf2.\nFeature monitorfm1that runs a monitoring job every hour on the featuresf1andf2.\nFeature monitorfm2that runs a monitoring job every day on the featuresf3andf4.\nFeature monitorfm2that runs a monitoring job every day on the featuresf3andf4.\nFeature monitor job(FeatureMonitorJob):\nAFeatureMonitorJobresource contains the feature statistics and information\nretrieved when a feature monitoring job is run. It can also contain information about\nanomalies, such as feature drift, detected in the feature data.\nFeature monitor job(FeatureMonitorJob):\nAFeatureMonitorJobresource contains the feature statistics and information\nretrieved when a feature monitoring job is run. It can also contain information about\nanomalies, such as feature drift, detected in the feature data.\nFor more information about how to create feature monitoring resources, seeMonitor features for anomalies.\nTo serve features for online predictions, you must define and\nconfigure at least one online serving cluster, and associate it with your feature\ndata source or Feature Registry resources. In Vertex AI Feature Store,\nthe online serving cluster is called anonline storeinstance. An online store\ninstance can contain multiplefeature viewinstances, where each feature view\nis associated with a feature data source.\nTo set up online serving, you must create the following\nVertex AI Feature Store resources:\nOnline store(FeatureOnlineStore):\nAFeatureOnlineStoreresource represents an online serving cluster instance and contains the\nonline serving configuration, such as the number of online serving nodes. An\nonline store instance doesn't specify the source of the feature data, but\ncontainsFeatureViewresources that specify the feature data sources in either\nBigQuery or the Feature Registry. For information about how to\ncreate an online store instance, seeCreate an online store instance.\nOnline store(FeatureOnlineStore):\nAFeatureOnlineStoreresource represents an online serving cluster instance and contains the\nonline serving configuration, such as the number of online serving nodes. An\nonline store instance doesn't specify the source of the feature data, but\ncontainsFeatureViewresources that specify the feature data sources in either\nBigQuery or the Feature Registry. For information about how to\ncreate an online store instance, seeCreate an online store instance.\nFeature view(FeatureView):\nAFeatureViewresource is a logical collection of features in an online store\ninstance. When you create a feature view, you can specify the location of the\nfeature data source in either of the following ways:Associate one or more feature groups and features from the Feature Registry. A\nfeature group specifies the location of the BigQuery data source.\nA feature within the feature group points to a specific feature column\nwithin that data source.Alternatively, associate a BigQuery source table or view.For information about how to create feature view instances within an online store,\nseeCreate a feature view.\nFeature view(FeatureView):\nAFeatureViewresource is a logical collection of features in an online store\ninstance. When you create a feature view, you can specify the location of the\nfeature data source in either of the following ways:\nAssociate one or more feature groups and features from the Feature Registry. A\nfeature group specifies the location of the BigQuery data source.\nA feature within the feature group points to a specific feature column\nwithin that data source.\nAssociate one or more feature groups and features from the Feature Registry. A\nfeature group specifies the location of the BigQuery data source.\nA feature within the feature group points to a specific feature column\nwithin that data source.\nAlternatively, associate a BigQuery source table or view.\nAlternatively, associate a BigQuery source table or view.\nFor information about how to create feature view instances within an online store,\nseeCreate a feature view.\nFor example, figure 3 illustrates a feature view comprising feature columnsf2andf4, which are sourced from two separate feature groups associated\nwith a BigQuery table.\nVertex AI Feature Store provides the following types of online serving\nfor real-time online predictions:\nBigtable online servingis useful for serving large data volumes\n(terabytes of data). It's similar to online serving in\nVertex AI Feature Store (Legacy) and provides improved\ncaching to mitigate hotspotting. Bigtable online serving doesn't supportembeddings. If you need to serve large\nvolumes of data that are frequently updated and don't need to serve embeddings,\nuse Bigtable online serving.\nBigtable online servingis useful for serving large data volumes\n(terabytes of data). It's similar to online serving in\nVertex AI Feature Store (Legacy) and provides improved\ncaching to mitigate hotspotting. Bigtable online serving doesn't supportembeddings. If you need to serve large\nvolumes of data that are frequently updated and don't need to serve embeddings,\nuse Bigtable online serving.\nOptimized online servinglets you online serve features\nat ultra-low latencies. Although online serving latencies depend on\nthe workload, Optimized online serving can provide lower latencies than\nBigtable online serving and is recommended for most scenarios.\nOptimized online serving also supports embeddings management.To use Optimized online serving, you need to configure either a public\nendpoint or a dedicated Private Service Connect endpoint.\nOptimized online servinglets you online serve features\nat ultra-low latencies. Although online serving latencies depend on\nthe workload, Optimized online serving can provide lower latencies than\nBigtable online serving and is recommended for most scenarios.\nOptimized online serving also supports embeddings management.\nTo use Optimized online serving, you need to configure either a public\nendpoint or a dedicated Private Service Connect endpoint.\nTo learn how to set up online serving in Vertex AI Feature Store\nafter you set up features, seeOnline serving types.\nBecause you don't need to copy or import your feature data from BigQuery\nto a separate offline store in Vertex AI, you can use the data management\nand export capabilities of BigQuery to do the following:\nQuery feature data, includinghistorical data at a point in time.\nQuery feature data, includinghistorical data at a point in time.\nPreprocessandexportfeature data for model training and batch predictions.\nPreprocessandexportfeature data for model training and batch predictions.\nFor more information about machine learning using BigQuery, seeBigQuery ML introduction.\nfeature engineeringFeature engineering is the process of transforming raw machine learning (ML) data into features that can be used to train ML models or to make predictions.\nFeature engineering is the process of transforming raw machine learning (ML) data into features that can be used to train ML models or to make predictions.\nfeatureIn machine learning (ML), a feature is a characteristic or attribute of an instance or entity that's used as an input to train an ML model or to make predictions.\nIn machine learning (ML), a feature is a characteristic or attribute of an instance or entity that's used as an input to train an ML model or to make predictions.\nfeature valueA feature value corresponds to the actual and measurable value of a feature (attribute) of an instance or entity. A collection of feature values for the unique entity represent the feature record corresponding to the entity.\nA feature value corresponds to the actual and measurable value of a feature (attribute) of an instance or entity. A collection of feature values for the unique entity represent the feature record corresponding to the entity.\nfeature timestampA feature timestamp indicates when the set of feature values in a specific feature record for an entity were generated.\nA feature timestamp indicates when the set of feature values in a specific feature record for an entity were generated.\nfeature recordA feature record is an aggregation of all feature values that describe the attributes of a unique entity at a specific point in time.\nA feature record is an aggregation of all feature values that describe the attributes of a unique entity at a specific point in time.\nfeature registryA feature registry is a central interface for recording feature data sources that you want to serve for online predictions.  For more information, seeFeature Registry setup.\nA feature registry is a central interface for recording feature data sources that you want to serve for online predictions.  For more information, seeFeature Registry setup.\nfeature groupA feature group is a feature registry resource that corresponds to a BigQuery source table or view containing feature data. A feature view might contain features and can be thought of as a logical grouping of feature columns in the data source.\nA feature group is a feature registry resource that corresponds to a BigQuery source table or view containing feature data. A feature view might contain features and can be thought of as a logical grouping of feature columns in the data source.\nfeature servingFeature serving is the process of exporting or fetching feature values for training or inference. In Vertex AI, there are two types of feature serving—online serving and offline serving. Online serving retrieves the latest feature values of a subset of the feature data source for online predictions. Offline or batch serving exports high volumes of feature data—including historical data—for offline processing, such as ML model training.\nFeature serving is the process of exporting or fetching feature values for training or inference. In Vertex AI, there are two types of feature serving—online serving and offline serving. Online serving retrieves the latest feature values of a subset of the feature data source for online predictions. Offline or batch serving exports high volumes of feature data—including historical data—for offline processing, such as ML model training.\noffline storeThe offline store is a storage facility storing recent and historical feature data, which is typically used for training ML models. An offline store also contains the latest feature values, which you can serve for online predictions.\nThe offline store is a storage facility storing recent and historical feature data, which is typically used for training ML models. An offline store also contains the latest feature values, which you can serve for online predictions.\nonline storeIn feature management, an online store is a storage facility for the latest feature values to be served for online predictions.\nIn feature management, an online store is a storage facility for the latest feature values to be served for online predictions.\nfeature viewA feature view is a logical collection of features materialized from a BigQuery data source to an online store instance. A feature view stores and periodically refreshes the customer's feature data, which is refreshed periodically from the BigQuery source. A feature view is associated with the feature data storage either directly or through associations to feature registry resources.\nA feature view is a logical collection of features materialized from a BigQuery data source to an online store instance. A feature view stores and periodically refreshes the customer's feature data, which is refreshed periodically from the BigQuery source. A feature view is associated with the feature data storage either directly or through associations to feature registry resources.\nAll Vertex AI Feature Store resources must be located in the same\nregion or the same multi-regional location as your BigQuery data\nsource. For example, if the feature data source is located inus-central1,\nyou must create yourFeatureOnlineStoreinstance only inus-central1or in theUSmulti-region location.\nVertex AI Feature Store is integrated with Dataplex to\nprovide feature governance capabilities, including feature metadata. Online\nstore instances, feature views, and feature groups are automatically registered\nas data assets in Data Catalog, a Dataplex feature that\ncatalogs metadata from these resources. You can then use the metadata search\ncapability of Dataplex to search for, view, and manage the\nmetadata for these resources. For more information about searching for\nVertex AI Feature Store resources in Dataplex, seeSearch for resource metadata in Data Catalog.\nYou can add labels to resources during or after the resource creation.\nFor more information about adding labels to existing\nVertex AI Feature Store resources, seeUpdate labels.\nVertex AI Feature Store only supports the version0for features.\nVertex AI Feature Store lets you set up feature monitoring to\nretrieve feature statistics and detect anomalies in feature data. You can either\nset up monitoring schedules to periodically run monitoring jobs, or manually\nrun a monitoring job. For more information about setting up feature\nmonitoring and running feature monitoring jobs, seeMonitor features for anomalies.\nOptimized online serving in Vertex AI Feature Store supports\nembedding management. You can store embeddings in BigQuery as\nregulardoublearrays. Using the embedding management capabilities of\nVertex AI Feature Store, you can perform vector similarity searches\nto retrieve entities that are approximate nearest neighbors for a specified\nentity or embedding value.\nTo use embedding management in Vertex AI Feature Store, you need to do the following:\nSet up the BigQuery data source to support embeddings by including\ntheembeddingcolumn. Optionally, include filtering and crowding columns.\nFor more information, seeData source preparation guidelines.\nSet up the BigQuery data source to support embeddings by including\ntheembeddingcolumn. Optionally, include filtering and crowding columns.\nFor more information, seeData source preparation guidelines.\nCreate an online store instance for Optimized online serving.\nCreate an online store instance for Optimized online serving.\nSpecify theembeddingcolumn while creating the feature view. For more\ninformation about how to create a feature view that supports embeddings,\nseeConfigure vector retrieval for a feature view.\nSpecify theembeddingcolumn while creating the feature view. For more\ninformation about how to create a feature view that supports embeddings,\nseeConfigure vector retrieval for a feature view.\nFor information about how to perform a vector similarity search in Vertex AI Feature Store,\nseePerform a vector search for entities.\nVertex AI Feature Store retains the latest feature values for a\nunique ID, based on the timestamp associated with the feature values in the data\nsource. There's no data retention limit in the online store.\nBecause the offline store is provisioned by BigQuery, data\nretention limits or quotas from BigQuery might apply to the\nfeature data source, including historical feature values.Learn more about quotas and limits in BigQuery.\nVertex AI Feature Store enforces quotas and limits to help you\nmanage resources by setting usage limits, and to protect the community of\nGoogle Cloud users by preventing unforeseen spikes in usage. To efficiently\nuse Vertex AI Feature Store resources without hitting these\nconstraints, review theVertex AI Feature Store quotas and limits.\nFor information about resource usage pricing for Vertex AI Feature Store, seeVertex AI Feature Store pricing.\nUse the following samples and tutorials to learn more about Vertex AI Feature Store.Online feature serving and fetching of BigQuery data with Vertex AI Feature Store Bigtable online servingIn this tutorial, you learn how to use Bigtable online serving in Vertex AI Feature Store for online serving and fetching of feature values in BigQuery.Open in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to use Bigtable online serving in Vertex AI Feature Store for online serving and fetching of feature values in BigQuery.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to use Optimized online serving in Vertex AI Feature Store for serving and fetching of feature values from BigQuery.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to use Vertex AI Feature Store for online serving and vector retrieval of feature values in BigQuery.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to enable feature view Service Agents and grant each feature view access to the specific source data that is used.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to chunk user-provided data, and then generate embedding vectors for each chunk using a Large Language Model (LLM) that has embedding generation capabilities. The resulting embedding vector dataset can then be loaded into Vertex AI Feature Store, enabling fast feature retrieval and efficient online serving.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to build a low-latency vector search system for your Gen AI application using BigQuery vector search and Vertex AI Feature Store.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nIn this tutorial, you learn how to configure an IAM policy to control access to resources and data stored within Vertex AI Feature Store.\nOpen in Colab|Open in Colab Enterprise|View on GitHub\nLearn how toset up your data in BigQuery.\nLearn how toset up your data in BigQuery.\nLearn how to createfeature groupsandfeatures.\nLearn how to createfeature groupsandfeatures.\nLearn how tocreate an online store instance.\nLearn how tocreate an online store instance.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/sql/docs/mysql",
    "title": "Cloud SQL for MySQL documentation",
    "content": "Home\nCloud SQL\nDocumentation\nMySQL\nCloud SQL for MySQL is a fully-managed database service that\nhelps you set up, maintain, manage, and administer your\nMySQL relational databases on Google Cloud Platform.\nFor information specific to MySQL, see theMySQL documentationorlearn more about Cloud SQL for MySQL.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate instances\nCreate instances\nConnection overview\nConnection overview\nEnable and disable high availability on an instance\nEnable and disable high availability on an instance\nCreate and manage MySQL databases\nCreate and manage MySQL databases\nCreate and manage MySQL users\nCreate and manage MySQL users\nExport and import using SQL dump file\nExport and import using SQL dump file\nExport and import using CSV files\nExport and import using CSV files\nCreate backups\nCreate backups\nCreate read replicas\nCreate read replicas\ngcloud sql command-line\ngcloud sql command-line\nUse the Cloud SQL Admin API\nUse the Cloud SQL Admin API\nREST API\nREST API\nBest practices\nBest practices\nPerformance tips\nPerformance tips\nAuthorize requests\nAuthorize requests\nConfigure VPC Service Controls\nConfigure VPC Service Controls\nCloud SQL Admin API error messages\nCloud SQL Admin API error messages\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nTroubleshoot\nTroubleshoot\nCloud SQL feature support by database engine\nCloud SQL feature support by database engine\nRelease notes\nRelease notes\nBilling questions\nBilling questions\nGet support\nGet support\nSecurity bulletins\nSecurity bulletins\nGoogle Cloud Fundamentals: Core Infrastructure\nThese lectures, demos, and hands-on labs give you an overview of Google Cloud products and services so that you can learn the value of Google Cloud and how to incorporate cloud-based solutions into your business strategies.\nArchitecting with Google Cloud: Design and Process\nThis course features a combination of lectures, design activities, and hands-on labs to show you how to use proven design patterns on Google Cloud to build highly reliable and efficient solutions and operate deployments that are highly available and cost-effective.\nConverting and optimizing queries from Oracle Database to Cloud SQL for MySQL\nDiscusses the basic query differences between Oracle® and Cloud SQL for MySQL, and how features in Oracle map to features in Cloud SQL for MySQL.Migration\nMigrating Oracle users to Cloud SQL for MySQL: Terminology and functionality\nPart of a series that provides key information and guidance related to planning and performing Oracle 11g/12c database migrations to Cloud SQL for MySQL version 5.7, second-generation instances.Migration\nData residency overview\nLearn how to use Cloud SQL to enforce data residency requirements for data.Data residencydata\nUse Secret Manager to handle secrets in Cloud SQL\nLearn how to use Secret Manager to store sensitive information about Cloud SQL instances\n  and users as secrets.Secret Managersecret\nPython SQLAlchemy\nUse SQLAlchemy with your Cloud SQL for MySQL database\nNode.js sample\nConnecting to your Cloud SQL for MySQL database in Node.js\nPHP PDO\nConnecting your Cloud SQL for MySQL database using PHP PDO\nGo web app sample\nSimple examples of connecting to Cloud SQL for MySQL using Go\n.NET sample\nThis sample application demonstrates how to store data in Google Cloud SQL with a MySQL database when running in Google App Engine Flexible Environment.\nJava servlet\nConnecting to Cloud SQL for MySQL from a Java application\nTerraform for Cloud SQL networking\nUse Terraform to create Cloud SQL for MySQL instances with private networking options.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data",
    "title": "View Carbon Footprint data",
    "content": "Home\nCarbon Footprint\nDocumentation\nThe Carbon Footprint dashboard displays estimated greenhouse gas\nemissions associated with the usage ofcovered Google Cloud servicesfor the\nselected billing account.\nTo access the Carbon Footprint dashboard, you need the following\nIAM permissions:\nbilling.accounts.getCarbonInformation\nbilling.accounts.list\nbilling.accounts.get\nThese IAM permissions enable you to do all of the following:\nList and view the names of billing accounts.\nAccess Carbon Footprint data associated with these billing accounts.\nTo obtain these IAM permissions, a billing account administrator must grant you one or more IAM roles listed below for the billing account that contain such permissions:\nroles/billing.carbonViewer\nroles/billing.admin\nroles/billing.viewer\nRead more aboutCloud Carbon IAM permission and rolesandCloud Billing access control.\nThe Carbon Footprint dashboard is located in theToolssection\nwithin Google Cloud console.\nGo to Carbon Footprint\nCarbon Footprint data is computed automatically for your billing\naccount, there is no API to enable or setup required.\nIt can take up to 21 days for data of the previous month to become available.\nIf you have the properIAM permissionand data is\navailable, data is displayed on the dashboard.\nCarbon footprint data is given in metric tons of CO2equivalent\n(tCO2e) in the\nUI dashboard and in kilograms of CO2equivalent\n(kgCO2e) in the data export.\nThe dashboard is divided into two tabs:Market-based emissionsandLocation-based emissions. These two tabs display emissions data estimated using\ndifferent Scope 2 carbon accounting definitions according to the Greenhouse Gas\nProtocol (GHGP). Scope 1 and Scope 3 emissions are identical across the two tabs.\nIn summary:\nMarket-based emissions: This tab displays emissions data broken\ndown by scope 1, scope 2 market-based, and scope 3 GHG emissions. Learn more\naboutscope 2 market-based emissions methodology.\nLocation-based emissions: This tab displays emissions data broken down by\nscope 1, scope 2 location-based, and scope 3 GHG emissions. Learn more aboutscope 2 location-based emissions methodology.\nEach tab displays an overview of the estimated greenhouse gas emissions\nassociated with the usage ofcovered Google Cloud servicesfor the selected billing account.\nAnnual carbon footprint: The total estimated greenhouse gas emissions\nassociated with the usage ofcovered Google Cloud servicesfor the selected billing account over the past 12 complete months.\nCarbon footprint for the past month: The total estimated greenhouse gas\nemissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account for the most recent completed month, compared\nto the previous month.\nCarbon Footprint has integrated with Google Cloud Active Assist\nUnattended Project Recommender, which analyzes usage activity across all\nprojects, provides you with the recommendations to reclaim or shut down unattended\nprojects, and helps you reduce both cost and carbon emissions. If your billing\naccount has associated projects that are deemed \"unattended\", you will see the\n\"Recommendations to reduce emissions\" card next to your annual and monthly\nsummary cards.\nTo dig deeper into the specifics of your carbon emissions, the dashboard includes\nfour charts:\nMonthly carbon emissions (in both market-based emissions tab and location-based emissions tab):\nDisplays the total estimated greenhouse gas emissions associated with the usage\nofcovered Google Cloud servicesfor\nthe selected billing account over all available months, broken down by month.\nCarbon emissions by region (in both market-based emissions tab and location-based emissions tab):\nDisplays the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the\nselected billing account for the past complete month broken down by Google Cloud\nregion.\nCarbon emissions by project (only in the location-based emissions tab):\nDisplays the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the\nselected billing account for the past complete month broken down by Google Cloud project.\nCarbon emissions by product (only in the location-based emissions tab):\nDisplays the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the\nselected billing account for the past complete month broken down by Google Cloud\nservice.\nExport your carbon footprint\nCreate custom dashboards with the exported data.\nUnderstand the methodology behind Carbon Footprint\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-29 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI provides a managed training service that lets you\noperationalize large scale model training. You can use Vertex AI to run\ntraining applications based on any machine learning (ML) framework on\nGoogle Cloud infrastructure. For the following popular ML frameworks,\nVertex AI also has integrated support that simplifies the preparation\nprocess for model training and serving:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nThis page explains the benefits of custom training on Vertex AI, the workflow\ninvolved, and the various training options that are available.\nThere are several challenges to operationalizing model training. These\nchallenges include the time and cost needed to train models, the depth of skills\nrequired to manage the compute infrastructure, and the need to provide\nenterprise-level security. Vertex AI addresses these challenges while\nproviding a host of other benefits.\nModel training on Vertex AI is a fully managed service that\n      requires no administration of physical infrastructure. You can train\n      ML models without the need to provision or manage servers. You only pay\n      for the compute resources that you consume. Vertex AI also\n      handles job logging, queuing, and monitoring.\nModel training on Vertex AI is a fully managed service that\n      requires no administration of physical infrastructure. You can train\n      ML models without the need to provision or manage servers. You only pay\n      for the compute resources that you consume. Vertex AI also\n      handles job logging, queuing, and monitoring.\nVertex AI training jobs are optimized for ML model training, which\n      can provide faster performance than directly running your training\n      application on a GKE cluster. You can also identify\n      and debug performance bottlenecks in your training job by using\n      Cloud Profiler.\nVertex AI training jobs are optimized for ML model training, which\n      can provide faster performance than directly running your training\n      application on a GKE cluster. You can also identify\n      and debug performance bottlenecks in your training job by using\n      Cloud Profiler.\nReduction Serveris an all-reduce algorithm in Vertex AI that can increase\n      throughput and reduce latency of multi-node distributed training on NVIDIA\n      graphics processing units (GPUs). This optimization helps reduce the time\n      and cost of completing large training jobs.\nReduction Serveris an all-reduce algorithm in Vertex AI that can increase\n      throughput and reduce latency of multi-node distributed training on NVIDIA\n      graphics processing units (GPUs). This optimization helps reduce the time\n      and cost of completing large training jobs.\nHyperparameter tuning jobsrun multiple trials of your training application using different\n      hyperparameter values. You specify a range of values to test and\n      Vertex AI discovers the optimal values for your model within that\n      range.\nHyperparameter tuning jobsrun multiple trials of your training application using different\n      hyperparameter values. You specify a range of values to test and\n      Vertex AI discovers the optimal values for your model within that\n      range.\nVertex AI provides the following enterprise security features:VPC peeringto limit network access.VPC Service Controlsto mitigate the risks\nof data exfiltration.Customer-managed encryption keysto help you meet\nspecific compliance or regulatory requirements related to data protection.Identity and Access Managementfor fine-grained control over\nservice account access.Data isolation with single-tenant project boundaries.\nVertex AI provides the following enterprise security features:\nVPC peeringto limit network access.\nVPC Service Controlsto mitigate the risks\nof data exfiltration.\nCustomer-managed encryption keysto help you meet\nspecific compliance or regulatory requirements related to data protection.\nIdentity and Access Managementfor fine-grained control over\nservice account access.\nData isolation with single-tenant project boundaries.\nVertex AI provides a suite ofintegrated MLOps toolsand features\nthat you can use for the following purposes:Orchestrate end-to-end ML workflows.Perform feature engineering.Run experiments.Manage and iterate your models.Track ML metadata.Monitor and evaluate model quality.\nVertex AI provides a suite ofintegrated MLOps toolsand features\nthat you can use for the following purposes:\nOrchestrate end-to-end ML workflows.\nPerform feature engineering.\nRun experiments.\nManage and iterate your models.\nTrack ML metadata.\nMonitor and evaluate model quality.\nThe following diagram shows a high-level overview of the custom training\nworkflow on Vertex AI. The sections that follow describe each step in\ndetail.\nFor the best performance and support, use one of the\nfollowing Google Cloud services as your data source:\nCloud Storage\nBigQuery\nNFS shares on Google Cloud\nFor a comparison of these services, seeData preparation overview.\nYou can also specify aVertex AI managed datasetas the data source when using a training pipeline to train your model. Training\na custom model and an AutoML model using the same dataset lets you\ncompare the performance of the two models.\nTo prepare your training application for use on Vertex AI, do\nthe following:\nImplement training code best practices for Vertex AI.\nDetermine a type of container image to use.\nPackage your training application into a supported format based on the\nselected container image type.\nYour training application should implement thetraining code best practices for Vertex AI.\nThese best practices relate to the ability of your training application to do\nthe following:\nAccess Google Cloud services.\nLoad input data.\nEnable autologging for experiment tracking.\nExport model artifacts.\nUse the environment variables of Vertex AI.\nEnsure resilience to VM restarts.\nVertex AI runs your training application in aDocker container image.\nA Docker container image is a self-contained software package that includes code\nand all dependencies, which can run in almost any computing environment. You can\neither specify the URI of aprebuilt container imageto use, or create and upload acustom container imagethat has your training application and dependencies pre-installed.\nThe following table shows the differences between prebuilt and custom container\nimages:\nPython source distribution.\nSingle Python file.\nGreater customization and control.\nNon-Python training applications.\nPrivate or custom dependencies.\nTraining applications that use an ML framework or framework version that has\n    no prebuilt container image available.\nAfter you've determined the type of container image to use, package your\ntraining application into one of the following formats based on the container\nimage type:\nSingle Python file for use in a prebuilt containerWrite your training application as a single Python file and use theVertex AI SDK for Pythonto create aCustomJoborCustomTrainingJobclass. The Python file is packaged into a\nPython source distribution and installed to a prebuilt container image.\nDelivering your training application as a single Python file is suitable for\nprototyping. For production training applications, you'll likely have your\ntraining application arranged into more than one file.\nSingle Python file for use in a prebuilt container\nWrite your training application as a single Python file and use theVertex AI SDK for Pythonto create aCustomJoborCustomTrainingJobclass. The Python file is packaged into a\nPython source distribution and installed to a prebuilt container image.\nDelivering your training application as a single Python file is suitable for\nprototyping. For production training applications, you'll likely have your\ntraining application arranged into more than one file.\nPython source distribution for use in a prebuilt containerPackage your training applicationinto one or more Python source distributions and upload them to a\nCloud Storage bucket. Vertex AI installs the source distributions\nto a prebuilt container image when you create a training job.\nPython source distribution for use in a prebuilt container\nPackage your training applicationinto one or more Python source distributions and upload them to a\nCloud Storage bucket. Vertex AI installs the source distributions\nto a prebuilt container image when you create a training job.\nCustom container imageCreate your own Docker container imagethat has your training application and dependencies pre-installed, and\nupload it to Artifact Registry. If your training application is written in Python,\nyou canperform these steps by using one Google Cloud CLI command.\nCustom container image\nCreate your own Docker container imagethat has your training application and dependencies pre-installed, and\nupload it to Artifact Registry. If your training application is written in Python,\nyou canperform these steps by using one Google Cloud CLI command.\nA Vertex AI training job performs the following tasks:\nProvisions one (single node training) or more (distributed training) virtual\nmachines (VMs).\nRuns your containerized training application on the provisioned VMs.\nDeletes the VMs after the training job completes.\nVertex AI offersthree types of training jobsfor running your training application:\nCustom jobA custom job\n(CustomJob)\nruns your training application. If you're using a prebuilt container image,\nmodel artifacts are output to the specified Cloud Storage bucket. For\ncustom container images, your training application can also output model\nartifacts to other locations.\nCustom job\nA custom job\n(CustomJob)\nruns your training application. If you're using a prebuilt container image,\nmodel artifacts are output to the specified Cloud Storage bucket. For\ncustom container images, your training application can also output model\nartifacts to other locations.\nHyperparameter tuning jobA hyperparameter tuning job\n(HyperparameterTuningJob)\nruns multiple trials of your training\napplication using different hyperparameter values until it produces model\nartifacts with the optimal performing hyperparameter values. You specify the\nrange of hyperparameter values to test and the metrics to optimize for.\nHyperparameter tuning job\nA hyperparameter tuning job\n(HyperparameterTuningJob)\nruns multiple trials of your training\napplication using different hyperparameter values until it produces model\nartifacts with the optimal performing hyperparameter values. You specify the\nrange of hyperparameter values to test and the metrics to optimize for.\nTraining pipelineA training pipeline\n(CustomTrainingJob)\nruns a custom job or hyperparameter tuning job and optionally exports the\nmodel artifacts to Vertex AI to create a model resource. You can\nspecify a Vertex AI managed dataset as your data source.\nTraining pipeline\nA training pipeline\n(CustomTrainingJob)\nruns a custom job or hyperparameter tuning job and optionally exports the\nmodel artifacts to Vertex AI to create a model resource. You can\nspecify a Vertex AI managed dataset as your data source.\nWhen creating a training job, specify the compute resources to use\nfor running your training application and configure your container settings.\nSpecify the compute resourcesto\nuse for a training job. Vertex AI supports single-node training, where\nthe training job runs on one VM, anddistributed training, where the training\njob runs on multiple VMs.\nThe compute resources that you can specify for your training job are as follows:\nVM machine typeDifferent machine types offer different CPUs, memory size, and bandwidth.\nVM machine type\nDifferent machine types offer different CPUs, memory size, and bandwidth.\nGraphics processing units (GPUs)You can add one or more GPUs to A2 or N1 type VMs. If your training\napplication is designed to use GPUs, adding GPUs can significantly improve\nperformance.\nGraphics processing units (GPUs)\nYou can add one or more GPUs to A2 or N1 type VMs. If your training\napplication is designed to use GPUs, adding GPUs can significantly improve\nperformance.\nTensor Processing Units (TPUs)TPUs are designed specifically for accelerating machine learning workloads.\nWhen using a TPU VM for training, you can specify only one worker pool.\nThat worker pool can have only one replica.\nTensor Processing Units (TPUs)\nTPUs are designed specifically for accelerating machine learning workloads.\nWhen using a TPU VM for training, you can specify only one worker pool.\nThat worker pool can have only one replica.\nBoot disksYou can use SSDs (default) or HDDs for your boot disk. If your training\napplication reads and writes to disk, using SSDs can improve performance.\nYou can also specify the size of your boot disk based on the amount of\ntemporary data that your training application writes to disk. Boot disks can\nhave between 100 GiB (default) and 64,000 GiB. All VMs in a worker pool must\nuse the same type and size of boot disk.\nBoot disks\nYou can use SSDs (default) or HDDs for your boot disk. If your training\napplication reads and writes to disk, using SSDs can improve performance.\nYou can also specify the size of your boot disk based on the amount of\ntemporary data that your training application writes to disk. Boot disks can\nhave between 100 GiB (default) and 64,000 GiB. All VMs in a worker pool must\nuse the same type and size of boot disk.\nThecontainer configurationsthat you need to make depend on whether you're using a prebuilt or custom\ncontainer image.\nPrebuilt container configurations:Specify the URI of the prebuilt container image that you want to use.If your training application is packaged as a Python source\ndistribution, specify the Cloud Storage URI where the package is\nlocated.Specify the entry point module of your training application.Optional: Specify a list of command-line arguments to pass to the entry\npoint module of your training application.\nPrebuilt container configurations:\nSpecify the URI of the prebuilt container image that you want to use.\nIf your training application is packaged as a Python source\ndistribution, specify the Cloud Storage URI where the package is\nlocated.\nSpecify the entry point module of your training application.\nOptional: Specify a list of command-line arguments to pass to the entry\npoint module of your training application.\nCustom container configurations:Specify the URI of your custom container image, which can be a URI from\nArtifact Registry or Docker Hub.Optional: Override theENTRYPOINTorCMDinstructions in your\ncontainer image.\nCustom container configurations:\nSpecify the URI of your custom container image, which can be a URI from\nArtifact Registry or Docker Hub.\nOptional: Override theENTRYPOINTorCMDinstructions in your\ncontainer image.\nAfter your data and training application are prepared, run your training\napplication by creating one of the following training jobs:\nCreate a custom job.\nCreate a hyperparameter tuning job.\nCreate a training pipeline.\nTo create the training job, you can use the Google Cloud console, Google Cloud CLI,\nVertex AI SDK for Python, or the Vertex AI API.\nYour training application likely outputs one or more model artifacts to a\nspecified location, usually a Cloud Storage bucket. Before you can get\npredictions in Vertex AI from your model artifacts, firstimport the model artifacts into Vertex AI Model Registry.\nLike container images for training, Vertex AI gives you the choice of\nusingprebuiltorcustomcontainer images for\npredictions. If a prebuilt container image for predictions is available for your\nML framework and framework version, we recommend using a prebuilt container\nimage.\nGet predictionsfrom your model.\nEvaluate your model.\nTry theHello custom trainingtutorial for step-by-step instructions on training a TensorFlow Keras image\nclassification model on Vertex AI.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-12 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",
    "title": "Ray on Vertex AI overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nRayis an open-source framework for scaling AI and\nPython applications. Ray provides the infrastructure to perform distributed\ncomputing and parallel processing for your machine learning (ML) workflow.\nIf you already use Ray, you can use the same open source Ray code to write\nprograms and develop applications on Vertex AI with minimal changes.\nYou can then use Vertex AI's integrations with other Google Cloud\nservices such asVertex AI PredictionandBigQueryas part of your machine learning workflow.\nIf you already use Vertex AI and need a simpler way to manage compute\nresources, you can use Ray code to scale training.\nUse Colab Enterprise and Vertex AI SDK for Python to connect to the Ray Cluster.\nRay clusters are built in to ensure capacity availability for critical ML\nworkloads or during peak seasons. Unlike custom jobs, where the training\nservice releases the resource after job completion, Ray clusters remain\navailable until deleted.\nNote: Use long running Ray clusters in these scenarios:\nIf you are submitting the same Ray job multiple times and can benefit from\ndata and image caching by running the jobs on the same long running\nRay clusters.\nIf you run many short-lived Ray jobs where the actual processing time is\nshorter than the job startup time, it may be beneficial to have a\nlong-running cluster.\nRay clusters on Vertex AI can be set up either with public or private\nconnectivity. The following diagrams show the architecture and workflow for\nRay on Vertex AI. SeePublic or private connectivityfor more information.\nCreate the Ray cluster on Vertex AI using the following options:a. Use the Google Cloud console to create the Ray cluster on Vertex AI.b. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python.\nCreate the Ray cluster on Vertex AI using the following options:\na. Use the Google Cloud console to create the Ray cluster on Vertex AI.\nb. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python.\nConnect to the Ray cluster on Vertex AI for interactive development\nusing the following options:a. UseColab Enterprisein the Google Cloud console for seamless connection.b. Use any Python environment accessible to the public internet.\nConnect to the Ray cluster on Vertex AI for interactive development\nusing the following options:\na. UseColab Enterprisein the Google Cloud console for seamless connection.\nb. Use any Python environment accessible to the public internet.\nDevelop your application and train your model on the Ray cluster on\nVertex AI:Use the Vertex AI SDK for Python in your preferred environment\n(Colab Enterprise or any Python notebook).Write a Python script using your preferred environment.Submit a Ray Job to the Ray cluster on Vertex AI using the\nVertex AI SDK for Python, Ray Job CLI, or Ray Job Submission API.\nDevelop your application and train your model on the Ray cluster on\nVertex AI:\nUse the Vertex AI SDK for Python in your preferred environment\n(Colab Enterprise or any Python notebook).\nUse the Vertex AI SDK for Python in your preferred environment\n(Colab Enterprise or any Python notebook).\nWrite a Python script using your preferred environment.\nWrite a Python script using your preferred environment.\nSubmit a Ray Job to the Ray cluster on Vertex AI using the\nVertex AI SDK for Python, Ray Job CLI, or Ray Job Submission API.\nSubmit a Ray Job to the Ray cluster on Vertex AI using the\nVertex AI SDK for Python, Ray Job CLI, or Ray Job Submission API.\nDeploy the trained model to an online Vertex AI endpoint for live\nprediction.\nDeploy the trained model to an online Vertex AI endpoint for live\nprediction.\nUse BigQueryto manage your data.\nUse BigQueryto manage your data.\nThe following diagram shows the architecture and workflow for Ray on Vertex AI\nafter you set up your Google Cloud project and VPC network, which is optional:\nSet up your (a) Google project and (b) VPC network.\nSet up your (a) Google project and (b) VPC network.\nCreate the Ray cluster on Vertex AI using the following options:a. Use the Google Cloud console to create the Ray cluster on Vertex AI.b. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python.\nCreate the Ray cluster on Vertex AI using the following options:\na. Use the Google Cloud console to create the Ray cluster on Vertex AI.\nb. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python.\nConnect to the Ray cluster on Vertex AI through a VPC peered network using\nthe following options:UseColab Enterprisein the\nGoogle Cloud console.Use aVertex AI Workbenchnotebook.\nConnect to the Ray cluster on Vertex AI through a VPC peered network using\nthe following options:\nUseColab Enterprisein the\nGoogle Cloud console.\nUseColab Enterprisein the\nGoogle Cloud console.\nUse aVertex AI Workbenchnotebook.\nUse aVertex AI Workbenchnotebook.\nDevelop your application and train your model on the Ray cluster on Vertex AI using the following options:Use the Vertex AI SDK for Python in your preferred environment\n(Colab Enterprise or a Vertex AI Workbench notebook).Write a Python script using your preferred environment. Submit a Ray Job\nto the Ray cluster on Vertex AI using the Vertex AI SDK for Python,\nRay Job CLI, or Ray dashboard.\nDevelop your application and train your model on the Ray cluster on Vertex AI using the following options:\nUse the Vertex AI SDK for Python in your preferred environment\n(Colab Enterprise or a Vertex AI Workbench notebook).\nUse the Vertex AI SDK for Python in your preferred environment\n(Colab Enterprise or a Vertex AI Workbench notebook).\nWrite a Python script using your preferred environment. Submit a Ray Job\nto the Ray cluster on Vertex AI using the Vertex AI SDK for Python,\nRay Job CLI, or Ray dashboard.\nWrite a Python script using your preferred environment. Submit a Ray Job\nto the Ray cluster on Vertex AI using the Vertex AI SDK for Python,\nRay Job CLI, or Ray dashboard.\nDeploy the trained model to an online Vertex AI endpoint for predictions.\nDeploy the trained model to an online Vertex AI endpoint for predictions.\nUse BigQuery to manage your data.\nUse BigQuery to manage your data.\nPricing for Ray on Vertex AI is calculated as follows:\nThe compute resources you use are charged based on the machine configuration\nyou select when creating your Ray cluster on Vertex AI. For Ray on Vertex AI pricing, see thepricing page.\nThe compute resources you use are charged based on the machine configuration\nyou select when creating your Ray cluster on Vertex AI. For Ray on Vertex AI pricing, see thepricing page.\nRegarding Ray clusters, you are only charged during RUNNING and UPDATINGstates.\nNo other states are charged.\nThe amount charged is based on the actual cluster size at the moment.\nRegarding Ray clusters, you are only charged during RUNNING and UPDATINGstates.\nNo other states are charged.\nThe amount charged is based on the actual cluster size at the moment.\nWhen you perform tasks using the Ray cluster on Vertex AI, logs are\nautomatically generated and charged based onCloud Logging\npricing.\nWhen you perform tasks using the Ray cluster on Vertex AI, logs are\nautomatically generated and charged based onCloud Logging\npricing.\nIf you deploy your model to an endpoint for online predictions, see the\"Prediction and explanation\"section of the Vertex AI\npricing page.\nIf you deploy your model to an endpoint for online predictions, see the\"Prediction and explanation\"section of the Vertex AI\npricing page.\nIf you use BigQuery with Ray on Vertex AI, seeBigQuery pricing.\nIf you use BigQuery with Ray on Vertex AI, seeBigQuery pricing.\nSet up for Ray on Vertex AI\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGoogle Cloud NetApp Volumes\nDocumentation\nGuides\nThis page provides an overview of Google Cloud NetApp Volumes and how it works.\nNetApp Volumes is a fully managed, cloud-based data storage\nservice that provides advanced data management capabilities and highly scalable\nperformance.\nNetApp Volumes helps to accelerate deployment times, manage your\nworkloads and applications, and migrate workloads to the cloud while keeping the\nperformance and features of on-premises storage.\nNetApp Volumes lets you move file-based applications to\nGoogle Cloud. It has support for Network File System (NFSv3 and NFSv4.1) and\nServer Message Block (SMB) protocols built-in, so you don't need to re-architect\nyour applications and can continue to get persistent storage for your\napplications.\nNetApp Volumes offers four service levels: Flex, Standard,\nPremium, and Extreme. Performance, features, and capabilities vary by service\nlevel.\nNetApp Volumes offers the following features. For a comparison of\nfeatures across service levels, seeservice levels.\nFully-managed service: provides fully-managed service with no operations,\nintegrated with theGoogle Cloud console\nFully-managed service: provides fully-managed service with no operations,\nintegrated with theGoogle Cloud console\nVolume provisioning: provisions volumes from 1 GiB to 1 PiB in seconds\nVolume provisioning: provisions volumes from 1 GiB to 1 PiB in seconds\nMultiprotocol support: supports NFSv3, NFSv4.1, and SMB 2.1, 3.0, and\n3.1.1 protocols.\nMultiprotocol support: supports NFSv3, NFSv4.1, and SMB 2.1, 3.0, and\n3.1.1 protocols.\nAutomated snapshots: protects data with automated, efficient snapshots.\nAutomated snapshots: protects data with automated, efficient snapshots.\nAuto-tiering: moves infrequently used data transparently to affordable\ncold storage.\nAuto-tiering: moves infrequently used data transparently to affordable\ncold storage.\nBackup: provides manual and automated backups for long-term retention.\nBackup: provides manual and automated backups for long-term retention.\nVolume replication: enables business continuity with asynchronous volume\nreplication across Google Cloud.\nVolume replication: enables business continuity with asynchronous volume\nreplication across Google Cloud.\nHigh availability: provides high availability with options for\nmulti-region redundancy, backed by theNetApp Volumes service level agreement\nHigh availability: provides high availability with options for\nmulti-region redundancy, backed by theNetApp Volumes service level agreement\nRapid cloning: accelerates application development with rapid cloning\nRapid cloning: accelerates application development with rapid cloning\nMultiple service level offerings: offers multiple service levels based on\nlocation, allowing you to pick a service level that best fits your needs:Flex: highly available, general purpose storage with advanced data\nmanagement capabilities.Performance:Default: up to 16 KiBps per GiB of storage pool capacity shared\nby all volumes in the pool inselected regions.\nMaximum of 1.6 GiBps per pool.Custom: independent provisioning of capacity and performance with\nzonal pools in selected regions. You can provision throughput from\n64 MiBps to a maximum of 5 GiBps in increments of\n1 MiBps. Each MiBps provisioned throughput includes 16 IOPS.\nAdditional IOPS beyond the included throughput of 16 IOPS can be\nprovisioned as required up to a maximum of 160,000 IOPS. For more\ninformation about available regions, seeSupported regions for Flex custom performance.Sample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, containerized workloads, and Google Cloud VMware Engine.Standard: highly available, general purpose storage with advanced data\nmanagement capabilities.Performance: up to 16 KiBps per GiB of volume capacity.\nMaximum of 1.6 GiBps per volume.Sample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, and Google Cloud VMware Engine.Premium: highly available, high-performance storage with advanced data\nmanagement capabilities.Performance: up to 64 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.Sample use cases: performance-critical workloads requiring low\nlatency, for example, Windows and enterprise NFS, self-managed\ndatabases and file shares, virtual desktop infrastructure (VDI), and\nVMware Engine.Extreme: highly available, high-throughput storage with advanced\ndata management capabilities.Performance: up to 128 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.Sample use cases: performance-critical workloads requiring high\nthroughput and low latency, for example, Windows and enterprise NFS,\nself-managed databases and file shares, VDI, and\nVMware Engine.\nMultiple service level offerings: offers multiple service levels based on\nlocation, allowing you to pick a service level that best fits your needs:\nFlex: highly available, general purpose storage with advanced data\nmanagement capabilities.Performance:Default: up to 16 KiBps per GiB of storage pool capacity shared\nby all volumes in the pool inselected regions.\nMaximum of 1.6 GiBps per pool.Custom: independent provisioning of capacity and performance with\nzonal pools in selected regions. You can provision throughput from\n64 MiBps to a maximum of 5 GiBps in increments of\n1 MiBps. Each MiBps provisioned throughput includes 16 IOPS.\nAdditional IOPS beyond the included throughput of 16 IOPS can be\nprovisioned as required up to a maximum of 160,000 IOPS. For more\ninformation about available regions, seeSupported regions for Flex custom performance.Sample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, containerized workloads, and Google Cloud VMware Engine.\nFlex: highly available, general purpose storage with advanced data\nmanagement capabilities.\nPerformance:Default: up to 16 KiBps per GiB of storage pool capacity shared\nby all volumes in the pool inselected regions.\nMaximum of 1.6 GiBps per pool.Custom: independent provisioning of capacity and performance with\nzonal pools in selected regions. You can provision throughput from\n64 MiBps to a maximum of 5 GiBps in increments of\n1 MiBps. Each MiBps provisioned throughput includes 16 IOPS.\nAdditional IOPS beyond the included throughput of 16 IOPS can be\nprovisioned as required up to a maximum of 160,000 IOPS. For more\ninformation about available regions, seeSupported regions for Flex custom performance.\nPerformance:\nDefault: up to 16 KiBps per GiB of storage pool capacity shared\nby all volumes in the pool inselected regions.\nMaximum of 1.6 GiBps per pool.\nDefault: up to 16 KiBps per GiB of storage pool capacity shared\nby all volumes in the pool inselected regions.\nMaximum of 1.6 GiBps per pool.\nCustom: independent provisioning of capacity and performance with\nzonal pools in selected regions. You can provision throughput from\n64 MiBps to a maximum of 5 GiBps in increments of\n1 MiBps. Each MiBps provisioned throughput includes 16 IOPS.\nAdditional IOPS beyond the included throughput of 16 IOPS can be\nprovisioned as required up to a maximum of 160,000 IOPS. For more\ninformation about available regions, seeSupported regions for Flex custom performance.\nCustom: independent provisioning of capacity and performance with\nzonal pools in selected regions. You can provision throughput from\n64 MiBps to a maximum of 5 GiBps in increments of\n1 MiBps. Each MiBps provisioned throughput includes 16 IOPS.\nAdditional IOPS beyond the included throughput of 16 IOPS can be\nprovisioned as required up to a maximum of 160,000 IOPS. For more\ninformation about available regions, seeSupported regions for Flex custom performance.\nSample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, containerized workloads, and Google Cloud VMware Engine.\nSample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, containerized workloads, and Google Cloud VMware Engine.\nStandard: highly available, general purpose storage with advanced data\nmanagement capabilities.Performance: up to 16 KiBps per GiB of volume capacity.\nMaximum of 1.6 GiBps per volume.Sample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, and Google Cloud VMware Engine.\nStandard: highly available, general purpose storage with advanced data\nmanagement capabilities.\nPerformance: up to 16 KiBps per GiB of volume capacity.\nMaximum of 1.6 GiBps per volume.\nPerformance: up to 16 KiBps per GiB of volume capacity.\nMaximum of 1.6 GiBps per volume.\nSample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, and Google Cloud VMware Engine.\nSample use cases: common enterprise workloads such as Network File\nSystem (NFS) and Server Message Block (SMB) file shares, SAP shared\nfiles, and Google Cloud VMware Engine.\nPremium: highly available, high-performance storage with advanced data\nmanagement capabilities.Performance: up to 64 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.Sample use cases: performance-critical workloads requiring low\nlatency, for example, Windows and enterprise NFS, self-managed\ndatabases and file shares, virtual desktop infrastructure (VDI), and\nVMware Engine.\nPremium: highly available, high-performance storage with advanced data\nmanagement capabilities.\nPerformance: up to 64 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.\nPerformance: up to 64 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.\nSample use cases: performance-critical workloads requiring low\nlatency, for example, Windows and enterprise NFS, self-managed\ndatabases and file shares, virtual desktop infrastructure (VDI), and\nVMware Engine.\nSample use cases: performance-critical workloads requiring low\nlatency, for example, Windows and enterprise NFS, self-managed\ndatabases and file shares, virtual desktop infrastructure (VDI), and\nVMware Engine.\nExtreme: highly available, high-throughput storage with advanced\ndata management capabilities.Performance: up to 128 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.Sample use cases: performance-critical workloads requiring high\nthroughput and low latency, for example, Windows and enterprise NFS,\nself-managed databases and file shares, VDI, and\nVMware Engine.\nExtreme: highly available, high-throughput storage with advanced\ndata management capabilities.\nPerformance: up to 128 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.\nPerformance: up to 128 KiBps per GiB exclusive to volume.\nMaximum of 4.5 GiBps per volume. 30 GiBps with large capacity\nvolumes.\nSample use cases: performance-critical workloads requiring high\nthroughput and low latency, for example, Windows and enterprise NFS,\nself-managed databases and file shares, VDI, and\nVMware Engine.\nSample use cases: performance-critical workloads requiring high\nthroughput and low latency, for example, Windows and enterprise NFS,\nself-managed databases and file shares, VDI, and\nVMware Engine.\nNetApp Volumes provides fully managed NFS and SMB remote file\nsystems as a service. Service administrators create and manage remote file\nsystems as volumes and share them with NFS and SMB clients over a network.\nClients such as Compute Engine VMs mount file system volumes, their users, and\nthe applications within the client store files in the file system volumes. You\ncan control access using Windows or UNIX-based permission models.\nYou can use Google Cloud NetApp Volumes using the following tools:\nGoogle Cloud SDK: theGoogle Cloud command line interfacelets you interact with NetApp Volumes through a terminal\nGoogle Cloud SDK: theGoogle Cloud command line interfacelets you interact with NetApp Volumes through a terminal\nGoogle Cloud console: theGoogle Cloud consoleprovides a\nvisual interface that gives you a holistic view of your applications and\nprojects\nGoogle Cloud console: theGoogle Cloud consoleprovides a\nvisual interface that gives you a holistic view of your applications and\nprojects\nTerraform Google Cloud Platform Provider: NetApp Volumes\nresources are part of the GoogleTerraform provider.\nFor more information about how to provision NetApp Volumes\nresources using Terraform, see introduction toTerraform integration.\nTerraform Google Cloud Platform Provider: NetApp Volumes\nresources are part of the GoogleTerraform provider.\nFor more information about how to provision NetApp Volumes\nresources using Terraform, see introduction toTerraform integration.\nNetApp Volumes uses theGoogle Cloud Private Service Access framework,\nwhich creates a private connection linking your Virtual Private Cloud (VPC)\nto the NetApp Volumes VPC. The Google Cloud private\nservice access framework assigns private addresses (RFC 1918) or non-private\naddresses (non-RFC 1918) to it using the Service Networking API and\nVPC peering constructs.\nNetwork peering is integrated in the storage pool creation workflow. All volumes\nin a pool are accessible from Network-attached storage (NAS) clients on the same\nVPC, but are subject to NAS access control. For Shared VPC,\nthis enables data access across different projects. You can't attach a single\nvolume or pool to multiple VPCs.\nIndependent of data access at the VPC level, all resources\nbelong only to the project they're created in and can only be managed within\nthat projectIdentity and Access Management (IAM)protects\nmanagement access.\nNetApp Volumes is available in several regions. For details about\nregion availability, seeNetApp Volumes locations.\nRead aboutservice levelsof\nGoogle Cloud NetApp Volumes.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-20 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/industry",
    "title": "Industry solutions",
    "content": "Home\nDocumentation\nTransform your business with Google Cloud solutions for specific industries like retail, healthcare, and financial services.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nTransform AML detection by replacing rules-based transaction monitoring with AI.\nDigitally transform your healthcare and life sciences business though data-powered innovation.\nTransform audience experiences with innovation and insights.\nProvide Google Search-quality product search, browsing, and recommendations to your retail customers.\nUse AI to solve problems at scale, increase quality hires, and improve subscriber acquisition and retention.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/network-connectivity/docs/interconnect",
    "title": "Cloud Interconnect documentation",
    "content": "Home\nNetwork Connectivity\nDocumentation\nCloud Interconnect\nCloud Interconnect extends your external network to the Google network through a\n  high-availability, low-latency connection. You can use Dedicated Interconnect\n  to connect directly to Google. Alternatively, you can use Partner Interconnect\n  to connect to Google through a supported service provider. You can use\n  Cross-Cloud Interconnect to connect to your network that's hosted by another\n  cloud service provider.Learn more\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCloud Interconnect overview\nCloud Interconnect overview\nDedicated Interconnect\nDedicated Interconnect\nPartner Interconnect\nPartner Interconnect\nCross-Cloud Interconnect\nCross-Cloud Interconnect\nPartner Cross-Cloud Interconnect for OCI\nPartner Cross-Cloud Interconnect for OCI\nCross-Site Interconnect\nCross-Site Interconnect\nHA-VPN over Cloud Interconnect\nHA-VPN over Cloud Interconnect\nMACsec for Cloud Interconnect\nMACsec for Cloud Interconnect\nCreate Dedicated Interconnect connections\nCreate Dedicated Interconnect connections\nCreate Partner Interconnect connections\nCreate Partner Interconnect connections\nCreate Cross-Cloud Interconnect connections for AWS\nCreate Cross-Cloud Interconnect connections for AWS\nCreate Cross-Cloud Interconnect connections for Azure\nCreate Cross-Cloud Interconnect connections for Azure\nCreate Cross-Cloud Interconnect connections for OCI\nCreate Cross-Cloud Interconnect connections for OCI\nCreate Cross-Cloud Interconnect connections for Alibaba\nCreate Cross-Cloud Interconnect connections for Alibaba\nCreate Partner Cross-Cloud Interconnect connections for OCI\nCreate Partner Cross-Cloud Interconnect connections for OCI\nCreate Cross-Site Interconnect connections\nCreate Cross-Site Interconnect connections\nCloud Interconnect APIs\nCloud Interconnect APIs\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nTroubleshooting\nTroubleshooting\nRelease notes\nRelease notes\nService Level Agreement\nService Level Agreement\nTCP optimization for network performance in Google Cloud and hybrid scenarios\nLearn about ways to improve connection latency between processes within Google Cloud, including how to compute correct settings for decreasing the latency of TCP connections.Network Connectivity\nMigration to Google Cloud: Transferring your large datasets\nThis tutorial explores the process of adding your data to Google Cloud, from planning a data transfer to using best practices in implementing a plan.Network ConnectivityMigration\nPatterns for connecting other cloud service providers with Google Cloud\nDecide how to connect Google Cloud with other cloud service providers (CSP) such as Amazon Web Services (AWS), Microsoft Azure, Oracle Cloud Infrastructure, and Alibaba Cloud.Cloud InterconnectPartner InterconnectDedicated Interconnect\nCalculate network throughput over Interconnect\nThis tutorial shows you how to calculate network throughput, within Google Cloud and for your on-premises or third-party cloud locations that are connected using Cloud Interconnect.Network Connectivity\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nDocumentation\nSecurity\nThis content was last updated in July 2024, and represents the status quo\nas of the time it was written. Google's security policies and systems may change\ngoing forward, as we continually improve protection for our customers.\nDownload pdf version\nTraditionally, businesses have looked to the public cloud to save costs,\nexperiment with new technology, and provide growth capacity. Increasingly,\nbusinesses are also looking to the public cloud for their security, realizing\nthat cloud providers can invest more than the businesses can in technology,\npeople, and processes to deliver a more secure infrastructure.\nAs a cloud innovator, Google understands security in the cloud. Our cloud\nservices are designed to deliver better security than many on-premises\napproaches. We make security a priority in our operations—operations that serve\nusers across the world.\nSecurity drives our organizational structure, culture, training priorities, and\nhiring processes. It shapes the design of our data centers and the technology\nthat they house. It's central to our everyday operations and disaster\nplanning, including how we address threats. It's prioritized in the way we\nhandle customer data, our account controls, our compliance audits, and our\ncertifications.\nThis document describes our approach to security, privacy, and compliance for\nGoogle Cloud, which is our suite of public cloud products and services. The\ndocument focuses on the physical, administrative, and technical controls that we\nhave deployed to help protect your data.\nGoogle's culture stresses the importance of protecting the large volume of\ninformation that belongs to our customers. This culture influences our hiring\nprocesses and employee onboarding. We advance and reinforce data protection\nguidelines and technologies through ongoing training and events that are focused\non security and privacy.\nOur dedicated security team includes some of the world's foremost experts in\ninformation security, application security, cryptography, and network security.\nThis team maintains our defense systems, develops security review processes,\nbuilds security infrastructure, and implements our security policies. The team\nactively scans for security vulnerabilities using commercial and custom tools.\nThe team also conducts penetration tests and performs quality assurance and\nsecurity reviews.\nMembers of the security team review security plans for our networks and\nservices, and they provide project-specific consulting services to our product\nand engineering teams. For example, our cryptography engineers review product\nlaunches that include cryptography implementations. The security team monitors\nfor suspicious activity on our networks and addresses information security\nthreats as needed. The team also performs routine security evaluations and\naudits, which can involve engaging outside experts to conduct regular security\nassessments.\nWe have long enjoyed a close relationship with the security research community,\nand we greatly value their help with identifying potential vulnerabilities in\nGoogle Cloud and other Google products. Our security teams take part in\nresearch and outreach activities to benefit the online community. For example,\nwe runProject Zero,\nwhich is a team of security researchers who are dedicated to researching\nzero-day vulnerabilities. Some examples of this research are the discovery of\ntheSpectreexploit, theMeltdownexploit, thePOODLE SSL 3.0 exploit,\nandcipher suite weaknesses.\nGoogle's security engineers and researchers actively participate andpublishin the academic security community and the privacy research community. They also\norganize and participate inopen-source projectsand academic conferences. Google's security teams have published an in-depth\naccount of our practices and experience in theBuilding Secure and Reliable Systemsbook.\nOurVulnerability Reward\nProgramoffers rewards in the tens of thousands of dollars for each confirmed\nvulnerability. The program encourages researchers to report design and\nimplementation issues that might put customer data at risk. In 2023, we awarded\nresearchers over 10 million dollars in prize money. To help improve the security\nof open-source code, the Vulnerability Reward Program also provides avariety\nof initiativesto\nresearchers. For more information about this program, including the rewards that\nwe've given, seeBug Hunters Key\nStats.\nOur world-class cryptographers participate in industry-leading cryptography\nprojects. For example, we designed theSecure AI Framework (SAIF)to help secure AI systems. In addition, to protect TLS connections against\nquantum computer attacks, we developed thecombined elliptic-curve and post-quantum (CECPQ2) algorithm.\nOur cryptographers developedTink,\nwhich is an open-source library of cryptographic APIs. We also use Tink in our\ninternal products and services.\nFor more information about how you can report security issues, seeHow Google handles security vulnerabilities.\nAll Google employees undergo security and privacy training as part of the\norientation process, and they receive ongoing security and privacy training\nthroughout their Google careers. During orientation, new employees agree to ourCode of Conduct,\nwhich highlights our commitment to keeping customer data safe and secure.\nDepending on their job role, employees might be required to take additional\ntraining on specific aspects of security. For example, the information security\nteam instructs new engineers on secure coding practices, product design, and\nautomated vulnerability testing tools. Engineers attend regular security\nbriefings and receive security newsletters that cover new threats, attack\npatterns, mitigation techniques, and more.\nSecurity and privacy are an ever-changing area, and we recognize that dedicated\nemployee engagement is a key means of raising awareness. We host regular\ninternal conferences that are open to all employees to raise awareness and drive\ninnovation in security and data privacy. We host events across global offices to\nraise awareness of security and privacy in software development, data handling,\nand policy enforcement.\nOur dedicated privacy team supports internal privacy initiatives that help\nimprove critical processes, internal tools, products, and privacy\ninfrastructure. The privacy team operates separately from product development\nand security organizations. They participate in Google product launches by\nreviewing design documentation and performing code reviews to ensure that\nprivacy requirements are followed. The team helps release products that\nincorporate strong privacy standards around the collection of user data.\nOur products are designed to provide users and administrators with meaningful\nprivacy configuration options. After products are launched, the privacy team\noversees ongoing automated processes to verify that data collected by the\nproducts is handled appropriately. In addition, the privacy team conducts\nresearch on privacy best practices for our emerging technologies. To understand\nhow we stay committed to user data privacy and to compliance with applicable\nprivacy regulations and laws, seeour commitment to complying with data\nprotection laws. For more\ninformation, see thePrivacy Resource\nCenter.\nWe have a dedicated internal audit team that reviews our products' compliance\nwith security laws and regulations around the world. As new auditing standards\nare created and existing standards are updated, the internal audit team\ndetermines what controls, processes, and systems are needed in order to help\nmeet them. This team supports independent audits and assessments by third\nparties. For more information, seeSupport for compliance requirementslater in this document.\nSecurity is an integral part of our cloud operations, not an afterthought. This\nsection describes our vulnerability management programs, malware prevention\nprogram, security monitoring, and incident management programs.\nOur internal vulnerability management process actively scans for security\nthreats across technology stacks. This process uses a combination of\ncommercial, open-source, and purpose-built in-house tools, and includes the\nfollowing:\nQuality assurance processes\nSoftware security reviews\nIntensive automated and manual penetration efforts, including extensive\nRed Team exercises\nExternal audits\nThe vulnerability management organization and its partners are responsible for\ntracking and following up on vulnerabilities. Because security improves only\nafter issues are fully addressed, automation pipelines continually reassess the\nstate of patch deployment to mitigate vulnerabilities and flag incorrect or\npartial deployment.\nTo help improve detection capabilities, the vulnerability management\norganization focuses on high-quality indicators that separate noise from signals\nthat indicate real threats. The organization also fosters interaction with the\nindustry and with the open-source community. For example, they run aPatch Reward Programfor theTsunaminetwork security scanner, which rewards developers who create open-source\ndetectors for vulnerabilities.\nFor more about vulnerabilities that we have mitigated, seeGoogle Cloud security bulletins.\nGoogle maintains malware protections for our core products (like Gmail,\nGoogle Drive, Google Chrome, YouTube, Google Ads, and\nGoogle Search) that use a variety of malware detection\ntechniques. To discover malware files proactively, we use web crawling, file\ndetonation, custom static detection, dynamic detection, and machine-learning\ndetection. We also use multiple antivirus engines.\nTo help protect our employees, we use the built-in advanced security\ncapabilities of Chrome Enterprise Premium and the Enhanced Safe Browsing feature in\nGoogle Chrome. These capabilities enable proactive detection of phishing and\nmalware sites as our employees browse the web. We also enable the most rigorous\nsecurity settings that are available in Google Workspace, such as\nGmail Security Sandbox, to proactively scan suspicious attachments.\nLogs from these capabilities feed into our security monitoring systems, as\ndescribed in the following section.\nOur security monitoring program is focused on information that's gathered from\ninternal network traffic, from employee actions on systems, and from outside\nknowledge of vulnerabilities. A core Google principle is to aggregate and store\nsecurity telemetry data in one location for unified security analysis.\nAt many points across our global network, internal traffic is inspected for\nsuspicious behavior, such as the presence of traffic that might indicate botnet\nconnections. We use a combination of open-source and commercial tools to capture\nand parse traffic so that we can perform this analysis. A proprietary\ncorrelation system built on top of our technology also supports this analysis.\nWe supplement network analysis by examining system logs to identify unusual\nbehavior, such as attempts to access customer data.\nOur security engineers review inbound security reports and monitor public\nmailing lists, blog posts, and wikis. Automated network analysis and automated\nanalysis of system logs helps determine when an unknown threat might exist; if\nthe automated processes detect an issue, they escalate it to our security staff.\nWe have a rigorous incident-management process for security events that might\naffect the confidentiality, integrity, or availability of systems or data. Our\nsecurity incident-management program aligns with the NIST guidance on handling\nincidents (NIST SP\n800–61). Key\nmembers of our staff are trained in forensics and in handling evidence in\npreparation for an event, including the use of third-party and proprietary\ntools.\nWe test incident response plans for key areas, such as systems that store\ncustomer data. These tests consider various scenarios, including insider\nthreats and software vulnerabilities. To help ensure the swift resolution of\nsecurity incidents, the Google security team is available 24/7 to all employees.\nIf an incident impacts your data, Google or its partners inform you and\nour team investigates the incident. For more information about our data incident\nresponse process, seeData incident response process.\nGoogle Cloud runs on a technology platform that is designed and built to\noperate securely. We are an innovator in hardware, software, network, and system\nmanagement technologies. We design our servers, our proprietary operating\nsystem, and our geographically distributed data centers. Using the principles of\ndefense in depth, we've created an IT infrastructure that is more secure and\neasier to manage than more conventional technologies.\nOur focus on security and protection of data is amongour primary design\ncriteria. The physical\nsecurity in Google data centers is a layered security model. Physical security\nincludes safeguards like custom-designed electronic access cards, alarms,\nvehicle access barriers, perimeter fencing, metal detectors, and biometrics. In\naddition, to detect and track intruders, we use security measures such as laser\nbeam intrusion detection and 24/7 monitoring by high-resolution interior and\nexterior cameras. Access logs, activity records, and camera footage are\navailable in case an incident occurs. Experienced security guards, who have\nundergone rigorous background checks and training, routinely patrol our data\ncenters. As you get closer to the data center floor, security measures also\nincrease. Access to the data center floor is only possible through a security\ncorridor that implements multi-factor access control using security badges and\nbiometrics. Only approved employees with specific roles may enter. Very few\nGoogle employees will ever gain access to one of our data centers.\nInside our data centers, we employ security controls in thephysical-to-logical\nspace, defined as \"arm's length from a machine in a rack to the machine's\nruntime environment.\" These controls include hardware hardening, task-based\naccess control, anomalous event detection, and system self-defense. For more\ninformation, seeHow Google protects the physical-to-logical space in a data\ncenter.\nTo keep things running 24/7 and provide uninterrupted services, our data\ncenters have redundant power systems and environmental controls. Every critical\ncomponent has a primary and alternate power source, each with equal power.\nBackup generators can provide enough emergency electrical power to run each data\ncenter at full capacity. Cooling systems maintain a constant operating\ntemperature for servers and other hardware, which reduces the risk of service\noutages whileminimizing environmental impact.\nFire detection and suppression equipment help prevent damage to hardware. Heat\ndetectors, fire detectors, and smoke detectors trigger audible and visible\nalarms at security operations consoles and at remote monitoring desks.\nWe are the first major internet services company to get external certification\nof our high environmental, workplace safety, and energy management standards\nthroughout our data centers. For example, to demonstrate our commitment to\nenergy management practices, we obtained voluntaryISO 50001certifications for our data centers in Europe. For more information about how we\nreduce our environmental impact in Google Cloud, seeEfficiency.\nOur data centers have purpose-built servers and network equipment, some of\nwhich we design ourselves. While our servers are customized to maximize\nperformance, cooling, and power efficiency, they are also designed to help\nprotect against physical intrusion attacks. Unlike most commercially available\nhardware, our servers don't include unnecessary components such as video cards,\nchipsets, or peripheral connectors, all of which can introduce vulnerabilities.\nWe vet component vendors and choose components with care, working with vendors\nto audit and validate the security properties that are provided by the\ncomponents. We design custom chips, such asTitan,\nthat help us securely identify and authenticate legitimate Google devices at the\nhardware level, including the code that these devices use to boot up.\nServer resources are dynamically allocated. Dynamic allocation gives us\nflexibility for growth and lets us adapt quickly and efficiently to customer\ndemand by adding or reallocating resources. This environment is maintained by\nproprietary software that continually monitors systems for binary-level\nmodifications. Our automated, self-healing mechanisms are designed to enable us\nto monitor and remediate destabilizing events, receive notifications about\nincidents, and slow down potential compromises on the network.\nWe meticulously track the location and status of equipment within our data\ncenters using barcodes and asset tags. We deploy metal detectors and video\nsurveillance to help make sure that no equipment leaves the data center floor\nwithout authorization. If a component fails to pass a performance test at any\npoint during its lifecycle, it's removed from inventory and retired.\nOur storage devices, including hard drives, solid-state drives, and non-volatile\ndual inline memory modules (DIMMs), use technologies like full disk encryption\n(FDE) and drive locking to protect data at rest. When a storage device is\nretired, authorized individuals verify that the device is sanitized. They also\nperform a multiple-step verification process to ensure the device contains no\ndata. If a device cannot be erased for any reason, it's physically destroyed.\nPhysical destruction is performed using a shredder that breaks the device into\nsmall pieces, which are then recycled at a secure facility. Each data center\nadheres to a strict disposal policy and any variances are immediately addressed.\nFor more information, seeData deletion on\nGoogle Cloud.\nWe proactively seek to limit the opportunities for vulnerabilities to be\nintroduced by using source control protections and two-party reviews. We also\nprovide libraries that prevent developers from introducing certain classes of\nsecurity bugs. For example, we have libraries and frameworks that are designed\nto eliminate XSS vulnerabilities in web apps. We also have automated tools for\nautomatically detecting security bugs; these tools include fuzzers, static\nanalysis tools, and web security scanners.\nFor more information, seeSafe software development.\nGoogle Cloud services are designed to deliver better security than many\non-premises solutions. This section describes the main security controls that we\nuse to help protect your data.\nEncryption adds a layer of defense for protecting data. Encryption ensures that\nif an attacker gets access to your data, the attacker cannot read the data\nwithout also having access to the encryption keys. Even if an attacker gets\naccess to your data (for example, by accessing the wire connection between data\ncenters or by stealing a storage device), they won't be able to understand or\ndecrypt it.\nEncryption provides an important mechanism in how we help protect the privacy of\nyour data. It allows systems to manipulate data—for example, for backup—and\nengineers to support our infrastructure, without providing access to content for\nthose systems or employees.\nBy default, Google Cloud uses several layers of encryption to protect user\ndata that's stored in Google production data centers. Encryption is applied at\nthe application layer, the storage device layer, or both layers.\nFor more information about encryption at rest, including encryption key\nmanagement and Keystore, seeEncryption at rest in Google Cloud.\nData can be vulnerable to unauthorized access as it travels across the internet\nor within networks. Traffic between your devices and theGoogle Front End (GFE)is encrypted using strong encryption protocols such as TLS.\nFor more information, seeEncryption in transit in Google Cloud.\nSoftware supply chain integrity ensures that the underlying code and binaries\nfor the services that process your data are verified and that they pass\nattestation tests. In Google Cloud, we developedBinary Authorization for Borg\n(BAB)to review and authorize\nproduction software that we deploy. BAB helps ensure that only authorized code\ncan process your data. In addition to BAB, we use hardware security chips\n(called Titan) that we deploy on servers, devices, and peripherals. These chips\noffer core security features such as secure key storage, root of trust, and\nsigning authority.\nTo help secure your software supply chain, you can implementBinary Authorizationto enforce your policies before deploying your code. For information about\nsecuring your supply chain, seeSLSA.\nGoogle Cloud supports data encryption for data in use withConfidential Computing.\nConfidential Computing provides hardware isolation and attestation using a\nTrusted Execution Environment (TEE). Confidential Computing protects\nworkloads by performing computation in cryptographic isolation, which helps to\nensure confidentiality in a multi-tenant cloud environment. This type of\ncryptographically isolated environment helps prevent unauthorized access or\nmodifications to applications and data while the applications and data are in\nuse. TEE provides independently verifiable attestations that attest to the\nsystem state and code run. Confidential Computing might be a good option\nfor organizations that manage sensitive and regulated data and that need\nverifiable security and privacy assurances.\nIn other cloud services and on-premises solutions, customer data travels\nbetween devices across the public internet in paths known ashops. The number\nof hops depends on the optimal route between the customer's ISP and the data\ncenter. Each additional hop introduces a new opportunity for data to be attacked\nor intercepted. Because our global network is linked to most ISPs in the world,\nour network limits hops across the public internet, and therefore helps limit\naccess to that data by bad actors.\nOur network uses multiple layers of defense—defense in depth—to help protect the\nnetwork against external attacks. Only authorized services and protocols that\nmeet our security requirements are allowed to traverse it; anything else is\nautomatically dropped. To enforce network segregation, we use firewalls and\naccess control lists. Traffic is routed through GFE servers to help detect\nand stop malicious requests and distributed denial-of-service (DDoS) attacks.\nLogs are routinely examined to reveal any exploitation of programming errors.\nAccess to networked devices is restricted to only authorized employees.\nOur global infrastructure allows us to runProject\nShield. Project Shield provides free,\nunlimited protection to websites that are vulnerable to DDoS attacks that are\nused to censor information. Project Shield is available for news websites, human\nrights websites, and election-monitoring websites.\nOur IP data network consists of our own fiber, of publicly available fiber, and\nof undersea cables. This network allows us to deliver highly available and\nlow-latency services across the globe.\nWe design the components of our platform to be highly redundant. This redundancy\napplies to our server design, to how we store data, to network and internet\nconnectivity, and to the software services themselves. This \"redundancy of\neverything\" includes exception handling and creates a solution that is not\ndependent on a single server, data center, or network connection.\nOur data centers are geographically distributed to minimize the effects of\nregional disruptions on global products, such as when natural disasters or local\noutages occur. If hardware, software, or a network fails, platform services and\ncontrol planes are automatically and swiftly shifted from one facility to\nanother so that platform services can continue without interruption.\nOur highly redundant infrastructure also helps you protect your business from\ndata loss. You can create and deploy Google Cloud resources across\nmultiple regions and zones to build resilient and highly available systems. Our\nsystems are designed to minimize downtime or maintenance windows for when we\nneed to service or upgrade our platform. For more information about how\nGoogle Cloud builds resilience and availability into its core\ninfrastructure and services, from design through operations, see theGoogle Cloud infrastructure reliability guide.\nSome Google Cloud services are not available in all geographies. Some\nservice disruptions are temporary (due to an unanticipated event, such as a\nnetwork outage), but other service limitations are permanent due to\ngovernment-imposed restrictions. Our comprehensiveTransparency\nReportandstatus\ndashboardshowrecent and ongoing\ndisruptions of trafficand availability of Google Cloud services. We provide this data to help\nyou analyze and understand the availability of services.\nThis section describes how we restrict access to data and how we respond to\ndata requests from law enforcement agencies.\nData that you store on our systems is yours. We don't scan your data for\nadvertising purposes, we don't sell it to third parties, and we don't use it to\ntrain our AI models without your permission. TheData Processing\nAddendumfor Google Cloud describes our\ncommitment to protecting your data. That document states that we won't process\ndata for any purpose other than to meet our contractual obligations. If you\nchoose to stop using our services, we provide tools that let you\ntake your data with you, without penalty or additional cost. For more\ninformation about our commitments for Google Cloud, see ourtrust principles.\nOur infrastructure is designed to logically isolate each customer's data from\nthe data of other customers and users, even when it's stored on the same\nphysical server. Only a small group of employees have access to customer\ndata. Access rights and levels are based on an employee's job\nfunction and role, using the principles of least privilege and need-to-know that\nmatch access privileges to defined responsibilities. Our employees are granted\nonly a limited set of default permissions to access company resources, such as\nemployee email and Google's internal employee portal. Requests for additional\naccess must follow a formal process that involves a request and an approval from\nthe data or system owner, manager, or other executives, as dictated by our\nsecurity policies.\nApprovals are managed by workflow tools that maintain audit records of all\nchanges. These tools control both the modification of authorization settings and\nthe approval process to help ensure that approval policies are consistently\napplied. An employee's authorization settings are used to control access to\nresources, including data and systems for Google Cloud products. Support\nservices are provided only to authorized customer administrators. Our dedicated\nsecurity teams, privacy teams, and internal audit teams monitor and audit\nemployee access, and we provide audit logs to you throughAccess Transparencyfor Google Cloud. Also, when you enableAccess Approval,\nour support personnel and our engineers require your explicit approval to access\nyour data.\nAs the data owner, you are primarily responsible for responding to law\nenforcement data requests. However, like many technology companies, we receive\ndirect requests from governments and courts to disclose customer information.\nGoogle has operational policies and procedures and other organizational measures\nin place to help protect against unlawful or excessive requests for user data by\npublic authorities. When we receive such a request, our team reviews the request\nto make sure that it satisfies legal requirements and Google's policies.\nGenerally speaking, for us to comply, the request must be made in writing,\nissued under an appropriate law, and signed by an authorized official of the\nrequesting agency.\nWe believe that the public deserves to know the full extent to which governments\nrequest information from us. We became the first company to start regularly\npublishing reports about government data requests. Detailed information about\ndata requests and our response to them is available in ourTransparency Report.\nIt's our policy to notify you about requests for your data unless we are\nspecifically prohibited by law or court order from doing so. For more\ninformation, seeGovernment Requests for Cloud Customer Data.\nFor most data-processing activities, we provide our services in our own\ninfrastructure. However, we might engage some third-party suppliers to provide\nservices related to Google Cloud, including customer support and technical\nsupport. Before onboarding a supplier, we assess their security and privacy\npractices. This assessment checks whether the supplier provides a level of\nsecurity and privacy that is appropriate for their access to data and for the\nscope of the services that they are engaged to provide. After we have assessed\nthe risks that are presented by the third-party supplier, the supplier is\nrequired to enter into appropriate security, confidentiality, and privacy\ncontract terms.\nFor more information, see theSupplier Code of Conduct.\nGoogle Cloud regularly undergoes independent verification of its\nsecurity, privacy, and compliance controls, and receives certifications,\nattestations, and audit reports to demonstrate compliance. Our information\nsecurity includes specific customer data privacy-related controls that help keep\ncustomer data secure.\nSome key international standards that we are audited against are the following:\nISO/IEC 27001 (Information Security Management)\nISO/IEC 27017 (Cloud Security)\nISO/IEC 27018 (Cloud Privacy)\nISO/IEC 27701 (Privacy)\nIn addition, ourSOC 2andSOC 3reports are available to our customers.\nWe also participate in sector and country-specific frameworks, such asFedRAMP(US government),BSI C5(Germany), andMTCS(Singapore). We provide resource documents and mappings for certain frameworks\nwhere formal certifications or attestations might not be required or applied.\nIf you operate in regulated industries, such as finance, government, healthcare,\nor education, Google Cloud provides products and services that help you be\ncompliant with numerous industry-specific requirements. SeeAssured Workloads\noverviewfor information about how you can\nimplement regulatory requirements in\nGoogle Cloud.\nFor a complete listing of our compliance offerings, see theCompliance resource center.\nWe maintain a robust insurance program for many risk types, including cyber and\nprivacy liability insurance coverage. These policies include coverage for\nGoogle Cloud in events such as unauthorized use or access of our network;\nregulatory action where insurable; failure to adequately protect confidential\ninformation; notification costs; and crisis management costs, including forensic\ninvestigation.\nSecurity is a shared responsibility. Generally, you are responsible for securing\nwhat you bring to the cloud, whereas we are responsible for protecting the cloud\nitself. Therefore, while you're always responsible for securing your data, we\nare responsible for securing the underlying infrastructure. The following image\nvisualizes this relationship as the shared responsibility model, which describes\nthe responsibilities that we and you have in Google Cloud.\nIn the infrastructure as a service (IaaS) model, only the hardware, storage, and\nnetwork are our responsibility. In the software as a service (SaaS) model, the\nsecurity of everything except the data and its access and usage are our\nresponsibility.\nGoogle Cloud offers a range of security services that you can take\nadvantage of to secure your cloud environment at scale. For more information,\nseeSecurity and identity products in Google Cloud.\nYou can also find more information in oursecurity best practices center.\nThe protection of your data is a primary design consideration for our\ninfrastructure, products, and operations. Our scale of operations and our\ncollaboration with the security research community enable us to address\nvulnerabilities quickly, and often to prevent them entirely. We run our own\nservices, such as Search, YouTube, and\nGmail, on the same infrastructure that we make available to our\ncustomers, who benefit directly from our security controls and practices.\nWe offer a level of protection that few public cloud\nproviders or private enterprise IT teams can match. Protecting data is\ncore to our business, so we make extensive investments in security, resources,\nand expertise at a scale that others cannot. Our investment frees you to focus\non your business and innovation. Our strong contractual commitments help you\nmaintain control over your data and how it's processed. We don't use your data\nfor advertising or any purpose other than to deliver Google Cloud\nservices.\nMany innovative organizations trust us with their most valuable asset: their\ndata. We will continue to invest in the security of Google Cloud services to let\nyou benefit from our services in a secure and transparent manner.\nTo learn more about our security culture and philosophy, readBuilding Secure and Reliable Systems (O'Reilly book).\nFor information about our novel approach to cloud security, readBeyondProd,\nwhich describes how to protect code change and access to user data in\nmicroservices.\nTo adopt similar security principles for your own workloads, deploy theenterprise foundations blueprint.\nTo learn more about Google Workspace security, seeGoogle Workspace security.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama",
    "title": "Self-deployed Llama modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nLlama is a collection of open models developed by Meta that you can fine-tune\nand deploy on Vertex AI. Llama offers pre-trained and instruction-tuned\ngenerative text and multimodal models.\nThe Llama 4 family of models is a collection of multimodal models that use the\nMixture-of-Experts (MoE) architecture. By using the MoE architecture, models\nwith very large parameter counts can activate a subset of those parameters for\nany given input, which leads to more efficient inferences. Additionally, Llama\n4 uses early fusion, which integrates text and vision information from the\ninitial processing stages. This method enables Llama 4 models to more\neffectively grasp complex, nuanced relationships between text and images.\nModel Garden on Vertex AI offers two Llama 4 models: Llama 4\nScout and Llama 4 Maverick.\nFor more information, see theLlama\n4model card in\nModel Garden or view theIntroducing Llama 4 on Vertex AI\nblog post.\nLlama 4 Maverick is the largest and most capable Llama 4 model, offering\nindustry-leading capabilities on coding, reasoning, and image benchmarks. It\nfeatures 17 billion active parameters out of 400 billion total parameters with\n128 experts. Llama 4 Maverick uses alternating dense and MoE layers, where each\ntoken activates a shared expert plus one of the 128 routed experts. You can use\nthe model as a pretrained (PT) model or instruction-tuned (IT) model with FP8\nsupport. The model is pretrained on 200 languages and optimized for high-quality\nchat interactions through a refined post-training pipeline.\nLlama 4 Maverick is multimodal and has a 1M context length. It is suited for\nadvanced image captioning, analysis, precise image understanding, visual\nQ&A, creative text generation, general-purpose AI assistants, and sophisticated\nchatbots requiring top-tier intelligence and image understanding.\nLlama 4 Scout delivers state-of-the-art results for its size class with a large\n10 million token context window, outperforming previous Llama generations and\nother open and proprietary models on several benchmarks. It features 17 billion\nactive parameters out of the 109 billion total parameters with 16 experts and is\navailable as a pretrained (PT) or instruction-tuned (IT) model. Llama 4 Scout is\nsuited for retrieval tasks within long contexts and tasks that demand reasoning\nover large amounts of information, such as summarizing multiple large documents,\nanalyzing extensive user interaction logs for personalization and reasoning\nacross large codebases.\nLlama 3.3 is a text-only 70B instruction-tuned model that provides enhanced\nperformance relative to Llama 3.1 70B and to Llama 3.2 90B when used for\ntext-only applications. Moreover, for some applications, Llama 3.3 70B\napproaches the performance of Llama 3.1 405B.\nFor more information, see theLlama\n3.3model card in\nModel Garden.\nLlama 3.2 enables developers to build and deploy the latest generative AI models\nand applications that use Llama's capabilities to ignite new innovations,\nsuch as image reasoning. Llama 3.2 is also designed to be more accessible for\non-device applications. The following list highlights Llama 3.2 features:\nOffers a more private and personalized AI experience, with on-device\nprocessing for smaller models.\nOffers models that are designed to be more efficient, with reduced\nlatency and improved performance, making them suitable for a wide range of\napplications.\nBuilt on top of the Llama Stack, which makes building and\ndeploying applications easier. Llama Stack is a standardized interface for\nbuilding canonical toolchain components and agentic applications.\nSupports vision tasks, with a new model architecture that integrates\nimage encoder representations into the language model.\nThe 1B and 3B models are lightweight text-only models that support on-device use\ncases such as multilingual local knowledge retrieval, summarization, and\nrewriting.\nLlama 11B and 90B models are small and medium-sized multimodal models with image\nreasoning. For example, they can analyze visual data from charts to provide more\naccurate responses and extract details from images to generate text\ndescriptions.\nFor more information, see theLlama\n3.2model card in\nModel Garden.\nWhen using the 11B and 90B, there are no restriction when you send\ntext-only prompts. However, if you include an image in your prompt, the image\nmust be at beginning of your prompt, and you can include only one image. You\ncannot, for example, include some text and then an image.\nLlama 3.1 collection of multilingual large language models (LLMs) is a\ncollection of pre-trained and instruction-tuned generative models in 8B, 70B and\n405B sizes (text in/text out). The Llama 3.1 instruction tuned text-only models\n(8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform\nmany of the available open source and closed chat models on common industry\nbenchmarks.\nFor more information, see theLlama\n3.1model card in\nModel Garden.\nThe Llama 3 instruction-tuned models are a collection of LLMs optimized for\ndialogue use cases. Llama 3 models outperform many of the available open source\nchat models on common industry benchmarks.\nFor more information, see theLlama\n3model card in\nModel Garden.\nThe Llama 2 LLMs is a collection of pre-trained and fine-tuned generative text\nmodels, ranging in size from 7B to 70B parameters.\nFor more information, see theLlama\n2model card in\nModel Garden.\nMeta's Code Llama models are designed for code synthesis,\nunderstanding, and instruction.\nFor more information, see theCode\nLlamamodel card in\nModel Garden.\nLlama Guard 3 builds on the capabilities of Llama Guard 2, adding\nthree new categories: Defamation, Elections, and Code Interpreter Abuse.\nAdditionally, this model is multilingual and has a prompt format that is\nconsistent with Llama 3 or later instruct models.\nFor more information, see theLlama\nGuardmodel card in\nModel Garden.\nFor more information about Model Garden, seeExplore AI models in Model Garden.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-13 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models",
    "title": "Use Hugging Face ModelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nHugging Faceprovides pre-trained models, fine-tuning scripts, and development APIs that make the process of creating and discovering LLMs easier.Model Gardencan serveText Embeddings,Text To Image,Text Generation, andImage Text To Textmodels in HuggingFace.\nYou can deploy supported Hugging Face models in Vertex AI or\nGoogle Kubernetes Engine (GKE). The deployment option\nyou choose can depend on the model you're using and how much control\nyou want over your workloads.\nVertex AI offers a managed platform for building and scaling\nmachine learning projects without in-house MLOps expertise. You can use\nVertex AI as the downstream application that serves the\nHugging Face models. We recommend using\nVertex AI if you want end-to-end MLOps capabilities, value-added ML\nfeatures, and a serverless experience for streamlined development.\nTo deploy a supported Hugging Face model in Vertex AI, go\nto Model Garden.Go to Model Garden\nTo deploy a supported Hugging Face model in Vertex AI, go\nto Model Garden.\nGo to Model Garden\nGo to theOpen models on Hugging Facesection and clickShow\nmore.\nGo to theOpen models on Hugging Facesection and clickShow\nmore.\nFind and select a model to deploy.\nFind and select a model to deploy.\nOptional: For theDeployment environment, selectVertex AI.\nOptional: For theDeployment environment, selectVertex AI.\nOptional: Specify the deployment details.\nOptional: Specify the deployment details.\nClickDeploy.\nClickDeploy.\nTo get started, see the following examples:\nSome models have detailed model cards and the deployment settings are verified by Google, such asgoogle/gemma-3-27b-it,meta-llama/Llama-4-Scout-17B-16E-Instruct,Qwen/QwQ-32B,BAAI/bge-m3,intfloat/multilingual-e5-large-instruct,black-forest-labs/FLUX.1-dev, andHuggingFaceFW/fineweb-edu-classifier.\nSome models have the deployment settings verified by Google but no detailed model cards, such asNousResearch/Genstruct-7B.\nSome models have deployment settings generated automatically.\nSome models have automatically generated deployment settings that are based on model metadata, such as some latest trending models intext generation,text embeddings,text to image generation, andimage text to text.\nGoogle Kubernetes Engine (GKE) is the Google Cloud solution\nfor managed Kubernetes that provides scalability, security, resilience, and cost\neffectiveness. We recommend this option if you have existing Kubernetes\ninvestments, your organization has in-house MLOps expertise, or if you need\ngranular control over complex AI/ML workloads with unique security, data\npipeline, and resource management requirements.\nTo deploy a supported Hugging Face model in GKE, go\nto Model Garden.Go to Model Garden\nTo deploy a supported Hugging Face model in GKE, go\nto Model Garden.\nGo to Model Garden\nGo to theOpen models on Hugging Facesection and clickShow\nmore.\nGo to theOpen models on Hugging Facesection and clickShow\nmore.\nFind and select a model to deploy.\nFind and select a model to deploy.\nFor theDeployment environment, selectGKE.\nFor theDeployment environment, selectGKE.\nFollow the deployment instructions.\nFollow the deployment instructions.\nTo get started, see the following examples:\nSome models have detailed model cards and verified deployment settings, such asgoogle/gemma-3-27b-it,meta-llama/Llama-4-Scout-17B-16E-Instruct, andQwen/QwQ-32B.\nSome models have verified deployment settings, but no detailed model cards, such asNousResearch/Genstruct-7B.\nWe automatically add the latest, most popular Hugging Face models to Model Garden.\nThis process includes the automatic generation of a deployment configuration for\neach model.\nTo address concerns regarding vulnerabilities and malicious code, we\nuse theHugging Face Malware Scannerto assess the safety\nof files within each Hugging Face model repository on a daily basis. If a\nmodel repository is flagged as containing malware, we immediately remove the\nmodel from the Hugging Face gallery page.\nWhile a model being designated assupported by Vertex AIsignifies that it\nhas undergone testing and is deployable on Vertex AI, we don't guarantee\nthe absence of vulnerabilities or malicious code. We recommend that you conduct\nyour own security verifications before deploying any model in your production\nenvironment.\nThe default deployment configuration that is provided\nwith the one-click deployment option can't satisfy every requirement\ngiven the diverse range of use cases and varying\npriorities with latency, throughput, cost, and accuracy.\nTherefore, you can initially experiment with the one-click\ndeployment to establish a baseline, and then fine-tune the deployment\nconfigurations by using the Colab notebook (vLLM,TGI,TEI,HF pytorch inference)\nor the Python SDK. This iterative approach lets you to tailor the\ndeployment to your precise needs to get the best possible performance for\nyour specific application.\nIf you're looking for a specific model that's not listed in\nModel Garden, the model is not supported by\nVertex AI. The following sections describe the reasoning and what you can\ndo.\nThe following reasons explain why a model might not be in Model Garden:\nIt's not a top trending model: We often prioritize models that are widely popular\nand have strong community interest.\nIt's not yet compatible: The model might not work with a supported\nserving container. For example, thevLLM containerfortext-generationandimage-text-to-textmodels.\nUnsupported pipeline tasks: The model has ataskwhich we don't yet\nfully support at the moment. We support the following tasks:text-generation,text2text-generation,text-to-image,feature-extraction,sentence-similarity,\nandimage-text-to-text.\nYou can still work with models that available in Model Garden:\nDeploy it yourself using the Colab Notebook: We have the following Colab\nNotebooks: (vLLM,TGI,TEI,HF pytorch inference),\nwhich provide the flexibility to deploy models with custom configurations. This\ngives you complete control over the process.\nSubmit a Feature Request: work with your support engineer and submit a\nfeature request through the Model Garden, or refer toVertex Generative AI supportfor additional help.\nKeep an eye on updates: We regularly add new models to Model Garden.\nThe model you're looking for might become available in the future, so check\nback periodically!\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/access-resources",
    "title": "Access and resource management",
    "content": "Home\nDocumentation\nOrganize, analyze, and manage access to your Google Cloud resources and services.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nDefine who can access resources in your organization.\nManage internal enterprise solutions and Google Cloud APIs.\nOptimize your service usage, monitor application and resource health, and identify disruptive events.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/java/docs/spring",
    "title": "Spring Framework supportStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nJava\nDocumentation\nGuides\nThe Spring Cloud GCP project brings thePivotal-developedSpring Framework\nto the Google Cloud APIs. Spring simplifies application development by\nproviding the infrastructure for enterprise applications to accomplish common\ntasks, such as exposing services and interacting with databases and messaging\nsystems.\nTheSpring Cloud GCPpage provides the full list of features and the documentation on how to get\nstarted.\nYou can find the source code and additional resources in theGitHub repository.\nThe repository also containscode samplesto help you develop your application.\nThereference documentationprovides detailed information on how to integrate Google Cloud APIs\nwith your Spring and Spring Boot applications.\nLearn how to get started using Spring on Google Cloud. Thesecodelabsare designed to get\nyou up and running quickly using GCP products and resources.\nSee our latest talks and presentations.\nContribute to the project byfiling an issueor submitting apull request.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-17 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/compute-area",
    "title": "Compute technology",
    "content": "Home\nDocumentation\nRun your workloads on virtual machines with specialized offerings for ML, high-performance computing, and other workloads to match your needs.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate a VM.\nOS images for virtual machines.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/workbench/instances",
    "title": "Vertex AI Workbench instances\n documentation",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI Workbench\nVertex AI Workbench instances are Jupyter notebook-based development environments\n    for the entire data science workflow. Vertex AI Workbench instances are prepackaged withJupyterLaband have a preinstalled suite of deep learning packages, including support for the\n    TensorFlow and PyTorch frameworks.Learn more.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate a Vertex AI Workbench instance\nCreate a Vertex AI Workbench instance\nIntroduction to Vertex AI Workbench instances\nIntroduction to Vertex AI Workbench instances\nQuery data in BigQuery from within JupyterLab\nQuery data in BigQuery from within JupyterLab\nAdd a conda environment\nAdd a conda environment\nManage your conda environment\nManage your conda environment\nChange machine type and configure GPUs of a Vertex AI Workbench instance\nChange machine type and configure GPUs of a Vertex AI Workbench instance\nPricing\nPricing\nRelease notes\nRelease notes\nGet support\nGet support\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings",
    "title": "Embeddings APIs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nEmbeddings are numerical representations of text, images, or videos that capture\nrelationships between inputs. Machine learning models, especially generative\nAI models, are suited for creating embeddings by identifying patterns within\nlarge datasets. Applications can use embeddings to process and produce\nlanguage, recognizing complex meanings and semantic relationships specific to\nyour content. You interact with embeddings every time you complete a\nGoogle Search or see music streaming recommendations.\nEmbeddings work by converting text, image, and video into arrays of floating\npoint numbers, called vectors. These vectors are designed to capture the meaning\nof the text, images, and videos. The length of the embedding array is called the\nvector's dimensionality. For example, one passage of text might be represented\nby a vector containing hundreds of dimensions. Then, by calculating the\nnumerical distance between the vector representations of two pieces of text, an\napplication can determine the similarity between the objects.\nVertex AI supports two types of embeddings models, text and multimodal.\nSome common use cases for text embeddings include:\nSemantic search: Search text ranked by semantic similarity.\nClassification: Return the class of items whose text attributes are\nsimilar to the given text.\nClustering: Cluster items whose text attributes are similar to the given\ntext.\nOutlier Detection: Return items where text attributes are least related\nto the given text.\nConversational interface: Clusters groups of sentences which can lead to similar\nresponses, like in a conversation-level embedding space.\nIf you want to develop a book recommendation chatbot, the first thing to do is\nto use a deep neural network (DNN) to convert each book into an embedding\nvector, where one embedding vector represents one book. You can feed, as input\nto the DNN, just the book title or just the text content. Or you can use both of\nthese together, along with any other metadata describing the book, such as the\ngenre.\nThe embeddings in this example could be comprised of thousands of book titles\nwith summaries and their genre, and it might have representations for books likeWuthering Heightsby Emily Brontë andPersuasionby Jane Austen that are\nsimilar to each other (small distance between numerical representation). Whereas\nthe numerical representation for the bookThe Great Gatsbyby F. Scott\nFitzgerald would be further, as the time period, genre, and summary is less\nsimilar.\nThe inputs are the main influence to the orientation of the embedding space. For\nexample, if we only had book title inputs, then two books with similar titles,\nbut very different summaries, could be close together. However, if we include\nthe title and summary, then these same books are less similar (further away) in\nthe embedding space.\nWorking with generative AI, this book-suggestion chatbot could summarize,\nsuggest, and show you books which you might like (or dislike), based on your\nquery.\nSome common use cases for multimodal embeddings include:\nImage and text use cases:Image classification: Takes an image as input and predicts one or more\nclasses (labels).Image search: Search relevant or similar images.Recommendations: Generate product or ad recommendations based on images.\nImage and text use cases:\nImage classification: Takes an image as input and predicts one or more\nclasses (labels).\nImage search: Search relevant or similar images.\nRecommendations: Generate product or ad recommendations based on images.\nImage, text, and video use cases:Recommendations: Generate product or advertisement recommendations based\non videos (similarity search).Video content searchUsing semantic search: Take a text as an input, and return a set of\nranked frames matching the query.Using similarity search:Take a video as an input, and return a set of videos matching the\nquery.Take an image as an input, and return a set of videos matching the\nquery.Video classification: Takes a video as input and predicts one or more\nclasses.\nImage, text, and video use cases:\nRecommendations: Generate product or advertisement recommendations based\non videos (similarity search).\nVideo content search\nUsing semantic search: Take a text as an input, and return a set of\nranked frames matching the query.\nUsing similarity search:Take a video as an input, and return a set of videos matching the\nquery.Take an image as an input, and return a set of videos matching the\nquery.\nTake a video as an input, and return a set of videos matching the\nquery.\nTake an image as an input, and return a set of videos matching the\nquery.\nVideo classification: Takes a video as input and predicts one or more\nclasses.\nOnline retailers are increasingly leveraging multimodal embeddings to enhance\ncustomer experience. Every time you see personalized product recommendations\nwhile shopping, and get visual results from a text search, you are interacting\nwith an embedding.\nIf you want to create a multimodal embedding for an online retail use case,\nstart by processing each product image to generate a unique image embedding,\nwhich is a mathematical representation of its visual style, color palette, key\ndetails, and more. Simultaneously, convert product descriptions, customer\nreviews, and other relevant textual data into text embeddings that capture their\nsemantic meaning and context. By merging these image and text embeddings into a\nunified search and recommendation engine, the store can offer personalized\nrecommendations of visually similar items based on a customer's browsing history\nand preferences. Additionally, it enables customers to search for products using\nnatural language descriptions, with the engine retrieving and displaying the\nmost visually similar items that match their search query. For example, if a\ncustomer searches \"Black summer dress\", the search engine can display dresses\nwhich are black, and also are in summer dress cuts,  made out of lighter\nmaterial, and might be sleeveless. This powerful combination of visual and\ntextual understanding creates a streamlined shopping experience that enhances\ncustomer engagement, satisfaction, and ultimately can drive sales.\nTo learn more about embeddings, seeMeet AI's multitool: Vector embeddings.\nTo take a foundational ML crash course on embeddings, seeEmbeddings.\nTo learn more about how to store vector embeddings in a database, see theDiscoverpage and theOverview of Vector Search.\nTo learn about responsible AI best practices and Vertex AI's safety\nfilters, seeResponsible AI.\nTo learn how to get embeddings, see the following documents:Get text embeddingsGet multimodal embeddings\nGet text embeddings\nGet multimodal embeddings\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/costs-usage",
    "title": "Costs and usage management",
    "content": "Home\nDocumentation\nManage costs and usage across Google Cloud products and services.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nTrack your Google Cloud costs, analyze your billing data, control and optimize your costs, and take advantage of committed use discounts.\nOptimize your Google Cloud resource usage and increase efficiency.\nMeasure, report, and reduce your Cloud carbon emissions.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nYou can evaluate the performance of foundation models and your tuned generative\nAI models on Vertex AI. The models are evaluated using a set of metrics\nagainst an evaluation dataset that you provide. This page explains how\ncomputation-based model evaluation through the evaluation pipeline service\nworks, how to create and format the evaluation dataset, and how to perform the\nevaluation using the Google Cloud console, Vertex AI API, or the\nVertex AI SDK for Python.\nTo evaluate the performance of a model, you first create an evaluation dataset\nthat contains prompt and ground truth pairs. For each pair, the prompt is the\ninput that you want to evaluate, and the ground truth is the ideal response for\nthat prompt. During evaluation, the prompt in each pair of the evaluation\ndataset is passed to the model to produce an output. The output generated by the\nmodel and the ground truth from the evaluation dataset are used to compute the\nevaluation metrics.\nThe type of metrics used for evaluation depends on the task that you are\nevaluating. The following table shows the supported tasks and the metrics used\nto evaluate each task:\nModel evaluation is supported for the following models:\ntext-bison: Base and tuned versions.\ntext-bison: Base and tuned versions.\nGemini: All tasks except classification.\nGemini: All tasks except classification.\nThe evaluation dataset that's used for model evaluation includes prompt and\nground truth pairs that align with the task that you want to evaluate. Your\ndataset must include a minimum of 1 prompt and ground truth pair and at least\n10 pairs for meaningful metrics. The more examples you\ngive, the more meaningful the results.\nYour evaluation dataset must be inJSON Lines(JSONL)\nformat where each line contains a single prompt and ground truth pair specified\nin theinput_textandoutput_textfields, respectively. Theinput_textfield contains the prompt that you want to evaluate, and theoutput_textfield\ncontains the ideal response for the prompt.\nThe maximum token length forinput_textis 8,192, and the maximum token length\nforoutput_textis 1,024.\nYou can eithercreate a new Cloud Storage bucketor use an existing one to store your dataset file. The bucket must be in the\nsame region as the model.\nAfter your bucket is ready,uploadyour dataset file to the bucket.\nYou can evaluate models by using the REST API or the Google Cloud console.\nTo perform this task, you must grantIdentity and Access Management (IAM)roles to each of the following service accounts:\nVertex AI User\n            (roles/aiplatform.user)\nStorage Object User\n            (roles/storage.objectUser)\nDepending on your input and output data sources, you may also need to grant the Vertex AI Pipelines\n  Service Account additional roles:\nTo create a model evaluation job, send aPOSTrequest by using thepipelineJobsmethod.\nBefore using any of the request data,\n  make the following replacements:\nPROJECT_ID: The Google Cloud project that runs the\n    pipeline components.\nPIPELINEJOB_DISPLAYNAME: A display\n    name for the pipelineJob.\nLOCATION: The region to run the pipeline components.\n    Currently, onlyus-central1is supported.\nDATASET_URI: The Cloud Storage URI of your\n    reference dataset. You can specify one or multiple URIs. This parameter supportswildcards. To learn more about\n    this parameter, seeInputConfig.\nOUTPUT_DIR: The Cloud Storage URI to store\n    evaluation output.\nMODEL_NAME: Specify a publisher model or a tuned\n    model resource as follows:Publisher model:publishers/google/models/MODEL@MODEL_VERSIONExample:publishers/google/models/text-bison@002Tuned model:projects/PROJECT_NUMBER/locations/LOCATION/models/ENDPOINT_IDExample:projects/123456789012/locations/us-central1/models/1234567890123456789The evaluation job doesn't impact any existing deployments of the model or their resources.\nPublisher model:publishers/google/models/MODEL@MODEL_VERSIONExample:publishers/google/models/text-bison@002\nExample:publishers/google/models/text-bison@002\nTuned model:projects/PROJECT_NUMBER/locations/LOCATION/models/ENDPOINT_IDExample:projects/123456789012/locations/us-central1/models/1234567890123456789\nExample:projects/123456789012/locations/us-central1/models/1234567890123456789\nThe evaluation job doesn't impact any existing deployments of the model or their resources.\nEVALUATION_TASK: The task that you want to\n    evaluate the model on. The evaluation job computes a set of metrics relevant to that specific\n    task. Acceptable values include the following:summarizationquestion-answeringtext-generationclassification\nsummarization\nquestion-answering\ntext-generation\nclassification\nINSTANCES_FORMAT: The format of your dataset.\n    Currently, onlyjsonlis supported. To learn more about this parameter, seeInputConfig.\nPREDICTIONS_FORMAT: The format of the\n    evaluation output. Currently, onlyjsonlis supported. To learn more about this\n    parameter, seeInputConfig.\nMACHINE_TYPE: (Optional) The machine type for\n    running the evaluation job. The default value ise2-highmem-16. For a list of\n    supported machine types, seeMachine types.\nSERVICE_ACCOUNT: (Optional) The service\n    account to use for running the evaluation job. To learn how to create a custom service account,\n    seeConfigure a service account with granular permissions.\n    If unspecified, theVertex AI Custom Code Service Agentis used.\nNETWORK: (Optional) The fully qualified name of the\n    Compute Engine network to peer the evaluatiuon job to. The format of the network name isprojects/PROJECT_NUMBER/global/networks/NETWORK_NAME. If you\n    specify this field, you need to have aVPC Network Peering for\n    Vertex AI. If left unspecified, the evaluation job is not peered with any network.\nKEY_NAME: (Optional) The name of the customer-managed\n    encryption key (CMEK). If configured, resources created by the evaluation job is encrypted using\n    the provided encryption key. The format of the key name isprojects/PROJECT_ID/locations/REGION/keyRings/KEY_RING/cryptoKeys/KEY.\n    The key needs to be in the same region as the evaluation job.\nHTTP method and URL:\nRequest JSON body:\nTo send your request, choose one of these options:\nSave the request body in a file namedrequest.json,\n      and execute the following command:\nSave the request body in a file namedrequest.json,\n      and execute the following command:\nYou should receive a JSON response similar to the following. Note thatpipelineSpechas been truncated to save space.\nTo learn how to install or update the Vertex AI SDK for Python, seeInstall the Vertex AI SDK for Python.\n      \n        For more information, see theVertex AI SDK for Python API reference documentation.\nTo create a model evaluation job by using the Google Cloud console, perform\nthe following steps:\nIn the Google Cloud console, go to theVertex AI Model Registrypage.Go to\n    Vertex AI Model RegistryClick the name of the model that you want to evaluate.In theEvaluatetab, clickCreate evaluationand configure as\n    follows:Objective: Select the task that you want to evaluate.Target column or field: (Classification only) Enter the target\n        column for prediction. Example:ground_truth.Source path: Enter or select the URI of your evaluation dataset.Output format: Enter the format of the evaluation output.\n        Currently, onlyjsonlis supported.Cloud Storage path: Enter or select the URI to store evaluation\n        output.Class names: (Classification only) Enter the list of possible\n        class names.Number of compute nodes: Enter the number of compute nodes to run\n        the evaluation job.Machine type: Select a machine type to use for running the\n        evaluation job.ClickStart evaluation\nGo to\n    Vertex AI Model Registry\nClick the name of the model that you want to evaluate.\nIn theEvaluatetab, clickCreate evaluationand configure as\n    follows:\nObjective: Select the task that you want to evaluate.\nTarget column or field: (Classification only) Enter the target\n        column for prediction. Example:ground_truth.\nSource path: Enter or select the URI of your evaluation dataset.\nOutput format: Enter the format of the evaluation output.\n        Currently, onlyjsonlis supported.\nCloud Storage path: Enter or select the URI to store evaluation\n        output.\nClass names: (Classification only) Enter the list of possible\n        class names.\nNumber of compute nodes: Enter the number of compute nodes to run\n        the evaluation job.\nMachine type: Select a machine type to use for running the\n        evaluation job.\nClickStart evaluation\nYou can find the evaluation results in the Cloud Storage output directory\nthat you specified when creating the evaluation job. The file is namedevaluation_metrics.json.\nFor tuned models, you can also view evaluation results in the Google Cloud console:\nIn the Vertex AI section of the Google Cloud console, go to\ntheVertex AI Model Registrypage.Go to\n   Vertex AI Model Registry\nIn the Vertex AI section of the Google Cloud console, go to\ntheVertex AI Model Registrypage.\nGo to\n   Vertex AI Model Registry\nClick the name of the model to view its evaluation metrics.\nClick the name of the model to view its evaluation metrics.\nIn theEvaluatetab, click the name of the evaluation run that you want\nto view.\nIn theEvaluatetab, click the name of the evaluation run that you want\nto view.\nLearn aboutgenerative AI evaluation.\nLearn about online evaluation withGen AI Evaluation Service.\nLearn how totune a foundation model.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-15 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions",
    "title": "Model versions and lifecycleStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nStable model: A publicly released version of the model that is available and\nsupported for production use starting on the release date. A stable model\nversion is typically released with aretirement date, which indicates the last\nday that the model is available. After this date, the model is no longer\naccessible or supported by Google.\nLatest stable model: The latest version within the model family\nrecommended for new and active projects and should be the target for\nmigrations from earlier versions. SeeLatest stable models.\nLegacy stable model: A model version that's been superseded by the Latest\nStable Model. Although legacy stable models are still supported, you should\nstrongly consider migrating to the latest model to receive the latest features\nand improvements. Access to legacy stable models might be restricted for new\nprojects. SeeLegacy stable models.\nRetired model: The model version is past its retirement date and has been\npermanently deactivated. Retired models are no longer accessible or supported by\nGoogle. API requests referencing a retired model ID typically returns a 404\nerror. SeeRetired models.\nRecommended upgrade: The latest stable model that we recommend switching to.\nLatest stable models tend to offer better performance and more capabilities as\ncompared to legacy stable models. See the recommended upgrades in theLegacy stable modelsandRetired modelssections.\nThe following table lists the latest stable models:\nThe following table lists legacy stable models:\n*: Restricted for new projects.\nTo learn how to migrate to a latest stable model, seeMigrate your application to Gemini 2 with the Vertex AI Gemini API.\nThis guide gives you a set of migration steps that aims to minimize some\npotential risks involved in model migration and helps you use new models in an\noptimal way.\nHowever, if you don't have time to follow the guide and just need to quickly\nresolve the errors caused by models reaching their retirement dates, do the following:\nUpdate your application to point to the recommended upgrades.\nTest all mission critical features to make sure everything works as expected.\nDeploy the updates like you normally would.\nThe auto-updated alias of a Gemini model always points to the latest\nstable model. When a new latest stable model is available, the auto-updated\nalias automatically points to the new version.\nThe following table shows the auto-updated aliases for Gemini models\nand the latest stable models that they point to.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "content": "Home\nVirtual Private Cloud\nDocumentation\nGuides\nThis document provides an overview of Private Service Connect.\nPrivate Service Connect is a capability of Google Cloud networking that\nallowsconsumersto accessmanaged servicesprivately from\ninside their VPC network. Similarly, it allows managed serviceproducersto host these services in their own separate VPC\nnetworks and offer a private connection to their consumers. For example, when\nyou use Private Service Connect to access Cloud SQL, you are\nthe service consumer, and Google is the service producer.\nWith Private Service Connect, consumers can use their own internal IP\naddresses to access services without leaving their VPC networks.\nTraffic remains entirely within Google Cloud. Private Service Connect\nprovides service-oriented access between consumers and producers with granular\ncontrol over how services are accessed.\nPrivate Service Connect supports access to the following types of\nmanaged services:\nPublished VPC-hosted services, which include the following:Google published\nservices,\nsuch as Apigee or the GKE control planeThird-party published\nservicesprovided by Private Service Connect partnersIntra-organizationpublished services, where the\nconsumer and producer might be two different VPC networks\nwithin the same company\nGoogle published\nservices,\nsuch as Apigee or the GKE control plane\nThird-party published\nservicesprovided by Private Service Connect partners\nIntra-organizationpublished services, where the\nconsumer and producer might be two different VPC networks\nwithin the same company\nGoogle\nAPIs,\nsuch as Cloud Storage or BigQuery\nFigure 1.Private Service Connect lets you send\n    traffic to endpoints and backends that forward the traffic to managed\n    services, including Google APIs and published services. Private Service Connect interfaces let managed services\n    initiate connections to consumer VPC networks.\nPrivate Service Connect provides private connectivity that has\nthe following characteristics:\nService-oriented design:Producer services are published through load\nbalancers that expose a single IP address to the consumer\nVPC network. Consumer traffic that accesses producer services\nis unidirectional and can only access the service IP address, rather than\nhaving access to an entire peered VPC network.\nExplicit authorization:Private Service Connect provides\nan authorization model that gives consumers and producers granular control,\nensuring that only the intended service endpoints and no other resources can\nconnect to a service.\nNo shared dependencies:Traffic between consumer and producers uses NAT\nso that no IP address coordination or other shared resource dependencies\nexist between the consumer and producer VPC networks. This\nindependence simplifies deployment and lets you more easily scale managed\nservices.\nLine-rate performance:Private Service Connect traffic\ngoes directly from consumer clients to producer backends without\nintermediate hops or proxies. NAT is performed directly on the physical host\nmachines that host the consumer and producer VMs, which reduces latency and\nincreases bandwidth capacity. The bandwidth capacity of\nPrivate Service Connect is limited only by the bandwidth\ncapacity of the client and server machines that are directly communicating.\nPrivate Service Connect is available in different types that\nprovide different capabilities and modes of communication.\nService producerspublish their applications to consumers by creating\nPrivate Service Connect services.Service consumersaccess\nthose Private Service Connect services directly through one of\nthese Private Service Connect types:\nPrivate Service Connect endpoints: Endpoints\nare deployed by using forwarding rules that provide the consumer an IP\naddress that is mapped to the Private Service Connect\nservice.\nPrivate Service Connect backends: Backends are\ndeployed by using network endpoint groups (NEGs) that let consumers direct\ntraffic to their load balancer before reaching a\nPrivate Service Connect service.\nService producers can initiate connections to service consumers by usingPrivate Service Connect interfaces.\nPrivate Service Connect interfaces provide bidirectional\ncommunication and can be used in the same VPC network as\nendpoints and backends.\nPrivate Service Connect endpoints are internal IP addresses in a\nconsumer VPC network that can be directly accessed by clients in\nthat network. Endpoints are created by deploying aforwarding\nrulethat references aservice attachmentor abundle of\nGoogle APIs.\nThe following diagram shows a Private Service Connect endpoint\nthat targets a published service that is running in a separate\nVPC network and organization.\nPrivate Service Connect endpoints and published services let two\nindependent companies communicate with each other by using internal IP addresses.\nFor more information, seeAbout accessing published services through\nendpoints.\nFigure 2.Private Service Connect lets you send\n    traffic to endpoints that forward the traffic to published services in\n    another VPC network.\nSimilarly, a Private Service Connect endpoint can be used to\naccess Google APIs such as Cloud Storage or BigQuery.\nThis functionality is similar to Private Google Access, except that you can\nuse your own internal IP addresses for endpoints.\nPrivate Service Connect lets you more directly control routing\nand create as many endpoints as necessary for your network. For more\ninformation, seeAbout accessing Google APIs through\nendpoints.\nFigure 3.Private Service Connect lets you send\n    traffic to endpoints that forward the traffic to Google APIs.\nPrivate Service Connect backends let Google Cloud load\nbalancers send traffic through Private Service Connect to reach\npublished services or Google APIs. The backends are deployed through\nPrivate Service Connectnetwork endpoint groups\n(NEGs)that reference a producer service attachment or a supported Google API. Placing a\nload balancer in front of a managed service provides the consumer with more\nvisibility and control than is possible through a\nPrivate Service Connect endpoint. Backends let you create\nconfigurations such as the following:\nCustomer-owned domains and certificates in front of managed services\nConsumer-controlled failover between managed services in different\nregions\nCentralized security configuration and access control for managed services\nThe following diagram shows an internal Application Load Balancer deployed with\nPrivate Service Connect backends that reference a published\nservice. There are two load balancers in the configuration:\nThe consumer load balancer that provides control, visibility, and security\nof traffic to the service.\nThe producer load balancer that load balances traffic across the service\nbackends.\nFigure 4.Private Service Connect lets you send\n    traffic to backends that forward the traffic to published services.\nSimilarly to Private Service Connect endpoints, backends also\nsupport targeting Google APIs. The following diagram shows an internal Application Load Balancer\nthat targets a Cloud Storage bucket and terminates traffic by using a\ncustomer-owned domain.\nFigure 5.Private Service Connect lets you send\n    traffic to backends that forward the traffic to a regional Google API.\nAPrivate Service Connect interfaceis a special type ofnetwork interfacethat refers to anetwork attachment.\nA service producer can create a Private Service Connect interface\nand request a connection to a network attachment. If the service consumer\naccepts the connection, Google Cloud allocates the interface an IP address\nfrom a subnet in the consumer VPC network that's specified by the\nnetwork attachment. The VM of the Private Service Connect\ninterface has a second standard network interface that connects to the\nproducer's VPC network.\nA connection between a Private Service Connect interface and a\nnetwork attachment is similar to the connection between a\nPrivate Service Connectendpointand aservice attachment, but\nit has two key differences:\nA Private Service Connect interface lets a producer\nVPC network initiate connections to a consumer VPC\nnetwork (managed service egress). An endpoint works in the reverse direction,\nletting a consumer VPC network initiate connections to a producer\nVPC network (managed service ingress).\nA Private Service Connect interface connection is transitive.\nThis means that workloads in a producer network can initiate connections to\nother workloads that areconnected to the consumer VPC network.\nPrivate Service Connect endpoints can only initiate connections\nto the producer VPC network.\nFigure 6.Private Service Connect interfaces\n  let service producers initiate connections to service consumers.\nManaged services are services that are owned and managed by someone other than\nthe service consumer. Private Service Connect can be used to\naccess managed services that are owned by Google, third-party software as a\nservice (SaaS) companies, or other teams within the consumer's own company. Both\npublished services and Google APIs can be targets of\nPrivate Service Connect.\nPublished servicesare VPC-hosted\nservices that are deployed in the producer's VPC network and are\naccessed from the consumer's VPC network. Publishing a service\nlets the service producer own and control the deployment of the service in their\nown VPC network. Published services can include the following:\nGoogle\nservices,\nsuch as GKE, Apigee, or Cloud Composer.\nThese services run in tenant projects and VPC networks that\nare managed by Google.\nThird-party\nservices,\nwhere third parties offer private access to a published service in\nGoogle Cloud.\nIntra-organization services, where a single company has clients accessing\ninternal applications across different VPC networks. Some\norganizations use separate VPC networks for internal\nsegmentation. Given that configuration, one team can offer a managed service\nto a different team that operates in a separate VPC network.\nService\nattachmentsare resources that are used to create Private Service Connect\npublished services.\nService attachments can be accessed by usingendpointsorbackends. Multiple backends or\nendpoints can connect to the same service attachment, which lets multiple\nVPC networks or multiple consumers access the same service\ninstance.\nA service attachment targets a producer load balancer and lets clients in a\nconsumer VPC network access the load balancer. The service\nattachment configuration defines the following:\nA consumer accept list that defines which consumers are allowed to connect\nto the service.\nTheNAT subnetwhere\ntranslated traffic is sourced from in the producer VPC\nnetwork.\nAn optionalDNS\ndomain, if\nprovided, that is used in theDNS entries for\nendpointsthat are\nautomatically created in the consumer's Cloud DNS zone.\nUsing Private Service Connect to access Google APIs is an\nalternative to using Private Google Access or the public domain names\nfor Google APIs. In this case, the producer is Google.\nGoogle APIs can be accessed by using endpoints or backends.\nEndpoints let you target abundle of global Google\nAPIs, or asingle regional Google API.\nBackends let you target asingle global Google\nAPIorsingle regional Google API.\nUsing Private Service Connect lets you do the following:\nCreate one or more internal IP addresses to access Google APIs for different\nuse cases.\nDirect on-premises traffic to specific IP addresses and regions when\naccessing Google APIs.\nCentralize Google API traffic through asupported load balancerto apply your own certificates, security policies, or observability.\nLearn aboutaccessing published services through endpoints.\nLearn aboutaccessing Google APIs through endpoints.\nLearn aboutbackends.\nLearn aboutpublishing services.\nComplete a codelab touse Private Service Connect to publish and consume services with GKE.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design",
    "title": "Introduction to promptingStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nTo see an example of prompt design,\n      run the \"Intro to prompt design\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nThis page introduces some basic concepts to get you started in designing prompts. A prompt is a\nnatural language request submitted to a language model to receive a response back. Prompts can\ncontain questions, instructions, contextual information, few-shot examples, and partial input for\nthe model to complete or continue. After the model receives a prompt, depending on the type of model\nbeing used, it can generate text, embeddings, code, images, videos, music, and more.\nPrompt designis the process of creating prompts that elicit the desired response from\nlanguage models. Writing well structured prompts can be an essential part of ensuring accurate, high\nquality responses from a language model. The iterative process of repeatedly updating prompts and\nassessing the model's responses is sometimes calledprompt engineering.\nGemini models often perform well without the need for prompt engineering, especially for\nstraightforward tasks. However, for complex tasks, effective prompt engineering still plays an\nimportant role.\nYou can include whatever information you want in a prompt that you think is important for the\ntask at hand. Generally, prompt content fall within one of the following components:\nTask (required)\nSystem instructions (optional)\nFew-shot examples (optional)\nContextual information (optional)\nA task is the text in the prompt that you want the model to provide a response for. Tasks are\ngenerally provided by a user and can be a question or some instructions on what to do.\nExample question task:\nExample instruction task:\nSystem instructions are instructions that get passed to the model before any user input in the\nprompt. You can add system instructions in the dedicatedsystemInstructionparameter.\nIn the following example, system instructions are used to dictate the style and tone of the\nmodel, while adding constraints to what it can and can't talk about:\nFew-shot examples are examples that you include in a prompt to show the model what getting it\nright looks like. Few-shot examples are especially effective at dictating the style and tone of the\nresponse and for customizing the model's behavior.\nContextual information, or context, is information that you include in the prompt that the model\nuses or references when generating a response. You can include contextual information in different\nformats, like tables or text.\nThere are a few use cases where the model is not expected to fulfill the user's requests.\nParticularly, when the prompt is encouraging a response that is not aligned with Google's values or\npolicies, the model might refuse to respond and provide a fallback response.\nHere are a few cases where the model is likely to refuse to respond:\nHate Speech:Prompts with negative or harmful content targeting identity and/or protected attributes.\nHarassment:Malicious, intimidating, bullying, or abusive prompts targeting another individual.\nSexually Explicit:Prompts that contains references to sexual acts or other lewd content.\nDangerous Content:Prompts that promote or enable access to harmful goods, services, and activities.\nTo learn about task-specific guidance for common use cases check out the\nfollowing pages:\nMultimodal prompts\nOverview of prompting strategies\nChat prompts\nImage generation and editing prompts\nLearn aboutprompting strategies.\nExplore more examples of prompts in thePrompt gallery.\nLearn how to optimize prompts for use withGoogle modelsby using theVertex AI prompt optimizer (Preview).\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart",
    "title": "Quickstart: Send text prompts to Gemini using Vertex AI StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nYou can use Vertex AI Studio to design, test, and manage prompts for\nGoogle'sGeminilarge language models\n(LLMs) and third-party models. Vertex AI Studio supports certain\nthird-party models that are offered on Vertex AI asmodels as a\nservice (MaaS), such as\nAnthropic's Claude models and Meta's Llama models.\nIn this quickstart, you:\nSend these prompts to the Gemini API using samples from the\ngenerative AI prompt gallery, including the following:A summarization text promptA code generation prompt\nA summarization text prompt\nA code generation prompt\nView the code used to generate the responses\nThis quickstart requires you to complete the following steps to set up a\nGoogle Cloud project and enable the Vertex AI API.\nSign in to your Google Cloud account. If you're new to\n        Google Cloud,create an accountto evaluate how our products perform in\n        real-world scenarios. New customers also get $300 in free credits to\n        run, test, and deploy workloads.\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.Note: If you don't plan to keep the\n    resources that you create in this procedure, create a project instead of\n    selecting an existing project. After you finish these steps, you can\n    delete the project, removing all resources associated with the project.Go to project selector\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.\nGo to project selector\nMake sure that billing is enabled for your Google Cloud project.\nMake sure that billing is enabled for your Google Cloud project.\nEnable the Vertex AI API.Enable the API\nEnable the Vertex AI API.\nEnable the API\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.Note: If you don't plan to keep the\n    resources that you create in this procedure, create a project instead of\n    selecting an existing project. After you finish these steps, you can\n    delete the project, removing all resources associated with the project.Go to project selector\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.\nGo to project selector\nMake sure that billing is enabled for your Google Cloud project.\nMake sure that billing is enabled for your Google Cloud project.\nEnable the Vertex AI API.Enable the API\nEnable the Vertex AI API.\nEnable the API\nA prompt is a natural language request submitted to a language model that\ngenerates a response. Prompts can contain questions, instructions, contextual\ninformation,few-shot examples,\nand partial input for the model to complete. After the model receives a prompt,\ndepending on the type of model used, it can generate text, embeddings, code,\nimages, videos, music, and more.\nThe sample prompts in Vertex AI Studioprompt galleryare predesigned to help demonstrate model capabilities. Each prompt is\npreconfigured with specified model and parameter values so you can open the\nsample prompt and clickSubmitto generate a response.\nSend a summarization text prompt to the Vertex AI Gemini API. A summarization\ntask extracts the most important information from text. You can provide\ninformation in the prompt to help the model create a summary, or ask the model\nto create a summary on its own.\nGo to thePrompt gallerypage from the Vertex AI\nsection in the Google Cloud console.Go to prompt gallery\nGo to thePrompt gallerypage from the Vertex AI\nsection in the Google Cloud console.Go to prompt gallery\nIn theTasksdrop-down menu, selectSummarize.\nIn theTasksdrop-down menu, selectSummarize.\nOpen theAudio summarizationcard.This sample prompt includes an audio file and requests a summary of the file\ncontents in a bulleted list.\nOpen theAudio summarizationcard.\nThis sample prompt includes an audio file and requests a summary of the file\ncontents in a bulleted list.\nNotice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model\nby clickingSwitch model.\nNotice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model\nby clickingSwitch model.\nClickSubmitto generate the summary.The output is displayed in the response.\nClickSubmitto generate the summary.\nThe output is displayed in the response.\nTo view the Vertex AI API code used to generate the transcript\nsummary, clickBuild with code>Get code.In theGet codepanel, you can choose your preferred language to get the\nsample code for the prompt, or you can open the Python code in a\nColab Enterprise notebook.\nTo view the Vertex AI API code used to generate the transcript\nsummary, clickBuild with code>Get code.\nIn theGet codepanel, you can choose your preferred language to get the\nsample code for the prompt, or you can open the Python code in a\nColab Enterprise notebook.\nSend a code generation prompt to the Vertex AI Gemini API. A code generation task generates code\nusing a natural language description.\nGo to thePrompt gallerypage from the Vertex AI\nsection in the Google Cloud console.Go to prompt gallery\nGo to thePrompt gallerypage from the Vertex AI\nsection in the Google Cloud console.Go to prompt gallery\nIn theTasksdrop-down menu, selectCode.\nIn theTasksdrop-down menu, selectCode.\nOpen theGenerate code from commentscard.This sample prompt includes asystem instructionthat tells the model how to respond and some incomplete Java methods.\nOpen theGenerate code from commentscard.\nThis sample prompt includes asystem instructionthat tells the model how to respond and some incomplete Java methods.\nNotice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model\nby clickingSwitch model.\nNotice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model\nby clickingSwitch model.\nTo complete each method by generating code in the areas marked<WRITE CODE HERE>, clickSubmit.The output is displayed in the response.\nTo complete each method by generating code in the areas marked<WRITE CODE HERE>, clickSubmit.\nThe output is displayed in the response.\nTo view the Vertex AI API code used to generate the transcript\nsummary, clickBuild with code>Get code.In theGet codepanel, you can choose your preferred language to get the\nsample code for the prompt, or you can open the Python code in a\nColab Enterprise notebook.\nTo view the Vertex AI API code used to generate the transcript\nsummary, clickBuild with code>Get code.\nIn theGet codepanel, you can choose your preferred language to get the\nsample code for the prompt, or you can open the Python code in a\nColab Enterprise notebook.\nSee anintroduction to prompt design.\nLearn aboutdesigning multimodal promptsandchat prompts.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-15 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/",
    "title": "No title",
    "content": "Home\nDocumentation\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/azure",
    "title": "GKE on Azure documentation",
    "content": "Home\nGoogle Kubernetes Engine (GKE)\nGKE Enterprise\nClusters\nDocumentation\nGKE on Azure\nGKE on Azure  lets you manage\n    GKE clusters running on Azure infrastructure through theGKE Multi-Cloud API.\n    Combined withConnect,\n   GKE on Azure lets you manage\n    GKE clusters on both Google Cloud and Azure from the\n    Google Cloud console.\nWhen you create a cluster with GKE on Azure, Google creates the Azure\n    resources you need and brings up a cluster on your behalf. You can then deploy your\n   workloads with thegcloudandkubectlcommand-line tools.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nQuickstart: Deploy an application on GKE on Azure\nQuickstart: Deploy an application on GKE on Azure\nPrerequisites\nPrerequisites\nCreate a cluster\nCreate a cluster\nArchitecture\nArchitecture\nSupported VM sizes\nSupported VM sizes\nSupported regions\nSupported regions\ngcloud container azure commands\ngcloud container azure commands\nGet support\nGet support\nRelease notes\nRelease notes\nQuotas and limits\nQuotas and limits\nPricing\nPricing\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nThis page provides an overview of Vertex AI Model Monitoring.\nVertex AI Model Monitoring lets you run monitoring jobs as needed or on a\nregular schedule to track the quality of your tabular models. If you've set\nalerts, Vertex AI Model Monitoring informs you when metrics surpass a\nspecified threshold.\nFor example, assume that you have a model that predicts customer lifetime value.\nAs customer habits change, the factors that predict customer spending also\nchange. Consequently, the features and feature values that you used to train\nyour model before might not be relevant for making predictions today. This\ndeviation in the data is known as drift.\nVertex AI Model Monitoring can track and alert you when deviations exceed\na specified threshold. You can then re-evaluate or retrain your model to ensure\nthe model is behaving as intended.\nFor example, Vertex AI Model Monitoring can provide visualizations like in\nthe following figure, which overlays two graphs from two datasets. This\nvisualization lets you quickly compare and see deviations between the two sets\nof data.\nVertex AI Model Monitoring provides two offerings: v2 and v1.\nModel Monitoring v2 is inPreviewand is the latest offering that associates all\nmonitoring tasks with a model version. In contrast, Model Monitoring v1 is Generally\nAvailable and is configured on Vertex AI endpoints.\nIf you need production-level support and want to monitor a model that's deployed\non a Vertex AI endpoint, use Model Monitoring v1. For all other use cases, use\nModel Monitoring v2, which provides all the capabilities of Model Monitoring v1 and more. For more\ninformation, see the overview for each version:\nModel Monitoring v2 overview\nModel Monitoring v1 overview\nFor existing Model Monitoring v1 users, Model Monitoring v1 is maintained as is. You aren't required to\nmigrate to Model Monitoring v2. If you want to migrate, you can use both versions\nconcurrently until you have fully migrated to Model Monitoring v2 to help you avoid\nmonitoring gaps during your transition.\nPreview\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA products and features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nModel Monitoring v2 lets you track metrics over time after you configure a model monitor and\nrun monitoring jobs. You can run on-demand monitoring jobs or set up scheduled\nruns. By using scheduled runs, Model Monitoring\nautomatically runs monitoring jobs based on a schedule that you define.\nThe metrics and thresholds you monitor are mapped tomonitoring objectives.\nFor each model version, you can specify one or more monitoring objectives. The\nfollowing table details each objective:\nMeasures the distribution of input feature values compared to a baseline\n    data distribution.\nL-Infinity\nJensen Shannon Divergence\nMeasures the model's predictions data distribution compared to a baseline\n    data distribution.\nL-Infinity\nJensen Shannon Divergence\nMeasures the change in contribution of features to a model's prediction\n    compared to a baseline. For example, you can track if a highly important\n    feature suddenly drops in importance.\nAfter a model is deployed in production, the input data can deviate from the\ndata that was used to train the model or the distribution of feature data in\nproduction could shift significantly over time. Model Monitoring v2 can monitor changes in\nthe distribution of production data compared to the training data or to track\nthe evolution of production data distribution over time.\nSimilarly, for prediction data, Model Monitoring v2 can monitor changes in the distribution\nof predicted outcomes compared to the training data or production data\ndistribution over time.\nFeature attributions indicate how much each feature in your model contributed to\nthe predictions for each given instance. Attribution scores are proportional to\nthe contribution of the feature to a model's prediction. They are typically\nsigned, indicating whether a feature helps push the prediction up or down.\nAttributions across all features must add up to the model's prediction score.\nBy monitoring feature attributions, Model Monitoring v2 tracks changes in a feature's\ncontributions to a model's predictions over time. A change in a key feature's\nattribution score often signals that the feature has changed in a way that can\nimpact the accuracy of the model's predictions.\nFor more information about feature attributions and metrics, seeFeature-based\nexplanationsandSampled Shapley method.\nYou must first register your models in Vertex AI Model Registry. If you\nare serving models outside of Vertex AI, you don't need to\nupload the model artifact. You then create a model monitor, which you associate\nwith a model version, and define your model schema. For some models, such as\nAutoML models, the schema is provided for you.\nIn the model monitor, you can optionally specify default configurations such as\nmonitoring objectives, a training dataset, monitoring output location, and\nnotification settings. For more information, seeSet up model\nmonitoring.\nAfter you create a model monitor, you can run a monitoring job on demand or\nschedule regular jobs for continuous monitoring. When you run a job,\nModel Monitoring uses the default configuration set in the\nmodel monitor unless you provide a different monitoring configuration. For\nexample, if you provide different monitoring objectives or a different\ncomparison dataset, Model Monitoring uses the job's\nconfigurations instead of the default configuration from the model monitor. For\nmore information, seeRun a monitoring job.\nYou are not charged for Model Monitoring v2 during thePreview. You are still\ncharged for the usage of other services, such as Cloud Storage,\nBigQuery, Vertex AI batch predictions,\nVertex Explainable AI, and Cloud Logging.\nThe following tutorials demonstrate how to use the Vertex AI SDK for\nPython to set up Model Monitoring v2 for your model.\nTo learn more,\n      run the \"Model Monitoring for Vertex AI Custom Model Batch Prediction Job\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo learn more,\n      run the \"Model Monitoring for Vertex AI custom model online prediction\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo learn more,\n      run the \"Model Monitoring for Models Outside Vertex AI\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nTo help you maintain a model's performance, Model Monitoring v1 monitors the model's\nprediction input data for featureskewanddrift:\nTraining-serving skewoccurs when the feature data distribution in\nproduction deviates from the feature data distribution used to train the model.\nIf the original training data is available, you can enable skew detection to\nmonitor your models for training-serving skew.\nTraining-serving skewoccurs when the feature data distribution in\nproduction deviates from the feature data distribution used to train the model.\nIf the original training data is available, you can enable skew detection to\nmonitor your models for training-serving skew.\nPrediction driftoccurs when feature data distribution in production changes\nsignificantly over time. If the original training data isn't available, you can\nenable drift detection to monitor the input data for changes over time.\nPrediction driftoccurs when feature data distribution in production changes\nsignificantly over time. If the original training data isn't available, you can\nenable drift detection to monitor the input data for changes over time.\nYou can enable both skew and drift detection.\nModel Monitoring v1 supports feature skew and drift\ndetection forcategoricalandnumericalfeatures:\nCategoricalfeatures are data limited by number of possible values,\ntypically grouped by qualitative properties. For example, categories such as\nproduct type, country, or customer type.\nCategoricalfeatures are data limited by number of possible values,\ntypically grouped by qualitative properties. For example, categories such as\nproduct type, country, or customer type.\nNumericalfeatures are data that can be any numeric value. For example,\nweight and height.\nNumericalfeatures are data that can be any numeric value. For example,\nweight and height.\nOnce the skew or drift for a model's feature exceeds an alerting threshold that\nyou set, Model Monitoring v1 sends you an email alert. You\ncan also view the distributions for each feature over time to evaluate whether\nyou need to retrain your model.\nTo detect drift for v1, Vertex AI Model Monitoring usesTensorFlow\nData Validation (TFDV)to calculate the distributions\nanddistance scores.\nCalculate thebaselinestatistical distribution:For skew detection, the baseline is the statistical\ndistribution of the feature's values in the training data.For drift detection, the baseline is the statistical distribution\nof the feature's values seen in production in the past.The distributions for categorical and numerical features are calculated as\nfollows:For categorical features, the computed distribution is the number or\npercentage of instances of each possible value of the feature.For numerical features, Vertex AI Model Monitoring divides the range\nof possible feature values into equal intervals and computes the number or\npercentage of feature values that falls in each interval.The baseline is calculated when youcreate a Vertex AI Model Monitoring\njob, and is only recalculated if you update the training dataset\nfor the job.\nCalculate thebaselinestatistical distribution:\nFor skew detection, the baseline is the statistical\ndistribution of the feature's values in the training data.\nFor skew detection, the baseline is the statistical\ndistribution of the feature's values in the training data.\nFor drift detection, the baseline is the statistical distribution\nof the feature's values seen in production in the past.\nFor drift detection, the baseline is the statistical distribution\nof the feature's values seen in production in the past.\nThe distributions for categorical and numerical features are calculated as\nfollows:\nFor categorical features, the computed distribution is the number or\npercentage of instances of each possible value of the feature.\nFor categorical features, the computed distribution is the number or\npercentage of instances of each possible value of the feature.\nFor numerical features, Vertex AI Model Monitoring divides the range\nof possible feature values into equal intervals and computes the number or\npercentage of feature values that falls in each interval.\nFor numerical features, Vertex AI Model Monitoring divides the range\nof possible feature values into equal intervals and computes the number or\npercentage of feature values that falls in each interval.\nThe baseline is calculated when youcreate a Vertex AI Model Monitoring\njob, and is only recalculated if you update the training dataset\nfor the job.\nCalculate the statistical distribution of the latest feature values seen in\nproduction.\nCalculate the statistical distribution of the latest feature values seen in\nproduction.\nCompare the distribution of the latest feature values in production against\nthe baseline distribution by calculating adistance score:For categorical features, the distance score is calculated using theL-infinity distance.For numerical features, the distance score is calculated using theJensen-Shannon divergence.\nCompare the distribution of the latest feature values in production against\nthe baseline distribution by calculating adistance score:\nFor categorical features, the distance score is calculated using theL-infinity distance.\nFor categorical features, the distance score is calculated using theL-infinity distance.\nFor numerical features, the distance score is calculated using theJensen-Shannon divergence.\nFor numerical features, the distance score is calculated using theJensen-Shannon divergence.\nWhen the distance score between two statistical distributions exceeds the\nthreshold you specify, Vertex AI Model Monitoringidentifies the anomaly\nas skew or drift.\nWhen the distance score between two statistical distributions exceeds the\nthreshold you specify, Vertex AI Model Monitoringidentifies the anomaly\nas skew or drift.\nThe following example shows skew or drift between the baseline\nand latest distributions of a categorical feature:\nThe following example shows skew or drift between the baseline\nand latest distributions of a numerical feature:\nFor cost efficiency, you can set aprediction request sampling rateto\nmonitor a subset of the production inputs to a model.\nFor cost efficiency, you can set aprediction request sampling rateto\nmonitor a subset of the production inputs to a model.\nYou can set a frequency at which a deployed model's recently logged inputs are\nmonitored for skew or drift. Monitoring frequency determines the timespan, or\nmonitoring window size, of logged data that is analyzed in each monitoring run.\nYou can set a frequency at which a deployed model's recently logged inputs are\nmonitored for skew or drift. Monitoring frequency determines the timespan, or\nmonitoring window size, of logged data that is analyzed in each monitoring run.\nYou can specify alerting thresholds for each feature you want to monitor. An\nalert is logged when the statistical distance between the input feature\ndistribution and its corresponding baseline exceeds the specified threshold. By\ndefault, every categorical and numerical feature is monitored, with threshold\nvalues of 0.3.\nYou can specify alerting thresholds for each feature you want to monitor. An\nalert is logged when the statistical distance between the input feature\ndistribution and its corresponding baseline exceeds the specified threshold. By\ndefault, every categorical and numerical feature is monitored, with threshold\nvalues of 0.3.\nAn online prediction endpoint can host multiple models. When you enable skew or\ndrift detection on an endpoint, the following configuration parameters are\nshared across all models hosted in that endpoint:Type of detectionMonitoring frequencyFraction of input requests monitoredFor the otherconfiguration parameters, you can set different\nvalues for each model.\nAn online prediction endpoint can host multiple models. When you enable skew or\ndrift detection on an endpoint, the following configuration parameters are\nshared across all models hosted in that endpoint:\nType of detection\nMonitoring frequency\nFraction of input requests monitored\nFor the otherconfiguration parameters, you can set different\nvalues for each model.\nGet started with Model Monitoring v2\nProvide schema to Model Monitoring v1\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/model-registry/introduction",
    "title": "Introduction to Vertex AI Model RegistryStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nTo see an example of getting started with Vertex AI Model Registry,\n      run the \"Get started with Vertex AI Model Registry\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nThe Vertex AI Model Registry is a central repository where you can manage\nthe lifecycle of your ML models. From the Model Registry,\nyou have an overview of your models so you can better organize, track,\nand train new versions. When you have a model version you would like to deploy,\nyou can assign it to an endpoint directly from the registry,\nor using aliases, deploy models to an endpoint.\nThe Vertex AI Model Registry supports custom models and all\nAutoML data types - text, tabular, image, and video. The\nModel Registry\ncan also support BigQuery ML models. If you have models trained in\nBigQuery ML, you can register them with the\nModel Registry without needing to export them from\nBigQuery ML or import them into the Model Registry.\nFrom the model version details page you can evaluate, deploy to an endpoint,\nset up batch prediction, and view specific model details. The Vertex AI Model Registry\nprovides a straightforward and streamlined interface to manage and deploy your\nbest models to production.\nThere are many valid workflows for working in the Model Registry.\nTo get started, you might want to follow these guidelines to understand what you can\ndo in the Model Registry and at what stage in your model-training journey.\nImport models to the Model Registry.\nCreate new models, assign a model version the default alias, ready for production.\nAdd other aliases, or labels to help you manage and organize your models and model versions.\nDeploy your models to an endpoint for online prediction.\nRun batch prediction, and start your model evaluation pipeline.\nView your model details and view performance metrics from the model details page.\nTo learn more about how to integrate your BigQuery ML models with\nVertex AI, see theBigQuery ML documentation.\nDataplex's Data Catalog service is a fully managed, scalable\nmetadata management service that provides a centralized location to search\nfor models across projects and regions.\nFor details, seeUse Data Catalog to search for model and dataset resources.\nTo get started using Vertex AI Model Registry, seeImport models to Vertex AI Model RegistryVersioning with Vertex AI Model RegistryHow to use aliases with Vertex AI Model RegistryIntegrate a BigQuery ML model with Model RegistryHow to copy a model to a different region from the Vertex AI Model Registry\nImport models to Vertex AI Model Registry\nVersioning with Vertex AI Model Registry\nHow to use aliases with Vertex AI Model Registry\nIntegrate a BigQuery ML model with Model Registry\nHow to copy a model to a different region from the Vertex AI Model Registry\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/start/explore-models",
    "title": "Overview of Model GardenStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nModel Garden is an AI/ML model library that helps you discover, test,\ncustomize, and deploy models and assets from Google and Google partners.\nWhen you're working with AI models, Model Garden provides the following\nadvantages:\nAvailable models are all grouped in a single location\nModel Garden provides a consistent deployment pattern for different\ntypes of models\nModel Garden provides built-in integration with other parts of\nVertex AI such as model tuning, evaluation, and serving\nServing generative AI models can be difficult—Vertex AI handles\nmodel deployment and serving for you\nTo view the list of available Vertex AI and open source foundation,\ntunable, and task-specific models, go to the Model Garden page in the\nGoogle Cloud console.\nGo to Model Garden\nThe model categories available in Model Garden are:\nTo filter models in the filter pane, specify the following:\nModalities: Click the modalities (data types) that you want in the\nmodel.\nTasks: Click the task that you want the model to perform.\nFeatures: Click the features that you want in the model.\nProvider: Click the provider of the model.\nTo learn more about each model, click its model card.\nFor a list of models available in Model Garden, seeModels available in Model Garden.\nGoogle does thorough testing and benchmarking on the serving and tuning\ncontainers that we provide. Active vulnerability scanning is also applied to\ncontainer artifacts.\nThird-party models from featured partners undergo model checkpoint scans to\nensure authenticity. Third-party models from HuggingFace Hub are scanned\ndirectly by HuggingFace and theirthird-party scannerfor malware, pickle files, Keras Lambda layers, and secrets. Models deemed\nunsafe from these scans are flagged by HuggingFace and blocked from deployment\nin Model Garden. Models deemed suspicious or those that have the\nability to potentially execute remote code are indicated in\nModel Garden but can still be deployed. We recommend you perform a\nthorough review of any suspicious model before deploying it\nwithin Model Garden.\nFor the open source models in Model Garden, you are charged for use of\nfollowing on Vertex AI:\nModel tuning: You are charged for the compute resources used at the same\nrate as custom training. Seecustom training pricing.\nModel deployment: You are charged for the compute resources used to\ndeploy the model to an endpoint. Seepredictions pricing.\nColab Enterprise: SeeColab Enterprise pricing.\nYou can set aModel Garden organization\npolicyat the organization, folder, or\nproject level to control access to specific models in Model Garden. For\nexample, you can allow access to specific models that you've vetted and deny\naccess to all others.\nFor more information about the deployment options and customizations that you\ncan do with models in Model Garden, view the resources in the\nfollowing sections, which include links to tutorials, references, notebooks, and\nYouTube videos.\nLearn more about customizing deployments and advance serving features.\nDeploy and serve open source model using Python SDK, CLI, REST API, or consoleDeveloper blog: Introducing the new Vertex AI Model Garden CLI and SDKDeploy open models by using the SDK tutorial notebookGet started with Vertex AI Model Garden SDK notebook\nDeveloper blog: Introducing the new Vertex AI Model Garden CLI and SDK\nDeploy open models by using the SDK tutorial notebook\nGet started with Vertex AI Model Garden SDK notebook\nDeploying and fine-tuning Gemma 3 in Model Garden YouTube video\nDeploying Gemma and making predictions\nServe open models with a Hex-LLM container on Cloud\nTPUs\nUse prefix caching and speculative decoding with\nHex-LLM or vLLM tutorial notebook\nUse vLLM to serve text-only and multimodel language models on Cloud GPUsText-only models tutorial notebookMultimodal models tutorial notebook\nText-only models tutorial notebook\nMultimodal models tutorial notebook\nUse xDiT GPU serving container for image and video generation\nServing Gemma 2 with multiple LoRA adapters with HuggingFace DLC for PyTorch inference tutorial on Medium\nUse custom handles to serve PaliGemma for image captioning with HuggingFace DLC for PyTorch inference tutorial on LinkedIn\nDeploy and serve a model that uses Spot VMs or a Compute Engine reservation tutorial notebook\nDeploy and serve a HuggingFace model\nLearn more about tuning models to tailor responses for specific use cases.\nWorkbench fine-tuning tutorial notebook\nFine-tuning and evaluation tutorial notebook\nDeploying and fine-tuning Gemma 3 in Model Garden YouTube video\nLearn more about assessing model responses with Vertex AI\nEvaluate Gemma 2 with the generative AI evaluation service YouTube video\nModel and user journey-specific Model Garden notebooks\nVertex AI open model serving, fine-tuning and evaluation notebooks\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCloud NGFW\nDocumentation\nGuides\nFirewall Rules Logging lets you audit, verify, and analyze the\neffects of your firewall rules. For example, you can determine if a firewall\nrule designed to deny traffic is functioning as intended.\nFirewall Rules Logging  is also useful if you need to determine how\nmany connections are affected by a given firewall rule.\nYou enable Firewall Rules Logging individually for each firewall rule\nwhose connections you need to log. Firewall Rules Logging is an option\nfor any firewall rule, regardless of the action (allowordeny) or direction\n(ingress or egress) of the rule.\nFirewall Rules Logging logs traffic to and fromCompute Engine\nvirtual machine (VM) instances. This includes\nGoogle Cloud products built on Compute Engine\nVMs, such asGoogle Kubernetes Engine (GKE)\nclustersandApp Engine flexible environmentinstances.\nWhen you enable logging for a firewall rule, Google Cloud creates an entry\ncalled aconnection recordeach time the rule allows or denies traffic. You\ncan view these records inCloud Logging, and you can export logs\nto any destination that Cloud Logging export supports.\nEach connection record contains the source and destination IP addresses, the\nprotocol and ports, date and time, and a reference to the firewall rule that\napplied to the traffic.\nFirewall Rules Logging is available for both VPC\nfirewall rules and hierarchical firewall policies.\nFor information about viewing logs, seeUse\nFirewall Rules Logging.\nFirewall Rules Logging has the following specifications:\nYou can only enable Firewall Rules Logging for rules in aVirtual Private Cloud (VPC) network.Legacy networksarenotsupported.\nFirewall Rules Logging only records TCP and UDP connections. Although you cancreate a firewall rule applicable to other\nprotocols, you cannot log their connections.\nIf you want to also log other protocols, consider usingPacket Mirroring.\nYoucannotenable Firewall Rules Logging for theimplied deny ingress and implied allow egress rules.\nLog entries are written from the perspective of VMs. Log entries\nare only created if a firewall rule has logging enabled and if the rule\napplies to traffic sent to or from the VM. Entries are created according to\nthe connection logging limits on abest effortbasis.\nThe number of connections that can belogged in a given\nintervalis based on the machine type.\nChanges to firewall rules can be viewed inVPC audit logs.\nA log entry is generated each time that a firewall rule with logging enabled applies\nto traffic. A given packet flow can generate more than one log entry in total.\nHowever, from the perspective of a given VM, at most only one log entry can be\ngenerated if the firewall rule that applies to it has logging enabled.\nThe following examples demonstrate how firewall logs work.\nIn this example:\nTraffic between VM instances in theexample-netVPC network in theexample-projproject is considered.\nThe two VM instances are:VM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region).VM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region).\nVM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region).\nVM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region).\nRule A: An egress deny firewall rule has a target of all instances in the\nnetwork, a destination of10.20.0.99(VM2), and applies to TCP port 80.Logging is enabled for this rule.\nLogging is enabled for this rule.\nRule B: An ingress allow firewall rule has a target of all instances in the\nnetwork, a source of10.10.0.99(VM1), and applies to TCP port 80.Logging is also enabled for this rule.\nLogging is also enabled for this rule.\nThe followinggcloudcommands can be used to create the firewall rules:\nRule A: egress deny rule for TCP 80, applicable to all instances,\ndestination10.20.0.99:gcloud compute firewall-rules create rule-a \\\n    --network example-net \\\n    --action deny \\\n    --direction EGRESS \\\n    --rules tcp:80 \\\n    --destination-ranges 10.20.0.99/32 \\\n    --priority 10 \\\n    --enable-logging\nRule A: egress deny rule for TCP 80, applicable to all instances,\ndestination10.20.0.99:\nRule B: ingress allow rule for TCP 80, applicable to all instances,\nsource10.10.0.99:gcloud compute firewall-rules create rule-b \\\n    --network example-net \\\n    --action allow \\\n    --direction INGRESS \\\n    --rules tcp:80 \\\n    --source-ranges 10.10.0.99/32 \\\n    --priority 10 \\\n    --enable-logging\nRule B: ingress allow rule for TCP 80, applicable to all instances,\nsource10.10.0.99:\nSuppose VM1 attempts to connect to VM2 on TCP port 80. The following firewall\nrules are logged:\nA log entry for rule A from the perspective of VM1 is generated as VM1\nattempts to connect to10.20.0.99(VM2).\nBecause rule A actually blocks the traffic, rule B is never considered, so\nthere is no log entry for rule B from the perspective of VM2.\nThe firewall log record is generated in the following example.\nIn this example:\nTraffic between VM instances in theexample-netVPC network in theexample-projproject is considered.\nThe two VM instances are:VM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region).VM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region).\nVM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region).\nVM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region).\nRule A: An egress allow firewall rule has a target of all instances in the\nnetwork, a destination of10.20.0.99(VM2), and applies to TCP port 80.Logging is enabled for this rule.\nLogging is enabled for this rule.\nRule B: An ingress allow firewall rule has a target of all instances in the\nnetwork, a source of10.10.0.99(VM1), and applies to TCP port 80.Logging is also enabled for this rule.\nLogging is also enabled for this rule.\nThe followinggcloudcommands can be used to create the two firewall rules:\nRule A: egress allow rule for TCP 80, applicable to all instances,\ndestination10.20.0.99(VM2):gcloud compute firewall-rules create rule-a \\\n    --network example-net \\\n    --action allow \\\n    --direction EGRESS \\\n    --rules tcp:80 \\\n    --destination-ranges 10.20.0.99/32 \\\n    --priority 10 \\\n    --enable-logging\nRule A: egress allow rule for TCP 80, applicable to all instances,\ndestination10.20.0.99(VM2):\nRule B: ingress allow rule for TCP 80, applicable to all instances,\nsource10.10.0.99(VM1):gcloud compute firewall-rules create rule-b \\\n    --network example-net \\\n    --action allow \\\n    --direction INGRESS \\\n    --rules tcp:80 \\\n    --source-ranges 10.10.0.99/32 \\\n    --priority 10 \\\n    --enable-logging\nRule B: ingress allow rule for TCP 80, applicable to all instances,\nsource10.10.0.99(VM1):\nSuppose VM1 attempts to connect to VM2 on TCP port 80. The following firewall\nrules are logged:\nA log entry for rule A from the perspective of VM1 is generated as VM1\nconnects to10.20.0.99(VM2).\nA log entry for rule B from the perspective of VM2 is generated as VM2 allows\nincoming connections from10.10.0.99(VM1).\nThe firewall log record reported by VM1 is generated in the following example.\nThe firewall log record reported by VM2 is generated in the following example.\nIn this example:\nTraffic from a system outside theexample-netVPC network to a VM\ninstance in that network is considered. The network is in theexample-projproject.\nThe system on the internet has IP address203.0.113.114.\nVM1 in zoneus-west1-ahas IP address10.10.0.99in thewest-subnet(us-west1region).\nRule C: An ingress allow firewall rule has a target of all instances in the\nnetwork, a source of any IP address (0.0.0.0/0), and applies to TCP port 80.Logging is enabled for this rule.\nLogging is enabled for this rule.\nRule D: An egress deny firewall rule has a target of all instances in the\nnetwork, a destination of any IP address (0.0.0.0/0), and applies to all\nprotocols.Logging is also enabled for this rule.\nLogging is also enabled for this rule.\nThe followinggcloudcommands can be used to create the firewall rules:\nRule C: ingress allow rule for TCP 80, applicable to all instances,\nany source:gcloud compute firewall-rules create rule-c \\\n    --network example-net \\\n    --action allow \\\n    --direction INGRESS \\\n    --rules tcp:80 \\\n    --source-ranges 0.0.0.0/0 \\\n    --priority 10 \\\n    --enable-logging\nRule C: ingress allow rule for TCP 80, applicable to all instances,\nany source:\nRule D: egress deny rule for all protocols, applicable to all instances,\nany destination:gcloud compute firewall-rules create rule-d \\\n    --network example-net \\\n    --action deny \\\n    --direction EGRESS \\\n    --rules all \\\n    --destination-ranges 0.0.0.0/0 \\\n    --priority 10 \\\n    --enable-logging\nRule D: egress deny rule for all protocols, applicable to all instances,\nany destination:\nSuppose the system with IP address203.0.113.114attempts to connect to VM1\non TCP port 80. The following happens:\nA log entry for rule C from the perspective of VM1 is generated as VM1 accepts\ntraffic from203.0.113.114.\nDespite rule D, VM1 is allowed to reply to the incoming request because\nGoogle Cloud firewall rules are stateful. If the incoming request is\nallowed, established responses cannot be blocked by any kind of egress rule.\nBecause rule D does not apply, it is never considered, so\nthere is no log entry for rule D.\nThe firewall log record is generated in the following example.\nSubject to thespecifications,\na log entry is created in Cloud Logging for each firewall rule that has\nlogging enabled if that rule applies to traffic to or from a VM instance. Log\nrecords are included in the JSON payload field of a LoggingLogEntry.\nLog records contain base fields, which are the core fields of every log record,\nand metadata fields that add additional information. You can control whether\nmetadata fields are included. If you omit them, you can save on storage costs.\nSome log fields support values that are also fields. These fields can have more\nthan one piece of data in a given field. For example, theconnectionfield is\nof theIpConnectionformat, which contains the source and destination IP\naddress and port, plus the protocol, in a single field. These fields are\ndescribed in the following tables.\nTo set up logging and view logs, seeUse Firewall Rules Logging.\nTo get insights about how your firewall rules are being used,\nseeFirewall Insights.\nTo store, search, analyze, monitor, and alert on log data and events, seeCloud Logging.\nTo route log entries, seeConfigure and manage sinks.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-24 UTC."
  },
  {
    "url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCloud Healthcare API\nDocumentation\nGuides\nThe Healthcare Natural Language API is a part of the Cloud Healthcare API that\nuses natural language models to extract healthcare information from medical\ntext.\nThis conceptual guide explains the basics of using the\nHealthcare Natural Language API, including:\nThe types of requests you can make to the Healthcare Natural Language API\nHow to construct requests to the Healthcare Natural Language API\nHow to handle responses from the Healthcare Natural Language API\nThe Healthcare Natural Language API extracts healthcare information from\nmedical text. This healthcare information can include:\nMedical concepts, such as medications, procedures, and medical conditions\nFunctional features, such as temporal relationships, subjects, and certainty\nassessments\nRelations, such as side effects and medication dosage\nThe Healthcare Natural Language API offers pre-trained natural language models to\nextract medical concepts and relationships from medical text. The\nHealthcare Natural Language API maps text into a predefined set ofmedical knowledge categories.\nAutoML Entity Extraction for Healthcare lets you create a custom\nentity extraction model trained using your own annotated medical text and using\nyour own categories. For more information, see theAutoML Entity Extraction for Healthcare documentation.\nThe Healthcare Natural Language API is available in the following locations:\nThe Healthcare Natural Language API inspects medical text for medical concepts and\nrelations. You perform entity analysis using theanalyzeEntitiesmethod.\nThe Healthcare Natural Language API is a REST API and consists of JSON requests\nand responses. The following sections show how to extract different medical\ninsights from a given medical text:\nExtract entities, relations, and contextual attributes\nInclude licensed vocabularies\nExtract output as a FHIR R4 bundle\nThe entity analysis request contains the following fields:\ndocumentContent:\nThe data for the request, which consists of medical text. The maximum size of\nthe medical text is 20,000 Unicode characters.\nlicensedVocabularies[]:\nOptional. TheSNOMED CTvocabulary. Available only for US users.\nalternativeOutputFormat:\nOptional. The FHIR bundle format.\nThe entity analysis returns a set of detected medical knowledge mentions, medical\nconcepts, and relations between medical knowledge mentions, including the\nfollowing:\nentityMentions: occurrences of medical knowledge entities in the source\nmedical text. Each entity mention has the following fields:mentionId: a unique identifier for an entity mention in the response.type: themedical knowledge categoryof\nthe entity mention.text: consists of thetextContentfield, and describes the excerpt of\nthe medical text containing the entity mention, andoffset, the location\nof the entity mention in the source medical text.temporalAssessment: specifies how the linked entity relates to the\nentity mention, one ofCURRENT,CLINICAL_HISTORY,FAMILY_HISTORY,UPCOMING, orOTHER.certaintyAssessment: the negation or qualification of the medical\nconcept, one ofLIKELY,SOMEWHAT_LIKELY,UNCERTAIN,SOMEWHAT_UNLIKELY,UNLIKELY, orCONDITIONAL.subject: specifies the subject that the medical concept relates to,\none ofPATIENT,FAMILY_MEMBER, orOTHER.linkedEntities: a list of medical concepts that might be related to this\nentity mention. Linked entities specify theentityId, which\nlinks a medical concept to an an entity inentities.\nentityMentions: occurrences of medical knowledge entities in the source\nmedical text. Each entity mention has the following fields:\nmentionId: a unique identifier for an entity mention in the response.\ntype: themedical knowledge categoryof\nthe entity mention.\ntext: consists of thetextContentfield, and describes the excerpt of\nthe medical text containing the entity mention, andoffset, the location\nof the entity mention in the source medical text.\ntemporalAssessment: specifies how the linked entity relates to the\nentity mention, one ofCURRENT,CLINICAL_HISTORY,FAMILY_HISTORY,UPCOMING, orOTHER.\ncertaintyAssessment: the negation or qualification of the medical\nconcept, one ofLIKELY,SOMEWHAT_LIKELY,UNCERTAIN,SOMEWHAT_UNLIKELY,UNLIKELY, orCONDITIONAL.\nsubject: specifies the subject that the medical concept relates to,\none ofPATIENT,FAMILY_MEMBER, orOTHER.\nlinkedEntities: a list of medical concepts that might be related to this\nentity mention. Linked entities specify theentityId, which\nlinks a medical concept to an an entity inentities.\nentities: describes the medical concepts from the linked entities fields.\nEach entity is described using the following fields:entityId: a unique identifier from thelinkedEntitiesfield.preferredTerm: a preferred term for the medical concept.vocabularyCodes: the representation of the medical concept in\nsupportedmedical vocabularies.\nentities: describes the medical concepts from the linked entities fields.\nEach entity is described using the following fields:\nentityId: a unique identifier from thelinkedEntitiesfield.\npreferredTerm: a preferred term for the medical concept.\nvocabularyCodes: the representation of the medical concept in\nsupportedmedical vocabularies.\nrelationships: define directed relationships between entity mentions.\nIn the sample, the subject of the relationship is \"Insulin regimen human\" and\nthe object of the relationship is \"5 units\".\nrelationships: define directed relationships between entity mentions.\nIn the sample, the subject of the relationship is \"Insulin regimen human\" and\nthe object of the relationship is \"5 units\".\nconfidence: an indication of the model's confidence in the relationship as a\nnumber between 0 and 1.\nconfidence: an indication of the model's confidence in the relationship as a\nnumber between 0 and 1.\nApart from the listed fields, the response might also contain theadditionalInfofield, which states any additional description about the entity mention type.\nSeeAdditional information.\nThe Healthcare Natural Language API only supports extracting healthcare information from English text.\nThe Healthcare Natural Language API supports the following medical vocabularies:\nFoundational Model of Anatomy\nGene Ontology\nHUGO Gene Nomenclature Committee\nHuman Phenotype Ontology\nICD-10 Procedure Coding System\nICD-10-CM\nICD-9-CM\nLOINC\nMeSH\nMedlinePlus Health Topics\nMetathesaurus Names\nNCBI Taxonomy\nNCI Thesaurus\nNational Drug File\nOnline Mendelian Inheritance in Man\nRXNORM\nSNOMED CT (available for US users only)\nThe Healthcare Natural Language API assigns a medical knowledge category to theentityMentions.typefield.\nA list of supported medical knowledge categories is as follows. The entity mention types that belong to the oncology, social determinants of health (SDOH), and protected health information (PHI) groups are only available inPreview:\nNote: HIPAA classifies the age of a person as PHI only when it's above 90. For more information, seeSummary of the HIPAA Privacy Rule.\nThe Healthcare Natural Language API can infer functional features, or attributes,\nof an entity mention from context. For example, in the statement\n\"Kusuma's mother has diabetes\", the condition \"diabetes\" has the functional\nfeature ofsubjectFAMILY_MEMBER.\nTemporal relationships, returned in thetemporalAssessmentfield, describe\nhow this entity mention relates to the subject temporally.\nThe Healthcare Natural Language API supports the following temporal relationships:\nCURRENT\nCLINICAL_HISTORY\nFAMILY_HISTORY\nUPCOMING\nOTHER\nSubjects, returned in thesubjectfield, describe the individual the entity\nmention relates to.\nThe Healthcare Natural Language API supports the following subjects:\nPATIENT\nFAMILY_MEMBER\nOTHER\nCertainty assessments, returned in thecertaintyAssessmentfield, describe\nthe original note taker's confidence. For example, if the original note\ncontains \"The patient has a sore throat\", the certainty assessment\nreturns aLIKELYvalue to indicate the note taker's confidence that it\nwas likely that the patient had a sore throat. If the original note contains \"The patient does\nnot have a sore throat\", the certainty assessment returns anUNLIKELYvalue to indicate the note taker's confidence that it was unlikely\nthat the patient had a sore throat.\nCertainty assessments can be one of the following values:\nLIKELY\nSOMEWHAT_LIKELY\nUNCERTAIN\nSOMEWHAT_UNLIKELY\nUNLIKELY\nCONDITIONAL\nTheadditionalInfofield provides additional details about an entity\nmention. For example, theadditionalInfofield for aDATEentity mention might\nconsist of details about the type of the date, categorized as one of the following:\nADMISSION_DATE\nCONSULTATION_DATE\nDISCHARGE_DATE\nSERVICE_DATE\nVISIT_DATE\nDIAGNOSIS_DATE\nMED_STARTED_DATE\nMED_ENDED_DATE\nNOTE_DATE\nPROCEDURE_DATE\nRADIATION_STARTED_DATE\nRADIATION_ENDED_DATE\nSTAGE_DATE\nThe Healthcare Natural Language API can infer relationships between entity\nmentions based on the surrounding medical text. In the response, the subject of\nthe relationship is identified bysubjectIdand the object of the\nrelationship is identified byobjectId.\nThe Healthcare Natural Language API supports the following relationships between\nentity mentions:\nWhen you request theanalyzeEntitiesmethod with thealternativeOutputFormatfield set toFHIR_BUNDLE, the response includes the following JSON objects:\nThe entity mentions, the entities, and the relationships\nA FHIR R4 bundle represented as a string, that includes all the entities, the\nentity mentions, and the relationships in JSON format\nTo create the FHIR R4 bundle, the Healthcare Natural Language API maps the entity mentions,\nentities, and relationships to FHIR resources and their elements. The following\ntable lists some of these mappings.\nTo extract entities from text as a FHIR R4 bundle, seeExtract output as a FHIR R4 bundle.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-28 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/quotas",
    "title": "Cloud Quotas documentation",
    "content": "Home\nDocumentation\nCloud Quotas\nCloud Quotas enables customers to manage quotas for all of their Google Cloud services. With\n    Cloud Quotas, users are able to easily monitor quota usage, create and modify quota\n    alerts, and request limit adjustments for quotas. Quotas are managed through the Cloud Quotas\n    dashboard or the Cloud Quotas API.Learn more.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nOverview\nOverview\nUnderstand quotas\nUnderstand quotas\nPredefined roles and permissions\nPredefined roles and permissions\nREST API\nREST API\nClient libraries\nClient libraries\nRelease notes\nRelease notes\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "content": "Home\nDocumentation\nAI/ML orchestration on GKE\nRun optimized AI/ML workloads withGoogle Kubernetes Engine (GKE)platform orchestration capabilities. With Google Kubernetes Engine (GKE), you can implement a robust, production-ready AI/ML platform with all the benefits of managed Kubernetes and these capabilities:\nInfrastructure orchestration that supports GPUs and TPUs for training and serving workloads at scale.\nFlexible integration with distributed computing and data processing frameworks.\nSupport for multiple teams on the same infrastructure to maximize utilization of resources\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nNEW!About model inference on GKE\nAbout model inference on GKE\nNEW!Run best practice inference with GKE Inference Quickstart recipes\nRun best practice inference with GKE Inference Quickstart recipes\nNEW!Serve LLMs like Deepseek-R1 671B or Llama 3.1 405B on GKE\nServe LLMs like Deepseek-R1 671B or Llama 3.1 405B on GKE\nTutorialServe Gemma using GPUs on GKE with vLLM\nServe Gemma using GPUs on GKE with vLLM\nTutorialServe an LLM using TPU Trillium on GKE with vLLM\nServe an LLM using TPU Trillium on GKE with vLLM\nTutorialDiscover more tutorials for model inference on GKE\nDiscover more tutorials for model inference on GKE\nNEW!Quickstart: Deploy GPU-accelerated Ray for AI workloads on GKE\nQuickstart: Deploy GPU-accelerated Ray for AI workloads on GKE\nNEW!Optimize GKE resource utilization for mixed AI/ML training and inference workloads\nOptimize GKE resource utilization for mixed AI/ML training and inference workloads\nVideoIntroduction to Cloud TPUs for machine learning.\nIntroduction to Cloud TPUs for machine learning.\nVideoBuild large-scale machine learning on Cloud TPUs with GKE\nBuild large-scale machine learning on Cloud TPUs with GKE\nVideoServing Large Language Models with KubeRay on TPUs\nServing Large Language Models with KubeRay on TPUs\nBlogMachine learning with JAX on Kubernetes with NVIDIA GPUs\nMachine learning with JAX on Kubernetes with NVIDIA GPUs\nNEW!Reference architecture for a batch processing platform on GKE\nReference architecture for a batch processing platform on GKE\nBest practiceOptimize GPU obtainability with flex-start provisioning mode\nOptimize GPU obtainability with flex-start provisioning mode\nBlogHigh performance AI/ML storage through Local SSD support on GKE\nHigh performance AI/ML storage through Local SSD support on GKE\nBlogSimplifying MLOps using Weights & Biases with Google Kubernetes Engine\nSimplifying MLOps using Weights & Biases with Google Kubernetes Engine\nBest practiceBest practices for running batch workloads on GKE\nBest practices for running batch workloads on GKE\nBest practiceRun cost-optimized Kubernetes applications on GKE\nRun cost-optimized Kubernetes applications on GKE\nBest practiceImproving launch time of Stable Diffusion on GKE by 4x\nImproving launch time of Stable Diffusion on GKE by 4x\nServe open source models using TPUs on GKE with Optimum TPU\nLearn how to deploy LLMs using Tensor Processing Units (TPUs) on GKE with the Optimum TPU serving framework from Hugging Face.TutorialAI/ML InferenceTPU\nCreate and use a volume backed by a Parallelstore instance in GKE\nLearn how to create storage backed by fully managed Parallelstore instances, and access them as volumes. The CSI driver is optimized for AI/ML training workloads involving smaller file sizes and random reads.TutorialAI/ML Data Loading\nAccelerate AI/ML data loading with Hyperdisk ML\nLearn how to how to simplify and accelerate the loading of AI/ML model weights on GKE using Hyperdisk ML.TutorialAI/ML Data Loading\nServe an LLM using TPUs on GKE with JetStream and PyTorch\nLearn how to serve a LLM using Tensor Processing Units (TPUs) on GKE with JetStream through PyTorch.TutorialAI/ML InferenceTPUs\nBest practices for optimizing LLM inference with GPUs on GKE\nLearn best practices for optimizing LLM inference performance with GPUs on GKE using the vLLM and Text Generation Inference (TGI) serving frameworks.TutorialAI/ML InferenceGPUs\nManage the GPU Stack with the NVIDIA GPU Operator on GKE\nLearn when to use the NVIDIA GPU operator and how to enable the NVIDIA GPU Operator on GKE.TutorialGPUs\nConfigure autoscaling for LLM workloads on TPUs\nLearn how to set up your autoscaling infrastructure by using the GKE Horizontal Pod Autoscaler (HPA) to deploy the Gemma LLM using single-host JetStream.TutorialTPUs\nFine-tune Gemma open models using multiple GPUs on GKE\nLearn how to fine-tune Gemma LLM using GPUs on GKE with the Hugging Face Transformers library.TutorialAI/ML InferenceGPUs\nDeploy a Ray Serve application with a Stable Diffusion model on GKE with TPUs\nLearn how to deploy and serve a Stable Diffusion model on GKE using TPUs, Ray Serve, and the Ray Operator add-on.TutorialAI/ML InferenceRayTPUs\nConfigure autoscaling for LLM workloads on GPUs with GKE\nLearn how to set up your autoscaling infrastructure by using the GKE Horizontal Pod Autoscaler (HPA) to deploy the Gemma LLM with the Hugging Face Text Generation Interface (TGI) serving framework.TutorialGPUs\nTrain Llama2 with Megatron-LM on A3 Mega virtual machines\nLearn how to run a container-based, Megatron-LM PyTorch workload on A3 Mega.TutorialAI/ML TrainingGPUs\nDeploy GPU workloads in Autopilot\nLearn how to request hardware accelerators (GPUs) in your GKE Autopilot workloads.TutorialGPUs\nServe a LLM with multiple GPUs in GKE\nLearn how to serve Llama 2 70B or Falcon 40B using multiple NVIDIA L4 GPUs with GKE.TutorialAI/ML InferenceGPUs\nGetting started with Ray on GKE\nLearn how to easily start using Ray on GKE by running a workload on a Ray cluster.TutorialRay\nServe an LLM on L4 GPUs with Ray\nLearn how to serve Falcon 7b, Llama2 7b, Falcon 40b, or Llama2 70b using the Ray framework in GKE.TutorialAI/ML InferenceRayGPUs\nOrchestrate TPU Multislice workloads using JobSet and Kueue\nLearn how to orchestrate a Jax workload on multiple TPU slices on GKE by using JobSet and Kueue.TutorialTPUs\nMonitoring GPU workloads on GKE with NVIDIA Data Center GPU Manager (DCGM)\nLearn how to observe GPU workloads on GKE with NVIDIA Data Center GPU Manager (DCGM).TutorialAI/ML ObservabilityGPUs\nQuickstart: Train a model with GPUs on GKE Standard clusters\nThis quickstart shows you how to deploy a training model with GPUs in GKE and store the predictions in Cloud Storage.TutorialAI/ML TrainingGPUs\nRunning large-scale machine learning on GKE\nThis video shows how GKE helps solve common challenges of training large AI models at scale, and the best practices for training and serving large-scale machine learning models on GKE.VideoAI/ML TrainingAI/ML Inference\nTensorFlow on GKE Autopilot with GPU acceleration\nThis blog post is a step-by-step guide to the creation, execution, and teardown of a Tensorflow-enabled Jupiter notebook.BlogAI/ML TrainingAI ML InferenceGPUs\nImplement a Job queuing system with quota sharing between namespaces on GKE\nThis tutorial uses Kueue to show you how to implement a Job queueing system, and configure workload resource and quota sharing between different namespaces on GKE.TutorialAI/ML Batch\nBuild a RAG chatbot with GKE and Cloud Storage\nThis tutorial shows you how to integrate a Large Language Model application based on retrieval-augmented generation with PDF files that you upload to a Cloud Storage bucket.TutorialAI/ML Data Loading\nAnalyze data on GKE using BigQuery, Cloud Run, and Gemma\nThis tutorial shows you how to analyze big datasets on GKE by leveraging BigQuery for data storage and processing, Cloud Run for request handling, and a Gemma LLM for data analysis and predictions.TutorialAI/ML Data Loading\nDistributed data preprocessing with GKE and Ray: Scaling for the enterprise\nLearn how to leverage GKE and Ray to efficiently preprocess large datasets for machine learning.MLOpsTrainingRay\nData loading best practices for AI/ML inference on GKE\nLearn how to speed up data loading times for your machine learning applications on Google Kubernetes Engine.InferenceHyperdisk MLCloud Storage FUSE\nSave on GPUs: Smarter autoscaling for your GKE inferencing workloads\nLearn how to optimize your GPU inference costs by fine-tuning GKE's Horizontal Pod Autoscaler for maximum efficiency.InferenceGPUHPA\nEfficiently serve optimized AI models with NVIDIA NIM microservices on GKE\nLearn how to deploy cutting-edge NVIDIA NIM microservices on GKE with ease and accelerate your AI workloads.AINVIDIANIM\nAccelerate Ray in production with new Ray Operator on GKE\nLearn how Ray Operator on GKE simplifies your AI/ML production deployments, boosting performance and scalability.AITPURay\nMaximize your LLM serving throughput for GPUs on GKE — a practical guide\nLearn how to maximize large language model (LLM) serving throughput for GPUs on GKE, including infrastructure decisions and model server optimizations.LLMGPUNVIDIA\nSearch engines made simple: A low-code approach with GKE and Vertex AI Agent Builder\nHow to build a search engine with Google Cloud, using Vertex AI Agent Builder, Vertex AI Search, and GKE.SearchAgentVertex AI\nLiveX AI reduces customer support costs with AI agents trained and served on GKE and NVIDIA AI\nHow LiveX AI uses GKE to build AI agents that enhance customer satisfaction and reduce costs.GenAINVIDIAGPU\nInfrastructure for a RAG-capable generative AI application using GKE\nReference architecture for running a generative AI application with retrieval-augmented\n             generation (RAG) using GKE, Cloud SQL, Ray, Hugging Face, and LangChain.GenAIRAGRay\nInnovating in patent search: How IPRally leverages AI with GKE and Ray\nHow IPRally uses GKE and Ray to build a scalable, efficient ML platform for faster patent searches with better accuracy.AIRayGPU\nPerformance deep dive of Gemma on Google Cloud\nLeverage Gemma on Cloud GPUs and Cloud TPUs for inference and training efficiency on GKE.AIGemmaPerformance\nGemma on GKE deep dive: New innovations to serve open generative AI models\nUse best-in-class Gemma open models to build portable, customizable AI applications and deploy them on GKE.AIGemmaPerformance\nAdvanced scheduling for AI/ML with Ray and Kueue\nOrchestrate Ray applications in GKE with KubeRay and Kueue.KueueRayKubeRay\nHow to secure Ray on Google Kubernetes Engine\nApply security insights and hardening techniques for training AI/ML workloads using Ray on GKE.AIRaySecurity\nDesign storage for AI and ML workloads in Google Cloud\nSelect the best combination of storage options for AI and ML workloads on Google Cloud.AIMLStorage\nAutomatic driver installation simplifies using NVIDIA GPUs in GKE\nAutomatically install Nvidia GPU drivers in GKE.GPUNVIDIAInstallation\nAccelerate your generative AI journey with NVIDIA NeMo framework on GKEE\nTrain generative AI models using GKE and NVIDIA NeMo framework.GenAINVIDIANeMo\nWhy GKE for your Ray AI workloads?\nImprove scalability, cost-efficiency, fault tolerance, isolation, and portability by using GKE for Ray workloads.AIRayScale\nRunning AI on fully managed GKE, now with new compute options, pricing and resource reservations\nGain improved GPU support, performance, and lower pricing for AI/ML workloads with GKE Autopilot.GPUAutopilotPerformance\nHow SEEN scaled output 89x and reduced GPU costs by 66% using GKE\nStartup scales personalized video output with GKE.GPUScaleContainers\nHow Spotify is unleashing ML Innovation with Ray and GKE\nHow Ray is transforming ML development at Spotify.MLRayContainers\nHow Ordaōs Bio takes advantage of generative AI on GKE\nOrdaōs Bio, one of the leading AI accelerators for biomedical research and discovery, is finding solutions to novel immunotherapies in oncology and chronic inflammatory disease.PerformanceTPUCost optimization\nGKE from a growing startup powered by ML\nHow Moloco, a Silicon Valley startup, harnessed the power of GKE and Tensor Flow Enterprise to supercharge its machine learning (ML) infrastructure.MLScaleCost optimization\nGoogle Kubernetes Engine (GKE) Samples\nView sample applications used in official GKE product tutorials.\nGKE AI Labs Samples\nView experimental samples for leveraging GKE to accelerate your AI/ML initiatives.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "content": "Home\nBigQuery\nDocumentation\nGuides\nBigQuery has built-in governance capabilities that simplify how\nyou discover, manage, monitor, govern, and use your data and AI assets.\nAdministrators, data stewards, data governance managers, and data custodians can\nuse the governance capabilities in BigQuery to do the following:\nDiscover data.\nCurate data.\nGather and enrich metadata.\nManage data quality.\nEnsure that data is used consistently and in compliance with organizational\npolicies.\nShare data at scale and in a secure fashion.\nAt the heart of BigQuery governance capabilities isuniversal catalog, a centralized inventory of all data assets in your organization.\nUniversal catalog holds business, technical, and runtime metadata for\nall of your data. It helps you discover relationships and semantics in the\nmetadata by applying artificial intelligence and machine learning.\nUniversal catalog brings together a data catalog and a fully\nmanaged runtime metastore. The metastore in BigQuery lets you use\nmultiple data processing engines to query a single copy of data with a single\nschema, without data duplication. The data processing engines that you can use\ninclude BigQuery, Apache Spark, Apache\nFlink, and Apache Hive. Your data can be stored in locations like\nBigQuery storage tables, BigQuery tables for Apache Iceberg, or\nBigLake external tables.\nBigQuery supports an end-to-end data lifecycle, from\ndiscovery to use of data. Universal catalog powers\nBigQuery governance features and capabilities. Governance\nfeatures are also available in\nDataplex.\nBigQuery discovers data across the organization in Google Cloud,\nwhether the data is in BigQuery, Spanner, Cloud SQL,\nPub/Sub, or Cloud Storage. BigQuery automatically\nextracts the metadata and stores it in universal catalog. For\nexample, you can use BigQuery to extract metadata for structured\nand unstructured data from Cloud Storage, and you can automatically\ncreate query-ready BigLake tables at scale. This lets you perform\nanalytics with an open source engine without data duplication.\nYou can also extract and catalog metadata from third-party data sources using\ncustom connectors.\nBigQuery offers the following data discovery\ncapabilities:\nSearch.Search for data and AI resources across projects and the\norganization. Within BigQuery in the Google Cloud console, usesemantic search(Preview) to search for resources by\nusing everyday language. Or, find resources by usingkeyword searchin Dataplex.\nAutomatic discovery of Cloud Storage\ndata.Scan for data\nin Cloud Storage buckets to extract and then catalog metadata.\nAutomatic discovery creates tables for both structured and unstructured\ndata.\nMetadata import.Import metadata at scale from\nthird-party systems into universal catalog. You can build\ncustom connectors to extract data from your data sources, and then run\nmanaged connectivity pipelines that orchestrate the metadata import\nworkflow.\nMetadata export.Export metadata at scale out of\nuniversal catalog. You can analyze the exported\nmetadata with BigQuery, or integrate the metadata into custom\napplications or programmatic processing workflows.\nTo improve the discoverability and usability of data, data stewards and\nadministrators can use BigQuery to review, update, and analyze\nmetadata. BigQuery data curation and stewardship capabilities\nhelp you ensure that your data is accurate, consistent, and aligned with your\norganization's policies.\nBigQuery offers the following data curation and\nstewardship capabilities:\nBusiness glossary(Preview).Improve\ncontext, collaboration, and search by defining your organization's\nterminology in a glossary. Identify data stewards for the terms, and attach\nterms to data asset fields.\nData insights.Gemini uses metadata to generate natural language questions about\nyour table and the SQL queries to answer them. These data insights help you\nuncover patterns, assess data quality, and perform statistical analysis.\nData profiling.Identify\ncommon statistical characteristics of the columns in BigQuery\ntables to understand and analyze your data more effectively.\nData quality.Define and run\ndata quality checks across tables in BigQuery and\nCloud Storage, and apply regular and ongoing data controls in\nBigQuery environments.\nData lineage.Track how data\nmoves through your systems: where it comes from, where it's passed to, and\nwhat transformations are applied to it. BigQuery supports\ndata lineage at the table- and column-levels.\nThe following table outlines next steps that you can take to learn more about\ncuration and data stewardship features:\nRun adata\n        profile scanto gain insights about your data, including the limits\n        or averages of your data.\nEnabledata lineagein your BigQuery project to\n        automatically record lineage information for BigQuery\n        operations like load, copy, and data modifications.\nSet up a recurringdata\n        quality scanto alert you to possible data issues by usingpredefined scan rules.\nSet upcustom data quality rulesfor your data quality scans so that your\n        scans are tailored to your specific needs.\nData access management is the process of defining, enforcing, and monitoring\nthe rules and policies governing who has access to data. Access management\nensures that data is only accessible to those who are authorized to access it.\nBigQuery offers the following security and access control\ncapabilities:\nIdentity and Access Management (IAM).IAM lets you control who\nhas access to your BigQuery resources, such as projects,\ndatasets, tables, and views. You can grant IAM\nroles to users, groups, and service accounts. These roles define what they\ncan do with your resources.\nColumn-level access\ncontrolsandrow-level access controls.Column-level and row-level access\ncontrols let you restrict access to specific columns and rows in a table,\nbased on user attributes or data values. This control lets you implement\nfine-grained access to help protect sensitive data from unauthorized access.\nData transfer management.VPC Service Controls lets you create perimeters around Google Cloud\nresources and control access to those resources based on your\norganization's policies.\nAudit logs.Audit logs provide you with a detailed\nrecord of user activity and system events in your organization. These logs\nhelp you enforce data governance policies and identify potential security\nrisks.\nData masking.Data masking lets you obscure sensitive\ndata in a table while still permitting authorized users to access the\nsurrounding data. Data masking can also obscure data that matches sensitive\ndata patterns, safeguarding against accidental data disclosure.\nEncryption.BigQuery automatically encrypts all data at\nrest and in transit, while letting you customize your encryption settings to\nmeet your specific requirements.\nThe following table outlines next steps that you can take to learn more about\naccess control features:\nTake a look atpredefined rolesin BigQuery and consider how to assign them based on theprinciple of least privilege.\nLearn how Google encrypts your dataat restandin transitby\n        default.\nFor greater flexibility and granularity in managing your\n        permissions, considercreating custom rolesthat match your needs.\nAddrowandcolumn controlsto help control access to specific rows and columns in your tables.\nEstablish an access perimeter around your Google Cloud\n        resources bysetting up VPC Service Controls.\nAddcolumn-level data\n        maskingto your table to share information through your organization\n        without revealing sensitive data.\nUseSensitive Data Protectionto scan your data for sensitive and\n        high-risk information, such as personally identifiable information\n        (PII), financial data, and health information.\nBigQuery lets you share data and insights at scale within and\nacross organizational boundaries. It has a robust security and privacy framework\nthrough a built-in data exchange platform. UsingBigQuery sharing,\nyou can discover, access, and consume a data library that's curated by a wide\nselection of data providers.\nBigQuery offers the following sharing capabilities:\nShare more than data.You can share a wide range of data and AI assets such as\nBigQuery datasets, tables, views, real-time streams with\nPub/Sub topics, SQL stored procedures, and BigQuery ML\nmodels.\nAccess Google datasets.Augment your analytics and ML initiatives with\nGoogle datasets from Search Trends, DeepMind WeatherNext models,\nGoogle Maps Platform, Google Earth Engine, and more.\nIntegrate with data governance\nprinciples.Data owners\nretain control over their data and have the ability to define and configure\nrules or policies to restrict access and usage.\nLive, zero-copy data sharing.Data is shared in place with no\nintegration, data movement, or replication needed, ensuring analysis is\nbased on the latest information. Linked datasets created are a live pointer\nto the shared asset.\nEnhance security posture.You can use access controls to reduce overprovisioning access,\nincluding built-in VPC Service Controls support.\nIncrease visibility with provider usage\nmetrics.Data\npublishers can view and monitor usage for shared assets such as the number\nof jobs executed, total bytes scanned, and subscribers for each\norganization.\nCollaborate on sensitive data with data clean\nrooms.Data clean\nrooms provide a security-enhanced environment in which multiple parties can\nshare, join, and analyze their data assets without moving or revealing the\nunderlying data.\nBuilt on BigQuery.You can build\non the scalability and massive processing capabilities in\nBigQuery, allowing for large scale collaborations.\nThe following table outlines next steps that you can take to learn more about\nsharing features:\nLearn how to create and manageexchangesandlistingsto start sharing within or\n        outside of your organization.\nShare real-time streaming data withPub/Sub topics.\nShare and collaborate on sensitive data withdata clean\n        rooms.\nFurther protect data exfiltration by configuringVPC Service Controlsaround your shared assets.\nCommercializeand sell your assets on Google Cloud Marketplace\nLearn aboutauthentication at Google.\nLearn aboutdata deletion on Google Cloud.\nLearn more aboutIAM best practices.\nLearn theresource hierarchy on Google Cloud.\nLearn aboutIAM on Google Cloud.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-14 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/product-list",
    "title": "No title",
    "content": "Home\nDocumentation\nCloud BillingBilling & cost management tools\nCloud IdentityManage users, devices & apps\nCloud QuotasManage service quotas & usage\nConfig ConnectorKubernetes add-on to manage resources\nDeployment ManagerTemplated infrastructure deployment\nIdentity PlatformCustomer identity access management\nInfra ManagerAutomate infrastructure resource deployment\nManaged Microsoft ADManaged Microsoft Active Directory\nRecommenderCloud usage recommendations & insights\nService CatalogManage internal enterprise solutions\nService UsageManage APIs & services in projects\nCloud TPUHardware acceleration for ML\nColab EnterpriseColab notebooks for enterprises\nDeep Learning ContainersPreconfigured containers for deep learning\nDeep Learning VMPreconfigured VMs for deep learning\nDocAIAnalyze, classify, search documents\nEnterprise Knowledge GraphConsolidate & reconcile organizational knowledge\nImmersive Stream for XRHost & serve interactive 3D & AR\nSpeech-to-TextConvert audio to text\nTensorFlow EnterpriseScalable TensorFlow development experience\nText-to-SpeechConvert text to audio\nTranslationLanguage detection & translation\nTranslation HubManaged document translation service\nVertex AIManaged platform for ML\nVertex AI VisionIngest, analyze & store video data\nVertex AI WorkbenchJupyter-based environment for data science\nVision APIImage recognition & classification\nAPI GatewayFully-managed API gateway\nApigeeNative API management platform\nApp HubView & understand app resources\nApplication IntegrationEnterprise application integrations\nArtifact AnalysisAutomated security scanning\nCloud BuildDevOps automation platform\nCloud CodeIntelliJ Google Cloud tools\nCloud DeployContinuous delivery for GKE\nCloud SchedulerManaged cron job service\nCloud ShellBrowser-based terminal/CLI\nCloud TasksAsynchronous task execution\nCloud WorkstationsCloud-based developer workstations\nDeveloper ConnectConnect source control systems\nEventarcAsynchronous task delivery\ngcloud CLIGoogle Cloud command-line tool\nIntegration ConnectorsEnterprise application connectivity\nPub/SubGlobal real-time messaging\nSecure Source ManagerManaged single-tenant source code repository\nService InfrastructureCross-organization foundational service platform\nSoftware supply chain securitySecure software supply chain\nTools for EclipseEclipse Google Cloud tools\nTools for PowerShellDeveloper powershell tools\nTools for Visual StudioVisual Studio Google Cloud tools\nWorkflowsHTTP services orchestration\nApp EngineManaged app platform\nBlockchain Node EngineFully-managed blockchain nodes\nBlockchain RPCRead/write to multiple blockchains\nCloud RunFully-managed serverless application platform\nGoogle Kubernetes Engine (GKE)Managed Kubernetes/containers\nAI HypercomputerSupercomputer architecture for AI\nBatchManaged batch processing service\nCapacity PlannerView & forecast compute usage\nCluster toolkitDeploy HPC, AI & ML workloads\nCompute EngineVMs, GPUs, TPUs, disks\nMigrate to ContainersMigrate VMs to Kubernetes Engine\nMigrate to VMsMigrate VMs to Compute Engine\nMigration CenterAccelerate end-to-end migration\nShielded VMsVM boot-time protection\nVM ManagerManage OS VM Fleets\nVMware EngineVMware as a service\nWorkload ManagerRule-based workload evaluation\nBigQueryData warehouse & analytics\nBlockchain AnalyticsIndexed blockchain data for analysis\nCloud ComposerManaged workflow orchestration service\nCloud Data FusionGraphically manage data pipelines\nDataflowStream/batch data processing\nDataformELT & SQL workflow tool\nDataplexCentrally manage & govern data\nDataprocManaged Spark & Hadoop\nDataproc ServerlessRun serverless Spark batch workloads\nDatastreamChange data capture/replication service\nLookerEnterprise BI & analytics\nAlloyDB for PostgreSQLScalable & performant PostgreSQL-compatible DB\nBigtablePetabyte-scale, low-latency, non-relational\nCloud SQLManaged MySQL, PostgreSQL, SQL Server\nDatabase Migration ServiceMigrate to Google Cloud databases\nFirestoreServerless NoSQL document DB\nMemorystore for MemcachedManaged Memcached service\nSpannerHorizontally scalable relational DB\nGKE Multi-CloudMulti-cloud cluster creation\nAnti Money Laundering AI (AML AI)Detect potential money laundering activity\nHealthcare Data EngineHealthcare system Google Cloud interoperability\nTalent SolutionsJob search with ML\nTelecom Network AutomationManaged cloud implementation of Nephio\nTelecom Subscriber Insights APIInsights for communication service providers\nVertex AI Search for retailPersonalized search for retail\nCloud CDNCache content near users\nCloud DNSProgrammable DNS serving\nCloud Intrusion Detection System (Cloud IDS)Detect network-based threats\nCloud InterconnectConnect networks to Google\nCloud Load BalancingMulti-region load distribution/balancing\nCloud NATNetwork address translation service\nCloud Next Generation Firewall (Cloud NGFW)Fully-distributed firewall service\nCloud RouterVPC/on-prem network route exchange (BGP)\nCloud Service MeshService mesh traffic management\nCloud VPNVirtual private network connection\nGoogle Cloud ArmorDDoS protection & WAF\nMedia CDNCDN for streaming & videos\nNetwork Connectivity CenterConnect VPC & on-prem\nNetwork Intelligence CenterNetwork monitoring & topology\nNetwork Service TiersPrice versus performance tiering\nSecure Web ProxySecure egress web traffic\nService ExtensionsAdd custom logic for edge applications\nVirtual Private Cloud (VPC)Software-defined networking\nVPC Service ControlsSecurity perimeters for service segregation\nError ReportingApp error reporting\nLoggingCentralized data & event logging\nMonitoringInfrastructure & application monitoring\nProfilerCPU & heap reporting\nTraceApp latency insights\nAccess Context ManagerFine-grained, attribute-based access control\nAccess TransparencyAudit cloud provider support access\nAdvisory NotificationsPrivacy & security event notifications\nAssured OSSGet verified open-source packages\nBinary AuthorizationKubernetes deploy-time security\nCertificate AuthorityCreate & manage private CAs\nCertificate ManagerAcquire & manage TLS certificates\nChrome Enterprise PremiumZero-trust secure access\nCloud Asset InventoryAll assets, one place\nCloud KMSHosted key management service\nConfidential VMEncrypt data in-use\nEndpoint VerificationAccess control for business devices\nGoogle SecOpsTool for SecOps, SIEM & SOAR\nIdentity and Access Management (IAM)Resource access control\nIdentity-Aware ProxyIdentity-based app access\nPolicy IntelligenceUnderstand policies & usage\nreCAPTCHAAdvanced bot & fraud detection\nResource ManagerCloud project metadata management\nSecret ManagerStore & manage secrets\nSecurity Command CenterSecurity management & data risk platform\nSensitive Data ProtectionClassify & redact sensitive data\nService HealthIdentify cloud service disruptions\nWeb RiskCheck URLs for safety\nBackup and DR ServiceBackup & DR SaaS\nCloud StorageMulti-class multi-region object storage\nFilestoreManaged network-attached storage\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-13 UTC."
  },
  {
    "url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCompute Engine\nDocumentation\nGuides\nYou can use GPUs on Compute Engine to accelerate specific workloads on\nyour VMs such as machine learning (ML) and data processing. To use GPUs, you\ncan either deploy an accelerator-optimized VM that has attached GPUs, or\nattach GPUs to an N1 general-purpose VM.\nCompute Engine provides GPUs for your VMs in passthrough mode so that\nyour VMs have direct control over the GPUs and their associated memory.\nFor more information about GPUs on Compute Engine, seeAbout GPUs.\nIf you have graphics-intensive workloads, such as 3D visualization,\n3D rendering, or virtual applications, you can use NVIDIA RTX virtual\nworkstations (formerly known as NVIDIA GRID).\nThis document provides an overview of the different GPU VMs that are\navailable on Compute Engine.\nTo view available regions and zones for GPUs on Compute Engine, seeGPUs regions and zone availability.\nFor compute workloads, GPUs are supported for the following machine types:\nA4 VMs: these VMs have NVIDIA B200 GPUs automatically\nattached.\nA3 VMs: these VMs have NVIDIA H100 80GB or NVIDIA H200\n141GB GPUs automatically attached.\nA2 VMs: these VMs have either NVIDIA A100 80GB or NVIDIA A100 40GB\nGPUs automatically attached.\nG2 VMs: these VMs have NVIDIA L4 GPUs automatically attached.\nN1 VMs: for these VMs, you can attach the following GPU models:\nNVIDIA T4, NVIDIA V100, NVIDIA P100, or NVIDIA P4.\nTo use NVIDIA B200 GPUs (nvidia-b200), you must use anA4 accelerator-optimizedmachine type. Each A4 machine type has a fixed GPU count, vCPU count, and memory\nsize.\n*GPU memory is the memory on a GPU device that can be used for temporary storage of\ndata. It is separate from the VM's memory and is specifically designed to handle the higher\nbandwidth demands of your graphics-intensive workloads.†A vCPU is implemented as a single hardware hyper-thread on one of\nthe availableCPU platforms.‡Maximum egress bandwidth cannot exceed the number given. Actual\negress bandwidth depends on the destination IP address and other factors.\nSeeNetwork bandwidth.\n*GPU memory is the memory on a GPU device that can be used for temporary storage of\ndata. It is separate from the VM's memory and is specifically designed to handle the higher\nbandwidth demands of your graphics-intensive workloads.†A vCPU is implemented as a single hardware hyper-thread on one of\nthe availableCPU platforms.‡Maximum egress bandwidth cannot exceed the number given. Actual\negress bandwidth depends on the destination IP address and other factors.\nSeeNetwork bandwidth.\nTo use NVIDIA H100 80GB or NVIDIA H200 141GB GPUs, you must use anA3 accelerator-optimizedmachine type. Each A3 machine type has a fixed GPU count, vCPU count, and memory\nsize.\nTo use NVIDIA H200 141GB GPUs, you must use the A3 Ultra machine type.\nThis machine type has H200 141GB GPUs (nvidia-h200-141gb)\nand provide the highest network performance. They are ideal for foundation\nmodel training and serving.\n*GPU memory is the memory on a GPU device that can be used for\ntemporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.†A vCPU is implemented as a single hardware hyper-thread on one of\nthe availableCPU platforms.‡Maximum egress bandwidth cannot exceed the number given. Actual\negress bandwidth depends on the destination IP address and other factors.\nSeeNetwork bandwidth.\n*GPU memory is the memory on a GPU device that can be used for\ntemporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.†A vCPU is implemented as a single hardware hyper-thread on one of\nthe availableCPU platforms.‡Maximum egress bandwidth cannot exceed the number given. Actual\negress bandwidth depends on the destination IP address and other factors.\nSeeNetwork bandwidth.\nTo use NVIDIA H100 80GB you have the following options:\nA3 Mega: these machine types have H100 80GB GPUs (nvidia-h100-mega-80gb)\nand are ideal for large-scale training and serving workloads.\nA3 High: these machine types have H100 80GB GPUs (nvidia-h100-80gb) and\nare well-suited for both training and serving tasks.\nA3 Edge: these machine types have H100 80GB GPUs (nvidia-h100-80gb),\nare designed specifically for serving, and are available in\nalimited set of regions.\nTo create Google Kubernetes Engine cluster, seeDeploy an A3 Mega cluster\n  with GKE.\nTo create a Slurm cluster, seeDeploy an A3 Mega Slurm cluster.\nTo create Spot VMs, set the provisioning model toSPOTwhen youCreate an accelerator-optimized\n    VM.\nTo create a resize request in a MIG, which uses DWS, seeCreate a MIG with GPU VMs.\nTo get started with A3 Edge machines, seeCreate an A3 VM with GPUDirect-TCPX enabled.\n800:for asia-south1 and northamerica-northeast2\n400:for all otherA3 Edge regions\n*GPU memory is the memory on a GPU device that can be used for\ntemporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.†A vCPU is implemented as a single hardware hyper-thread on one of\nthe availableCPU platforms.‡Maximum egress bandwidth cannot exceed the number given. Actual\negress bandwidth depends on the destination IP address and other factors.\nSeeNetwork bandwidth.\n*GPU memory is the memory on a GPU device that can be used for\ntemporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.†A vCPU is implemented as a single hardware hyper-thread on one of\nthe availableCPU platforms.‡Maximum egress bandwidth cannot exceed the number given. Actual\negress bandwidth depends on the destination IP address and other factors.\nSeeNetwork bandwidth.\nTo useNVIDIA A100GPUs on\nGoogle Cloud, you must use anA2 accelerator-optimizedmachine type. Each A2 machine type has a fixed GPU count, vCPU count, and memory size.\nA2 machine series are available in two types:\nA2 Ultra: these machine types have A100 80GB GPUs (nvidia-a100-80gb) and\nLocal SSD disks attached.\nA2 Standard: these machine types have A100 40GB GPUs (nvidia-tesla-a100)\nattached.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\nTo useNVIDIA L4GPUs\n(nvidia-l4ornvidia-l4-vws), you must use aG2 accelerator-optimizedmachine type.\nEach G2 machine type has a fixed number ofNVIDIA L4 GPUsand vCPUs attached. Each G2 machine type also has a default memory and a custom\nmemory range. The custom memory range defines the amount of memory that\nyou can allocate to your VM for each machine type. You can specify your custom\nmemory during VM creation.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\nYou can attach the following GPU models to anN1 machine typewith the\nexception of the N1 shared-core machine type.\nN1 VMs with lower numbers of GPUs are limited to a maximum number of vCPUs.\nIn general, a higher number of GPUs lets you create VM instances with a higher\nnumber of vCPUs and memory.\nYou can attachNVIDIA T4GPUs to N1 general-purpose VMs with the following VM configurations.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\nYou can attachNVIDIA P4GPUs to N1 general-purpose VMs with the following VM configurations.\n*GPU memory is the memory that is available on a GPU device\nthat can be used for temporary storage of data. It is separate from the VM's\nmemory and is specifically designed to handle the higher bandwidth demands of\nyour graphics-intensive workloads.†For VMs with attached NVIDIA P4 GPUs, Local SSD disks\nare only supported in zonesus-central1-candnorthamerica-northeast1-b.\n*GPU memory is the memory that is available on a GPU device\nthat can be used for temporary storage of data. It is separate from the VM's\nmemory and is specifically designed to handle the higher bandwidth demands of\nyour graphics-intensive workloads.\n†For VMs with attached NVIDIA P4 GPUs, Local SSD disks\nare only supported in zonesus-central1-candnorthamerica-northeast1-b.\nYou can attachNVIDIA V100GPUs to N1 general-purpose VMs with the following VM configurations.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.†For VMs with attached NVIDIA V100 GPUs, Local SSD disks\naren't supported inus-east1-c.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.†For VMs with attached NVIDIA V100 GPUs, Local SSD disks\naren't supported inus-east1-c.\nYou can attachNVIDIA P100GPUs to N1 general-purpose VMs with the following VM configurations.\nFor some NVIDIA P100 GPUs, the maximum CPU and memory that is available for\nsome configurations is dependent on the zone in which the GPU resource is running.\n1 to 64(us-east1-c, europe-west1-d, europe-west1-b)\n1 to 96(all P100 zones)\n1 to 208(us-east1-c, europe-west1-d, europe-west1-b)\n1 to 624(all P100 zones)\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\n*GPU memory is the memory available on a GPU device that can be used\nfor temporary storage of data. It is separate from the VM's memory and is\nspecifically designed to handle the higher bandwidth demands of your\ngraphics-intensive workloads.\nIf you have graphics-intensive workloads, such as 3D visualization, you can\ncreate virtual workstations that useNVIDIA RTX Virtual Workstations (vWS)(formerly known as NVIDIA GRID). When you create a virtual\nworkstation, an NVIDIA RTX Virtual Workstation (vWS) license is automatically added\nto your VM.\nFor information about pricing for virtual workstations, seeGPU pricing page.\nFor graphics workloads, NVIDIA RTX virtual workstation (vWS) models are available:\nG2 machine series: forG2 machine typesyou can enable\nNVIDIA L4 Virtual Workstations (vWS):nvidia-l4-vws\nG2 machine series: forG2 machine typesyou can enable\nNVIDIA L4 Virtual Workstations (vWS):nvidia-l4-vws\nN1 machine series: forN1 machine types, you can enable the following\nvirtual workstations:NVIDIA T4 Virtual Workstations:nvidia-tesla-t4-vwsNVIDIA P100 Virtual Workstations:nvidia-tesla-p100-vwsNVIDIA P4 Virtual Workstations:nvidia-tesla-p4-vws\nN1 machine series: forN1 machine types, you can enable the following\nvirtual workstations:\nNVIDIA T4 Virtual Workstations:nvidia-tesla-t4-vws\nNVIDIA P100 Virtual Workstations:nvidia-tesla-p100-vws\nNVIDIA P4 Virtual Workstations:nvidia-tesla-p4-vws\nThe following table describes the GPU memory size, feature availability,\nand ideal workload types of different GPU models that are available on\nCompute Engine.\nTo compare GPU pricing for the different GPU models and regions that are\navailable on Compute Engine, seeGPU pricing.\nThe following table describes the performance specifications of different GPU\nmodels that are available on Compute Engine.\n*To allow FP64 code to work correctly, a small number of FP64\n  hardware units are included in the T4, L4, and P4 GPU architecture.†TeraOperations per Second.\n*To allow FP64 code to work correctly, a small number of FP64\n  hardware units are included in the T4, L4, and P4 GPU architecture.†TeraOperations per Second.\n*For mixed precision training, NVIDIA B200, H200, H100, A100, and\nL4 GPUs also support thebfloat16data type.†For NVIDIA B200, H200, H100 and L4 GPUs, structural sparsity is\nsupported which you can use to double the performance value. The values shown\nare with sparsity. Specifications are one-half lower without sparsity.\n*For mixed precision training, NVIDIA B200, H200, H100, A100, and\nL4 GPUs also support thebfloat16data type.†For NVIDIA B200, H200, H100 and L4 GPUs, structural sparsity is\nsupported which you can use to double the performance value. The values shown\nare with sparsity. Specifications are one-half lower without sparsity.\nFor more information about GPUs on Compute Engine,\nseeAbout GPUs.\nReview theGPU regions and zones availability.\nReviewNetwork bandwidths and GPUs.\nLearn aboutGPU pricing.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-14 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/get-started",
    "title": "No title",
    "content": "Home\nDocumentation\nClick to show or hide setup steps by job function:\nEstablish administrators, billing accounts, and other settings in your Google Cloud environment.\nCloud Quotas overview\nGoogle Cloud deployment archetypes (Architecture Center)\nSet up billing, spending notifications, and resource structure to facilitate cost monitoring and optimization.\nMonitor costs using billing reports\nOptimize costs with FinOps hub\nResource hierarchy options for cost tracking\nImplement cost optimization strategies (Architecture Center)\nStart automating infrastructure and secure collaboration with teammates using Google Cloud tools and best practices.\nObservability in Google Cloud\nTerraform and Infrastructure Manager\nCI/CD pipeline for containerized apps (Architecture Center)\nGet basic API access and set up a development environment that can interact with Google Cloud services.\nBuild a generative AI application\nAnalyze sample data using Google Cloud products with minimal setup.\nSet up the bq command-line tool\nGemini in BigQuery overview\nData analytics design patterns (Architecture Center)\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/memorystore/docs/cluster",
    "title": "Memorystore for Redis Cluster documentation",
    "content": "Home\nDocumentation\nMemorystore\nMemorystore for Redis Cluster\nMemorystore for Redis Cluster is a fully managed Redis service for Google Cloud. Applications running\n    on Google Cloud can achieve extreme performance by leveraging the highly scalable, available,\n    secure Redis service without the burden of managing complex Redis deployments.\nMemorystore for Redis Cluster distributes (or \"shards\") your data across primary nodes and\n    replicates your data across optional replica nodes to ensure high availability. The horizontally\n    scalable cluster architecture provides better performance over vertically scalable architecture\n    because Redis performance is better on many smaller nodes instead of fewer larger nodes.\nMemorystore for Redis Cluster is based on and is compatible with open-source Redis\n    versions 7.2 and earlier and supports a subset of the total Redis command library.\nNot sure what database option is right for you? Learn more about ourdatabase services.\nLearn moreabout Memorystore for Redis Cluster.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nMemorystore for Redis Cluster overview\nMemorystore for Redis Cluster overview\nHigh availability and replicas\nHigh availability and replicas\nCluster and node specification\nCluster and node specification\nGeneral best practices\nGeneral best practices\nOperational guidelines\nOperational guidelines\nClient library code samples\nClient library code samples\nSupported commands\nSupported commands\nTerraform reference\nTerraform reference\nCreate instances\nCreate instances\nConnect to an instance\nConnect to an instance\nNetworking\nNetworking\nAbout in-transit encryption\nAbout in-transit encryption\nREST API reference\nREST API reference\nLocations\nLocations\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nRelease notes\nRelease notes\nService Level Agreement\nService Level Agreement\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Redis is a trademark of Redis Ltd. All rights therein are reserved to Redis Ltd. Any use by Google is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and Google. Memorystore is based on and is compatible with open-source Redis versions 7.2 and earlier and supports a subset of the total Redis command library.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nVector Search is a powerful vector search engine built on groundbreaking\ntechnology developed by Google Research. Leveraging theScaNNalgorithm, Vector Search lets you build next-generation search and\nrecommendation systems as well as generative AI applications.\nYou can benefit from the very same research and technology that power core Google\nproducts, including Google Search, YouTube, and Google Play.\nThis means you get the scalability, availability, and performance that's\ntrusted to handle massive datasets and deliver lightning-fast results at a\nglobal scale. With Vector Search, you have an enterprise-grade\nsolution for implementing cutting-edge semantic search capabilities in your own\napplications.\nBlog:Multimodal search with Vector Search\nNext 24 Infinite Nature Demo\nInfinite Fleurs: Discover AI-assisted creativity in full bloom\nExperience multimodal\n    AI with manga ONE PIECE\nVector Search interactive demo:\nCheck out the live demo for a realistic example of what vector search technology\ncan do and get a headstart with Vector Search.\nVector Search quickstart: Try\nVector Search in 30 minutes by building, deploying, and querying a\nVector Search index using a sample dataset. This tutorial covers setup, data\npreparation, index creation, deployment, querying, and cleanup.\nBefore you begin: Prepare your embeddings by\nchoosing and training a model, and preparing your data. Then, choose a public\nor private endpoint to deploy your query index to.\nVector Search pricing and pricing calculator:\nVector Search pricing includes the cost of virtual machines used\nto host deployed indexes, as well as expenses for building and updating indexes.\nEven a minimal setup (under $100 per month) can accommodate high throughput for\nmoderate-sized use cases. To estimate your monthly costs:\nGo toGoogle Cloud's pricing calculator.\nClickAdd to estimate.\nSearch for Vertex AI.\nClick theVertex AIbutton.\nChooseVertex AI Vector Searchfrom theService typedrop-down.\nKeep the default settings or configure your own. The estimated cost per month\nis shown in theCost detailspanel.\nManage Indexes and EndpointsIndex OverviewConfigure an indexCreate an indexCreate an index endpointQuery dataAdvanced topicsHybrid search\nManage Indexes and Endpoints\nIndex Overview\nConfigure an index\nCreate an index\nCreate an index endpoint\nQuery data\nAdvanced topics\nHybrid search\nAPIsVertex SDK versus Client LibrariesInstall the Vertex AI client librariesVertex AI SDK for PythonClass MatchingEngineIndexClass MatchingEngineIndexEndpointPython ClientPackage index_endpoint_servicePackage index_servicePackage match_serviceJava ClientClass IndexServiceClientClass IndexEndpointServiceClientClass MatchServiceClient\nAPIs\nVertex SDK versus Client Libraries\nInstall the Vertex AI client libraries\nVertex AI SDK for Python\nClass MatchingEngineIndex\nClass MatchingEngineIndexEndpoint\nPython Client\nPackage index_endpoint_service\nPackage index_service\nPackage match_service\nJava Client\nClass IndexServiceClient\nClass IndexEndpointServiceClient\nClass MatchServiceClient\nMore resourcesVector Search quotas and limitGet support\nMore resources\nVector Search quotas and limit\nGet support\nVector search technology is becoming a central hub for businesses using AI.\nSimilar to how relational databases function in IT systems, it connects various\nbusiness elements like documents, content, products, users, events, and other\nentities based on their relevance. Beyond searching conventional media like\ndocuments and images, Vector Search can also power intelligent\nrecommendations, match business problems with solutions, and even link IoT signals\nto monitoring alerts. It's a versatile tool that's essential for navigating the\ngrowing landscape of AI-enabled enterprise data.\nSearch / Information Retrieval\nRecommendationSystems\nHow Vertex AI vector search helps unlock high-performance gen AI apps:Vector Search powers diverse applications, including ecommerce,\n      RAG systems, and recommendation engines, alongside chatbots, multimodal\n      search, and more. Hybrid search further enhances results for niche terms.\n      Customers like Bloomreach, eBay, and Mercado Libre use Vertex AI\n      for its performance, scalability, and cost-effectiveness, achieving\n      benefits like faster search and increased conversions.\neBay uses Vector Search for recommendations:Highlights how eBay uses Vector Search for its recommendation\n      system. This technology allows eBay to find similar products within its\n      extensive catalog, improving the user experience.\nMercari leverages Google's vector search technology to create a new marketplace:Explains how Mercari uses Vector Search to improve its new\n      marketplace platform. Vector Search powers the platform's\n      recommendations, helping users find relevant products more effectively.\nVertex AI Embeddings for Text: Grounding LLMs made easy:Focuses on grounding LLMs using Vertex AI Embeddings for text data.\n      Vector Search plays an important role in finding relevant text\n      passages that ensure the model's responses are grounded in factual information.\nWhat is Multimodal Search: \"LLMs with vision\" change businesses:Discusses Multimodal Search, which combines LLMs with visual understanding.\n      It explains how Vector Search processes and compares both text\n      and image data, allowing for more comprehensive search experiences.\nUnlock multimodal search at scale: Combine text & image power with Vertex AI:Describes building a multimodal search engine with Vertex AI that\n      combines text and image search using a weighted Rank-Biased Reciprocal Rank\n      ensemble method. This improves user experience and provides more relevant\n      results.\nScaling deep retrieval with TensorFlow Recommenders and Vector Search:Explains how to build a playlist recommendation system using TensorFlow\n      Recommenders and Vector Search, covering deep retrieval models,\n      training, deployment, and scaling.\nGen AI: retrieval for RAG and Agents\nVertex AI and Denodo unlock enterprise data with Gen AI:Showcases how Vertex AI's integration with Denodo enables businesses\n      to use generative AI for gaining insights from their data. Vector Search\n      is key for efficiently accessing and analyzing relevant data within an\n      enterprise environment.\nInfinite Nature and the nature of industries: This 'wild' demo shows the\n      diverse possibilities of AI:Showcases a demo that illustrates AI's potential across different industries.\n      It utilizes Vector Search to power generative recommendations\n      and multimodal semantic search.\nInfinite Fleurs: Discover AI-assisted creativity in full bloom:Google's Infinite Fleurs, an AI experiment using Vector Search,\n      Gemini and Imagen models, generates unique flower bouquets based on\n      user prompts. This technology showcases AI's potential to inspire creativity\n      across various industries.\nLlamaIndex for RAG on Google Cloud:Describes how to use LlamaIndex to facilitate Retrieval Augmented Generation\n      (RAG) with large language models. LlamaIndex utilizes Vector Search\n      to retrieve relevant information from a knowledge base, resulting in more\n      accurate and contextually appropriate responses.\nRAG and grounding on Vertex AI:Examines RAG and grounding techniques on Vertex AI.\n      Vector Search helps identify relevant grounding information\n      during retrieval, which makes generated content more accurate and reliable.\nVector Search on LangChain:provides a guide to using Vector Search with LangChain for\n      building and deploying a vector database index for text data, including\n      question-answering and PDF processing.\nBI, data analytics, monitoring, and more\nEnabling real-time AI with Streaming Ingestion in Vertex AI:Explores Streaming Update in Vector Search and how it provides\n    real-time AI capabilities. This technology allows for real-time processing\n    and analysis of incoming data streams.\nYou can use the following resources to get started with Vector Search:\nNotebooks and solutions\nTutorials and training\nRelated products\nIn-depth deep dive material\nVertex AI Vector Search Quickstart:Provides an overview of Vector Search. It is designed for users\n      who are new to the platform and want to get started quickly.\nGetting Started with Text Embeddings and Vector Search:Introduces text embeddings and vector search. It explains how these\n      technologies work and how they can be used to improve search results.\nCombining Semantic & Keyword Search: A Hybrid Search Tutorial with Vertex AI\n      Vector Search:Provides instructions on how to use Vector Search for hybrid search.\n      It covers the steps involved in setting up and configuring a hybrid\n      search system.\nVertex AI RAG Engine with Vector Search:Explores the use of Vertex AI RAG Engine with Vector Search.\n      It discusses the benefits of using these two technologies together and\n      provides examples of how they can be used in real-world applications.\nInfrastructure for a RAG-capable generative AI application using Vertex AI\n      and Vector Search:Details the architecture\n      for building a generative AI application and RAG using\n      Vector Search, Cloud Run and Cloud Storage,\n      covering use cases, design choices, and key considerations.\nGetting Started with Vector Search and EmbeddingsVector Search is used to find similar or related items. It can be\nused for recommendations, search, chatbots, and text classification. The process\ninvolves creating embeddings, uploading them to Google Cloud, and indexing them\nfor querying. This lab focuses on text embeddings using Vertex AI, but\nembeddings can be generated for other data types.\nVector Search and EmbeddingsThis course introduces Vector Search and describes how it can be\nused to build a search application with large language model (LLM) APIs for\nembeddings. The course consists of conceptual lessons on Vector Search\nand text embeddings, practical demos on how to build Vector Search\non Vertex AI, and a practice lab.\nUnderstanding and Applying Text EmbeddingsThe Vertex AI Embeddings API generates text embeddings, which arenumerical representations of text used for tasks like identifying similar items.\nIn this course, you'll use text embeddings for tasks like classification and\nsemantic search, and combine semantic search with LLMs to build question-answering\nsystems using Vertex AI.\nMachine Learning Crash Course: EmbeddingsThis course introduces word embeddings, contrasting them with sparse representations.\nIt explores methods for obtaining embeddings and differentiates between static\nand contextual embeddings.\nVertex AI EmbeddingsProvides an overview of Embeddings API. Text and multimodal embedding use cases,\nalong with links to additional resources and related Google Cloud services.\nAI Applications ranking APIThe ranking API reranks documents based on relevance to a query using a pre-trained\nlanguage model, providing precise scores. It's ideal for improving search results\nfrom various sources including Vector Search.\nVertex AI Feature StoreLets you manage and serve feature data using BigQuery as the data source.\nIt provisions resources for online serving, acting as a metadata layer to serve\nthe latest feature values directly from BigQuery. Feature Store allows\nfor the instant retrieval of feature values for the items Vector Store returned\nfor queries.\nVertex AI PipelinesVertex AI Pipelines enables the automation, monitoring, and governance\nof your ML systems in a serverless manner by orchestrating ML workflows with ML\npipelines. You can run ML pipelines defined using Kubeflow Pipelines or the\nTensorFlow Extended (TFX) framework in batches. Pipelines allows for\nbuilding automated pipelines to generate embeddings, create and update\nVector Search indexes, and form an MLOps setup for production search\nand recommendation systems.\nEnhancing your gen AI use case with Vertex AI embeddings and task typesFocuses on improving Generative AI applications using Vertex AI Embeddings\nand task types. Vector Search can be used with task type embeddings\nto enhance the context and accuracy of generated content by finding more relevant\ninformation.\nTensorFlow RecommendersAn open-source library for building recommendation systems. It simplifies the\nprocess from data preparation to deployment and supports flexible model building.\nTFRS offers tutorials and resources and enables the creation of sophisticated\nrecommendation models.\nTensorFlow RankingTensorFlow Ranking is an open-source library for building scalable\nneural learning-to-rank (LTR) models. It supports various loss functions and\nranking metrics, with applications in search, recommendation, and other fields.\nThe library is actively developed by Google AI.\nAnnouncing ScaNN: Efficient Vector Similarity SearchGoogle's ScaNN, an algorithm for efficient vector similarity search, utilizes\na novel technique to improve accuracy and speed in finding nearest neighbors.\nIt outperforms existing methods and has broad applications in machine learning\ntasks requiring semantic search. Google's research efforts span various areas,\nincluding foundational ML and societal impacts of AI.\nSOAR: New algorithms for even faster Vector Search with ScaNNGoogle's SOAR algorithm improves Vector Search efficiency by introducing\ncontrolled redundancy, allowing faster searches with smaller indexes. SOAR assigns\nvectors to multiple clusters, creating \"backup\" search paths for improved performance.\nGet Started with Vector Search using Vertex AI\nVector Search is a powerful tool for building AI-powered applications.\nThis video introduces the technology and provides a step-by-step guide to getting\nstarted.\nLearn Hybrid Search with Vector Search\nVector Search can be used for hybrid search, allowing you to combine\nthe power of vector search with the flexibility and speed of a conventional\nsearch engine. This video introduces hybrid search and shows you how to use\nVector Search for hybrid search.\nYou're Already Using Vector Search! Here's How to Be an Expert\nDid you know you're probably using vector search every day without realizing it?\nFrom finding that elusive product on social media to tracking down a song stuck\nin your head, vector search is the AI magic behind these everyday experiences.\nNew \"task type\" embedding from the DeepMind team improves RAG search quality\nImprove the accuracy and relevance of your RAG systems with newtask typeembeddings developed by the Google DeepMind team. Watch along and learn about\nthe common challenges in RAG search quality and how task type embeddings can\neffectively bridge the semantic gap between questions and answers, leading to\nmore effective retrieval and enhanced RAG performance.\nThis list contains some important terminology that you'll need to understand to\nuse Vector Search:\nVector: A vector is a list of float values that has magnitude and direction.\nIt can be used to represent any kind of data, such as numbers, points in space,\nand directions.\nVector: A vector is a list of float values that has magnitude and direction.\nIt can be used to represent any kind of data, such as numbers, points in space,\nand directions.\nEmbedding: An embedding is a type of vector that's used to represent\ndata in a way that captures its semantic meaning. Embeddings are typically\ncreated using machine learning techniques, and they are often used in natural\nlanguage processing (NLP) and other machine learning applications.Dense embeddings: Dense embeddings represent the semantic meaning\nof text, using arrays that mostly contain non-zero values. With dense\nembeddings, similar search results can be returned based on semantic\nsimilarity.Sparse embeddings: Sparse embeddings represent text syntax,\nusing high-dimensional arrays that contain very few non-zero values compared\nto dense embeddings. Sparse embeddings are often used for keyword searches.\nEmbedding: An embedding is a type of vector that's used to represent\ndata in a way that captures its semantic meaning. Embeddings are typically\ncreated using machine learning techniques, and they are often used in natural\nlanguage processing (NLP) and other machine learning applications.\nDense embeddings: Dense embeddings represent the semantic meaning\nof text, using arrays that mostly contain non-zero values. With dense\nembeddings, similar search results can be returned based on semantic\nsimilarity.\nDense embeddings: Dense embeddings represent the semantic meaning\nof text, using arrays that mostly contain non-zero values. With dense\nembeddings, similar search results can be returned based on semantic\nsimilarity.\nSparse embeddings: Sparse embeddings represent text syntax,\nusing high-dimensional arrays that contain very few non-zero values compared\nto dense embeddings. Sparse embeddings are often used for keyword searches.\nSparse embeddings: Sparse embeddings represent text syntax,\nusing high-dimensional arrays that contain very few non-zero values compared\nto dense embeddings. Sparse embeddings are often used for keyword searches.\nHybrid search: Hybrid search uses both dense and sparse embeddings, which\nlets you search based on a combination of keyword search and\nsemantic search. Vector Search supports search based on dense\nembeddings, sparse embeddings, and hybrid search.\nHybrid search: Hybrid search uses both dense and sparse embeddings, which\nlets you search based on a combination of keyword search and\nsemantic search. Vector Search supports search based on dense\nembeddings, sparse embeddings, and hybrid search.\nIndex: A collection of vectors deployed together for similarity search.\nVectors can be added to or removed from an index. Similarity search\nqueries are issued to a specific index and search the vectors in that index.\nIndex: A collection of vectors deployed together for similarity search.\nVectors can be added to or removed from an index. Similarity search\nqueries are issued to a specific index and search the vectors in that index.\nGround truth: A term that refers to verifying machine\nlearning for accuracy against the real world, like a ground truth dataset.\nGround truth: A term that refers to verifying machine\nlearning for accuracy against the real world, like a ground truth dataset.\nRecall: The percentage of nearest neighbors returned by the index that are\nactually true nearest neighbors. For example, if a nearest neighbor query\nfor 20 nearest neighbors returned 19 of the ground truth nearest neighbors,\nthe recall is 19/20x100 = 95%.\nRecall: The percentage of nearest neighbors returned by the index that are\nactually true nearest neighbors. For example, if a nearest neighbor query\nfor 20 nearest neighbors returned 19 of the ground truth nearest neighbors,\nthe recall is 19/20x100 = 95%.\nRestrict: Feature that limits searches to a subset of the index by\nusing Boolean rules. Restrict is also referred to as \"filtering\". With\nVector Search, you can use numeric filtering and text attribute\nfiltering.\nRestrict: Feature that limits searches to a subset of the index by\nusing Boolean rules. Restrict is also referred to as \"filtering\". With\nVector Search, you can use numeric filtering and text attribute\nfiltering.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/migrate/virtual-machines/docs/5.0",
    "title": "Migrate to Virtual Machines documentation",
    "content": "Home\nMigrate to Virtual Machines\nDocumentation\nMigrate to Virtual Machines\nMigrate to Virtual Machines lets you migrate virtual machine (VM) instances and disks of VMs from\n  different migration sources such as vSphere on-premises data center, AWS cloud computing services,\n  Azure cloud computing services, and Google Cloud VMware Engine to VM instances or Persistent Disk volumes\n  on Google Cloud.Learn more.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nVM Migration lifecycle\nVM Migration lifecycle\nMigrate to Virtual Machines Architecture\nMigrate to Virtual Machines Architecture\nEnabling Migrate to Virtual Machines services\nEnabling Migrate to Virtual Machines services\nMigrate from an on-premises VMware source\nMigrate from an on-premises VMware source\nMigrate from VMware Engine\nMigrate from VMware Engine\nMigrate from an AWS source\nMigrate from an AWS source\nMigrate from an Azure source\nMigrate from an Azure source\nMigrate individual VMs\nMigrate individual VMs\nMigrate VM disks\nMigrate VM disks\nImport virtual disk images\nImport virtual disk images\nImport machine images\nImport machine images\nRelease notes\nRelease notes\nSupported operating systems\nSupported operating systems\nLocations\nLocations\nTroubleshooting\nTroubleshooting\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/storage-transfer/docs/overview",
    "title": "What is Storage Transfer Service?Stay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCloud Storage Transfer Service\nDocumentation\nGuides\nStorage Transfer Service enables seamless data movement across object and file\nstorage systems, including:\nAmazon S3, Azure Blob Storage, or Cloud Storage to Cloud Storage\nOn-premises storage to Cloud Storage, or Cloud Storage to on-premises\nBetween on-premises storage systems\nFrom publicly-accessible URLs to Cloud Storage\nFrom HDFS to Cloud Storage\nStorage Transfer Service is optimized for transfers involving more than 1TiB of data.\nFor smaller transfers, see ourrecommendations.\nWith Storage Transfer Service, you can:\nAutomate data transfers: Eliminate the need for manual processes and custom scripts.\nTransfer data at scale: Move petabytes of data quickly and reliably.\nOptimize network performance: Choose between Google-managed transfers for simplicity or self-hosted agents for granular control over network routing and bandwidth consumption.\nSupport diverse storage systems: Transfer data seamlessly between cloud providers and on-premises environments.\nMigrating data to Cloud Storage: Storage Transfer Service can be used to migrate\ndata from other cloud storage providers, on-premises data centers, or\nHTTP/HTTPS URLs to Cloud Storage.\nBackup: Replicate your data to Google Cloud, or create a copy of a\nCloud Storage bucket in another region\nData processing pipelines: Move data generated on other clouds, your data\ncenter, and the edge to Google Cloud for analytics usingBigQueryorDataproc.\nArchival: Move cold data from costly on-premises storage systems to\nCloud Storage to reduce storage cost.\nSecure\nEnd-to-end encryption protects your data in transit. Storage Transfer Service\nsupports TLS 1.3 for all HTTPS communication.\nData integrity validation ensures that your data is not corrupted during\ntransfer.\nPerformant\nHighly-parallelized architecture accelerates transfer speeds.\nAutomatic retries and load balancing ensure reliable transfers.\nFully managed\nNo need to manage infrastructure or write code.\nFocus on your applications, not data transfer.\nCheck to see if yoursource and sink combination is supported.\nThere are a number of ways that you can work with Storage Transfer Service:\nThe Google Cloud console\nREST APIs\nThegcloudcommand-line tool\nJava and Python client libraries\nSeeCreate transfersto get started.\nStorage Transfer Service does not provide an SLA, including for transfer performance or\nlatency, and some performance fluctuations may occur.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-17 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/get-started/",
    "title": "No title",
    "content": "Home\nDocumentation\nClick to show or hide setup steps by job function:\nEstablish administrators, billing accounts, and other settings in your Google Cloud environment.\nCloud Quotas overview\nGoogle Cloud deployment archetypes (Architecture Center)\nSet up billing, spending notifications, and resource structure to facilitate cost monitoring and optimization.\nMonitor costs using billing reports\nOptimize costs with FinOps hub\nResource hierarchy options for cost tracking\nImplement cost optimization strategies (Architecture Center)\nStart automating infrastructure and secure collaboration with teammates using Google Cloud tools and best practices.\nObservability in Google Cloud\nTerraform and Infrastructure Manager\nCI/CD pipeline for containerized apps (Architecture Center)\nGet basic API access and set up a development environment that can interact with Google Cloud services.\nBuild a generative AI application\nAnalyze sample data using Google Cloud products with minimal setup.\nSet up the bq command-line tool\nGemini in BigQuery overview\nData analytics design patterns (Architecture Center)\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction",
    "title": "Introduction to Vertex AI SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nAI Applications\nDocumentation\nGuides\nThis page introduces the key search and recommendations features of\nVertex AI Search.\nFor information about search and recommendations for media, seeIntroduction to\nAI Applications for media.\nVertex AI Search brings together the power of deep information\nretrieval, state-of-the-art natural language processing, and the latest in large\nlanguage model (LLM) processing to understand user intent and return the most\nrelevant results for the user.\nWith Vertex AI Search, you can build a Google-quality search app\non data you control. You also have the option to use the search results that you\nretrieve to ground generative AI LLM responses. For more information, see the \nblog postYour RAG powered by Google Search.\nWith recommendations, you can build a recommendations app across your data\nthat suggests content similar to the content that the user is viewing.\nVertex AI Search makes it easy to get started with high-quality search\nor recommendations based on data that you provide. As part of the setup\nexperience, you can:\nUse your existing Google Account or sign up for one.\nUse your existing Google Cloud project or create one.\nCreate an app and attach a data store to it. Provide data to search or\nrecommend by entering the URLs for your website content, importing your data\nfrom BigQuery or Cloud Storage, or importing FHIR R4 data\nfrom Cloud Healthcare API, or uploading through RESTful CRUD APIs. Syncing data\nfrom third-party data sources is available in Preview with allowlist.\nEmbed JavaScript widgets and API samples to integrate search or\nrecommendations into your website or applications.\nWith Vertex AI Search, you create a search or recommendations app and\nattach it to a data store. You import your data into a data store and index your\ndata. Apps and data stores have a one-to-one relationship.\nThere are various kinds of data stores that you can create, based on the type of\ndata you use. Each data store can contain one type of data:\nWebsite data: You can provide domains such asyourexamplewebsite.com/faqandyourexamplewebsite.com/eventsand enable\nsearch over the content at those domains.\nWebsite data: You can provide domains such asyourexamplewebsite.com/faqandyourexamplewebsite.com/eventsand enable\nsearch over the content at those domains.\nStructured data: A data store with structured data enables hybrid search\n(keyword and semantic) or recommendations over structured data such as a\nBigQuery table or NDJSON files. For example, you can enable search\nor recommendations over a product catalog for your ecommerce experience, a\nmovie catalog for movie search or recommendations, or a directory of doctors\nfor provider search or recommendations.\nStructured data: A data store with structured data enables hybrid search\n(keyword and semantic) or recommendations over structured data such as a\nBigQuery table or NDJSON files. For example, you can enable search\nor recommendations over a product catalog for your ecommerce experience, a\nmovie catalog for movie search or recommendations, or a directory of doctors\nfor provider search or recommendations.\nStructured data for media: A data store with a structured data schema\nthat is specific for the media industry. For example, a data store for media\nmight contain information about videos, news articles, music files, or\npodcasts.\nStructured data for media: A data store with a structured data schema\nthat is specific for the media industry. For example, a data store for media\nmight contain information about videos, news articles, music files, or\npodcasts.\nUnstructured data: An unstructured data store enables hybrid search\n(keyword and semantic) over data such as documents and images. For example, a\nfinancial institution can enable search over their private corpus (index) of\nfinancial research publications, or a biotech company can enable\nsearch over their private repository of medical research.\nUnstructured data: An unstructured data store enables hybrid search\n(keyword and semantic) over data such as documents and images. For example, a\nfinancial institution can enable search over their private corpus (index) of\nfinancial research publications, or a biotech company can enable\nsearch over their private repository of medical research.\nHealthcare data: A healthcare data store enables hybrid search (keyword\nand semantic) over healthcare FHIR R4 data imported from Cloud Healthcare API.\nFor example, a healthcare provider can search over a patient's clinical\nhistory using exploratory queries.\nHealthcare data: A healthcare data store enables hybrid search (keyword\nand semantic) over healthcare FHIR R4 data imported from Cloud Healthcare API.\nFor example, a healthcare provider can search over a patient's clinical\nhistory using exploratory queries.\nFor more information, seeAbout apps and data stores.\nYou can implement Vertex AI Search in any of the following ways:\nUse the Google Cloud console.Use theAI Applicationspage of the console for a\nquick-start experience using a web interface. From the\nconsole, you can create your search app, import your data,\ntest the user experience, and view analytics.\nUse the AI Applications API.Use the AI Applications API when you're ready\nto integrate search or recommendations into your website or applications.\nUse both the Google Cloud console and the API.You can set up your app\nand import your data using the console, for example, and then\nuse the API to test the user experience and integrate it into your website\nor application.\nAbout apps and data stores\nGet started with generic search\nGet started with generic recommendations\nGet started with media search\nGet started with media recommendations\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-21 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/cross-product-overviews",
    "title": "Access and resource management",
    "content": "Home\nDocumentation\nOrganize, analyze, and manage access to your Google Cloud resources and services.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nDefine who can access resources in your organization.\nManage internal enterprise solutions and Google Cloud APIs.\nOptimize your service usage, monitor application and resource health, and identify disruptive events.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/security",
    "title": "Security",
    "content": "Home\nDocumentation\nGoogle Cloud security products help organizations secure their cloud environment, protect their data, and comply with industry regulations.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nHandle key management for secrets, disks, images, and log retention.\nCentrally manage network resources, establish scalable segmentation for different security zones, and detect network threats.\nProtect your workloads against denial-of-service attacks, web application attacks, and other security threats.\nDetect vulnerabilities, threats, and misconfigurations.\nProvide unified, federated identity with least privilege policies to reduce the risk of data breaches and other security incidents.\nCollect, store, analyze, and monitor your organization's aggregated platform and system logs with a comprehensive solution.\nManage your resources in a secure and compliant way with visibility and control over your cloud environment.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCompute Engine\nDocumentation\nGuides\nThis document describes the features of Google Cloud Hyperdisk. Hyperdisk\nis the fastest and most efficient durable disk for Compute Engine.\nIf you need boot or data disks for your compute instances—virtual machine (VM) instances,\ncontainers, and bare metal instances—Google recommends using Hyperdisk.\nFor information about the other block storage options in\nCompute Engine, seeChoose a disk type.\nTo create a new Hyperdisk volume, seeCreate a Hyperdisk volume.\nWith Hyperdisk you can provision, manage, and scale your\nCompute Engine workloads without the cost and complexity of a typical\non-premises storage area network (SAN).\nHyperdisk volumes have the following features:\nFunction as physical disks: you can use a Hyperdisk volume\nwith a compute instance as if it were a physical disk attached to the instance.\nWhen you read to or write from a Hyperdisk volume,\ndata is transmitted over the network.\nFunction as physical disks: you can use a Hyperdisk volume\nwith a compute instance as if it were a physical disk attached to the instance.\nWhen you read to or write from a Hyperdisk volume,\ndata is transmitted over the network.\nHigher performance: Hyperdisk offers higher IOPS and\nthroughput than Persistent Disk by leveraging Google's Titanium\nstorage offload technology.\nHigher performance: Hyperdisk offers higher IOPS and\nthroughput than Persistent Disk by leveraging Google's Titanium\nstorage offload technology.\nCustomizable performance: you can choose the performance—IOPS and/or\nthroughput—of each Hyperdisk volume. You can also increase or\ndecrease a Hyperdisk volume's performance while it's in use.\nCustomizable performance: you can choose the performance—IOPS and/or\nthroughput—of each Hyperdisk volume. You can also increase or\ndecrease a Hyperdisk volume's performance while it's in use.\nSupport for high availability: in the unlikely event of a zonal or regional outage,\nyou can ensure high availability for your data by enabling one or both of the following\nfeatures:To protect data your data in case of a zonal outage, useHyperdisk Balanced High Availability.\nData on Hyperdisk Balanced High Availability volumes is synchronously replicated across two zones\nwithin the same region to protect against up to one zonal outage.To protect your data from a regional outage, maintain a replica\nof your data in another region by usingAsynchronous Replication.\nWhen you enable Asynchronous Replication for a disk, data in one region is\ncontinuously copied to a\nreplica in a secondary region. If a regional outage occurs, you canfailoveryour data to a secondary region. Asynchronous Replication is available for\nHyperdisk Balanced, Hyperdisk Balanced High Availability, and Hyperdisk Extreme volumes.\nSupport for high availability: in the unlikely event of a zonal or regional outage,\nyou can ensure high availability for your data by enabling one or both of the following\nfeatures:\nTo protect data your data in case of a zonal outage, useHyperdisk Balanced High Availability.\nData on Hyperdisk Balanced High Availability volumes is synchronously replicated across two zones\nwithin the same region to protect against up to one zonal outage.\nTo protect data your data in case of a zonal outage, useHyperdisk Balanced High Availability.\nData on Hyperdisk Balanced High Availability volumes is synchronously replicated across two zones\nwithin the same region to protect against up to one zonal outage.\nTo protect your data from a regional outage, maintain a replica\nof your data in another region by usingAsynchronous Replication.\nWhen you enable Asynchronous Replication for a disk, data in one region is\ncontinuously copied to a\nreplica in a secondary region. If a regional outage occurs, you canfailoveryour data to a secondary region. Asynchronous Replication is available for\nHyperdisk Balanced, Hyperdisk Balanced High Availability, and Hyperdisk Extreme volumes.\nTo protect your data from a regional outage, maintain a replica\nof your data in another region by usingAsynchronous Replication.\nWhen you enable Asynchronous Replication for a disk, data in one region is\ncontinuously copied to a\nreplica in a secondary region. If a regional outage occurs, you canfailoveryour data to a secondary region. Asynchronous Replication is available for\nHyperdisk Balanced, Hyperdisk Balanced High Availability, and Hyperdisk Extreme volumes.\nPortability: you can change the compute instance that a\nHyperdisk volume is attached to.\nPortability: you can change the compute instance that a\nHyperdisk volume is attached to.\nShareable between VMs: for high availability workloads, certain\nHyperdisk types can be shared by multiple VMs. Each VM has\nsimultaneous read-write or read-only access to the volume.\nShareable between VMs: for high availability workloads, certain\nHyperdisk types can be shared by multiple VMs. Each VM has\nsimultaneous read-write or read-only access to the volume.\nSupport for pooled capacity and performance: to simplify planning,\navoid overprovisioning storage, and reduce costs, you can purchase Hyperdisk\nstorage and performance in bulk by using Hyperdisk Storage Pools.\nSupport for pooled capacity and performance: to simplify planning,\navoid overprovisioning storage, and reduce costs, you can purchase Hyperdisk\nstorage and performance in bulk by using Hyperdisk Storage Pools.\nTo add Hyperdisk volumes to your workloads, you must choose a\nHyperdisk type. Each Hyperdisk type is designed and optimized for a\nspecific type of workload.\nThe following is a list of the available Hyperdisk types.\nHyperdisk Balanced\nHyperdisk Balanced High Availability\nHyperdisk Extreme\nHyperdisk Throughput\nHyperdisk ML\nFor most workloads, we recommend Hyperdisk Balanced.\nTo select a Hyperdisk type, compare your workload's type and its\nperformance requirements with the information in the following table. For\ndetailed information about a specific Hyperdisk\ntype, see the linked page in theRecommended Hyperdisk typecolumn.\nMost enterprise applications\nBoot disks\nVirtual desktops\nPostgres, MySQL\nDesigned to be the best fit for the majority of workloads\nBest combination of price and performance\nSupports simultaneous read-write access to the same volume from up to\n          8 instances\nHighly-available, mission-critical applications that require arecovery\n        point objectiveof 0\nOffers data replication in two zones within the same region for\n          quick failover\nSupports simultaneous read-write access to the same volume from up to\n          8 instances\nSAP HANA\nHigh-end SQL Server, Oracle, and in-memory RDBMS\nOffers the highest IOPS\nDesigned for workloads that need more than 5,000 MiB/s of\n          throughput or 350,000 IOPS, such as:High-performance computing (HPC)Machine learning, AI inference or trainingAccelerator-optimized workloads\nHigh-performance computing (HPC)\nMachine learning, AI inference or training\nAccelerator-optimized workloads\nSupports attaching a single volume in read-only mode to up to\n          2500 instances.\nOffers the highest read-only throughput\nScale out analytics workloads like Hadoop, Spark, and Kafka\nCold disks\nHigh throughput for bandwidth and capacity-intensive applications that\n          don't need high IOPS\nCost-effective data disks for cost-sensitive applications\n1You can't specify a throughput level for Hyperdisk Extreme\n   volumes. The provisioned throughput is based on the IOPS level you specify.2You can't specify an IOPS level for Hyperdisk Throughput and\n   Hyperdisk ML volumes. The provisioned IOPS is based on the throughput level you\n   specify.\nThe following is a summary of key Hyperdisk performance concepts:\nYou can configure the performance (IOPS and/or throughput) limit and size of each\nHyperdisk volume. You can also increase or decrease a\nHyperdisk volume's performance without changing its size.\nThe performance limit you specify is referred to as theprovisioned performance.\nThe provisioned performance isn't the expected performance, rather,\nit's the maximum performance the disk can achieve.\nThe actual performance for a Hyperdisk volume is the observed\nperformance while the volume is in use.\nFor a Hyperdisk volume to reach its provisioned performance,\nyou must attach it to a compute instance that supports the same level\nof performance or higher.\nFor a discussion of how Hyperdisk performance works, seeAbout Hyperdisk performance.For performance limits for each Hyperdisk type, seeHyperdisk performance limits.\nEach Hyperdisk type has different latency profiles. Google recommends\ncomparing Hyperdisk Throughput to the latency of a hard disk drive. You can compare\nthe latency for Hyperdisk Balanced, Hyperdisk Balanced High Availability, Hyperdisk Extreme, and Hyperdisk ML to the\nlatency of enterprise SSDs.\nHyperdisk Balanced and Hyperdisk Extreme offer sub-millisecond latency.\nThis section lists themachine seriesthat each Hyperdisk type supports.\nIf a machine series doesn't support Hyperdisk, use Persistent Disk.\nSelect one or more machine series to see the supported Hyperdisk types.\nC4AC4C4D(Preview)C3C3DN4N2N2DN1T2DT2AE2Z3H3C2C2DX4M4M3M2M1N1+GPUA4A3 (H200)A3 (H100)A2G2\nThis section lists the restrictions that apply to the machine series\nthat each Hyperdisk type supports.\nTo use Hyperdisk Balanced with A3 VMs, the VM must have at least 8 GPUs.\nTo use Hyperdisk Balanced with A3 VMs, the VM must have at least 8 GPUs.\nFor Hyperdisk Extreme, the following restrictions apply:A3 machine types require at least 4 GPUs.C3 machine type require at least 88 vCPUs.C3D machine types require at least 60 vCPUs.C4 machine types require at least 96 vCPUs.M1 machine types require at least 80 vCPUs.C4A, C4D (Preview), and M3\nmachine types require at least 64 vCPUs.M4 machine types require at least 112 vCPUs.N2 requires 80 or more vCPUs; Custom N2 machine types aren't supported.\nFor Hyperdisk Extreme, the following restrictions apply:\nA3 machine types require at least 4 GPUs.\nC3 machine type require at least 88 vCPUs.\nC3D machine types require at least 60 vCPUs.\nC4 machine types require at least 96 vCPUs.\nM1 machine types require at least 80 vCPUs.\nC4A, C4D (Preview), and M3\nmachine types require at least 64 vCPUs.\nM4 machine types require at least 112 vCPUs.\nN2 requires 80 or more vCPUs; Custom N2 machine types aren't supported.\nYou can't use Hyperdisk Throughput withc3-*-metalmachine types.\nYou can't use Hyperdisk Throughput withc3-*-metalmachine types.\nYou can share a Hyperdisk volume between multiple VMs by\nsimultaneously attaching the same volume to multiple VMs.\nThe following scenarios are supported:\nConcurrent read-write access to a single volume from multiple VMs.\nRecommended for clustered file systems and highly available workloads like\nSQL Server Failover Cluster Infrastructure. Supported for Hyperdisk Balanced and\nHyperdisk Balanced High Availability volumes.\nConcurrent read-write access to a single volume from multiple VMs.\nRecommended for clustered file systems and highly available workloads like\nSQL Server Failover Cluster Infrastructure. Supported for Hyperdisk Balanced and\nHyperdisk Balanced High Availability volumes.\nConcurrent read-only access to a single volume from multiple VMs.\nThis is more cost effective than having multiple disks with the same data.\nRecommended for accelerator-optimized machine learning workloads.\nSupported for Hyperdisk ML volumes.\nConcurrent read-only access to a single volume from multiple VMs.\nThis is more cost effective than having multiple disks with the same data.\nRecommended for accelerator-optimized machine learning workloads.\nSupported for Hyperdisk ML volumes.\nYou can't attach a Hyperdisk Throughput or Hyperdisk Extreme volume to more than one VM.\nTo learn about disk sharing, seeShare a disk between VMs.\nYou can protect your data in the rare event of a zonal or regional outage by\nenabling replication, that is, maintaining a copy of the data in another zone or\nregion.\nTo replicate data to another zone within the same region, you must use Hyperdisk Balanced High Availability\nvolumes. Hyperdisk Balanced High Availability is the only supported Hyperdisk type for\nzonal replication.\nFor more information, seeAbout synchronous disk replication.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't\nrepresent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.You create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).Because the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a\nHyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk.\nYou can protect your data in the unlikely event of a regional outage by enabling\nAsynchronous Replication. Asynchronous Replication maintains a copy of the data on your\nvolume in another region. For example, to protect a\nHyperdisk volume inus-west1,\nyou can use Asynchronous Replication to replicate the volume to a secondary volume in\ntheus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.\nYou can use Asynchronous Replication with the following Hyperdisk types:\nHyperdisk Balanced\nHyperdisk Extreme\nHyperdisk Balanced High Availability\nTo learn more about cross-regional replication, seeAsynchronous Replication.\nBy default, Compute Engine protects your Hyperdisk volumes with\nGoogle-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk\nvolumes with customer-managed encryption keys (CMEK).\nFor more information, seeAbout disk encryption.\nYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling\nConfidential mode for the disk when you create it.\nYou can use Confidential mode only with Hyperdisk Balanced disks that are attached to\nConfidential VMs.\nFor more information, seeConfidential mode for Hyperdisk Balanced volumes.\nCompute Engine distributes the data on Hyperdisk volumes\nacross several physical disks to ensure durability and optimize performance.\nDisk durability represents the probability of data loss, by design, for a typical\ndisk in a typical year. Hyperdisk data loss events are extremely\nrare and have historically been the result of coordinated hardware failures,\nsoftware bugs, or a combination of the two. Google takes many steps to mitigate\nthe industry-wide risk of silent data corruption.\nDurability is calculated with a set of assumptions about hardware failures,\nthe likelihood of catastrophic events, isolation practices and engineering\nprocesses in Google data centers, and the internal encodings used by each disk type.\nHuman error by a Google Cloud customer, such as when a customer accidentally\ndeletes a disk, is outside the scope of Hyperdisk durability.\nThe table below shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 Hyperdisk volumes, you would likely go a\nhundred years without losing a single one.\nHyperdisk volumes are mounted as a disk on a VM using the NVMe or\nSCSI interface, depending on the machine type of the instance.\nHyperdisk Storage Pools make it easier to lower your block storage total cost of\nownership and simplify block storage management. With Hyperdisk Storage Pools,\nyou can share a pool of capacity and performance across a maximum of 1,000 disks\nin a single project. Because storage pools offer thin-provisioning and data\nreduction, you can achieve higher efficiency.\nStorage pools simplify migrating your on-premises SAN to the\ncloud, and also make it easier to provide your workloads with the capacity and\nperformance that they need.\nYou create a storage pool with the estimated capacity and performance for\nall workloads in a project in a specific zone. You then create disks in this\nstorage pool and attach the disks to existing VMs. You can also create a\ndisk in the storage pool as part of creating a new VM. Each\nstorage pool contains one type of disk, such as Hyperdisk Throughput. There are two\ntypes of Hyperdisk Storage Pools:\nHyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced\ndisks\nHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best\nserved by Hyperdisk Throughput disks\nFor information about using Hyperdisk Storage Pools, seeAbout storage pools.\nYou are billed for the total provisioned capacity of your\nHyperdisk volumes until you delete them. You are charged per GiB\nper month. Additionally, you are billed for the following:\nHyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned\nthroughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and\n140 MiB/s throughput.\nHyperdisk Extreme charges a monthly rate based on the provisioned IOPS.\nHyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s).\nThere is no additional charge for attaching multiple VMs to a single\nHyperdisk ML volume.\nHyperdisk Throughput charges a monthly rate based on the provisioned throughput\n(in MiB/s).\nBecause the data for regional disks is written to two locations,\nthe cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.\nFor more pricing information, seeDisk pricing.\nHyperdisk volumes are not eligible for:\nResource-based committed use discounts (CUDs)\nSustained use discounts (SUDs)\nHyperdisk can be used with Spot VMs (or\npreemptible VMs). However, there are no discounted spot prices for\nHyperdisk.\nYou can't create amachine imagefrom a\nHyperdisk volume.\nYou can't back up a disk in multi-writer mode with snapshots or\nimages. You mustdisable multi-writer modefirst.\nYou can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.\nYou can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.\nYou can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.\nHyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.\nYou can attach a Hyperdisk ML volume to up to 100 VMs at most once every\n30 seconds.\nYou can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.\nIf you enable read-only mode for a Hyperdisk ML volume, you can't re-enable\nread-write mode.\nIf you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.\nConfidential VMs with AMD SEV on C3D machine types don't support\nHyperdisk Balanced and Hyperdisk Throughput.\nLearn how tocreate a Hyperdisk volume.\nLearn how toclone a Hyperdisk volume.\nLearn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.\nLearn aboutHyperdisk Storage Pools.\nReviewDisk pricinginformation.\nLearn how tooptimize performance of Hyperdisk.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "content": "Home\nBigQuery\nDocumentation\nGuides\nBigQuery sharing (formerly Analytics Hub) is a data exchange platform that\nlets you share data and insights at scale across organizational boundaries with\na robust security and privacy framework. Sharing lets you\ndiscover and access a data library curated by various data providers. This data\nlibrary also includes Google-provided datasets.\nFor example, you can use sharing to augment your\nanalytics and ML initiatives with third-party and Google datasets.\nAnalytics Hub Identity and Access Management (IAM) roles let you perform the\nfollowing sharing tasks:\nThe Analytics Hub Publisher role lets you monetize data by sharing\nit with your partner network or within your own organization in real time.Listingslet you share data without replicating the shared data.\nYou can build a catalog of analytics-ready data sources with granular\npermissions that let you deliver data to the right audiences. You can also\nmanage subscriptions and view the usage metrics for your listings.\nThe Analytics Hub Publisher role lets you monetize data by sharing\nit with your partner network or within your own organization in real time.Listingslet you share data without replicating the shared data.\nYou can build a catalog of analytics-ready data sources with granular\npermissions that let you deliver data to the right audiences. You can also\nmanage subscriptions and view the usage metrics for your listings.\nThe Analytics Hub Subscriber role lets you discover the data that\nyou are looking for, combine shared data with your existing data, and use\nthebuilt-in features of\nBigQuery. When you\nsubscribe to a listing, alinked datasetor linked Pub/Sub\nsubscription is created in\nyour project. You can manage your subscriptions by using theSubscription\nresource,\nwhich stores relevant information about the subscriber and represents the\nconnection between publisher and subscriber.\nThe Analytics Hub Subscriber role lets you discover the data that\nyou are looking for, combine shared data with your existing data, and use\nthebuilt-in features of\nBigQuery. When you\nsubscribe to a listing, alinked datasetor linked Pub/Sub\nsubscription is created in\nyour project. You can manage your subscriptions by using theSubscription\nresource,\nwhich stores relevant information about the subscriber and represents the\nconnection between publisher and subscriber.\nThe Analytics Hub Viewer role lets you browse through the\nresources that you have access to in Sharing and request\nthe publisher to access the shared data.\nThe Analytics Hub Viewer role lets you browse through the\nresources that you have access to in Sharing and request\nthe publisher to access the shared data.\nThe Analytics Hub Admin role lets you createdata\nexchangesthat enable data sharing, and then give\npermissions to data publishers and subscribers to access these data exchanges.\nThe Analytics Hub Admin role lets you createdata\nexchangesthat enable data sharing, and then give\npermissions to data publishers and subscribers to access these data exchanges.\nFor more information, seeConfigure Analytics Hub roles.\nSharing is built on a publish and subscribe model of\nGoogle Cloud data resources, allowing for zero-copy sharing in place.\nSharing supports the following Google Cloud resources:\nBigQuery datasets\nPub/Sub topics\nThe following diagram describes how a publisher shares assets:\nThe following sections describe the features in this workflow.\nShared resources are the unit of sharing by a publisher in\nBigQuery sharing.\nA shared dataset is a BigQuery dataset that is the unit of\ndata sharing in BigQuery sharing. The separation of compute and storage\nin the BigQuery architecture enables data publishers to share datasets\nwith as many subscribers as they want, without having to make multiple copies of\nthe data. As a publisher, you create or use an existing BigQuery\ndataset in your project with the following supported objects that you want to\ndeliver to your subscribers:\nAuthorized views\nAuthorized datasets\nBigQuery ML models\nExternal tables\nMaterialized views\nRoutinesNot all routines are supported in shared datasets. For more information, seeLimitations.\nRoutines\nNot all routines are supported in shared datasets. For more information, seeLimitations.\nTables\nTables\nTable snapshots\nTable snapshots\nViews\nViews\nShared datasets supportcolumn-level securityandrow-level security.\nA shared topic is aPub/Sub topicthat is the unit ofstreaming data sharing in BigQuery.\nAs a publisher, you create or use an existing Pub/Sub topic in your\nproject and distribute that with your subscribers.\nA data exchange is a container that enables self-service data sharing. It\ncontains listings that reference shared resources. Publishers and administrators\ncan grant access to subscribers at the exchange and the listing level. This\nmethod helps to avoid granting access on the underlying shared resources\nexplicitly. A subscriber can browse through data exchanges, discover data that\nthey can access, and subscribe to shared resources. When youcreate a data\nexchange, you\ncan assign a primary contact email to it. The primary contact email provides a\nway for users to contact the owner of a data exchange with questions or concerns\nabout the data exchange. A data exchange can be of the following types:\nPrivate data exchange.By default, a data exchange is private and only\nusers or groups that have access to that exchange can view or subscribe to its\nlistings.\nPublic data exchange.By default, a data exchange is private and only\nusers or groups that have access to that exchange can view or subscribe to its\nlistings. However, you can choose to make a data exchange public. Listings in\npublic data exchanges can bediscoveredandsubscribed tobyGoogle Cloud users (allAuthenticatedUsers).\nFor more information about public data exchanges, seeMake a data exchange public.\nThe Analytics Hub Admin role lets you create multiple data exchanges and manage\nother users performing sharing tasks.\nA listing is a reference to a shared resource that a publisher lists in\na data exchange. As a publisher, you can create a listing and specify the\nresource description, sample queries to run or sample message data, links to any\nrelevant documentation, and any additional information that can help subscribers\nto use your shared resource. When you create a listing, you can assign a primary\ncontact email, a provider name and contact, and a publisher name and contact.\nThe primary contact email provides a way for users to contact the owner of a\nlisting with questions or concerns about the data exchange. The provider name\nand contact is the information of the agency that originally provided the data\nfor the listing. This information is optional. The publisher name and contact is\nthe agency that published the data for use in BigQuery sharing. This\ninformation is optional. For more information, seeManage listings.\nA listing can be of the following two types based on the\nIdentity and Access Management (IAM) policy that is set for the listing and the type of data\nexchange that contains the listing:\nPublic listing.It is shared with allGoogle Cloud users (allAuthenticatedUsers).\nListings in a public data exchange are public listings. These listings can be\nreferences of afree public resourceor acommercial resource.\nIf the listing is of a commercial resource, subscribers can either request\naccess to the listing directly from the data provider, or they can\nbrowse and purchaseGoogle Cloud Marketplace-integrated commercial listings.\nPrivate listing.It is shared directly with individuals or\ngroups. For example, a private listing can reference marketing\nmetrics dataset that you share with other internal teams within your\norganization.\nThe following diagram describes how subscribers interact with\nshared resources:\nThe following sections describe the features in the subscriber workflow.\nLinked resources are created when subscribing to a Sharing\nlisting, connecting a subscriber to the underlying shared resource.\nA linked dataset is aread-onlyBigQuery dataset that serves as\na pointer or reference to a shared dataset. Subscribing to a listing creates a linked\ndataset in your project and not a copy of the dataset, so subscribers can read\nthe data but cannot add or update objects within it. When you query objects\nsuch as tables and views through a linked dataset, the data from the shared\ndataset is returned. For more information about linked datasets, seeView and subscribe to listings.\nLinked datasets are authorized to access tables and views of a shared dataset.\nSubscribers with linked datasets access tables and views of a shared dataset\nwithout any additional Identity and Access Management authorization.\nLinked datasets supports the following objects:\nAuthorized views\nAuthorized datasets\nAuthorized routines\nSubscribing to a listing with a shared topic creates a linked Pub/Sub\nsubscription in the subscriber project. No copies of the shared topic or message data are\ncreated. Subscribers of the linkedPub/Sub subscriptioncan access the messages published to the shared topic. Subscribers access message data of\na shared topic without any additional Identity and Access Management authorization. Publishers can manage\nsubscriptions both in Pub/Sub directly or through\nSharing subscription management. For more information about linked\nPub/Sub subscriptions, seeStream sharing with Pub/Sub.\nData egress options let\npublishers restrict the export by subscribers of data out of BigQuery\nlinked datasets.\nPublishers can enable data egress restriction on a listing, the results of a query, or\nboth. When data egress is restricted, the following restrictions are applied:\nCopy, clone, export, and snapshot APIs are disabled.\nCopy, clone, export, and snapshot APIs are disabled.\nCopy, clone, export, and snapshot options in the Google Cloud console are\ndisabled.\nCopy, clone, export, and snapshot options in the Google Cloud console are\ndisabled.\nConnecting the restricted dataset to the table explorer is disabled.\nConnecting the restricted dataset to the table explorer is disabled.\nBigQuery Data Transfer Service is disabled on the restricted dataset.\nBigQuery Data Transfer Service is disabled on the restricted dataset.\nCREATE TABLE AS SELECTstatementsandwriting to a destination tableare\ndisabled.\nCREATE TABLE AS SELECTstatementsandwriting to a destination tableare\ndisabled.\nCREATE VIEW AS SELECTstatementsand writing to a destination view are disabled.\nCREATE VIEW AS SELECTstatementsand writing to a destination view are disabled.\nWhen youcreate a\nlisting, you can\nset the appropriate data egress options.\nSharing has the following limitations:\nA shared dataset can have a maximum of 1,000 linked datasets.\nA shared dataset can have a maximum of 1,000 linked datasets.\nA shared topic can have amaximumof 10,000 Pub/Sub subscriptions. This limit includes linked Pub/Sub subscriptions and Pub/Sub subscriptions created outside of Sharing (for example, directly from Pub/Sub).\nA shared topic can have amaximumof 10,000 Pub/Sub subscriptions. This limit includes linked Pub/Sub subscriptions and Pub/Sub subscriptions created outside of Sharing (for example, directly from Pub/Sub).\nA dataset with unsupported resources cannot be selected as a shared dataset\nwhen youcreate a listing.\nFor more information about the BigQuery objects that\nSharing supports, seeShared datasetsin this document.\nA dataset with unsupported resources cannot be selected as a shared dataset\nwhen youcreate a listing.\nFor more information about the BigQuery objects that\nSharing supports, seeShared datasetsin this document.\nYou can't setIAM rolesorIAM\npolicieson\nindividual tables within a linked dataset. Apply them at the linked dataset\nlevel instead.\nYou can't setIAM rolesorIAM\npolicieson\nindividual tables within a linked dataset. Apply them at the linked dataset\nlevel instead.\nYou can't attachIAM tagson tables within a linked dataset. Apply them at the linked dataset level\ninstead.\nYou can't attachIAM tagson tables within a linked dataset. Apply them at the linked dataset level\ninstead.\nLinked datasets created before July 25, 2023, aren't backfilled by thesubscription resource. Only\nsubscriptions created after July 25, 2023 work with the API methods.\nLinked datasets created before July 25, 2023, aren't backfilled by thesubscription resource. Only\nsubscriptions created after July 25, 2023 work with the API methods.\nIf you are a publisher, the following BigQuery interoperability\nlimitations apply:Subscribers must be given explicit permissions to read the source\ndataset to be able to query views within linked datasets. To\ngrant access to views, as a best practice publishers shouldcreate authorized views. Authorized\nviews can grant subscribers access to the view data without giving them\naccess to the underlying source data.Thequery planreveals the shared\nview query and the routine query, including project IDs, and other\ndatasets involved in authorized views. Never include anything such as\nencryption keys that you consider sensitive in the shared view or routine\nquery.Shared datasets are indexed inData Catalog(deprecated) andBigQuery universal catalog.\nUpdates on a shared dataset, such as adding tables or views, are made\navailable to subscribers without any delay. However, in certain\nscenarios, for example, when there are more than one hundred subscribers or\ntables in a shared dataset, the updates might take up to 18 hours to\nget indexed in these services. Due to the delay in\nindexing, subscribers cannot search for these updated\nresources in the Google Cloud console immediately.Shared topics are indexed in Data Catalog (deprecated) anduniversal catalog, but\nyou cannot filter specifically for its resource type.If you have set uprow-level securityordata maskingpolicies on\nthe tables that are listed, then subscribers must be an\nEnterprise or Enterprise Plus customer to run the query\njob on the linked dataset. For information about editions, seeIntroduction to BigQuery editions.\nIf you are a publisher, the following BigQuery interoperability\nlimitations apply:\nSubscribers must be given explicit permissions to read the source\ndataset to be able to query views within linked datasets. To\ngrant access to views, as a best practice publishers shouldcreate authorized views. Authorized\nviews can grant subscribers access to the view data without giving them\naccess to the underlying source data.\nSubscribers must be given explicit permissions to read the source\ndataset to be able to query views within linked datasets. To\ngrant access to views, as a best practice publishers shouldcreate authorized views. Authorized\nviews can grant subscribers access to the view data without giving them\naccess to the underlying source data.\nThequery planreveals the shared\nview query and the routine query, including project IDs, and other\ndatasets involved in authorized views. Never include anything such as\nencryption keys that you consider sensitive in the shared view or routine\nquery.\nThequery planreveals the shared\nview query and the routine query, including project IDs, and other\ndatasets involved in authorized views. Never include anything such as\nencryption keys that you consider sensitive in the shared view or routine\nquery.\nShared datasets are indexed inData Catalog(deprecated) andBigQuery universal catalog.\nUpdates on a shared dataset, such as adding tables or views, are made\navailable to subscribers without any delay. However, in certain\nscenarios, for example, when there are more than one hundred subscribers or\ntables in a shared dataset, the updates might take up to 18 hours to\nget indexed in these services. Due to the delay in\nindexing, subscribers cannot search for these updated\nresources in the Google Cloud console immediately.\nShared datasets are indexed inData Catalog(deprecated) andBigQuery universal catalog.\nUpdates on a shared dataset, such as adding tables or views, are made\navailable to subscribers without any delay. However, in certain\nscenarios, for example, when there are more than one hundred subscribers or\ntables in a shared dataset, the updates might take up to 18 hours to\nget indexed in these services. Due to the delay in\nindexing, subscribers cannot search for these updated\nresources in the Google Cloud console immediately.\nShared topics are indexed in Data Catalog (deprecated) anduniversal catalog, but\nyou cannot filter specifically for its resource type.\nShared topics are indexed in Data Catalog (deprecated) anduniversal catalog, but\nyou cannot filter specifically for its resource type.\nIf you have set uprow-level securityordata maskingpolicies on\nthe tables that are listed, then subscribers must be an\nEnterprise or Enterprise Plus customer to run the query\njob on the linked dataset. For information about editions, seeIntroduction to BigQuery editions.\nIf you have set uprow-level securityordata maskingpolicies on\nthe tables that are listed, then subscribers must be an\nEnterprise or Enterprise Plus customer to run the query\njob on the linked dataset. For information about editions, seeIntroduction to BigQuery editions.\nIf you are a subscriber, the following BigQuery\ninteroperability limitations apply:Materialized views that refer to tables in the linked dataset aren't\nsupported.Takingsnapshotsof linked dataset\ntables isn't supported.Queries with linked datasets andJOINstatements that are larger than\n1 TB (physical storage) might fail. You cancontact supportto resolve this issue.You cannot useregion qualifierswithINFORMATION_SCHEMAviews toview metadata for your linked\ndataset.When querying forroutinesin a linked dataset,\nyou can only query forUser-defined functions(both SQL and Javascript UDFs) andTable functionsroutine types. Querying\nfor an unsupported routine type results in the error message:Querying routine typetypeis not yet supported on linked datasetdataset.Caution:As a publisher, be cautious when sharing datasets with\nunsupported routines. Unsupported routines might receive support in the\nfuture, causing previously inactive queries with unsupported routines to\nwork. Only include routines in shared datasets that you intend to share to\nsubscribers.\nIf you are a subscriber, the following BigQuery\ninteroperability limitations apply:\nMaterialized views that refer to tables in the linked dataset aren't\nsupported.\nMaterialized views that refer to tables in the linked dataset aren't\nsupported.\nTakingsnapshotsof linked dataset\ntables isn't supported.\nTakingsnapshotsof linked dataset\ntables isn't supported.\nQueries with linked datasets andJOINstatements that are larger than\n1 TB (physical storage) might fail. You cancontact supportto resolve this issue.\nQueries with linked datasets andJOINstatements that are larger than\n1 TB (physical storage) might fail. You cancontact supportto resolve this issue.\nYou cannot useregion qualifierswithINFORMATION_SCHEMAviews toview metadata for your linked\ndataset.\nYou cannot useregion qualifierswithINFORMATION_SCHEMAviews toview metadata for your linked\ndataset.\nWhen querying forroutinesin a linked dataset,\nyou can only query forUser-defined functions(both SQL and Javascript UDFs) andTable functionsroutine types. Querying\nfor an unsupported routine type results in the error message:Querying routine typetypeis not yet supported on linked datasetdataset.Caution:As a publisher, be cautious when sharing datasets with\nunsupported routines. Unsupported routines might receive support in the\nfuture, causing previously inactive queries with unsupported routines to\nwork. Only include routines in shared datasets that you intend to share to\nsubscribers.\nWhen querying forroutinesin a linked dataset,\nyou can only query forUser-defined functions(both SQL and Javascript UDFs) andTable functionsroutine types. Querying\nfor an unsupported routine type results in the error message:Querying routine typetypeis not yet supported on linked datasetdataset.Caution:As a publisher, be cautious when sharing datasets with\nunsupported routines. Unsupported routines might receive support in the\nfuture, causing previously inactive queries with unsupported routines to\nwork. Only include routines in shared datasets that you intend to share to\nsubscribers.\nThe following limitations apply for the usage metrics:You can't get the usage metrics for listings that were subscribed before\nJuly 20, 2023.External tableusage metrics for thenum_rows_processedandtotal_bytes_processedfields\nmight contain inaccurate data.Usage metrics for consumption are supported only for usage usingBigQuery jobs. Consumption\nby using the following resources is not supported:BigQuery Storage Read APItabledata.listBigQuery BI Engine queriesUsage metrics forviewsare only populated\nfor queries after April 22, 2024.Usage metrics aren't captured for linked Pub/Sub subscriptions\nin BigQuery (you can continue to see usage directly in\nPub/Sub).\nThe following limitations apply for the usage metrics:\nYou can't get the usage metrics for listings that were subscribed before\nJuly 20, 2023.\nYou can't get the usage metrics for listings that were subscribed before\nJuly 20, 2023.\nExternal tableusage metrics for thenum_rows_processedandtotal_bytes_processedfields\nmight contain inaccurate data.\nExternal tableusage metrics for thenum_rows_processedandtotal_bytes_processedfields\nmight contain inaccurate data.\nUsage metrics for consumption are supported only for usage usingBigQuery jobs. Consumption\nby using the following resources is not supported:BigQuery Storage Read APItabledata.listBigQuery BI Engine queries\nUsage metrics for consumption are supported only for usage usingBigQuery jobs. Consumption\nby using the following resources is not supported:\nBigQuery Storage Read API\ntabledata.list\nBigQuery BI Engine queries\nUsage metrics forviewsare only populated\nfor queries after April 22, 2024.\nUsage metrics forviewsare only populated\nfor queries after April 22, 2024.\nUsage metrics aren't captured for linked Pub/Sub subscriptions\nin BigQuery (you can continue to see usage directly in\nPub/Sub).\nUsage metrics aren't captured for linked Pub/Sub subscriptions\nin BigQuery (you can continue to see usage directly in\nPub/Sub).\nThe following limitations apply when subscribing to Salesforce Data Cloud data:Data Cloud data is shared as views. As a subscriber, you\ncan't access the underlying tables that the views reference.\nThe following limitations apply when subscribing to Salesforce Data Cloud data:\nData Cloud data is shared as views. As a subscriber, you\ncan't access the underlying tables that the views reference.\nBigQuery sharing is supported in the following regions and\nmulti-regions.\n1Data located in theEUmulti-region is not\nstored in theeurope-west2(London) oreurope-west6(Zürich) data\ncenters.\nThis section shows an example of how you can use sharing in\nBigQuery.\nSuppose you are a retailer and your organization has real-time\ndemand forecasting data in a Google Cloud project namedForecasting.\nYou want to share this demand forecasting data with hundreds of vendors in\nyour supply-chain system. Here's how you can share your data with vendors\nthrough BigQuery sharing:\nAdministrators\nAs the owner of theForecastingproject, you must first enable the\nAnalytics Hub API and then assign theAnalytics Hub Admin roleto a user who administers the data exchange in the project.\nThis administrator can perform the following tasks:\nCreate, update, delete, and share the data exchange in your organization'sForecastingproject.\nCreate, update, delete, and share the data exchange in your organization'sForecastingproject.\nManage otheradministratorswith the Analytics Hub Admin role.\nManage otheradministratorswith the Analytics Hub Admin role.\nManagepublishersby granting the Analytics Hub Publisher role to your\norganization's employees. If you want some employees to\nonly be able to update, delete, and share listings but not create them, then\nyou can grant them the Analytics Hub Listing Admin role.\nManagepublishersby granting the Analytics Hub Publisher role to your\norganization's employees. If you want some employees to\nonly be able to update, delete, and share listings but not create them, then\nyou can grant them the Analytics Hub Listing Admin role.\nManagesubscribersby granting the Analytics Hub\nSubscriber role to a Google group consisting of all vendors. If you want some\nvendors to only have view access to the available exchanges and listings then\nyou can grant them the Analytics Hub Viewer role. These vendors\naren't able to subscribe to listings.\nManagesubscribersby granting the Analytics Hub\nSubscriber role to a Google group consisting of all vendors. If you want some\nvendors to only have view access to the available exchanges and listings then\nyou can grant them the Analytics Hub Viewer role. These vendors\naren't able to subscribe to listings.\nFor more information, seeManage data exchanges.\nPublishers\nPublishers create the following listings for their datasets in theForecastingproject or in a different project:\nListing A: Demand Forecast Dataset 1\nListing B: Demand Forecast Dataset 2\nListing C: Demand Forecast Dataset 3\nAs a data provider, you cantrack the usage metricsfor your shared dataset. The usage metrics include the following details:\nJobs that run against your shared dataset.\nThe consumption details of your shared dataset by subscribers' projects and\norganization.\nThe number of rows and bytes processed by the job.\nFor more information, seeManage listings.\nSubscribers\nSubscribers can browse through listings that they have access to in data\nexchanges. They can also subscribe to these listings and add these datasets to\ntheir projects by creating a linked dataset. Vendors can then run queries on\nthese linked datasets and retrieve results in real time.\nFor more information, seeView and subscribe to listings.\nThere is no additional cost for managing data exchanges or listings.\nFor BigQuery datasets, publishers are charged for data storage,\nwhereas subscribers pay for queries that run against the shared data based on\neither on-demand or capacity-based pricing model. For information about pricing,\nseeBigQuery pricing.\nFor Pub/Sub, topic publishers are charged for the total number of bytes written (publish throughput) to the shared topic and network egress (if applicable). Subscribers are charged for the total number of bytes read (subscribe throughput) from the linked subscription and network egress (if applicable). SeePub/Sub pricingfor additional details.\nFor information about BigQuery sharing quotas, seeQuotas and\nlimits.\nSharing, as part of BigQuery, is compliant\nwith the following compliance programs:\nISO 27001\nISO 27017\nISO 27018\nSOC 1\nSOC 2\nSOC 3\nPCI\nPenetration Testing\nHIPAA\nHITRUST\nYou can set the ingress and egress rules needed to let publishers and\nsubscribers access data from projects that have\nVPC Service Controls perimeters. For more information, seeSharing VPC Service Controls rules.\nLearn how toview and subscribe to listings.\nLearn how to grantAnalytics Hub roles.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-05 UTC."
  },
  {
    "url": "https://cloud.google.com/looker/docs/studio",
    "title": "Welcome to Looker StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nDocumentation\nLooker Studio\nGuides\nVisualize your data through highly configurable charts and tables.\nEasily connect to a variety of data sources.\nShare your insights with your team or with the world.\nCollaborate on reports with your team.\nSpeed up your report creation process with built-in sample reports.\nLooker Studio is ano-costtool that turns your data into informative, easy to read, easy to share, and fully customizable dashboards and reports. Use the drag and drop report editor to:\nTell your data story withcharts, including line, bar, and pie charts, geo maps, area and bubble graphs, paginated data tables, pivot tables, and more.\nMake your reportsinteractivewith viewerfiltersanddate rangecontrols. Thedata controlturns any report into a flexible template report that anyone can use to see their own data.\nIncludelinksandclickable imagesto create product catalogs, video libraries, and other hyperlinked content.\nAnnotate and brand your reports withtextandimages.\nApplystylesandcolor themesthat make your data stories works of data visualization art.\nLearn moreabout creating reports.\nGet started quickly by using theLooker Studio marketing templates. Templates make it easy to visualize your data in a finished report.Learn more.\nWith Looker Studio, you can easily report on data from a wide variety of sources, without programming. In just a few moments, you can connect to data sets such as the following:\nDatabases, including BigQuery, MySQL, and PostgreSQL\nGoogle Marketing Platform products, including Google Ads, Google Analytics, Display & Video 360, Search Ads 360\nGoogle consumer products, such as Google Sheets, YouTube, and Search Console\nFlat files via CSV file upload and Cloud Storage\nSocial media platforms such as Facebook, Reddit, and Twitter\nBlended data from any combination of related sources\nLearn moreabout connecting to your data.\nIt's easy to share your insights with individuals, teams, or the world. Invite others to view or edit your reports, or send them links in scheduled emails. To tell your data stories as broadly as possible, you can embed your reports in other pages, such as Google Sites, blog posts, marketing articles, and annual reports.\nWhen you share a Looker Studio file with another editor, you can work it together in real time as a team.\nLearn more aboutsharing.\nLooker Studio's enterprise features let Cloud Identity and Google Workspace administrators manage users and control access to Looker Studio assets.\nLearn more aboutenterprise administrator features.\nGet even more enterprise-grade collaboration and administration features, along with access to Google Cloud Customer Care. Learn more aboutLooker Studio Pro.\nTry Looker Studio now\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-06 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nTo learn more,\n      run the \"Vertex AI Pipelines: Lightweight Python function-based components, and component I/O\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nVertex AI Pipelines lets you automate, monitor, and govern your machine\nlearning (ML) systems in a serverless manner by using ML pipelines to\norchestrate your ML workflows. You can batch run ML pipelines defined using\nthe Kubeflow Pipelines or the TensorFlow Extended\n(TFX) framework. To learn how to choose a framework for\ndefining your ML pipeline, seeInterfaces to define a pipeline.\nThis page provides an overview of the following:\nWhat is an ML pipeline?\nWhat is an ML pipeline?\nStructure of an ML pipeline\nStructure of an ML pipeline\nPipeline tasks and components\nPipeline tasks and components\nLife cycle of an ML pipeline\nLife cycle of an ML pipeline\nUse Vertex ML Metadata to track the lineage of ML artifacts\nUse Vertex ML Metadata to track the lineage of ML artifacts\nAdd pipeline runs to experiments\nAdd pipeline runs to experiments\nAn ML pipeline is a portable and extensible description of an MLOps workflow as a\nseries of steps called pipeline tasks. Each task performs a specific\nstep in the workflow to train and/or deploy an ML model.\nWith ML pipelines, you can apply MLOps strategies to automate and monitor\nrepeatable processes in your ML practice. For example, you can reuse a pipeline\ndefinition to continuously retrain a model on the latest\nproduction data. For more information about MLOps in Vertex AI,\nseeMLOps on Vertex AI.\nAn ML pipeline is a directed acyclic graph (DAG) of containerized pipeline tasks\nthat are interconnected using input-output dependencies. You can\nauthor each task either in Python or as a prebuilt container images.\nYou can define the pipeline as a DAG using either the Kubeflow Pipelines\nSDK or the TFX SDK, compile it to its YAML for intermediate\nrepresentation, and then run the pipeline. By default, pipeline tasks run in\nparallel. You can link the tasks to execute them in series.\nFor more information about pipeline tasks, seePipeline task.\nFor more information about the workflow for defining, compiling, and running the\npipeline, seeLife cycle of an ML pipeline.\nConsider an ML pipeline with the following steps:\nPrepare data: Prepare or preprocess training data.Input (from tasks within the same ML pipeline): None.Output: Prepared or preprocessed training data.\nPrepare data: Prepare or preprocess training data.\nInput (from tasks within the same ML pipeline): None.\nInput (from tasks within the same ML pipeline): None.\nOutput: Prepared or preprocessed training data.\nOutput: Prepared or preprocessed training data.\nTrain model: Use the prepared training data to train a model.Input: Prepared or preprocessed training data from pipeline taskPrepare data.Output: Trained model.\nTrain model: Use the prepared training data to train a model.\nInput: Prepared or preprocessed training data from pipeline taskPrepare data.\nInput: Prepared or preprocessed training data from pipeline taskPrepare data.\nOutput: Trained model.\nOutput: Trained model.\nEvaluate model: Evaluate the trained model.Input: Trained model from pipeline taskTrain model.\nEvaluate model: Evaluate the trained model.\nInput: Trained model from pipeline taskTrain model.\nDeploy: Deploy the trained model for predictions.Input: Trained model from pipeline taskTrain model.\nDeploy: Deploy the trained model for predictions.\nInput: Trained model from pipeline taskTrain model.\nWhen you compile your ML pipeline, the pipelines SDK you're using\n(Kubeflow Pipelines or TFX) analyzes the data\ndependencies between these tasks and creates the following workflow DAG:\nPrepare datadoesn't rely on other tasks within the same ML pipeline for\ninputs. Therefore, it can be the first step in the ML pipeline, or run\nconcurrently with other tasks.\nPrepare datadoesn't rely on other tasks within the same ML pipeline for\ninputs. Therefore, it can be the first step in the ML pipeline, or run\nconcurrently with other tasks.\nTrain modelrelies onPrepare datafor inputs. Therefore, it occurs\nafterPrepare data.\nTrain modelrelies onPrepare datafor inputs. Therefore, it occurs\nafterPrepare data.\nEvaluateandDeployboth depend on the trained model. Therefore, they\ncan run concurrently, but afterTrain model.\nEvaluateandDeployboth depend on the trained model. Therefore, they\ncan run concurrently, but afterTrain model.\nWhen you run your ML pipeline, Vertex AI Pipelines executes these tasks in the\nsequence described in the DAG.\nApipeline taskis an instantiation of apipeline componentwith specific inputs. While defining\nyour ML pipeline, you can interconnect multiple tasks to form a DAG, by routing\nthe outputs of one pipeline task to the inputs for the next pipeline task in\nthe ML workflow. You can also use the inputs for the ML pipeline as the inputs\nfor a pipeline task.\nA pipeline component is a self-contained set of code that performs a specific\nstep of an ML workflow, such as data preprocessing, model training, or\nmodel deployment. A component typically consists of the following:\nInputs: A component might have one or more input parameters and artifacts.\nInputs: A component might have one or more input parameters and artifacts.\nOutputs: Every component has one or more output parameters or artifacts.\nOutputs: Every component has one or more output parameters or artifacts.\nLogic: This is the component's executable code. For\ncontainerized components, the logic also contains the definition of the\nenvironment, or container image, where the component runs.\nLogic: This is the component's executable code. For\ncontainerized components, the logic also contains the definition of the\nenvironment, or container image, where the component runs.\nComponents are the basis of defining tasks in an ML pipeline. To define\npipeline tasks, you can either use predefinedGoogle Cloud Pipeline Componentsor create your own custom\ncomponents.\nUse predefined Google Cloud Pipeline Components if you want to use features of Vertex AI,\nsuch as AutoML, in your pipeline. To learn how to use Google Cloud Pipeline Components\nto define a pipeline, seeBuild a Pipeline.\nYou can author your own custom components to use in your ML pipeline. For more\ninformation about authoring custom components, seeBuild your own pipeline\ncomponents.\nTo learn how to author custom Kubeflow Pipelines components, see the\"Pipelines\nwith lightweight components based on Python functions\"Jupyter notebook on GitHub. To learn how to author custom TFX\ncomponents, see theTFX Python function component tutorialon theTensorFlow Extended in Production tutorials.\nA pipeline task is the instantiation of a pipeline component and performs a\nspecific step in your ML workflow. You can author ML pipeline tasks either using\nPython or as prebuilt container images.\nWithin a task, you can build on the on-demand compute capabilities of Vertex AI\nwith Kubernetes to scalably execute your code, or delegate your workload to another\nexecution engine, such as BigQuery, Dataflow, or\nDataproc Serverless.\nFrom definition to execution and monitoring, the lifecycle of an ML pipeline\ncomprises the following high-level stages:\nDefine: The process of defining an ML pipeline and its task is also\ncalled building a pipeline. In this stage, you need to perform the following\nsteps:Choose an ML framework: Vertex AI Pipelines supports ML\npipelines defined using the TFX or Kubeflow Pipelines\nframework. To learn how to choose a framework for building your pipeline, seeInterfaces to define a pipeline.Define pipeline tasks and configure pipeline: For more information, seeBuild a Pipeline.\nDefine: The process of defining an ML pipeline and its task is also\ncalled building a pipeline. In this stage, you need to perform the following\nsteps:\nChoose an ML framework: Vertex AI Pipelines supports ML\npipelines defined using the TFX or Kubeflow Pipelines\nframework. To learn how to choose a framework for building your pipeline, seeInterfaces to define a pipeline.\nChoose an ML framework: Vertex AI Pipelines supports ML\npipelines defined using the TFX or Kubeflow Pipelines\nframework. To learn how to choose a framework for building your pipeline, seeInterfaces to define a pipeline.\nDefine pipeline tasks and configure pipeline: For more information, seeBuild a Pipeline.\nDefine pipeline tasks and configure pipeline: For more information, seeBuild a Pipeline.\nCompile: In this stage, you need to perform the following steps:Generate your ML pipeline definition in a compiled YAML file for\nintermediate representation, which you can use to run your ML pipeline.Optional: You can upload the compiled YAML file as apipeline templateto a repository and reuse it to create ML pipeline runs.\nCompile: In this stage, you need to perform the following steps:\nGenerate your ML pipeline definition in a compiled YAML file for\nintermediate representation, which you can use to run your ML pipeline.\nGenerate your ML pipeline definition in a compiled YAML file for\nintermediate representation, which you can use to run your ML pipeline.\nOptional: You can upload the compiled YAML file as apipeline templateto a repository and reuse it to create ML pipeline runs.\nOptional: You can upload the compiled YAML file as apipeline templateto a repository and reuse it to create ML pipeline runs.\nRun: Create an execution instance of your ML pipeline using the compiled\nYAML file or a pipeline template. The execution instance of a pipeline\ndefinition is called apipeline run.You can create a one-time occurrence of a pipeline run or use thescheduler APIto create recurring pipeline runs from the same\nML pipeline definition. You can also clone an existing pipeline run. To learn\nhow to choose an interface to run an ML pipeline, seeInterfaces to run a\npipeline. For more information about how to\ncreate a pipeline run, seeRun a pipeline.\nRun: Create an execution instance of your ML pipeline using the compiled\nYAML file or a pipeline template. The execution instance of a pipeline\ndefinition is called apipeline run.\nYou can create a one-time occurrence of a pipeline run or use thescheduler APIto create recurring pipeline runs from the same\nML pipeline definition. You can also clone an existing pipeline run. To learn\nhow to choose an interface to run an ML pipeline, seeInterfaces to run a\npipeline. For more information about how to\ncreate a pipeline run, seeRun a pipeline.\nMonitor, visualize, and analyze runs: After you create a pipeline run,\nyou can do the following to monitor the performance, status, and costs of\npipeline runs:Configure email notifications for pipeline failures. For more information,\nseeConfigure email notifications.Use Cloud Logging to create log entries for monitoring events. For more\ninformation, seeView pipeline job logs.Visualize, analyze, and compare pipeline runs. For more information, seeVisualize and analyze pipeline results.Use Cloud Billing export to BigQuery to analyze pipeline run\ncosts. For more information, seeUnderstand pipeline run costs.\nMonitor, visualize, and analyze runs: After you create a pipeline run,\nyou can do the following to monitor the performance, status, and costs of\npipeline runs:\nConfigure email notifications for pipeline failures. For more information,\nseeConfigure email notifications.\nConfigure email notifications for pipeline failures. For more information,\nseeConfigure email notifications.\nUse Cloud Logging to create log entries for monitoring events. For more\ninformation, seeView pipeline job logs.\nUse Cloud Logging to create log entries for monitoring events. For more\ninformation, seeView pipeline job logs.\nVisualize, analyze, and compare pipeline runs. For more information, seeVisualize and analyze pipeline results.\nVisualize, analyze, and compare pipeline runs. For more information, seeVisualize and analyze pipeline results.\nUse Cloud Billing export to BigQuery to analyze pipeline run\ncosts. For more information, seeUnderstand pipeline run costs.\nUse Cloud Billing export to BigQuery to analyze pipeline run\ncosts. For more information, seeUnderstand pipeline run costs.\nOptional: stop or delete pipeline runs: There is no restriction on how\nlong you can keep a pipeline run active. You can optionally do the following:Stop a pipeline run.Pause or resume a pipeline run schedule.Delete an existing pipeline template, pipeline run, or pipeline run schedule.\nOptional: stop or delete pipeline runs: There is no restriction on how\nlong you can keep a pipeline run active. You can optionally do the following:\nStop a pipeline run.\nStop a pipeline run.\nPause or resume a pipeline run schedule.\nPause or resume a pipeline run schedule.\nDelete an existing pipeline template, pipeline run, or pipeline run schedule.\nDelete an existing pipeline template, pipeline run, or pipeline run schedule.\nA pipeline run is an execution instance of your ML pipeline definition. Each\npipeline run is identified by a unique run name. Using Vertex AI Pipelines,\nyou can create an ML pipeline run in the following ways:\nUse the compiled YAML definition of a pipeline\nUse the compiled YAML definition of a pipeline\nUse a pipeline template from the Template Gallery\nUse a pipeline template from the Template Gallery\nFor more information about how to create a pipeline run, seeRun a\npipeline. For more information about how to\ncreate a pipeline run from a pipeline template, seeCreate, upload, and use a\npipeline template.\nFor information about capturing and storing pipeline run metadata\nusing Vertex ML Metadata, seeUse Vertex ML Metadata to track the\nlineage of ML artifacts.\nFor information about using pipeline runs to experiment on your ML workflow\nusing Vertex AI Experiments, seeAdd your pipeline runs to\nexperiments.\nA pipeline run contains several artifacts and parameters, including pipeline\nmetadata. To understand changes in the performance or accuracy of your ML\nsystem, you need to analyze the metadata and the lineage of ML artifacts from\nyour ML pipeline runs. The lineage of an ML artifact includes all the factors\nthat contributed to its creation, along with metadata and references to artifacts\nderived from it.\nLineage graphs help you analyze upstream root cause and downstream impact.\nEach pipeline run produces a lineage graph of parameters and artifacts that are\ninput into the run, materialized within the run, and output from the run.\nMetadata that composes this lineage graph is stored in Vertex ML Metadata.\nThis metadata can also be synced to Dataplex.\nUse Vertex ML Metadata to track pipeline artifact lineageWhen you run a pipeline using Vertex AI Pipelines, all\nparameters and artifact metadata consumed and generated by the pipeline are stored in\nVertex ML Metadata. Vertex ML Metadata is a managed implementation of\nthe ML Metadata library in TensorFlow, and supports registering and\nwriting custom metadata schemas. When you create a pipeline run in\nVertex AI Pipelines, metadata from the pipeline run is\nstored in the default metadata store for the project and region where\nyou execute the pipeline.\nUse Vertex ML Metadata to track pipeline artifact lineage\nWhen you run a pipeline using Vertex AI Pipelines, all\nparameters and artifact metadata consumed and generated by the pipeline are stored in\nVertex ML Metadata. Vertex ML Metadata is a managed implementation of\nthe ML Metadata library in TensorFlow, and supports registering and\nwriting custom metadata schemas. When you create a pipeline run in\nVertex AI Pipelines, metadata from the pipeline run is\nstored in the default metadata store for the project and region where\nyou execute the pipeline.\nUse Dataplex to track pipeline artifact lineageDataplexis a global and cross-project\ndata fabric integrated with multiple systems within Google Cloud, such as\nVertex AI, BigQuery, and Cloud Composer.\n\n\nWithin Dataplex, you can search for a pipeline artifact and\nview its lineage graph. Note that to prevent artifact conflicts, any resource\ncatalogued in Dataplex is identified with afully qualified name (FQN).Learn about Dataplex usage costs.\nUse Dataplex to track pipeline artifact lineage\nDataplexis a global and cross-project\ndata fabric integrated with multiple systems within Google Cloud, such as\nVertex AI, BigQuery, and Cloud Composer.\n\n\nWithin Dataplex, you can search for a pipeline artifact and\nview its lineage graph. Note that to prevent artifact conflicts, any resource\ncatalogued in Dataplex is identified with afully qualified name (FQN).\nLearn about Dataplex usage costs.\nFor more information about tracking the lineage of ML artifacts using\nVertex ML Metadata and Dataplex, seeTrack the lineage of pipeline\nartifacts.\nFor more information about visualizing, analyzing, and comparing pipeline runs,\nseeVisualize and analyze pipeline results.\nFor a list of first-party artifact types defined in Google Cloud Pipeline Components, seeML Metadata artifact types.\nVertex AI Experiments lets you track and analyze various model\narchitectures, hyperparameters, and training environments to find the best model\nfor your ML use case. After you create an ML pipeline run, you can associate it\nwith an experiment or experiment run. By doing so, you can experiment with\ndifferent sets of variables, such as hyperparameters, number of training steps,\nor iterations.\nFor more information about experimenting with ML workflows using\nVertex AI Experiments, seeIntroduction to\nVertex AI Experiments.\nLearn about theinterfaces you can use to define and run pipelines using Vertex AI Pipelines.\nLearn about theinterfaces you can use to define and run pipelines using Vertex AI Pipelines.\nGet started bylearning how to define a pipeline using the Kubeflow Pipelines SDK.\nGet started bylearning how to define a pipeline using the Kubeflow Pipelines SDK.\nLearn how to run a pipeline.\nLearn how to run a pipeline.\nLearn aboutbest practices for implementing custom-trained ML models on Vertex AI.\nLearn aboutbest practices for implementing custom-trained ML models on Vertex AI.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/sql/docs/sqlserver",
    "title": "Cloud SQL for SQL Server documentation",
    "content": "Home\nCloud SQL\nDocumentation\nSQL Server\nCloud SQL for SQL Server is a managed database service that\n  helps you set up, maintain, manage, and administer your\n  SQL Server databases on Google Cloud.\n  For information specific to Microsoft SQL Server, see theSQL Server documentation.Learn more\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate instances\nCreate instances\nConnection Overview\nConnection Overview\nEnable and disable high availability on an instance\nEnable and disable high availability on an instance\nCreate and manage SQL Server databases\nCreate and manage SQL Server databases\nCreate and manage SQL Server users\nCreate and manage SQL Server users\nExporting and importing using BAK files\nExporting and importing using BAK files\nExport and import using SQL dump files\nExport and import using SQL dump files\nCreate backups\nCreate backups\nConfigure database flags\nConfigure database flags\ncloud sql command-line\ncloud sql command-line\nREST API\nREST API\nUse the Cloud SQL Admin API\nUse the Cloud SQL Admin API\nBest practices\nBest practices\nPerformance tips\nPerformance tips\nAuthorize requests\nAuthorize requests\nConfigure VPC Service Controls\nConfigure VPC Service Controls\nClient libraries and sample code for Cloud SQL\nClient libraries and sample code for Cloud SQL\nCloud SQL Admin API error messages\nCloud SQL Admin API error messages\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nTroubleshoot\nTroubleshoot\nCloud SQL feature support by database engine\nCloud SQL feature support by database engine\nRelease notes\nRelease notes\nBilling questions\nBilling questions\nGet support\nGet support\nGoogle Cloud Fundamentals: Core Infrastructure\nThese lectures, demos, and hands-on labs give you an overview of Google Cloud products and services so that you can learn the value of Google Cloud and how to incorporate cloud-based solutions into your business strategies.\nArchitecting with Google Cloud: Design and Process\nThis course features a combination of lectures, design activities, and hands-on labs to show you how to use proven design patterns on Google Cloud to build highly reliable and efficient solutions and operate deployments that are highly available and cost-effective.\nCreating a SQL Server instance integrated with Active Directory using Google Cloud SQL\nThis blog post describes the basic steps required to create a SQL Server instance integrated with Managed Service for Microsoft Active Directory.\nMigrating data from SQL Server 2017 to Cloud SQL for SQL Server using snapshot replication\nShows how to migrate data from Microsoft SQL Server 2017 Enterprise running on Compute Engine to Cloud SQL for SQL Server 2017 Enterprise.MigrationReplication\nMigrating data between SQL Server 2008 and Cloud SQL for SQL Server using backup files\nShows how to migrate data from SQL Server 2008 to Cloud SQL for SQL Server 2017 Enterprise.Migration\nData residency overview\nLearn how to use Cloud SQL to enforce data residency requirements for data.Data residencydata\nUse Secret Manager to handle secrets in Cloud SQL\nLearn how to use Secret Manager to store sensitive information about Cloud SQL instances\n  and users as secrets.Secret Managersecret\nPython SQLAlchemy\nUse SQLAlchemy with your Cloud SQL for SQL Server database\nNode.js sample\nConnecting to your Cloud SQL for SQL Server database in Node.js\nPHP PDO\nConnecting your Cloud SQL for SQL Server database using PHP PDO\nGo web app sample\nSimple examples of connecting to Cloud SQL for SQL Server using Go\nTerraform for Cloud SQL networking\nUse Terraform to create Cloud SQL for SQL Server instances with private networking options.\n.NET sample\nThis sample application demonstrates how to store data in Google Cloud SQL with a SQL Server database when running in Google App Engine Flexible Environment.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/workbench/managed",
    "title": "Vertex AI Workbench: Managed notebooks\n documentation",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI Workbench\nVertex AI Workbench managed notebooks isdeprecated. On\n    April 14, 2025, support for\n    managed notebooks will end and the ability to create managed notebooks instances\n    will be removed. Existing instances will continue to function\n    but patches, updates, and upgrades won't be available. To continue using\n    Vertex AI Workbench, we recommend that youmigrate\n    your managed notebooks instances to Vertex AI Workbench instances.\nManaged notebooks instances are Google-managed environments\n    with integrations and capabilities that help you set up and work in\n    an end-to-end Jupyter notebook-based production environment.Learn more.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate a managed notebooks instance\nCreate a managed notebooks instance\nIntroduction to managed notebooks\nIntroduction to managed notebooks\nQuery data in BigQuery tables from within JupyterLab\nQuery data in BigQuery tables from within JupyterLab\nRun a managed notebooks instance on a Dataproc cluster\nRun a managed notebooks instance on a Dataproc cluster\nRun notebook files with the executor\nRun notebook files with the executor\nAdd a custom container to a managed notebooks instance\nAdd a custom container to a managed notebooks instance\nPricing\nPricing\nRelease notes\nRelease notes\nGet support\nGet support\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/tech-area-overviews",
    "title": "AI and ML",
    "content": "Home\nDocumentation\nLeverage the power of AI/ML models and solutions to transform your organization and solve real-world problems.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nBuild AI applications with enterprise-grade scaling, security, and observability.\nApply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs.\nApply Google's state-of-the-art capabilities to handle your document management needs.\nApply Google's state-of-the-art capabilities to handle your industry-specific needs.\nApply Google's state-of-the-art capabilities to handle your video, images, vision, and augmented reality needs.\nApply Google's state-of-the-art capabilities to handle your search and recommendations needs.\nApply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs.\nTrain ML models from your data using AutoML or your preferred ML framework.\nApply operations best practices to monitor and improve your deployed ML models.\nAccelerate machine learning workloads.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/code/docs/intellij",
    "title": "Cloud Code for IntelliJ documentation",
    "content": "Home\nCloud Code\nDocumentation\nIntelliJ Cloud Code\nCloud Code for IntelliJ provides IDE support to help you develop and deploy\n  Cloud Run, Kubernetes, and App Engine applications, manage your Google Cloud\n  APIs and libraries, view your Cloud Storage content, and add new projects to Cloud Source\n  Repositories, among a wealth of functionality, bringing speed, harmony, and efficiency to your\n  development workflow.Learn more\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nSet up the Cloud Code extension\nSet up the Cloud Code extension\nQuickstart: Deploy a Kubernetes app with Cloud Code for IntelliJ\nQuickstart: Deploy a Kubernetes app with Cloud Code for IntelliJ\nQuickstart: Deploy a Cloud Run service with Cloud Code for IntelliJ\nQuickstart: Deploy a Cloud Run service with Cloud Code for IntelliJ\nGet started with the Gemini Code Assist plugin\nGet started with the Gemini Code Assist plugin\nManage Cloud APIs and Libraries\nManage Cloud APIs and Libraries\nWork with Google Cloud and Kubernetes YAML files\nWork with Google Cloud and Kubernetes YAML files\nExplore Kubernetes development in Cloud Code\nExplore Kubernetes development in Cloud Code\nRun and developing a Kubernetes application\nRun and developing a Kubernetes application\nDeploy to the App Engine Flexible Environment\nDeploy to the App Engine Flexible Environment\nDebug a Kubernetes application\nDebug a Kubernetes application\nDevelop a Cloud Run service locally\nDevelop a Cloud Run service locally\nTroubleshoot common installation issues\nTroubleshoot common installation issues\nGetting support\nGetting support\nRelease notes\nRelease notes\nIntelliJ version support\nIntelliJ version support\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/memorystore/docs/memcached",
    "title": "Memorystore for Memcached documentation",
    "content": "Home\nDocumentation\nMemorystore\nMemorystore for Memcached\nMemorystore for Memcached is a fully managed Memcached service for Google Cloud.\n  Applications running on Google Cloud can achieve extreme performance by\n  leveraging the highly scalable, available, secure Memcached service without the\n  burden of managing complex Memcached deployments.Learn more\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nQuickstart: Create a Memorystore for Memcached instance by using the Google Cloud console\nQuickstart: Create a Memorystore for Memcached instance by using the Google Cloud console\nQuickstart: Create a Memorystore for Memcached instance by using the gcloud CLI\nQuickstart: Create a Memorystore for Memcached instance by using the gcloud CLI\nOverview of Memorystore for Memcached\nOverview of Memorystore for Memcached\nMonitor Memcached instances\nMonitor Memcached instances\nConnect to a Memcached instance\nConnect to a Memcached instance\nCreate and manage Memcached instances\nCreate and manage Memcached instances\nConfigure Memcached instances\nConfigure Memcached instances\nUse the Auto Discovery service\nUse the Auto Discovery service\nEstablish a private services access connection\nEstablish a private services access connection\nSetting up client libraries\nSetting up client libraries\nREST API\nREST API\nPricing\nPricing\nRelease notes\nRelease notes\nBilling questions\nBilling questions\nQuotas and limits\nQuotas and limits\nGetting support\nGetting support\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nTo see an example of using Anthropic's Claude model on Vertex AI,\n      run the \"Use Anthropic's Claude model on Vertex AI\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nThe Anthropic Claude models on Vertex AI offer fully managed and\nserverless models as APIs. To use a Claude model on Vertex AI, send\na request directly to the Vertex AI API endpoint. Because the Anthropic\nClaude models use a managed API, there's no need to provision or manage\ninfrastructure.\nYou can stream your Claude responses to reduce the end-user latency perception.\nA streamed response uses server-sent events (SSE) to incrementally stream the\nresponse.\nYou pay for Claude models as you use them (pay as you go), or you pay a fixed\nfee when using [provisioned throughput][pt]. For pay-as-you-go pricing, seeAnthropic Claude models on the Vertex AI pricing\npage.\nThe following models are available from Anthropic to use in Vertex AI.\nTo access a Claude model, go to its Model Garden model card.\nAnthropic's Claude models support Vertex AI request-response\nlogging. Enable 30-day request-response logging of your prompt and completion\nactivity to track any model misuse by your users. For more information, seeLog\nrequests and responses.\nClaude Opus 4 is Anthropic's most intelligent model and is\nstate-of-the-art for coding and agent capabilities, especially agentic search.\nIt excels for customers needing frontier intelligence:\nAdvanced coding: Independently plan and execute complex development tasks\nend-to-end. It adapts to your style and maintains high code quality\nthroughout.\nLong-horizon tasks and complex problem solving (virtual collaborator):\nUnlock new use cases that involves long-horizon tasks that require memory,\nsustained reasoning, and long chains of actions.\nAI agents: Enable agents to tackle complex, multi-step tasks that require\npeak accuracy.\nAgentic search and research: Connect to multiple data sources to\nsynthesize comprehensive insights across repositories.\nContent creation: Create human-quality content with natural prose. Produce\nlong-form creative content, technical documentation, marketing copy, and\nfrontend design mockups.\nMemory and context management: Incorporates memory capabilities that allow\nit to effectively summarize and reference previous interactions.\nGo to the Claude Opus 4 model card\nClaude Sonnet 4 balances impressive performance for coding with the\nright speed and cost for high-volume use cases:\nCoding: Handle everyday development tasks with enhanced performance—power\ncode reviews, bug fixes, API integrations, and feature development with\nimmediate feedback loops.\nAI Assistants: Power production-ready assistants for real-time\napplications—from customer support automation to operational workflows that\nrequire both intelligence and speed.\nEfficient research: Perform focused analysis across multiple data sources\nwhile maintaining fast response times. Ideal for rapid business intelligence,\ncompetitive analysis, and real-time decision support.\nLarge-scale content: Generate and analyze content at scale with improved\nquality. Create customer communications, analyze user feedback, and produce\nmarketing materials with the right balance of quality and throughput.\nGo to the Claude Sonnet 4 model card\nClaude 3.7 Sonnet is Anthropic's most intelligent model to date\nand the first Claude model to offer extended thinking—the ability to solve\ncomplex problems with careful, step-by-step reasoning.\nClaude 3.7 Sonnet is a single model where you can balance speed\nand quality by choosing between standard thinking for near-instant responses or\nextended thinking for advanced reasoning.\nFor more information about extended thinking, see Anthropic'sdocumentation.\nClaude 3.7 Sonnet is optimized for the following use cases:\nAgentic coding - Claude 3.7 Sonnet is state-of-the-art for\nagentic coding, and can complete tasks across the entire software development\nlifecycle—from initial planning to bug fixes, maintenance to large refactors.\nIt offers strong performance in both planning and solving for complex coding\ntasks, making Claude 3.7 Sonnet an ideal choice to power\nend-to-end software development processes.\nCustomer-facing agents - Claude 3.7 Sonnet offers superior\ninstruction following, tool selection, error correction, and advanced\nreasoning for customer-facing agents and complex AI workflows.\nComputer use - Claude 3.7 Sonnet is our most accurate model for\ncomputer use, enabling developers to direct Claude to use computers the way\npeople do.\nContent generation and analysis - Claude 3.7 Sonnet excels at\nwriting and is able to understand nuance and tone in content to generate more\ncompelling content and analyze content on a deeper level.\nVisual data extraction - With Claude 3.7 Sonnet's robust vision\nskills, it is the right choice for teams that want to extract raw data from\nvisuals like charts or graphs as part of their AI workflow.\nGo to the Claude 3.7 Sonnet model card\nClaude 3.5 Sonnet v2 is a state-of-the-art model for\nreal-world software engineering tasks and agentic capabilities.\nClaude 3.5 Sonnet v2 delivers these advancements at the same\nprice and speed as Claude 3.5 Sonnet.\nThe upgraded Claude 3.5 Sonnet model is capable of interacting\nwith tools that can manipulate a computer desktop environment. For more\ninformation, see theAnthropic documentation.\nClaude 3.5 Sonnet is optimized for the following use cases:\nAgentic tasks and tool use - Claude 3.5 Sonnet offers\nsuperior instruction following, tool selection, error correction, and advanced\nreasoning for agentic workflows that require tool use.\nCoding - For software development tasks ranging from code migrations, code\nfixes, and translations, Claude 3.5 Sonnet offers strong\nperformance in both planning and solving for complex coding tasks.\nDocument Q&A - Claude 3.5 Sonnet combines strong context\ncomprehension, advanced reasoning, and synthesis to deliver accurate and\nhuman-like responses.\nVisual data extraction - With Claude 3.5 Sonnet leading vision\nskills, Claude 3.5 Sonnet can extract raw data from visuals\nlike charts or graphs as part of AI workflows.\nContent generation and analysis - Claude 3.5 Sonnet can\nunderstand nuance and tone in content, generating more compelling content and\nanalyzing content on a deeper level.\nGo to the Claude 3.5 Sonnet v2 model card\nClaude 3.5 Haiku, the next generation of Anthropic's fastest and\nmost cost-effective model, is optimal for use cases where speed and\naffordability matter. It improves on its predecessor across every skill set.\nClaude 3.5 Haiku is optimized for the following use cases:\nCode completions - With its rapid response time and understanding of\nprogramming patterns, Claude 3.5 Haiku excels at providing\nquick, accurate code suggestions and completions in real-time development\nworkflows.\nInteractive chat bots - Claude 3.5 Haiku's improved reasoning\nand natural conversation abilities make it ideal for creating responsive,\nengaging chatbots that can handle high volumes of user interactions\nefficiently.\nData extraction and labeling - Leveraging its improved analysis skills,\nClaude 3.5 Haiku efficiently processes and categorizes data,\nmaking it useful for rapid data extraction and automated labeling tasks.\nReal-time content moderation - With strong reasoning skills and content\nunderstanding, Claude 3.5 Haiku provides fast, reliable content\nmoderation for platforms that require immediate response times at scale.\nGo to the Claude 3.5 Haiku model card\nAnthropic's Claude 3 Opus is a powerful AI model with top-level performance on highly\ncomplex tasks. It can navigate open-ended prompts and sight-unseen scenarios\nwith remarkable fluency and human-like understanding.\nClaude 3 Opus is optimized for the following use cases:\nTask automation, such as interactive coding and planning, or running complex\nactions across APIs and databases.\nTask automation, such as interactive coding and planning, or running complex\nactions across APIs and databases.\nResearch and development tasks, such as research review, brainstorming and\nhypothesis generation, and product testing.\nResearch and development tasks, such as research review, brainstorming and\nhypothesis generation, and product testing.\nStrategy tasks, such as advanced analysis of charts and graphs, financials and\nmarket trends, and forecasting.\nStrategy tasks, such as advanced analysis of charts and graphs, financials and\nmarket trends, and forecasting.\nVision tasks, such as processing images to return text output. Also, analysis\nof charts, graphs, technical diagrams, reports, and other visual content.\nVision tasks, such as processing images to return text output. Also, analysis\nof charts, graphs, technical diagrams, reports, and other visual content.\nGo to the Claude 3 Opus model card\nAnthropic's Claude 3 Haiku is Anthropic's fastest vision and text model for\nnear-instant responses to basic queries, meant for seamless AI experiences\nmimicking human interactions.\nLive customer interactions and translations.\nLive customer interactions and translations.\nContent moderation to catch suspicious behavior or customer requests.\nContent moderation to catch suspicious behavior or customer requests.\nCost-saving tasks, such as inventory management and knowledge extraction from\nunstructured data.\nCost-saving tasks, such as inventory management and knowledge extraction from\nunstructured data.\nVision tasks, such as processing images to return text output, analysis\nof charts, graphs, technical diagrams, reports, and other visual content.\nVision tasks, such as processing images to return text output, analysis\nof charts, graphs, technical diagrams, reports, and other visual content.\nGo to the Claude 3 Haiku model card\nAnthropic's Claude 3.5 Sonnet outperforms Claude 3 Opus on a wide\nrange of Anthropic's evaluations, with the speed and cost of Anthropic's\nmid-tier Claude 3 Sonnet. Claude 3.5 Sonnet is\noptimized for the following use cases:\nCoding, such as writing, editing, and running code with sophisticated\nreasoning and troubleshooting capabilities.\nCoding, such as writing, editing, and running code with sophisticated\nreasoning and troubleshooting capabilities.\nHandle complex queries from customer support by understanding user context and\norchestrating multi-step workflows.\nHandle complex queries from customer support by understanding user context and\norchestrating multi-step workflows.\nData science and analysis by navigating unstructured data and leveraging\nmultiple tools to generate insights.\nData science and analysis by navigating unstructured data and leveraging\nmultiple tools to generate insights.\nVisual processing, such as interpreting charts and graphs that require visual\nunderstanding.\nVisual processing, such as interpreting charts and graphs that require visual\nunderstanding.\nWriting content with a more natural, human-like tone.\nWriting content with a more natural, human-like tone.\nGo to the Claude 3.5 Sonnet model card\nLearn how to use Anthropic's models.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/observability",
    "title": "Observability and monitoring",
    "content": "Home\nDocumentation\nDocumentation, guides, and resources for observability and monitoring across Google Cloud products and services.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nIntegrated monitoring, logging, and trace managed services for applications and systems running on Google Cloud and beyond.\nContinuous profiling of production systems for large-scale operations.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCloud Billing\nDocumentation\nGuides\nWith the FinOps hub, you can monitor and communicate your current savings,\nexplore new recommended opportunities to optimize costs, and plan your\noptimization goals. The FinOps hub presents all of your active savings\nand optimization opportunities in one dashboard. The FinOps hub automatically\ngenerates the dashboard based on historical usage metrics gathered by\nCloud Billing andRecommender,\nincluding recent usage and current commitments.\nThe FinOps hub uses Cloud Billing to retrieve cost data, and variousGoogle Cloud cost recommendersfor optimization and utilization metrics.\nTo access the FinOps hub for a Cloud Billing account — to view\nall available recommendations, the FinOps score, CUDs optimization metrics, and\nutilization insights — you need Cloud Billing permissions.\nDepending on the type of recommendation, you might also need project\npermissions to view the details and apply a recommendation.\nTo access the FinOps hub for a Cloud Billing account, and view\n all available recommendations, the FinOps score, CUDs optimization metrics,\n and utilization insights, you need one of the following predefinedCloud Billing IAM roleson your\n Cloud Billing account:\nBilling Account ViewerBilling Account AdministratorIf you prefer to use a custom role to access the FinOps hub, you need a\n  role with the followingpermissionson your Cloud Billing account:billing.accounts.getbilling.accounts.getSpendingInformationbilling.finOpsBenchmarkInformation.getbilling.finOpsHealthInformation.getrecommender.costRecommendations.listAllView details of a recommendation and apply the recommendationTo view the details and apply a recommendation, you needrecommender-specific permissionson the billing account or project.\n  Depending on thetype of recommendation, you might need the following permissions:Project Viewerrole for each of the projects that you want to see\n   recommendations for.Recommender Viewer roleon your Cloud Billing account.See thelist of FinOps hub cost recommendersfor the recommenders that\n    contribute to hub metrics.To learn how to grant permissions to view recommendation updates, seeRecommender overview.\nBilling Account Viewer\nBilling Account Administrator\nIf you prefer to use a custom role to access the FinOps hub, you need a\n  role with the followingpermissionson your Cloud Billing account:\nbilling.accounts.get\nbilling.accounts.getSpendingInformation\nbilling.finOpsBenchmarkInformation.get\nbilling.finOpsHealthInformation.get\nrecommender.costRecommendations.listAll\nTo view the details and apply a recommendation, you needrecommender-specific permissionson the billing account or project.\n  Depending on thetype of recommendation, you might need the following permissions:\nProject Viewerrole for each of the projects that you want to see\n   recommendations for.\nRecommender Viewer roleon your Cloud Billing account.\nSee thelist of FinOps hub cost recommendersfor the recommenders that\n    contribute to hub metrics.\nTo learn how to grant permissions to view recommendation updates, seeRecommender overview.\nFor more information aboutCloud Billing permissions,\n     see:\nOverview of Cloud Billing access control\nCreate custom roles for Cloud Billing\nUnderstanding predefined Identity and Access Management roles for Cloud Billing\nFor more information about Google Cloudproject permissions,\n     see:\nAccess control for projects with IAM\nIAM basic and predefined roles reference\nTo view the FinOps hub:\nIn the Google Cloud console, go to the FinOps hub.Go to FinOps hub\nIn the Google Cloud console, go to the FinOps hub.\nGo to FinOps hub\nAt the prompt, choose the Cloud Billing account for which you want to\nview the FinOps hub.\nAt the prompt, choose the Cloud Billing account for which you want to\nview the FinOps hub.\nTheFinOps hubdashboard summarizes your current cost optimizations and\nintroduces Google Cloud-recommended optimizations. Information on the\nFinOps hub reflects historical data collected.\nThe FinOps hub considers four optimization practices to create cost-saving\nrecommendations:\nTurning off idle resources\nRight-sizing instances\nOther configuration changes for certain resources\nPurchasing committed use discounts (CUDs)\nMetrics throughout the hub reflect how well you're using those optimizations and\nidentify additional optimization opportunities to reduce costs and improve\nFinOps practices.\nIn a recommendation, the estimated savings for a resource is calculated using\neither a custom contract price or a list price. The pricing type used tocalculate the estimated cost savingsis based on the contract type you might have with Google Cloud and your\nspecific role and cost view permissions on the Cloud Billing account.\nThe estimated savings in a recommendation doesn't consider existing committed\nuse discounts that you have purchased that might apply to the resource.\nTheOptimization summaryis a snapshot of how much you're saving with\noptimizations, current recommendations from Google Cloud, and how much\nmore you could save by adopting additional optimizations. TheOptimization\nsummaryincludes:\nLast month's realized savings- the total savings related to\nCUDs, right-sizing instances, and removing idle resources.Note:Realized savings don't calculate savings from turning off\n  idle resources. Additionally, you might see a negative total if you've\n  underutilized CUDs.\nLast month's realized savings- the total savings related to\nCUDs, right-sizing instances, and removing idle resources.\nActive Recommendations- the total number of\nGoogle Cloud-recommended optimizations available to you.\nRecommendations include suggestions to turn off idle resources, right-size\ninstances or other configuration changes, and purchase CUDs.\nActive Recommendations- the total number of\nGoogle Cloud-recommended optimizations available to you.\nRecommendations include suggestions to turn off idle resources, right-size\ninstances or other configuration changes, and purchase CUDs.\nPotential savings per month- the estimated amount of money you can save\nby applying all available recommendations. If there are multiple potential\nopportunities to save on the same Compute Engine resource, such as\npurchasing a resource-based CUD and a Compute flexible CUD, the FinOps hub\nde-duplicates the opportunities and only includes the recommendation that\nbrings you the higher savings.\nPotential savings per month- the estimated amount of money you can save\nby applying all available recommendations. If there are multiple potential\nopportunities to save on the same Compute Engine resource, such as\npurchasing a resource-based CUD and a Compute flexible CUD, the FinOps hub\nde-duplicates the opportunities and only includes the recommendation that\nbrings you the higher savings.\nCUD optimization rate- the percentage of how much of your CUD-eligible\nusage is covered by committed use discounts. We determine your CUD\noptimization rate by calculating how much of your usage across all products\nover the last 30 days can be converted to CUDs. For example, in the past 30\ndays, you might have spent $10,000 on Google Cloud products eligible\nfor CUDs, and received $4,500 worth of CUDs. In this example, you are using\n$4,500 out of $10,000 in CUDs opportunities, so yourCUD optimization rateis 45%.\nCUD optimization rate- the percentage of how much of your CUD-eligible\nusage is covered by committed use discounts. We determine your CUD\noptimization rate by calculating how much of your usage across all products\nover the last 30 days can be converted to CUDs. For example, in the past 30\ndays, you might have spent $10,000 on Google Cloud products eligible\nfor CUDs, and received $4,500 worth of CUDs. In this example, you are using\n$4,500 out of $10,000 in CUDs opportunities, so yourCUD optimization rateis 45%.\nInsights provided by Gemini Cloud Assist- If you enabledGemini Cloud Assist in Cloud Billing,\nthen you will see Gemini-provided key optimization and\nutilization insights in the Optimization summary widget:\nInsights provided by Gemini Cloud Assist- If you enabledGemini Cloud Assist in Cloud Billing,\nthen you will see Gemini-provided key optimization and\nutilization insights in the Optimization summary widget:\nTheFinOps scorecan help you gauge how well you're using Google Cloud\ntools to monitor and save costs, and how you can continue to optimize costs. The\nscore is a calculation based on how you follow optimization best practices,\nincluding the following:\nMonitoring spend by actively logging in and using Cloud Billing\ntools.\nUsing tools such as tags and labels to allocate costs for your resources.\nOptimizing resources by turning off idle resources and right-sizing\ninstances.\nPurchasing CUDs, including CUDs opportunities recommended by\nGoogle Cloud.\nCreating and monitoring budgets frequently.\nAutomating your cost management by using tools such as the billing\nBigQuery export and using the Budgets API.\nSelectImprove your scoreto review Google Cloud recommended\ncost-saving actions based on three stages of the cloud FinOps journey: inform,\noperate, and optimize.\nThe FinOps score also provides a peer benchmark score as a view of your\noptimization performance in the context of industry verticals based on\naggregated usage data. All customers are automatically opted into peer\nbenchmarking aggregation, but you can choose to opt out at any time. If you\nchoose to opt out, you no longer see CUD recommendations, the FinOps score, or\nthe peer benchmark score in the FinOps hub.\nThe FinOps and peer benchmark scores are based on data collected two days prior,\nand are updated daily. Your score might change depending on factors such as\nnew customers joining the peer benchmark score.\nTo opt in or out of participating in peer benchmarking, you must have the\nBilling Account Administrator role on your Cloud Billing account, and\nthedataprocessing.groupcontrols.updatepermission on your\nCloud Billing account, which is part of the Data Processing Controls\nResource Admin role.\nIn the Google Cloud console, open the Identity and Access Management (IAM)\nTransparency and Control Center for your Cloud Billing account.Go to Transparency and Control Center\nIn the Google Cloud console, open the Identity and Access Management (IAM)\nTransparency and Control Center for your Cloud Billing account.\nGo to Transparency and Control Center\nSelect your Cloud Billing account from the menu.\nSelect your Cloud Billing account from the menu.\nTo opt out of participating in peer benchmarking, in theData processing\ngroupstable, for theBillinggroup, clickDisable.If you want to opt in, selectEnable.\nTo opt out of participating in peer benchmarking, in theData processing\ngroupstable, for theBillinggroup, clickDisable.\nIf you want to opt in, selectEnable.\nWith thePotential savings/monthchart, you can focus on savings byserviceor byproject. The chart shows the total potential monthly\nsavings from all cost optimization opportunities broken down by the associated\nservice or project.\nFrom thePotential savings/monthchart, you can access theRecommendationsdashboard, where you can review all of the FinOps\nrecommendations, view the recommendation details, send recommendations to\nothers to review, and apply recommendations to optimize your cloud costs.\nTo open theRecommendationsdashboard,\nselectView all recommendations.\nTheTop recommendationswidget shows you the top 10 recommendations by\npotential cost savings. Each recommendation displays the estimated monthly\nsavings, the associated service, and a brief description of the recommendation.\nIf you have theRecommender Viewer roleon your Cloud Billing account, you can select a recommendation to\nget further details and to apply the recommendation. Depending on the type of\nrecommendation, you might also need project permissions to view the details\nand apply a recommendation.\n(Preview)Gain insights on resource utilization and the cost of potential waste. For\nresources that provide utilization metrics (such as Compute Engine,\nGoogle Kubernetes Engine, Cloud SQL, and Cloud Run), the chart shows the costs\nof provisioned resources with potentially wasted usage.\nTo determine waste, utilization insights uses various recommenders to help you\nidentify idle resources, right-sizing opportunities of over- or underprovisioned\nresources, and suboptimal configurations, and then summarizes the cost of the\npotential wasted usage for each service that returns utilization metrics.\nTo view the details of the insights and access cost optimization\nrecommendations, clickView utilization insightsto open theUtilization insightsdashboard.\nFor a high-level view of your savings from commitments, use theLast month's realized savingswidget, which breaks down your savings\nby the services that you have purchased commitments for.\nTheCarbon Footprint dashboardpresents the estimated greenhouse\ngas emissions from your Google Cloud usage, helping you optimize cloud spend\nwhile reducing carbon impact. For more information, visit theCarbon Footprint documentation.\nWhen you view theRecommendationsdashboard,\nin the list of recommendations, theRegioncolumn displays a green leaf\nnext to locations that have the lowest carbon impact.\nFor more information, seeLow CO2.\nContinue to optimize costs by taking advantage of Google's cost-saving\nrecommendations. To access and apply recommendations, take any of the\nfollowing actions:\nOn theFinOps scorewidget, clickImprove your scoreto access and apply recommended actions.\nOn thePotential savings/monthwidget, clickView all recommendationsto access theRecommendationsdashboard,\nthen click any of the recommendations to view the details of the\nrecommendation and take action.\nOn thePotential wasted usagewidget, clickView utilization insightsto access theUtilization insightsdashboard.\nOn theTop recommendationswidget, click any of the recommendations to view the details of the\nrecommendation and take action.\nTheFinOps hubdashboard receives metrics from various Google Cloud\ncost recommenders. Use the following table to learn more about each recommender.\nGoogle Cloud Well-Architected Framework: Cost optimization\nOptimize costs with committed use discounts\nPurchasing spend-based committed use discounts\nPurchasing resource-based committed use discounts\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-29 UTC."
  },
  {
    "url": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect",
    "title": "CDN Interconnect overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nNetwork Connectivity\nDocumentation\nGuides\nCDN Interconnect enables select third-party Content Delivery Network (CDN) providers\nto establish direct peering links with Google's edge network at various locations, which enables you\nto direct your traffic from your Virtual Private Cloud (VPC) networks to a provider's network.\nCDN Interconnect enables you to optimize your CDN population costs and use\ndirect connectivity to select CDN providers from Google Cloud.\nYour network traffic egressing from Google Cloud through one of these links benefits from\nthe direct connectivity to supported CDN providers and is billed automatically with reduced\npricing.\nIf your CDN provider is already part of the program, you don't have to do anything. Traffic from\nsupported Google Cloud locations to your CDN provider automatically takes advantage of the\ndirect connection and reduced pricing.\nWork with your supported CDN provider to learn what locations are supported\n  and how to correctly configure your deployment to use intra-region egress routes.\n  CDN Interconnect does not require any configuration or integration with\n  Cloud Load Balancing.\nIf your CDN provider is not part of the program, contact your CDN provider\n  and ask them to work with Google to get connected.\nHigh-volume egress traffic.If you're populating your CDN with large data files from\n  Google Cloud, you can use the CDN Interconnect links between\n  Google Cloud and selected providers to automatically optimize this traffic and save money.\nFrequent content updates.Cloud workloads that frequently update data stored in CDN\n  locations benefit from using CDN Interconnect because the direct link to the\n  CDN provider reduces latency for these CDN destinations.For example, if you have frequently updated data served by the CDN originally\n  hosted on Google Cloud, you might consider using CDN Interconnect.\nFrequent content updates.Cloud workloads that frequently update data stored in CDN\n  locations benefit from using CDN Interconnect because the direct link to the\n  CDN provider reduces latency for these CDN destinations.\nFor example, if you have frequently updated data served by the CDN originally\n  hosted on Google Cloud, you might consider using CDN Interconnect.\nThe special pricing for your traffic egressing from Google Cloud to a CDN\nprovider is automatic. Google works with approved CDN partners in supported\nlocations to allowlist provider IP addresses. This means that any data\nthat you send to your allowlisted CDN provider from Google Cloud is charged at\nthe reduced price. This reduced price applies only to IPv4 traffic. It does not apply to\nIPv6 traffic.\nTraffic between Google Cloud and pre-approved CDN Interconnect\nlocations is billed as follows:\nIngress traffic is free for all regions.\nEgress traffic rates apply only to data leaving Compute Engine or Cloud Storage. Egress charges\n      for CDN Interconnect appear on the invoice asCompute Engine Data Transfer Out via Carrier Peering Network.Forinter-regionCDN Interconnect traffic rates, seeInternet egress rates.\nForinter-regionCDN Interconnect traffic rates, seeInternet egress rates.\nIntra-regionpricing for CDN Interconnect applies only to intra-region\n  egress traffic that is sent toGoogle-approved\n  CDN Interconnect providersat specific locations that\n  Google approves for those providers.\nConsult with your CDN provider to verify that they are an approved provider, and\nif so, which of their CDN locations are approved for this program. They can help\nyou set up your deployment to use intra-region egress routes when using\nGoogle Cloud as the origin source.\nReview the list of CDN Interconnect service providers and choose\nthe option that best suits your needs.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/code/docs/vscode",
    "title": "Cloud Code for VS Code documentation",
    "content": "Home\nCloud Code\nDocumentation\nCloud Code for VS Code\nCloud Code for VS Code provides IDE support for the full development cycle of\n  Kubernetes and Cloud Run applications, from creating a cluster to\n  running your finished application, and for the development of APIs using Apigee.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nInstall the Cloud Code extension\nInstall the Cloud Code extension\nCode with Gemini Code Assist Standard and Enterprise\nCode with Gemini Code Assist Standard and Enterprise\nQuickstart: Deploy a Kubernetes app with Cloud Code for VS Code\nQuickstart: Deploy a Kubernetes app with Cloud Code for VS Code\nQuickstart: Deploy a Cloud Run service with Cloud Code for VS Code\nQuickstart: Deploy a Cloud Run service with Cloud Code for VS Code\nQuickstart: Deploy a Kubernetes app by using remote development\nQuickstart: Deploy a Kubernetes app by using remote development\nManage Cloud APIs and Libraries\nManage Cloud APIs and Libraries\nGet started with Kubernetes in Cloud Code\nGet started with Kubernetes in Cloud Code\nDebug a Kubernetes application\nDebug a Kubernetes application\nWork with Google Cloud and Kubernetes YAML\nWork with Google Cloud and Kubernetes YAML\nDevelop a Cloud Run service locally\nDevelop a Cloud Run service locally\nDevelop APIs using Apigee\nDevelop APIs using Apigee\nUsage statistics\nUsage statistics\nGetting support\nGetting support\nRelease notes\nRelease notes\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/ai-ml",
    "title": "AI and ML",
    "content": "Home\nDocumentation\nLeverage the power of AI/ML models and solutions to transform your organization and solve real-world problems.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nBuild AI applications with enterprise-grade scaling, security, and observability.\nApply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs.\nApply Google's state-of-the-art capabilities to handle your document management needs.\nApply Google's state-of-the-art capabilities to handle your industry-specific needs.\nApply Google's state-of-the-art capabilities to handle your video, images, vision, and augmented reality needs.\nApply Google's state-of-the-art capabilities to handle your search and recommendations needs.\nApply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs.\nTrain ML models from your data using AutoML or your preferred ML framework.\nApply operations best practices to monitor and improve your deployed ML models.\nAccelerate machine learning workloads.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/terraform",
    "title": "Terraform on Google Cloud documentation",
    "content": "Home\nDocumentation\nTerraform on Google Cloud\nLearn how to use Terraform to reliably provision infrastructure on Google Cloud.Learn more\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nTutorialGet started with Terraform\nGet started with Terraform\nReferenceBasic Terraform commands\nBasic Terraform commands\nTutorialStore Terraform state\nStore Terraform state\nTutorialHashiCorp tutorials\nHashiCorp tutorials\nReferenceGoogle Cloud provider\nGoogle Cloud provider\nBest practiceTerraform best practices\nTerraform best practices\nTutorialManage infrastructure as code\nManage infrastructure as code\nReferenceGet support for Terraform issues\nGet support for Terraform issues\nTutorialExport resources into Terraform\nExport resources into Terraform\nTutorialImport resources into Terraform state\nImport resources into Terraform state\nTutorialCreate a configuration with Service Catalog\nCreate a configuration with Service Catalog\nTechnicalUse policy validation\nUse policy validation\nLearn Terraform fundamentals\nIn this lab, you install Terraform and create a VM instance using Terraform.\nLearn how to automate Infrastructure on Google Cloud with Terraform\nIn this lab, you write infrastructure as code with Terraform.\nLearn how to build Cloud Infrastructure with Terraform\nIn this lab, you learn how to describe and launch cloud resources with Terraform.\nLearn about managing state\nIn this lab, you learn how to store Terraform state in Google Cloud Storage.\nLearn to use Terraform modules\nIn this lab, you learn how modules can address problems of code complexity, duplication, and reuse.\nLearn to use policy validation\nIn this lab, you learn how to enforce policies on Terraform configurations.\nResource samples\nFind samples to build your infrastructure.\nBlueprints\nFind deployable, reusable Terraform modules.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/datasets/overview",
    "title": "Overview of creating managed datasets on Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nYou can use a managed dataset to provide the source data used\nto train AutoML and custom models on Vertex AI. A managed\ndataset is required for AutoML and is optional for\ncustom training.\nYou can create managed datasets for training AutoML models by using the\nGoogle Cloud console or the Vertex AI API. The instructions for how to do this\nslightly vary based on your data type and model objective. Start by preparing\nyour training data.\nLearn how to create a managed dataset for the following types of image\nAutoML models:\nImage classification models\nImage object detection models\nLearn how to create a managed dataset for the following types of tabular\nAutoML models:\nTabular classification and regression models\nTabular forecasting models\nLearn how to create a managed dataset for the following types of text\nAutoML models:\nText classification models\nText entity extraction models\nText sentiment analysis models\nLearn how to create a managed dataset for the following types of video\nAutoML models:\nVideo action recognition models\nVideo classification models\nVideo object tracking models\nThe instructions on how to create a managed dataset for training custom models\nare the same, regardless of your data type or model objective.\nFor details, seeUse managed datasets.\nData Catalog is a fully managed, scalable metadata management service\nwithin Dataplex which provides a centralized location to search for datasets\nacross projects and regions.\nFor details, seeUse Data Catalog to search for model and dataset resourcesoverview.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/sql/docs/postgres",
    "title": "Cloud SQL for PostgreSQL documentation",
    "content": "Home\nCloud SQL\nDocumentation\nPostgreSQL\nCloud SQL for PostgreSQL is a fully-managed database service that\nhelps you set up, maintain, manage, and administer your\nPostgreSQL relational databases on Google Cloud Platform.\nLearn more\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate instances\nCreate instances\nConnection overview\nConnection overview\nEnable and disable high availability on an instance\nEnable and disable high availability on an instance\nCreate and manage PostgreSQL databases\nCreate and manage PostgreSQL databases\nCreate and manage PostgreSQL users\nCreate and manage PostgreSQL users\nExport and import using pg_dump, pg_dumpall, and pg_restore\nExport and import using pg_dump, pg_dumpall, and pg_restore\nExport and import using CSV files\nExport and import using CSV files\nCreate backups\nCreate backups\nCreate read replicas\nCreate read replicas\nBuild generative AI applications using Cloud SQL\nBuild generative AI applications using Cloud SQL\nClient libraries and sample code for Cloud SQL\nClient libraries and sample code for Cloud SQL\ngcloud sql command-line\ngcloud sql command-line\nUse the Cloud SQL Admin API\nUse the Cloud SQL Admin API\nREST API\nREST API\nBest practices\nBest practices\nPerformance tips\nPerformance tips\nAuthorize requests\nAuthorize requests\nConfigure VPC Service Controls\nConfigure VPC Service Controls\nCloud SQL Admin API error messages\nCloud SQL Admin API error messages\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nTroubleshoot\nTroubleshoot\nCloud SQL feature support by database engine\nCloud SQL feature support by database engine\nRelease notes\nRelease notes\nBilling questions\nBilling questions\nGetting support\nGetting support\nSecurity bulletins\nSecurity bulletins\nUsing Cloud SQL for PostgreSQL with Ruby on Rails 5\nLearn how to connect a Ruby on Rails 5 app to Cloud SQL for PostgreSQL.\nConnecting to Cloud SQL with Cloud Functions\nLearn how to connect to Cloud SQL from Cloud Functions.\nDeploying Pega using Compute Engine and Cloud SQL\nLearn how to deploy Pega Platform, which is a business process management and customer relationship management (CRM) platform with Cloud SQL for PostgreSQL.CRMReference architecture\nProduction launch checklist\nThis checklist provides recommended activities to complete for launching a commercial application that uses Cloud SQL. This checklist focuses on Cloud SQL-specific activities.ProductionLaunch\nData residency overview\nLearn how to use Cloud SQL to enforce data residency requirements for data.Data residencydata\nUse Secret Manager to handle secrets in Cloud SQL\nLearn how to use Secret Manager to store sensitive information about Cloud SQL instances\n  and users as secrets.Secret Managersecret\nPython SQLAlchemy\nUse SQLAlchemy with your Cloud SQL for PostgreSQL database\nNode.js sample\nConnecting to your Cloud SQL for PostgreSQL database in Node.js\nPHP PDO\nConnecting your Cloud SQL for PostgreSQL database using PHP PDO\nGo web app sample\nSimple examples of connecting to Cloud SQL for PostgreSQL using Go\n.NET sample\nThis sample application demonstrates how to store data in Google Cloud SQL with a PostgreSQL database when running in Google App Engine Flexible Environment.\nTerraform for Cloud SQL networking\nUse Terraform to create Cloud SQL for PostgreSQL instances with private networking options.\nJava servlet\nConnecting to Cloud SQL for PostgreSQL from a Java application\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCloud Key Management Service\nDocumentation\nGuides\nThis topic provides an overview of Cloud HSM and shows you how to create and\nuse HSM-protected encryption keys in Cloud Key Management Service.\nCloud HSM is a cloud-hosted Hardware Security Module (HSM) service that\nallows you to host encryption keys and perform cryptographic operations in a\ncluster ofFIPS 140-2 Level 3certified HSMs. Google\nmanages the HSM cluster for you, so you don't need to worry about clustering,\nscaling, or patching. Because Cloud HSM uses Cloud KMS as\nits front end, you can leverage all the conveniences and features that\nCloud KMS provides.\nWhen you create a key, you add it to a key ring in a given Google Cloud\nlocation. You can create a new key ring or use an existing one. In this topic,\nyou create a new key ring and add a new key to it.\nCreate a key ring in a Google Cloudlocationthat supports\nCloud HSM.\nGo to theKey Managementpage in the Google Cloud console.Go to Key Management\nGo to theKey Managementpage in the Google Cloud console.\nGo to Key Management\nClickCreate key ring.\nClickCreate key ring.\nForKey ring name, enter a name for your key ring.\nForKey ring name, enter a name for your key ring.\nForKey ring location, select a location like\"us-east1\".Note:Choose a location that is near the resources you want to protect.\nFor CMEK usage, your key ring should be in the same location as the resources\nyou use it with. For Cloud EKM keys, the location must be physically\nclose to your external key manager (EKM) vendor.\nForKey ring location, select a location like\"us-east1\".\nClickCreate.\nClickCreate.\nIn the Google Cloud console, activate Cloud Shell.Activate Cloud ShellIn your environment, run thegcloud kms keyrings createcommand:gcloudkmskeyringscreateKEY_RING\\--locationLOCATIONReplace the following:KEY_RING: the name of the key ring that contains the key.LOCATION: the Cloud KMS location of the key ring.For information on all flags and possible values, run the command with the--helpflag.\nIn the Google Cloud console, activate Cloud Shell.\nActivate Cloud Shell\nIn your environment, run thegcloud kms keyrings createcommand:gcloudkmskeyringscreateKEY_RING\\--locationLOCATIONReplace the following:KEY_RING: the name of the key ring that contains the key.LOCATION: the Cloud KMS location of the key ring.For information on all flags and possible values, run the command with the--helpflag.\nReplace the following:\nKEY_RING: the name of the key ring that contains the key.\nLOCATION: the Cloud KMS location of the key ring.\nFor information on all flags and possible values, run the command with the--helpflag.\nTo run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK.\nTo run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK.\nTo run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK.\nTo run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK.\nTo run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK.\nTo run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK.\nTo run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK.\nThese examples usecurlas an HTTP client\n  to demonstrate using the API. For more information about access control, seeAccessing the Cloud KMS API.\nReplace the following:\nPROJECT_ID: the ID of the project that contains the key ring.\nKEY_RING: the name of the key ring that contains the key.\nLOCATION: the Cloud KMS location of the key ring.\nSee theKeyRing.createAPI documentationfor more information.\nFollow these steps to create a Cloud HSM key on the specified key ring\nand location.\nGo to theKey Managementpage in the Google Cloud console.Go to the Key Management page\nGo to theKey Managementpage in the Google Cloud console.\nGo to the Key Management page\nClick the name of the key ring for which you will create a key.\nClick the name of the key ring for which you will create a key.\nClickCreate key.\nClickCreate key.\nIn theWhat type of key do you want to create?, chooseGenerated\nkey.\nIn theWhat type of key do you want to create?, chooseGenerated\nkey.\nIn theKey namefield, enter the name for your key.\nIn theKey namefield, enter the name for your key.\nClick theProtection leveldropdown and selectHSM.\nClick theProtection leveldropdown and selectHSM.\nClick thePurposedropdown and selectSymmetric encrypt/decrypt.\nClick thePurposedropdown and selectSymmetric encrypt/decrypt.\nAccept the default values forRotation periodandStarting on.\nAccept the default values forRotation periodandStarting on.\nClickCreate.\nClickCreate.\nTo use Cloud KMS on the command line, firstInstall or upgrade to the latest version of Google Cloud CLI.\nReplacekeywith a name for the new key. Replacekey-ringwith the name of the existing key ring where the key will be located. Replacelocationwith the Cloud KMS location for the key ring.\nFor information on all flags and possible values, run the command with the--helpflag.\nTo run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK.\nTo run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK.\nTo run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK.\nTo run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK.\nTo run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK.\nTo run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK.\nTo run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK.\nNow that you have a key, you can use that key to encrypt text or binary content.\nTo use Cloud KMS on the command line, firstInstall or upgrade to the latest version of Google Cloud CLI.\nReplace the following:\nKEY_NAME: the name of the key that you want to use for encryption.\nKEY_RING: the name of the key ring that contains the key.\nLOCATION: the Cloud KMS location that contains the key\nring.\nFILE_TO_ENCRYPT: the path to the file that you want to\nencrypt.\nENCRYPTED_OUTPUT: the path where you want to save the\nencrypted output.\nFor information on all flags and possible values, run the command with the--helpflag.\nTo run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK.\nTo run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK.\nTo run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK.\nTo run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK.\nTo run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK.\nTo run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK.\nTo run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK.\nThese examples usecurlas an HTTP client\n  to demonstrate using the API. For more information about access control, seeAccessing the Cloud KMS API.\nWhen using JSON and the REST API, content must be base64 encoded before it can\nbe encrypted by Cloud KMS.\nTip: You can base64-encode or decode data\nusing thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting\nlanguages typically include libraries for base64-encoding. For command-line\nexamples, seeBase64 Encodingin the\nCloud Vision API documentation.\nTip: You can base64-encode or decode data\nusing thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting\nlanguages typically include libraries for base64-encoding. For command-line\nexamples, seeBase64 Encodingin the\nCloud Vision API documentation.\nTo encrypt data, make aPOSTrequest and provide the appropriate project and\nkey information and specify the base64 encoded text to be encrypted in theplaintextfield of the request body.\nReplace the following:\nPROJECT_ID: the ID of the project that contains the key ring and\nkey that you want to use for encryption.\nLOCATION: the Cloud KMS location that contains the key\nring.\nKEY_RING: the key ring that contains the key that you want to use\nfor encryption.\nKEY_NAME: the name of the key that you want to use for encryption.\nPLAINTEXT_TO_ENCRYPT: the plaintext data that you want\nto encrypt. The plaintext must be base64 encoded before you call theencryptmethod.\nHere is an example payload with base64 encoded data:\nTo decrypt encrypted content, you must use the same key that was used to encrypt\nthe content.\nTo use Cloud KMS on the command line, firstInstall or upgrade to the latest version of Google Cloud CLI.\nReplace the following:\nKEY_NAME: the name of the key that you want to use for decryption.\nKEY_RING: the name of the key ring that contains the key.\nLOCATION: the Cloud KMS location that contains the key\nring.\nFILE_TO_DECRYPT: the path to the file that you want to\ndecrypt.\nDECRYPTED_OUTPUT: the path where you want to save the\ndecrypted output.\nFor information on all flags and possible values, run the command with the--helpflag.\nTo run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK.\nTo run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK.\nTo run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK.\nTo run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK.\nTo run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK.\nTo run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK.\nTo run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK.\nThese examples usecurlas an HTTP client\n  to demonstrate using the API. For more information about access control, seeAccessing the Cloud KMS API.\nDecrypted text that is returned in the JSON from Cloud KMS is\nbase64 encoded.\nTip: You can base64-encode or decode data\nusing thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting\nlanguages typically include libraries for base64-encoding. For command-line\nexamples, seeBase64 Encodingin the\nCloud Vision API documentation.\nTip: You can base64-encode or decode data\nusing thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting\nlanguages typically include libraries for base64-encoding. For command-line\nexamples, seeBase64 Encodingin the\nCloud Vision API documentation.\nTo decrypt encrypted data, make aPOSTrequest and provide the appropriate\nproject and key information and specify the encrypted text (also known asciphertext) to be decrypted in theciphertextfield of the request body.\nReplace the following:\nPROJECT_ID: the ID of the project that contains the key ring and\nkey that you want to use for decryption.\nLOCATION: the Cloud KMS location that contains the key\nring.\nKEY_RING: the key ring that contains the key that you want to use\nfor decryption.\nKEY_NAME: the name of the key that you want to use for decryption.\nENCRYPTED_DATA: the encrypted data that you want\nto decrypt.\nHere is an example payload with base64 encoded data:\nThe encryption example in this topic used a symmetric key with theprotection levelof HSM. To encrypt using an asymmetric key with the\nprotection level of HSM, follow the steps atEncrypting and decrypting data with an asymmetric keywith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM.\nThe encryption example in this topic used a symmetric key with theprotection levelof HSM. To encrypt using an asymmetric key with the\nprotection level of HSM, follow the steps atEncrypting and decrypting data with an asymmetric keywith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM.\nCreate the key ring in one of thesupported regions for Cloud HSM.\nCreate the key with protection level HSM.\nTo use an asymmetric key with theprotection levelof HSM for elliptic\ncurve signing or RSA signing, follow the steps atCreate and validate\nsignatureswith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM.\nTo use an asymmetric key with theprotection levelof HSM for elliptic\ncurve signing or RSA signing, follow the steps atCreate and validate\nsignatureswith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM.\nCreate the key ring in one of thesupported regions for Cloud HSM.\nCreate the key with protection level HSM.\nStartusing the API.\nStartusing the API.\nTake a look at theCloud KMS API Reference.\nTake a look at theCloud KMS API Reference.\nReadHow-to guidesto get started with creating, rotating, and setting\npermissions on keys.\nReadHow-to guidesto get started with creating, rotating, and setting\npermissions on keys.\nReadConceptsto better understand object hierarchy, key states, and\nkey rotation.\nReadConceptsto better understand object hierarchy, key states, and\nkey rotation.\nLearn aboutLoggingin Cloud KMS. Note that logging is\nbased on operations, and applies to keys with both HSM and software\nprotection levels.\nLearn aboutLoggingin Cloud KMS. Note that logging is\nbased on operations, and applies to keys with both HSM and software\nprotection levels.\nLearn more about how Cloud HSM protects your data in theCloud HSM architecture whitepaper.\nLearn more about how Cloud HSM protects your data in theCloud HSM architecture whitepaper.\nMessage size is limited to 8 KiB (as opposed to 64 KiB for\nCloud KMS software keys) for user-provided plaintext and\nciphertext, including theadditional authenticated data.\nMessage size is limited to 8 KiB (as opposed to 64 KiB for\nCloud KMS software keys) for user-provided plaintext and\nciphertext, including theadditional authenticated data.\nCloud HSM may not be available in certain multi or dual regions.\nFor details, seeSupported regions for Cloud HSM.\nCloud HSM may not be available in certain multi or dual regions.\nFor details, seeSupported regions for Cloud HSM.\nIf you use Cloud HSM keys with customer-managed encryption key\n(CMEK) integrations in other Google Cloud services, the locations you\nuse for the services must match the locations of your Cloud HSM\nkeys exactly. This applies to regional, dual-regional, and multi-regional\nlocations.For more information about CMEK integrations, see the relevant section ofEncryption at rest.\nIf you use Cloud HSM keys with customer-managed encryption key\n(CMEK) integrations in other Google Cloud services, the locations you\nuse for the services must match the locations of your Cloud HSM\nkeys exactly. This applies to regional, dual-regional, and multi-regional\nlocations.\nFor more information about CMEK integrations, see the relevant section ofEncryption at rest.\nCurrently key operations for asymmetric keys stored in Cloud HSM\nmay incur a noticeably greater latency compared to using\nCloud KMS software keys.\nCurrently key operations for asymmetric keys stored in Cloud HSM\nmay incur a noticeably greater latency compared to using\nCloud KMS software keys.\nGoogle Cloud offers additional HSM options, such as single-tenancy.Bare Metal Rack HSMis available for customers to host their own HSMs\nin Google-provided space. Inquire with your account representative for\nadditional information.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-15 UTC."
  },
  {
    "url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCompute Engine\nDocumentation\nGuides\nUse operating system (OS) images to create boot disks for your virtual machine\n(VM) instances. You can use one of the following OS image types:\nPublic OS imagesare provided and maintained by\nGoogle, open source communities, and third-party vendors. By default, all\nGoogle Cloud projects have access to these OS images and can use them tocreate VM instances.\nCustom OS imagesare available only to your\nGoogle Cloud project. You cancreate a custom OS imagefrom boot disks and other images. Then, use the custom OS image tocreate VM instances.\nSome OS images are also capable of runningcontainers on Compute Engine.\nCompute Engine offers many preconfigured public OS images that have\ncompatible Linux or Windows operating systems. Use these OS images tocreate and start instances.\nCompute Engine uses your selected image to create a persistent boot\ndisk for each VM. By default, the boot disk for a VM is the same\nsize as the image that you selected. If your VM requires a larger\nboot disk than the image size,resize the boot disk.\nTo see a full list of public OS images with each image's name, size, and\nversion number, you can use the Google Cloud console or the Google Cloud CLI.\nCompute Engine updates public OS images regularly, or when a patch for\na critical impact common vulnerability and exposure (CVE) is available.\nCompute Engine provides 64-bit versions of these public OS images.\nFor more information about each OS, including how each OS is customized to run\non Compute Engine, seeOperating system details.\nIn the Google Cloud console, go to theImagespage.Go to ImagesBy default, the Google Cloud console list all OS images available in theCompute Engine images,Deep Learning VM Images,\nandHPC imagesprojects.\nIn the Google Cloud console, go to theImagespage.\nGo to Images\nBy default, the Google Cloud console list all OS images available in theCompute Engine images,Deep Learning VM Images,\nandHPC imagesprojects.\nBy default, the gcloud CLI list all OS images available in theCompute Engine imagesprojects.\nA custom OS image is a boot disk image that you own and control access to. Use\ncustom OS images for the following tasks:\nImport a virtual diskto Compute Engine from your on-premises environment or from VMs that\nare running on your local workstation or on another cloud platform. You can\nmanuallyimport boot disk imagesto Compute Engine, but one disk at a time.Note:If you are planning to migrate several VMs to\nCompute Engine, consider using theVM migration service.\nImport a virtual diskto Compute Engine from your on-premises environment or from VMs that\nare running on your local workstation or on another cloud platform. You can\nmanuallyimport boot disk imagesto Compute Engine, but one disk at a time.\nCreate an imagefrom the boot disks of your existing Compute Engine\nVM instances. Then use that image tocreate new boot disksfor your VMs. This process lets you create new VMs that\nare preconfigured with the apps that you need without having to\nconfigure apublic OS imagefrom scratch.\nCreate an imagefrom the boot disks of your existing Compute Engine\nVM instances. Then use that image tocreate new boot disksfor your VMs. This process lets you create new VMs that\nare preconfigured with the apps that you need without having to\nconfigure apublic OS imagefrom scratch.\nCopy one image to another image by using either thegcloud CLIor theAPI. Use the same process that\nyou use tocreate an image,\nbut specify another image as the image source. You can also\ncreate an image from a custom image in a different project.\nCopy one image to another image by using either thegcloud CLIor theAPI. Use the same process that\nyou use tocreate an image,\nbut specify another image as the image source. You can also\ncreate an image from a custom image in a different project.\nSome guest operating system features are available only on certain OS images.\nFor example,multiqueue SCSIis\nenabled only on some public OS images.\nTo enable these features on your custom OS images, specify one or\nmore guest operating system features when youcreate a custom OS image.\nPremium OS images,\nwhether public or custom, incur licensing fees to run on\nCompute Engine. You have two options:\nAttach an on-demand/pay-as-you-go (PAYG) license\nBring your own license (BYOL)/Bring your own subscription (BYOS)For more information about licenses, seeLicense types and pricing.\nBring your own license (BYOL)/Bring your own subscription (BYOS)\nFor more information about licenses, seeLicense types and pricing.\nFor custom OS images, you also incur animage storage chargewhile you\n keep your custom OS image in your project.\nImage families help you manage images in your project by\ngrouping related images together, so that you can roll forward and\nroll back between specific image versions. An image family always points to the\nlatest version of an OS image that is not deprecated. Mostpublic OS imagesare grouped into an image\nfamily. For example, thedebian-11image family in thedebian-cloudproject\nalways points to the most recent Debian 11 image.\nIf you regularly update yourcustom OS imageswith newer configurations and software, you can group those images into a custom\nimage family. The image family always points to the most recent OS image in that\nfamily, so your instance templates and scripts can use that image without having\nto update references to a specific image version.\nAlso, because the image family never points to a deprecated image, you can roll\nthe image family back to a previous OS image version by deprecating the most\nrecent image in that family. Note that the rollback is possible only if the\nprevious image version is not deprecated. For more information, seeSetting image versions in an image family.\nFor best practices recommendations when working with image families, seeImage families best practices.\nThese are operating systems that you can run on Google Cloud,\nbut the partner or distributor is responsible for ensuring that these operating\nsystems work with Google Cloud features and that security updates are\nmaintained. For issues specific to the partner supported operating systems, you\nmust use either community resources or get enterprise-level support from the\npartner.\nThe following partner supported operating systems can run on Google Cloud.\nOracle Linux is an operating system that is offered by Oracle.\nOracle Linux images are available on Google Cloud provided by Oracle. You\ncan also import Oracle Linux images to Google Cloud.\nIf you require support that is specific to the Oracle Linux operating system,\nyou can either consult community resources, or get enterprise-level support\ndirectly from Oracle.\nImport Oracle Linux OS images\nTo import Oracle Linux OS image to Compute Engine, you can use the import tool\navailable from Migrate to Virtual Machines. This tool ensures the imported OS images\nare set up correctly for working in a Google Cloud environment. For detailed\ninstructions, seeImport virtual disk images.\nFor a list of the Oracle Linux OS versions supported for import, seeOperating systems supported by partners.\nCommunity-supported OS images are not directly supported by Google Cloud.\nIt is up to the project community to ensure that these OS images work with\nGoogle Cloud features and that security updates are maintained.\nCommunity-supported images are provided as-is by the project communities that\nbuild and maintain them.\nThe following community supported images can run on Google Cloud.\nAlmaLinux is an operating system offered by theAlmaLinux project.\nAlmaLinux images are available in thealmalinux-cloudproject. To\nlist AlmaLinux OS images, use the followinggcloudcommand:\nFedora Cloud is an operating system maintained by theFedora Cloud project.\nFedora Cloud images are available in thefedora-cloudproject. To list\nFedora Cloud OS images, use the followinggcloudcommand:\nFreeBSD is an operating system maintained by theFreeBSD\nproject.\nFreeBSD images are available in thefreebsd-org-cloud-devproject. To list\nFreeBSD OS images, use the followinggcloudcommand:\ngVNIC support for FreeBSD (Preview) is\navailable with release 14.0 and later. To use gVNIC with other releases, the\ndriver can beinstalled manually.\nTo create a VM that uses gVNIC with a FreeBSD release earlier than 14.0, you mustcreate a custom OS image that supports gVNICand then use that OS image when creating the VM.\nopenSUSEis a Linux-based operating system sponsored by SUSE. openSUSE images are\navailable in theopensuse-cloudproject. To list openSUSE OS images, use the\nfollowinggcloudcommand:\nThe following OS images are available for creating VMs that are optimized\nto run high performance computing (HPC) workloads on Compute Engine:\nFor CentOS 7:\nImage family:hpc-centos-7, Image project:cloud-hpc-image-public\nFor Rocky Linux 8:\nImage family:hpc-rocky-linux-8, Image project:cloud-hpc-image-public\nFor information about using this OS image, seeCreating an HPC-ready VM instance.\nView the source imagefor a\nVM instance.\nReadImage management best practices.\nLearn aboutSupport and maintenance policy for OS images.\nCreate and start an instance.\nRead aboutCompute Engine instances.\nCreate a custom image.\nBuild an image from scratch.\nIf you're new to Google Cloud, create an account to evaluate how\n        Compute Engine performs in real-world\n        scenarios. New customers also get $300 in free credits to run, test, and\n        deploy workloads.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/network-connectivity/docs/network-connectivity-center",
    "title": "Network Connectivity Center documentation",
    "content": "Home\nNetwork Connectivity\nDocumentation\nNetwork Connectivity Center\nNetwork Connectivity Center lets you use a hub-and-spoke architecture for network connectivity\nmanagement. With this architecture, you can conduct data\ntransfer between your sites. Network Connectivity Center lets you create VPC spokes to\nconnect VPC networks together for full\nmesh connectivity. It also\nincludes the Router appliance feature, which lets you use a third-party\nnetwork virtual appliance to establish site-to-site or\nsite-to-cloud connectivity.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nWorking with hubs and spokes\nWorking with hubs and spokes\nViewing logs and metrics\nViewing logs and metrics\nCreating router appliance instances\nCreating router appliance instances\nDeleting router appliance instances\nDeleting router appliance instances\nNetwork Connectivity Center overview\nNetwork Connectivity Center overview\nVPC spokes overview\nVPC spokes overview\nRouter appliance overview\nRouter appliance overview\nTopologies\nTopologies\nLocations\nLocations\nAccess control\nAccess control\nAudit logging\nAudit logging\nAPIs and gcloud reference\nAPIs and gcloud reference\nTroubleshooting\nTroubleshooting\nQuotas and limits\nQuotas and limits\nPartners\nPartners\nPricing\nPricing\nRelease notes\nRelease notes\nGetting support\nGetting support\nBilling questions\nBilling questions\nConnecting two branch offices using HA VPN spokes\nThis tutorial describes how to use a Network Connectivity Center hub and Cloud VPN spokes to set\n             up data transfer between two branch offices.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGoogle Cloud Observability\nLogging\nDocumentation\nGuides\nThis document provides a conceptual overview of Cloud Audit Logs.\nGoogle Cloud services write audit logs that record administrative activities\nand accesses within your Google Cloud resources. Audit logs help you\nanswer \"who did what, where, and when?\" within your Google Cloud resources\nwith the same level of transparency as in on-premises environments. Enabling\naudit logs helps your security, auditing, and compliance entities monitor\nGoogle Cloud data and systems for possible vulnerabilities or external\ndata misuse.\nFor a list of Google Cloud services that provide audit logs, seeGoogle Cloud services with audit logs. All\nGoogle Cloud services will eventually provide audit logs.\nFor an overview of Google Workspace audit logs, seeAudit logs for Google Workspace.\nTo view audit logs, you must have the appropriateIdentity and Access Management (IAM)permissions and roles:\nTo get the permissions that\n      you need to get read-only access to Admin Activity, Policy Denied, and\nSystem Event audit logs,\n    \n      ask your administrator to grant you theLogs Viewer(roles/logging.viewer)\n     IAM role on your project.If you have only the Logs Viewer role(roles/logging.viewer), then you\ncannot view Data Access audit logs that are in the_Defaultbucket.To get the permissions that\n      you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs,\n    \n      ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer)\n     IAM role on your project.The Private Logs Viewer role(roles/logging.privateLogViewer)includes the\npermissions contained in the Logs Viewer role (roles/logging.viewer),\nand those necessary to read Data Access audit logs in the_Defaultbucket.For more information about the IAM permissions and roles that\napply to audit logs data, seeAccess control with IAM.Types of audit logsCloud Audit Logs provides the following audit logs for each\nGoogle Cloud project, folder, and organization:Admin Activity audit logsData Access audit logsSystem Event audit logsPolicy Denied audit logsNote:Log entries written by Cloud Audit Logs are immutable.Admin Activity audit logsAdmin Activity audit logs contain log entries for API calls or other actions\nthat modify the configuration or metadata of resources. For example, these logs\nrecord when users create VM instances or change Identity and Access Management permissions.Admin Activity audit logs are always written; you can't configure, exclude, or\ndisable them. Even if you disable the Cloud Logging API, Admin Activity audit\nlogs are still generated.For a list of services that write Admin Activity audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logsData Access audit logs contain API calls that read the configuration or metadata\nof resources, as well as user-driven API calls that create, modify, or read\nuser-provided resource data.Publicly available resources that have the Identity and Access Management policiesallAuthenticatedUsersorallUsersdon't generate  audit logs. Resources\nthat can  be accessed without logging into a Google Cloud,\nGoogle Workspace, Cloud Identity, or Drive Enterprise account don't\ngenerate audit logs. This helps protect end-user identities and information.Data Access audit logs—except for\nBigQuery Data Access audit logs—are disabled by default because\naudit logs can be quite large. If you want Data\nAccess audit logs to be written for Google Cloud services other than\nBigQuery, you must explicitly enable them. Enabling the logs might\nresult in your Google Cloud project being charged for the additional logs\nusage. For instructions on enabling and configuring Data Access audit logs, seeEnable Data Access audit logs.For a list of services that write Data Access audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logs are stored in the_Defaultlog bucket unless\nyou've routed them elsewhere. For more information, see theStoring and routing audit logssection of this page.System Event audit logsSystem Event audit logs contain log entries for Google Cloud actions that\nmodify the configuration of resources. System Event audit logs are generated by\nGoogle Cloud systems; they aren't driven by direct user action.System Event audit logs are always written; you can't configure, exclude, or\ndisable them.For a list of services that write System Event audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Policy Denied audit logsPolicy Denied audit logs are recorded when a Google Cloud service denies access\nto a user orservice accountbecause of a security\npolicy violation.Policy Denied audit logs are generated by default and your\nGoogle Cloud project is charged for the logs storage. You can't disable Policy\nDenied audit logs, but you can useexclusion filtersto prevent Policy\nDenied audit logs from being stored in Cloud Logging.For a list of services that write Policy Denied audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Audit log entry structureEvery audit log entry in Cloud Logging is an object of typeLogEntry. What distinguishes an audit log entry from other log\nentries is theprotoPayloadfield; this field contains anAuditLogobject that stores the audit logging data.To understand how to read and interpret audit log entries, and for a sample of\nan audit log entry, seeUnderstanding audit logs.Log nameCloud Audit Logs log names include the following:Resource identifiers indicating the Google Cloud project or\nother Google Cloud entity that owns the audit logs.The stringcloudaudit.googleapis.com.A string that indicates whether the log contains Admin Activity, Data Access,\nPolicy Denied, or System Event audit logging data.The following are the audit log names, including variables for the resource\nidentifiers:projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivityprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fpolicyfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Factivityfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fdata_accessfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2FpolicybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2FactivitybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fpolicyorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Factivityorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fdata_accessorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2FpolicyCaller identities in audit logsAudit logs record the identity that performed the logged operations on the\nGoogle Cloud resource. The caller's identity is held in theAuthenticationInfofield ofAuditLogobjects.Audit logging doesn't redact the caller's principal email address for any\naccess that succeeds or for any write operation.For read-only operations that fail with a \"permission denied\" error,\nAudit Logging might redact the caller's principal email address unless the\ncaller is a service account.In addition to the conditions listed above, the following applies to certain\nGoogle Cloud services:Legacy App Engine API: Identities aren't\ncollected.BigQuery: Caller\nidentities and IP addresses, as well as some resource names, are redacted\nfrom the audit logs, unless certain conditions are met.Cloud Storage: When Cloud Storage\nusage logs are enabled, Cloud Storage writes usage data to the\nCloud Storage bucket, which generates Data Access audit logs for the\nbucket. The generated Data Access audit log has its caller identity\nredacted.Firestore: If a JSON Web Token\n(JWT) was used for third-party authentication, thethirdPartyPrincipalfield includes the token's header and payload. For example, audit logs for\nrequests authenticated withFirebase Authenticationinclude that\nrequest'sauth token.VPC Service Controls: For\nPolicy Denied audit logs, the following redaction occurs:Parts of the caller email addresses might be redacted and replaced by\nthree period characters....Some caller email addresses belonging to the domaingoogle.comare\nredacted and replaced bygoogle-internal.Organization Policy:\nParts of the caller email addresses might be redacted and replaced by\nthree period characters....IP address of the caller in audit logsThe IP address of the caller is held in theRequestMetadata.callerIpfield of\ntheAuditLogobject:For a caller from the internet, the address is a public IPv4 or IPv6\naddress.For calls made from inside the internal production network from one\nGoogle Cloud service to another, thecallerIpis redacted to \"private\".For a caller from a Compute Engine VM with a external IP address, thecallerIpis the external address of the VM.For a caller from a Compute Engine VM without a external IP address, if\nthe VM is in the same organization or project as the accessed resource, thencallerIpis the VM's internal IPv4 address. Otherwise, thecallerIpis\nredacted to \"gce-internal-ip\". For more information, seeVPC network overview.Viewing audit logsYou can query for all audit logs or you can query for logs by their\naudit log name. The audit log name includes theresource identifierof the Google Cloud project, folder, billing account, or\norganization for which you want to view audit logging information.\nYour queries can specify indexedLogEntryfields.\nFor more information about querying your logs, seeBuild queries in the Logs ExplorerThe Logs Explorer lets you view filter individual log entries. If you want\nto use SQL to analyze groups of log entries, then use theLog Analyticspage. For more information, see:Query and view logs in Log Analytics.Sample queries for security insights.Chart query results.Most audit logs can be viewed in Cloud Logging by using the\nGoogle Cloud console, the Google Cloud CLI, or the Logging API.\nHowever, for audit logs related to billing, you can only use the\nGoogle Cloud CLI or the Logging API.ConsoleIn the Google Cloud console, you can use the Logs Explorer\nto retrieve your audit log entries for your Google Cloud project, folder,\nor organization:Note:You can't view audit logs for Cloud Billing accounts in the\nGoogle Cloud console. You must use the API or the gcloud CLI.In the Google Cloud console, go to theLogs Explorerpage:Go toLogs ExplorerIf you use the search bar to find this page, then select the result whose subheading isLogging.Select an existing Google Cloud project, folder, or organization.To display all audit logs, enter either of the following queries\ninto the query-editor field, and then clickRun query:logName:\"cloudaudit.googleapis.com\"protoPayload.\"@type\"=\"type.googleapis.com/google.cloud.audit.AuditLog\"To display the audit logs for a specific resource and audit log type,\nin theQuery builderpane, do the following:InResource type, select the Google Cloud resource whose\naudit logs you want to see.InLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.ClickRun query.If you don't see these options, then there aren't any audit logs of\nthat type available in the Google Cloud project, folder, or\norganization.If you're experiencing issues when trying to view logs in the\nLogs Explorer, see thetroubleshootinginformation.For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer.gcloudThe Google Cloud CLI provides a command-line interface to the\nLogging API. Supply a valid resource identifier in each of the log\nnames. For example, if your query includes aPROJECT_ID, then the\nproject identifier you supply must refer to the currently selected\nGoogle Cloud project.To read your Google Cloud project-level audit log entries, run\nthe following command:gcloud logging read \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" \\\n    --project=PROJECT_IDTo read your folder-level audit log entries, run the following command:gcloud logging read \"logName : folders/FOLDER_ID/logs/cloudaudit.googleapis.com\" \\\n    --folder=FOLDER_IDTo read your organization-level audit log entries, run the following\ncommand:gcloud logging read \"logName : organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com\" \\\n    --organization=ORGANIZATION_IDTo read your Cloud Billing account-level audit log entries, run the following command:gcloud logging read \"logName : billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com\" \\\n    --billing-account=BILLING_ACCOUNT_IDAdd the--freshnessflagto your command to read logs that are more than 1 day old.For more information about using the gcloud CLI, seegcloud logging read.RESTWhen building your queries, supply a valid resource identifier in each of\nthe log names. For example, if your query includes aPROJECT_ID,\nthen the project identifier you supply must refer to the currently selected\nGoogle Cloud project.For example, to use the Logging API to view your project-level\naudit log entries, do the following:Go to theTry this APIsection in the documentation for theentries.listmethod.Put the following into theRequest bodypart of theTry this\nAPIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.{\n  \"resourceNames\": [\n    \"projects/PROJECT_ID\"\n  ],\n  \"pageSize\": 5,\n  \"filter\": \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\"\n}ClickExecute.Storing and routing audit logsCloud Logging useslog bucketsas\ncontainers that store and organize your logs data. For each billing account,\nGoogle Cloud project, folder, and organization, Logging\nautomatically creates two log buckets,_Requiredand_Default, and\ncorrespondingly namedsinks.Cloud Logging_Requiredbuckets store Admin Activity audit logs\nand System Event audit logs. You can't prevent Admin Activity or System Event\naudit logs from being stored. You also can't configure the sink that routes\nlog entries to the_Requiredbuckets.Admin Activity audit logs and System Event audit logs are always stored in the_Requiredbucket in the project where the logs were generated.If you route Admin Activity audit logs and System Event audit logs to a\ndifferent project, then those logs don't pass through the_Defaultor_Requiredsink of the destination project. Therefore, these logs aren't stored\nin the_Defaultlog bucket or the_Requiredlog bucket of the destination\nproject. To store these logs, create a log sink in the destination project.\nFor more information, seeRoute logs to supported destinations.The_Defaultbuckets, by default, store any enabled Data Access\naudit logs as well as Policy Denied audit logs. To prevent Data Access audit\nlogs from being stored in the_Defaultbuckets, you can disable them. To\nprevent any Policy Denied audit logs from being stored in the_Defaultbuckets, you can exclude them by modifying their sinks' filters.You can also route your audit log entries to user-defined\nCloud Logging buckets at the Google Cloud project level or to supported\ndestinations outside of Logging using sinks. For instructions\non routing logs, seeRoute logs to supported destinations.When configuring your log sinks' filters, you need to specify the audit log\ntypes you want to route; for filtering examples, seeSecurity logging queries.If you want to route audit log entries for a Google Cloud organization,\nfolder, or billing account, seeCollate and route organization-level logs to supported destinations.Audit log retentionFor details on how long log entries are retained by Logging,\nsee the retention information inQuotas and limits: Logs retention periods.Access controlIAM permissions and roles determine your ability to access audit\nlogs data in theLogging API, theLogs Explorer, and theGoogle Cloud CLI.For detailed information about the IAM permissions and roles you\nmight need, seeAccess control with IAM.Quotas and limitsFor details on logging usage limits, including the maximum sizes of audit logs,\nseeQuotas and limits.PricingCloud Logging doesn't charge to route logs to a\nsupported destination; however, the destination might apply charges.\nWith the exception of the_Requiredlog bucket,\nCloud Logging charges to stream logs into log buckets and\nfor storage longer than the default retention period of the log bucket.Cloud Logging doesn't charge for copying logs,\nfor creatinglog scopesoranalytics views,\nor for queries issued through theLogs ExplorerorLog Analyticspages.For more information, see the following documents:Cloud Logging pricing summaryDestination costs:Cloud Storage pricingBigQuery pricingPub/Sub pricingCloud Logging pricingVPC flow log generation chargesapply when you send and then exclude your Virtual Private Cloud flow logs from Cloud Logging.What's nextLearn how toread and understand audit logs.Learn how toenable Data Access audit logs.Reviewbest practicesfor\nCloud Audit Logs.Learn aboutAccess Transparency,\nwhich provides logs of actions taken by Google Cloud staff when accessing\nyour Google Cloud content.\nTo get the permissions that\n      you need to get read-only access to Admin Activity, Policy Denied, and\nSystem Event audit logs,\n    \n      ask your administrator to grant you theLogs Viewer(roles/logging.viewer)\n     IAM role on your project.If you have only the Logs Viewer role(roles/logging.viewer), then you\ncannot view Data Access audit logs that are in the_Defaultbucket.\nTo get the permissions that\n      you need to get read-only access to Admin Activity, Policy Denied, and\nSystem Event audit logs,\n    \n      ask your administrator to grant you theLogs Viewer(roles/logging.viewer)\n     IAM role on your project.\nIf you have only the Logs Viewer role(roles/logging.viewer), then you\ncannot view Data Access audit logs that are in the_Defaultbucket.\nTo get the permissions that\n      you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs,\n    \n      ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer)\n     IAM role on your project.The Private Logs Viewer role(roles/logging.privateLogViewer)includes the\npermissions contained in the Logs Viewer role (roles/logging.viewer),\nand those necessary to read Data Access audit logs in the_Defaultbucket.For more information about the IAM permissions and roles that\napply to audit logs data, seeAccess control with IAM.Types of audit logsCloud Audit Logs provides the following audit logs for each\nGoogle Cloud project, folder, and organization:Admin Activity audit logsData Access audit logsSystem Event audit logsPolicy Denied audit logsNote:Log entries written by Cloud Audit Logs are immutable.Admin Activity audit logsAdmin Activity audit logs contain log entries for API calls or other actions\nthat modify the configuration or metadata of resources. For example, these logs\nrecord when users create VM instances or change Identity and Access Management permissions.Admin Activity audit logs are always written; you can't configure, exclude, or\ndisable them. Even if you disable the Cloud Logging API, Admin Activity audit\nlogs are still generated.For a list of services that write Admin Activity audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logsData Access audit logs contain API calls that read the configuration or metadata\nof resources, as well as user-driven API calls that create, modify, or read\nuser-provided resource data.Publicly available resources that have the Identity and Access Management policiesallAuthenticatedUsersorallUsersdon't generate  audit logs. Resources\nthat can  be accessed without logging into a Google Cloud,\nGoogle Workspace, Cloud Identity, or Drive Enterprise account don't\ngenerate audit logs. This helps protect end-user identities and information.Data Access audit logs—except for\nBigQuery Data Access audit logs—are disabled by default because\naudit logs can be quite large. If you want Data\nAccess audit logs to be written for Google Cloud services other than\nBigQuery, you must explicitly enable them. Enabling the logs might\nresult in your Google Cloud project being charged for the additional logs\nusage. For instructions on enabling and configuring Data Access audit logs, seeEnable Data Access audit logs.For a list of services that write Data Access audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logs are stored in the_Defaultlog bucket unless\nyou've routed them elsewhere. For more information, see theStoring and routing audit logssection of this page.System Event audit logsSystem Event audit logs contain log entries for Google Cloud actions that\nmodify the configuration of resources. System Event audit logs are generated by\nGoogle Cloud systems; they aren't driven by direct user action.System Event audit logs are always written; you can't configure, exclude, or\ndisable them.For a list of services that write System Event audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Policy Denied audit logsPolicy Denied audit logs are recorded when a Google Cloud service denies access\nto a user orservice accountbecause of a security\npolicy violation.Policy Denied audit logs are generated by default and your\nGoogle Cloud project is charged for the logs storage. You can't disable Policy\nDenied audit logs, but you can useexclusion filtersto prevent Policy\nDenied audit logs from being stored in Cloud Logging.For a list of services that write Policy Denied audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.Audit log entry structureEvery audit log entry in Cloud Logging is an object of typeLogEntry. What distinguishes an audit log entry from other log\nentries is theprotoPayloadfield; this field contains anAuditLogobject that stores the audit logging data.To understand how to read and interpret audit log entries, and for a sample of\nan audit log entry, seeUnderstanding audit logs.Log nameCloud Audit Logs log names include the following:Resource identifiers indicating the Google Cloud project or\nother Google Cloud entity that owns the audit logs.The stringcloudaudit.googleapis.com.A string that indicates whether the log contains Admin Activity, Data Access,\nPolicy Denied, or System Event audit logging data.The following are the audit log names, including variables for the resource\nidentifiers:projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivityprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fpolicyfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Factivityfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fdata_accessfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2FpolicybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2FactivitybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fpolicyorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Factivityorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fdata_accessorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2FpolicyCaller identities in audit logsAudit logs record the identity that performed the logged operations on the\nGoogle Cloud resource. The caller's identity is held in theAuthenticationInfofield ofAuditLogobjects.Audit logging doesn't redact the caller's principal email address for any\naccess that succeeds or for any write operation.For read-only operations that fail with a \"permission denied\" error,\nAudit Logging might redact the caller's principal email address unless the\ncaller is a service account.In addition to the conditions listed above, the following applies to certain\nGoogle Cloud services:Legacy App Engine API: Identities aren't\ncollected.BigQuery: Caller\nidentities and IP addresses, as well as some resource names, are redacted\nfrom the audit logs, unless certain conditions are met.Cloud Storage: When Cloud Storage\nusage logs are enabled, Cloud Storage writes usage data to the\nCloud Storage bucket, which generates Data Access audit logs for the\nbucket. The generated Data Access audit log has its caller identity\nredacted.Firestore: If a JSON Web Token\n(JWT) was used for third-party authentication, thethirdPartyPrincipalfield includes the token's header and payload. For example, audit logs for\nrequests authenticated withFirebase Authenticationinclude that\nrequest'sauth token.VPC Service Controls: For\nPolicy Denied audit logs, the following redaction occurs:Parts of the caller email addresses might be redacted and replaced by\nthree period characters....Some caller email addresses belonging to the domaingoogle.comare\nredacted and replaced bygoogle-internal.Organization Policy:\nParts of the caller email addresses might be redacted and replaced by\nthree period characters....IP address of the caller in audit logsThe IP address of the caller is held in theRequestMetadata.callerIpfield of\ntheAuditLogobject:For a caller from the internet, the address is a public IPv4 or IPv6\naddress.For calls made from inside the internal production network from one\nGoogle Cloud service to another, thecallerIpis redacted to \"private\".For a caller from a Compute Engine VM with a external IP address, thecallerIpis the external address of the VM.For a caller from a Compute Engine VM without a external IP address, if\nthe VM is in the same organization or project as the accessed resource, thencallerIpis the VM's internal IPv4 address. Otherwise, thecallerIpis\nredacted to \"gce-internal-ip\". For more information, seeVPC network overview.Viewing audit logsYou can query for all audit logs or you can query for logs by their\naudit log name. The audit log name includes theresource identifierof the Google Cloud project, folder, billing account, or\norganization for which you want to view audit logging information.\nYour queries can specify indexedLogEntryfields.\nFor more information about querying your logs, seeBuild queries in the Logs ExplorerThe Logs Explorer lets you view filter individual log entries. If you want\nto use SQL to analyze groups of log entries, then use theLog Analyticspage. For more information, see:Query and view logs in Log Analytics.Sample queries for security insights.Chart query results.Most audit logs can be viewed in Cloud Logging by using the\nGoogle Cloud console, the Google Cloud CLI, or the Logging API.\nHowever, for audit logs related to billing, you can only use the\nGoogle Cloud CLI or the Logging API.ConsoleIn the Google Cloud console, you can use the Logs Explorer\nto retrieve your audit log entries for your Google Cloud project, folder,\nor organization:Note:You can't view audit logs for Cloud Billing accounts in the\nGoogle Cloud console. You must use the API or the gcloud CLI.In the Google Cloud console, go to theLogs Explorerpage:Go toLogs ExplorerIf you use the search bar to find this page, then select the result whose subheading isLogging.Select an existing Google Cloud project, folder, or organization.To display all audit logs, enter either of the following queries\ninto the query-editor field, and then clickRun query:logName:\"cloudaudit.googleapis.com\"protoPayload.\"@type\"=\"type.googleapis.com/google.cloud.audit.AuditLog\"To display the audit logs for a specific resource and audit log type,\nin theQuery builderpane, do the following:InResource type, select the Google Cloud resource whose\naudit logs you want to see.InLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.ClickRun query.If you don't see these options, then there aren't any audit logs of\nthat type available in the Google Cloud project, folder, or\norganization.If you're experiencing issues when trying to view logs in the\nLogs Explorer, see thetroubleshootinginformation.For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer.gcloudThe Google Cloud CLI provides a command-line interface to the\nLogging API. Supply a valid resource identifier in each of the log\nnames. For example, if your query includes aPROJECT_ID, then the\nproject identifier you supply must refer to the currently selected\nGoogle Cloud project.To read your Google Cloud project-level audit log entries, run\nthe following command:gcloud logging read \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" \\\n    --project=PROJECT_IDTo read your folder-level audit log entries, run the following command:gcloud logging read \"logName : folders/FOLDER_ID/logs/cloudaudit.googleapis.com\" \\\n    --folder=FOLDER_IDTo read your organization-level audit log entries, run the following\ncommand:gcloud logging read \"logName : organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com\" \\\n    --organization=ORGANIZATION_IDTo read your Cloud Billing account-level audit log entries, run the following command:gcloud logging read \"logName : billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com\" \\\n    --billing-account=BILLING_ACCOUNT_IDAdd the--freshnessflagto your command to read logs that are more than 1 day old.For more information about using the gcloud CLI, seegcloud logging read.RESTWhen building your queries, supply a valid resource identifier in each of\nthe log names. For example, if your query includes aPROJECT_ID,\nthen the project identifier you supply must refer to the currently selected\nGoogle Cloud project.For example, to use the Logging API to view your project-level\naudit log entries, do the following:Go to theTry this APIsection in the documentation for theentries.listmethod.Put the following into theRequest bodypart of theTry this\nAPIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.{\n  \"resourceNames\": [\n    \"projects/PROJECT_ID\"\n  ],\n  \"pageSize\": 5,\n  \"filter\": \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\"\n}ClickExecute.\nTo get the permissions that\n      you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs,\n    \n      ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer)\n     IAM role on your project.The Private Logs Viewer role(roles/logging.privateLogViewer)includes the\npermissions contained in the Logs Viewer role (roles/logging.viewer),\nand those necessary to read Data Access audit logs in the_Defaultbucket.\nTo get the permissions that\n      you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs,\n    \n      ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer)\n     IAM role on your project.\nThe Private Logs Viewer role(roles/logging.privateLogViewer)includes the\npermissions contained in the Logs Viewer role (roles/logging.viewer),\nand those necessary to read Data Access audit logs in the_Defaultbucket.\nFor more information about the IAM permissions and roles that\napply to audit logs data, seeAccess control with IAM.\nCloud Audit Logs provides the following audit logs for each\nGoogle Cloud project, folder, and organization:\nAdmin Activity audit logs\nData Access audit logs\nSystem Event audit logs\nPolicy Denied audit logs\nAdmin Activity audit logs contain log entries for API calls or other actions\nthat modify the configuration or metadata of resources. For example, these logs\nrecord when users create VM instances or change Identity and Access Management permissions.\nAdmin Activity audit logs are always written; you can't configure, exclude, or\ndisable them. Even if you disable the Cloud Logging API, Admin Activity audit\nlogs are still generated.\nFor a list of services that write Admin Activity audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.\nData Access audit logs contain API calls that read the configuration or metadata\nof resources, as well as user-driven API calls that create, modify, or read\nuser-provided resource data.\nPublicly available resources that have the Identity and Access Management policiesallAuthenticatedUsersorallUsersdon't generate  audit logs. Resources\nthat can  be accessed without logging into a Google Cloud,\nGoogle Workspace, Cloud Identity, or Drive Enterprise account don't\ngenerate audit logs. This helps protect end-user identities and information.\nData Access audit logs—except for\nBigQuery Data Access audit logs—are disabled by default because\naudit logs can be quite large. If you want Data\nAccess audit logs to be written for Google Cloud services other than\nBigQuery, you must explicitly enable them. Enabling the logs might\nresult in your Google Cloud project being charged for the additional logs\nusage. For instructions on enabling and configuring Data Access audit logs, seeEnable Data Access audit logs.\nFor a list of services that write Data Access audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.\nData Access audit logs are stored in the_Defaultlog bucket unless\nyou've routed them elsewhere. For more information, see theStoring and routing audit logssection of this page.\nSystem Event audit logs contain log entries for Google Cloud actions that\nmodify the configuration of resources. System Event audit logs are generated by\nGoogle Cloud systems; they aren't driven by direct user action.\nSystem Event audit logs are always written; you can't configure, exclude, or\ndisable them.\nFor a list of services that write System Event audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.\nPolicy Denied audit logs are recorded when a Google Cloud service denies access\nto a user orservice accountbecause of a security\npolicy violation.\nPolicy Denied audit logs are generated by default and your\nGoogle Cloud project is charged for the logs storage. You can't disable Policy\nDenied audit logs, but you can useexclusion filtersto prevent Policy\nDenied audit logs from being stored in Cloud Logging.\nFor a list of services that write Policy Denied audit logs and detailed\ninformation about which activities generate those logs, seeGoogle Cloud services with audit logs.\nEvery audit log entry in Cloud Logging is an object of typeLogEntry. What distinguishes an audit log entry from other log\nentries is theprotoPayloadfield; this field contains anAuditLogobject that stores the audit logging data.\nTo understand how to read and interpret audit log entries, and for a sample of\nan audit log entry, seeUnderstanding audit logs.\nCloud Audit Logs log names include the following:\nResource identifiers indicating the Google Cloud project or\nother Google Cloud entity that owns the audit logs.\nResource identifiers indicating the Google Cloud project or\nother Google Cloud entity that owns the audit logs.\nThe stringcloudaudit.googleapis.com.\nThe stringcloudaudit.googleapis.com.\nA string that indicates whether the log contains Admin Activity, Data Access,\nPolicy Denied, or System Event audit logging data.\nA string that indicates whether the log contains Admin Activity, Data Access,\nPolicy Denied, or System Event audit logging data.\nThe following are the audit log names, including variables for the resource\nidentifiers:\nAudit logs record the identity that performed the logged operations on the\nGoogle Cloud resource. The caller's identity is held in theAuthenticationInfofield ofAuditLogobjects.\nAudit logging doesn't redact the caller's principal email address for any\naccess that succeeds or for any write operation.\nFor read-only operations that fail with a \"permission denied\" error,\nAudit Logging might redact the caller's principal email address unless the\ncaller is a service account.\nIn addition to the conditions listed above, the following applies to certain\nGoogle Cloud services:\nLegacy App Engine API: Identities aren't\ncollected.\nBigQuery: Caller\nidentities and IP addresses, as well as some resource names, are redacted\nfrom the audit logs, unless certain conditions are met.\nBigQuery: Caller\nidentities and IP addresses, as well as some resource names, are redacted\nfrom the audit logs, unless certain conditions are met.\nCloud Storage: When Cloud Storage\nusage logs are enabled, Cloud Storage writes usage data to the\nCloud Storage bucket, which generates Data Access audit logs for the\nbucket. The generated Data Access audit log has its caller identity\nredacted.\nCloud Storage: When Cloud Storage\nusage logs are enabled, Cloud Storage writes usage data to the\nCloud Storage bucket, which generates Data Access audit logs for the\nbucket. The generated Data Access audit log has its caller identity\nredacted.\nFirestore: If a JSON Web Token\n(JWT) was used for third-party authentication, thethirdPartyPrincipalfield includes the token's header and payload. For example, audit logs for\nrequests authenticated withFirebase Authenticationinclude that\nrequest'sauth token.\nVPC Service Controls: For\nPolicy Denied audit logs, the following redaction occurs:Parts of the caller email addresses might be redacted and replaced by\nthree period characters....Some caller email addresses belonging to the domaingoogle.comare\nredacted and replaced bygoogle-internal.\nVPC Service Controls: For\nPolicy Denied audit logs, the following redaction occurs:\nParts of the caller email addresses might be redacted and replaced by\nthree period characters....\nParts of the caller email addresses might be redacted and replaced by\nthree period characters....\nSome caller email addresses belonging to the domaingoogle.comare\nredacted and replaced bygoogle-internal.\nSome caller email addresses belonging to the domaingoogle.comare\nredacted and replaced bygoogle-internal.\nOrganization Policy:\nParts of the caller email addresses might be redacted and replaced by\nthree period characters....\nThe IP address of the caller is held in theRequestMetadata.callerIpfield of\ntheAuditLogobject:\nFor a caller from the internet, the address is a public IPv4 or IPv6\naddress.\nFor calls made from inside the internal production network from one\nGoogle Cloud service to another, thecallerIpis redacted to \"private\".\nFor a caller from a Compute Engine VM with a external IP address, thecallerIpis the external address of the VM.\nFor a caller from a Compute Engine VM without a external IP address, if\nthe VM is in the same organization or project as the accessed resource, thencallerIpis the VM's internal IPv4 address. Otherwise, thecallerIpis\nredacted to \"gce-internal-ip\". For more information, seeVPC network overview.\nYou can query for all audit logs or you can query for logs by their\naudit log name. The audit log name includes theresource identifierof the Google Cloud project, folder, billing account, or\norganization for which you want to view audit logging information.\nYour queries can specify indexedLogEntryfields.\nFor more information about querying your logs, seeBuild queries in the Logs Explorer\nThe Logs Explorer lets you view filter individual log entries. If you want\nto use SQL to analyze groups of log entries, then use theLog Analyticspage. For more information, see:\nQuery and view logs in Log Analytics.\nSample queries for security insights.\nChart query results.\nMost audit logs can be viewed in Cloud Logging by using the\nGoogle Cloud console, the Google Cloud CLI, or the Logging API.\nHowever, for audit logs related to billing, you can only use the\nGoogle Cloud CLI or the Logging API.\nIn the Google Cloud console, you can use the Logs Explorer\nto retrieve your audit log entries for your Google Cloud project, folder,\nor organization:\nIn the Google Cloud console, go to theLogs Explorerpage:Go toLogs ExplorerIf you use the search bar to find this page, then select the result whose subheading isLogging.\nIn the Google Cloud console, go to theLogs Explorerpage:Go toLogs Explorer\nGo toLogs Explorer\nIf you use the search bar to find this page, then select the result whose subheading isLogging.\nSelect an existing Google Cloud project, folder, or organization.\nSelect an existing Google Cloud project, folder, or organization.\nTo display all audit logs, enter either of the following queries\ninto the query-editor field, and then clickRun query:logName:\"cloudaudit.googleapis.com\"protoPayload.\"@type\"=\"type.googleapis.com/google.cloud.audit.AuditLog\"\nTo display all audit logs, enter either of the following queries\ninto the query-editor field, and then clickRun query:\nTo display the audit logs for a specific resource and audit log type,\nin theQuery builderpane, do the following:InResource type, select the Google Cloud resource whose\naudit logs you want to see.InLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.ClickRun query.If you don't see these options, then there aren't any audit logs of\nthat type available in the Google Cloud project, folder, or\norganization.If you're experiencing issues when trying to view logs in the\nLogs Explorer, see thetroubleshootinginformation.For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer.\nTo display the audit logs for a specific resource and audit log type,\nin theQuery builderpane, do the following:\nInResource type, select the Google Cloud resource whose\naudit logs you want to see.\nInResource type, select the Google Cloud resource whose\naudit logs you want to see.\nInLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.\nInLog name, select the audit log type that you want to see:\nFor Admin Activity audit logs, selectactivity.\nFor Data Access audit logs, selectdata_access.\nFor System Event audit logs, selectsystem_event.\nFor Policy Denied audit logs, selectpolicy.\nClickRun query.\nClickRun query.\nIf you don't see these options, then there aren't any audit logs of\nthat type available in the Google Cloud project, folder, or\norganization.\nIf you're experiencing issues when trying to view logs in the\nLogs Explorer, see thetroubleshootinginformation.\nFor more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer.\nThe Google Cloud CLI provides a command-line interface to the\nLogging API. Supply a valid resource identifier in each of the log\nnames. For example, if your query includes aPROJECT_ID, then the\nproject identifier you supply must refer to the currently selected\nGoogle Cloud project.\nTo read your Google Cloud project-level audit log entries, run\nthe following command:\nTo read your folder-level audit log entries, run the following command:\nTo read your organization-level audit log entries, run the following\ncommand:\nTo read your Cloud Billing account-level audit log entries, run the following command:\nAdd the--freshnessflagto your command to read logs that are more than 1 day old.\nFor more information about using the gcloud CLI, seegcloud logging read.\nWhen building your queries, supply a valid resource identifier in each of\nthe log names. For example, if your query includes aPROJECT_ID,\nthen the project identifier you supply must refer to the currently selected\nGoogle Cloud project.\nFor example, to use the Logging API to view your project-level\naudit log entries, do the following:\nGo to theTry this APIsection in the documentation for theentries.listmethod.\nGo to theTry this APIsection in the documentation for theentries.listmethod.\nPut the following into theRequest bodypart of theTry this\nAPIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.{\n  \"resourceNames\": [\n    \"projects/PROJECT_ID\"\n  ],\n  \"pageSize\": 5,\n  \"filter\": \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\"\n}\nPut the following into theRequest bodypart of theTry this\nAPIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.\nClickExecute.\nClickExecute.\nCloud Logging useslog bucketsas\ncontainers that store and organize your logs data. For each billing account,\nGoogle Cloud project, folder, and organization, Logging\nautomatically creates two log buckets,_Requiredand_Default, and\ncorrespondingly namedsinks.\nCloud Logging_Requiredbuckets store Admin Activity audit logs\nand System Event audit logs. You can't prevent Admin Activity or System Event\naudit logs from being stored. You also can't configure the sink that routes\nlog entries to the_Requiredbuckets.\nAdmin Activity audit logs and System Event audit logs are always stored in the_Requiredbucket in the project where the logs were generated.\nIf you route Admin Activity audit logs and System Event audit logs to a\ndifferent project, then those logs don't pass through the_Defaultor_Requiredsink of the destination project. Therefore, these logs aren't stored\nin the_Defaultlog bucket or the_Requiredlog bucket of the destination\nproject. To store these logs, create a log sink in the destination project.\nFor more information, seeRoute logs to supported destinations.\nThe_Defaultbuckets, by default, store any enabled Data Access\naudit logs as well as Policy Denied audit logs. To prevent Data Access audit\nlogs from being stored in the_Defaultbuckets, you can disable them. To\nprevent any Policy Denied audit logs from being stored in the_Defaultbuckets, you can exclude them by modifying their sinks' filters.\nYou can also route your audit log entries to user-defined\nCloud Logging buckets at the Google Cloud project level or to supported\ndestinations outside of Logging using sinks. For instructions\non routing logs, seeRoute logs to supported destinations.\nWhen configuring your log sinks' filters, you need to specify the audit log\ntypes you want to route; for filtering examples, seeSecurity logging queries.\nIf you want to route audit log entries for a Google Cloud organization,\nfolder, or billing account, seeCollate and route organization-level logs to supported destinations.\nFor details on how long log entries are retained by Logging,\nsee the retention information inQuotas and limits: Logs retention periods.\nIAM permissions and roles determine your ability to access audit\nlogs data in theLogging API, theLogs Explorer, and theGoogle Cloud CLI.\nFor detailed information about the IAM permissions and roles you\nmight need, seeAccess control with IAM.\nFor details on logging usage limits, including the maximum sizes of audit logs,\nseeQuotas and limits.\nCloud Logging doesn't charge to route logs to a\nsupported destination; however, the destination might apply charges.\nWith the exception of the_Requiredlog bucket,\nCloud Logging charges to stream logs into log buckets and\nfor storage longer than the default retention period of the log bucket.\nCloud Logging doesn't charge for copying logs,\nfor creatinglog scopesoranalytics views,\nor for queries issued through theLogs ExplorerorLog Analyticspages.\nFor more information, see the following documents:\nCloud Logging pricing summary\nDestination costs:Cloud Storage pricingBigQuery pricingPub/Sub pricingCloud Logging pricingVPC flow log generation chargesapply when you send and then exclude your Virtual Private Cloud flow logs from Cloud Logging.\nDestination costs:\nCloud Storage pricing\nBigQuery pricing\nPub/Sub pricing\nCloud Logging pricing\nVPC flow log generation chargesapply when you send and then exclude your Virtual Private Cloud flow logs from Cloud Logging.\nLearn how toread and understand audit logs.\nLearn how toenable Data Access audit logs.\nReviewbest practicesfor\nCloud Audit Logs.\nLearn aboutAccess Transparency,\nwhich provides logs of actions taken by Google Cloud staff when accessing\nyour Google Cloud content.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-19 UTC."
  },
  {
    "url": "https://cloud.google.com/network-connectivity/docs/vpn",
    "title": "Cloud VPN documentation",
    "content": "Home\nNetwork Connectivity\nDocumentation\nCloud VPN\nCloud VPN securely extends your peer network to Google's network\n  through an IPsec VPN tunnel. Traffic is encrypted and travels between the two\n  networks over the public internet. Cloud VPN is useful for\n  low-volume data connections. For additional connection options, see theHybrid Connectivityproduct page.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nMoving to HA VPN from Classic VPN\nMoving to HA VPN from Classic VPN\nCreating an HA VPN gateway to a peer VPN gateway\nCreating an HA VPN gateway to a peer VPN gateway\nConfiguring firewall rules\nConfiguring firewall rules\nConfiguring the peer VPN gateway\nConfiguring the peer VPN gateway\nUsing third-party VPNs with Cloud VPN\nUsing third-party VPNs with Cloud VPN\nViewing logs and metrics\nViewing logs and metrics\nBest practices for Cloud VPN\nBest practices for Cloud VPN\nCloud VPN overview\nCloud VPN overview\nCloud VPN topologies\nCloud VPN topologies\nAdvanced configurations\nAdvanced configurations\nAPIs and gcloud reference\nAPIs and gcloud reference\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nTroubleshooting\nTroubleshooting\nClassic VPN partial deprecation\nClassic VPN partial deprecation\nRelease notes\nRelease notes\nSLA\nSLA\nSecurely connecting to VM instances\nLearn how to build HA VPN connections between Google Cloud and AWS.Network Connectivity\nDeploying HA VPN with Terraform\nThis tutorial demonstrates how to use Terraform to deploy the high-availability VPN resources on Google Cloud that are used in the VPN interoperability guides.Network Connectivity\nTCP optimization for network performance in Google Cloud and hybrid scenarios\nLearn about ways to improve connection latency between processes within Google Cloud, including how to compute correct settings for decreasing the latency of TCP connections.Network Connectivity\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/storage",
    "title": "Storage",
    "content": "Home\nDocumentation\nData storage, backup, and disaster recovery.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate and execute backup plans for databases, VMs, and file systems to protect all of your data resources consistently and efficiently.\nChoose your block, file, or object storage options.\nTransfer your data into, within, or from Google Cloud.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/networking",
    "title": "Networking",
    "content": "Home\nDocumentation\nConnect your networks and workloads, load balance traffic, and secure your network.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nSet up your Virtual Private Cloud network and connect it to your other networks.\nMake your services available at scale to your internal or external customers.\nBlock unauthorized traffic and implement threat prevention and detection services.\nMonitor and troubleshoot your Google Cloud network.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGoogle Cloud Observability\nDocumentation\nGoogle Cloud Managed Service for Prometheus is Google Cloud's fully managed, multi-cloud,\ncross-project solution forPrometheusandOpenTelemetrymetrics. It lets you globally monitor and alert on your workloads, using\nPrometheus and OpenTelemetry, without having to manually manage and operate Prometheus at scale.\nManaged Service for Prometheus collects metrics from Prometheus exporters and\nlets you query the data globally using PromQL, meaning that you can keep\nusing any existingGrafanadashboards,\nPromQL-based alerts, and workflows. It is hybrid- and multi-cloud\ncompatible, can monitor Kubernetes, VMs, and serverless workloads on\nCloud Run, retains data\nfor 24 months, and maintains portability by staying\ncompatible with upstream Prometheus. You can also supplement your Prometheus\nmonitoring by queryingover 6,500 free\nmetricsin Cloud Monitoring, includingfree GKE\nsystem metrics, using PromQL.\nThis document gives an overview of the managed service, and further documents\ndescribe how to set up and run the service. To receive regular updates about new\nfeatures and releases, submit the optionalsign-up form.\nHear how The Home Depot uses Managed Service for Prometheus to get\nunified observability across 2,200 stores running on-prem Kubernetes clusters:\nGoogle Cloud Managed Service for Prometheus gives you the familiarity of Prometheus backed\nby the global, multi-cloud, and cross-project infrastructure of\nCloud Monitoring.\nManaged Service for Prometheus is built on top of Monarch, the\nsameglobally scalable datastoreused for Google's own monitoring.\nBecause Managed Service for Prometheus uses the same backend and APIs asCloud Monitoring, both Cloud Monitoring metrics\nand metrics ingested by Managed Service for Prometheus are queryable\nby usingPromQL in Cloud Monitoring,Grafana, orany other tool that\ncan read the Prometheus API.\nIn a standard Prometheus deployment, data collection, query evaluation, rule and\nalert evaluation, and data storage are all handled within a single Prometheus\nserver. Managed Service for Prometheus splits responsibilities for these\nfunctions into multiple components:\nData collectionis handled by managed collectors,\nself-deployed collectors, the OpenTelemetry Collector, orthe Ops Agent, which scrape local exporters and forward\nthe collected data to Monarch. These collectors can be used for\nKubernetes, serverless, and traditional VM workloads and can run everywhere,\nincluding other clouds and on-prem deployments.\nQuery evaluationis handled by Monarch,\nwhich executes queries and unions results across all Google Cloud\nregions and across up to 3,500\nGoogle Cloud projects.\nRule and alert evaluationis handled either by \nwritingPromQL alerts in Cloud Monitoringwhich fully execute in the cloud, or by using locally run\nand locally configured rule evaluator components which execute rules and\nalerts against the global Monarch data store and forward any\nfired alerts toPrometheus AlertManager.\nData storageis handled by Monarch, which\nstores all Prometheus data for 24 months at no\nadditional cost.\nGrafana connects to the global Monarch data store instead of\nconnecting to individual Prometheus servers. If you have\nManaged Service for Prometheus collectors configured in all your\ndeployments, then this single Grafana instance gives you a unified view of all\nyour metrics across all your clouds.\nYou can use Managed Service for Prometheus in one of four modes: withmanaged data collection, withself-deployed data\ncollection, withthe OpenTelemetry Collector, or\nwiththe Ops Agent.\nManaged Service for Prometheus offers an operator for managed data collection\nin Kubernetes environments. We recommend that you use managed collection; using\nit eliminates the complexity of deploying, scaling, sharding, configuring, and\nmaintaining Prometheus servers. Managed collection is supported for both\nGKE and non-GKE Kubernetes environments.\nWith self-deployed data collection, you manage your Prometheus installation as\nyou always have. The only difference from upstream Prometheus is that you run\nthe Managed Service for Prometheus drop-in replacement binary instead of the\nupstream Prometheus binary.\nThe OpenTelemetry Collector can be used to scrape Prometheus exporters and send\ndata to Managed Service for Prometheus. OpenTelemetry supports a\nsingle-agent strategy for all signals, where one collector can be used for\nmetrics (including Prometheus metrics), logs, and traces in any environment.\nYou can configure the Ops Agent on any Compute Engine instance to scrape and\nsend Prometheus metrics to the global datastore. Using an agent greatly\nsimplifies VM discovery and eliminates the need to install, deploy, or configure\nPrometheus in VM environments.\nIf you have a Cloud Run service that writesPrometheus metricsorOTLP metrics,\nthen you can use a sidecar and Managed Service for Prometheus to\nsend the metrics to Cloud Monitoring.\nTo collect Prometheus metrics from Cloud Run, use thePrometheus sidecar.\nTo collect OTLP metrics from Cloud Run, use theOpenTelemetry sidecar.\nYou can run managed, self-deployed, and OpenTelemetry collectors in\non-prem deployments and on any cloud. Collectors running outside of Google Cloud\nsend data to Monarch for long-term storage and global querying.\nWhen choosing between collection options, consider the following:\nManaged collection:Google's recommended approach for all Kubernetes environments.Deployed by using the GKE UI, the gcloud CLI,\nthekubectlCLI, or Terraform.Operation of Prometheus—generating scrape configurations, scaling\ningestion, scoping rules to the right data, and so forth—is fully\nhandled by the Kubernetes operator.Scraping and rules are configured by using lightweight custom resources\n(CRs).Good for those who want a more hands-off, fully managed experience.Intuitive migration fromprometheus-operatorconfigs.Supports most current Prometheus use cases.Full assistance from Google Cloud technical support.\nManaged collection:\nGoogle's recommended approach for all Kubernetes environments.\nDeployed by using the GKE UI, the gcloud CLI,\nthekubectlCLI, or Terraform.\nOperation of Prometheus—generating scrape configurations, scaling\ningestion, scoping rules to the right data, and so forth—is fully\nhandled by the Kubernetes operator.\nScraping and rules are configured by using lightweight custom resources\n(CRs).\nGood for those who want a more hands-off, fully managed experience.\nIntuitive migration fromprometheus-operatorconfigs.\nSupports most current Prometheus use cases.\nFull assistance from Google Cloud technical support.\nSelf-deployed collection:A drop-in replacement for the upstream Prometheus binary.You can use your preferred deployment mechanism, likeprometheus-operatoror manual\ndeployment.Scraping is configured by using your preferred methods, like annotations\nor prometheus-operator.Scaling and functional sharding is done manually.Good for quick integration into more complex existing setups. You can\nreuse your existing configs and run upstream Prometheus and\nManaged Service for Prometheus side by side.Rules and alerts typically run within individual Prometheus servers,\nwhich might be preferable for edge deployments as local rule evaluation\ndoes not incur any network traffic.Might support long-tail use cases that aren't yet supported by managed\ncollection, such aslocal aggregationsto reduce\ncardinality.Limited assistance from Google Cloud technical support.\nSelf-deployed collection:\nA drop-in replacement for the upstream Prometheus binary.\nYou can use your preferred deployment mechanism, likeprometheus-operatoror manual\ndeployment.\nScraping is configured by using your preferred methods, like annotations\nor prometheus-operator.\nScaling and functional sharding is done manually.\nGood for quick integration into more complex existing setups. You can\nreuse your existing configs and run upstream Prometheus and\nManaged Service for Prometheus side by side.\nRules and alerts typically run within individual Prometheus servers,\nwhich might be preferable for edge deployments as local rule evaluation\ndoes not incur any network traffic.\nMight support long-tail use cases that aren't yet supported by managed\ncollection, such aslocal aggregationsto reduce\ncardinality.\nLimited assistance from Google Cloud technical support.\nThe OpenTelemetry Collector:A single collector that can collect metrics (including Prometheus\nmetrics) from any environment and send them to any compatible backend.\nCan also be used to collect logs and traces and send them to any\ncompatible backend, including Cloud Logging and Cloud Trace.Deployed in any compute or Kubernetes environment either manually or by\nusing Terraform. Can be used to send metrics from stateless environments\nsuch as Cloud Run.Scraping is configured using Prometheus-like configs in the collector's\nPrometheus receiver.Supports push-based metric collection patterns.Metadata is injected from any cloud using resource detector processors.Rules and alerts can be executed using a Cloud Monitoring alerting\npolicy or the stand-alone rule evaluator.Best supports cross-signal workflows and features such as exemplars.Limited assistance from Google Cloud technical support.\nThe OpenTelemetry Collector:\nA single collector that can collect metrics (including Prometheus\nmetrics) from any environment and send them to any compatible backend.\nCan also be used to collect logs and traces and send them to any\ncompatible backend, including Cloud Logging and Cloud Trace.\nDeployed in any compute or Kubernetes environment either manually or by\nusing Terraform. Can be used to send metrics from stateless environments\nsuch as Cloud Run.\nScraping is configured using Prometheus-like configs in the collector's\nPrometheus receiver.\nSupports push-based metric collection patterns.\nMetadata is injected from any cloud using resource detector processors.\nRules and alerts can be executed using a Cloud Monitoring alerting\npolicy or the stand-alone rule evaluator.\nBest supports cross-signal workflows and features such as exemplars.\nLimited assistance from Google Cloud technical support.\nThe Ops Agent:The easiest way to collect and send Prometheus metric data originating\nfrom Compute Engine environments, including both Linux and Windows\ndistros.Deployed by using the gcloud CLI, the Compute Engine UI,\nor Terraform.Scraping is configured using Prometheus-like configs in the Agent's\nPrometheus receiver, powered by OpenTelemetry.Rules and alerts can be executed using Cloud Monitoring or the\nstand-alone rule evaluator.Comes bundled with optional Logging agents andprocess\nmetrics.Full assistance from Google Cloud technical support.\nTo get started, seeGet started with managed collection,Get\nstarted with self-deployed collection,Get started with the\nOpenTelemetry Collector, orGet started with the\nOps Agent.\nThe Ops Agent:\nThe easiest way to collect and send Prometheus metric data originating\nfrom Compute Engine environments, including both Linux and Windows\ndistros.\nDeployed by using the gcloud CLI, the Compute Engine UI,\nor Terraform.\nScraping is configured using Prometheus-like configs in the Agent's\nPrometheus receiver, powered by OpenTelemetry.\nRules and alerts can be executed using Cloud Monitoring or the\nstand-alone rule evaluator.\nComes bundled with optional Logging agents andprocess\nmetrics.\nFull assistance from Google Cloud technical support.\nTo get started, seeGet started with managed collection,Get\nstarted with self-deployed collection,Get started with the\nOpenTelemetry Collector, orGet started with the\nOps Agent.\nIf you use the managed service outside of Google Kubernetes Engine or Google Cloud, some\nadditional configuration might be necessary; seeRun managed collection outside\nof Google Cloud,Run self-deployed collection outside of\nGoogle Cloud, orAdd OpenTelemetry\nprocessors.\nManaged Service for Prometheus supports any query UI that can call the\nPrometheus query API, including Grafana and the Cloud Monitoring UI. Existing\nGrafana dashboards continue to work when switching from local Prometheus to\nManaged Service for Prometheus, and you can continue using PromQL found in\npopular open-source repositories and on community forums.\nYou can use PromQL to queryover 6,500 free\nmetricsin Cloud Monitoring, even without sending data to\nManaged Service for Prometheus. You can also use PromQL to queryfree\nKubernetes metrics,custom metricsandlog-based metrics.\nFor information on how to configure Grafana to query\nManaged Service for Prometheus data, seeQuery using Grafana.\nFor information on how to query Cloud Monitoring metrics using PromQL, seePromQL in Cloud Monitoring.\nManaged Service for Prometheus provides both a fully cloud-based alerting\npipeline and a stand-alone rule evaluator, both of which\nevaluate rules against all Monarch data\naccessible in ametrics scope. Evaluating rules against a\nmulti-project metrics scope eliminates the need to co-locate all data of\ninterest on a single Prometheus server or within a single Google Cloud project,\nand it lets you set IAM permissions on groups of projects.\nBecause all rule evaluation options accept the standard Prometheusrule_filesformat, you can easily migrate to Managed Service for Prometheus by copy-pasting\nexisting rules or by copy-pasting rules found in popular open source\nrepositories. For those using self-deployed collectors, you can continue to\nevaluate recording rules locally in your collectors. The results of recording\nand alerting rules are stored in Monarch, just like directly\ncollected metric data. You can also migrate your Prometheus alerting rules to\nPromQL-based alerting policies in Cloud Monitoring.\nFor alert evaluation with Cloud Monitoring, seePromQL alerts in Cloud Monitoring.\nFor rule evaluation with managed collection, seeManaged rule evaluation and\nalerting.\nFor rule evaluation with self-deployed collection, the OpenTelemetry Collector,\nand the Ops Agent, seeSelf-deployed rule evaluation and\nalerting.\nFor information on reducing cardinality using recording rules on self-deployed\ncollectors, seeCost controls and attribution.\nAll Managed Service for Prometheus data is stored for\n24 months at no additional cost.\nManaged Service for Prometheus supports a minimum scrape interval of\n5 seconds. Data is stored at full\ngranularity for 1 week, then is downsampled to 1-minute points for the next 5\nweeks, then is downsampled to 10-minute points and stored for the remainder of\nthe retention period.\nManaged Service for Prometheus has no limit on the number of active\ntime series or total time series.\nFor more information, seeQuotas and limits within the\nCloud Monitoring documentation.\nManaged Service for Prometheus is a Google Cloud product, and\nbilling and usage quotas apply.\nBilling for the service is based primarily on\nthe number of metric samples ingested into storage. There is also a\nnominal charge for read API calls. Managed Service for Prometheus does\nnot charge for storage or retention of metric data.\nFor current pricing, seeGoogle Cloud Managed Service for Prometheus pricing summary.\nTo estimate your bill based on your expected number of time series or your\nexpected samples per second, see the Cloud Operations tab within theGoogle Cloud Pricing Calculator.\nFor tips on how to lower your bill or determine the sources of high costs,\nseeCost controls and attribution.\nFor information about the rationale for the pricing model, seePricing for controllability and predictability.\nFor pricing examples, seePricing example based on samples ingested.\nManaged Service for Prometheus shares ingest and read quotas with\nCloud Monitoring. The default ingest quota is 500 QPS per project with up to\n200 samples in a single call, equivalent to 100,000 samples per second. The\ndefault read quota is 100 QPS permetrics scope.\nYou can increase these quotas to support your metric and query\nvolumes. For information about managing quotas and requesting quota increases,\nseeWorking with quotas.\nManaged Service for Prometheus is part of Cloud Monitoring and therefore\ninherits certain agreements and certifications from Cloud Monitoring,\nincluding (but not limited to):\nTheGoogle Cloud terms of service\nTheOperations Service Level Agreement (SLA)\nUS DISAandFedRAMPcompliance levels\nVPC-SC (VPC Service Controls)support\nGet started withmanaged collection.\nGet started withself-deployed collection.\nGet started withthe OpenTelemetry Collector.\nGet started withthe Ops Agent.\nUse PromQL in Cloud Monitoring to query Prometheus metrics.\nUse Grafana to query Prometheus metrics.\nQuery Cloud Monitoring metricsusing PromQL.\nRead up onbest practices and view architecture diagrams.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-15 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models",
    "title": "Overview of self-deployed modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nModel Garden offers both self-deploy open and partner models\nthat you can deploy and serve on Vertex AI. These models are different\nfrom themodel-as-a-service (MaaS)offerings, which are serverless and\nrequire no manual deployment.\nWhen you self deploy models, you deploy them securely within your\n Google Cloud project and VPC network.\nOpen models provide pretrained capabilities for various AI tasks, including\nGemini models that excel in multimodal processing. An open model is\nfreely available, you are free to publish its outputs, and it can be used\nanywhere as long as you adhere to its licensing terms.Vertex AIoffers both open (also known asopen weight)\nand open source models.\nWhen you use an open model with Vertex AI, you use Vertex AI for\nyour infrastructure. You can also use open models with other infrastructure\nproducts, such as PyTorch or Jax.\nMany open models are considered open weight large language models (LLMs). Open\nmodels provide more transparency than models that aren't open weight. A\nmodel's weights are the numerical values stored in the model's neural network\narchitecture that represent learned patterns and relationships from the data a\nmodel is trained on. The pretrained parameters, or weights, of open weight\nmodels are released. You can use an open weight model for inference and tuning\nwhile details such as the original dataset, model architecture, and training\ncode aren't provided.\nOpen models differ from open source AI models. While open models often expose\nthe weights and the core numerical representation of learned patterns, they\ndon't necessarily provide the full source code or training details. Providing\nweights offers a level of AI model transparency, allowing you to\nunderstand the model's capabilities without needing to build it yourself.\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nModel Garden helps you purchase and manage model licenses from partners\nwho offer proprietary models as a self deploy option. After you purchase\naccess to a model from Cloud Marketplace, you can choose to deploy on\non-demand hardware or use your Compute Engine reservations and\ncommitted use discounts to meet your budget requirements. You are charged for\nmodel usage and for the Vertex AI infrastructure that you use.\nTo request usage of a self-deploy partner model, find the relevant model in\ntheModel Garden console, clickContact sales, and\nthen complete the form, which initiates contact with a Google Cloud sales\nrepresentative.\nFor more information about deploying and using partner models, seeDeploy a\npartner model and make prediction requests.\nConsider the following limitations when using self-deploy partner models:\nUnlike with open models, you cannot export weights.\nIf you VPC Service Controls set up for your project, you can't upload\nmodels, which prevents you from deploying partner models.\nFor endpoints, only theshared public endpointtype is\nsupported.\nFor more information about Model Garden, seeOverview of\nModel Garden.\nFor more information about deploying models, seeUse models in\nModel Garden.\nUse Gemma open models\nUse Llama open models\nUse Hugging Face open models\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nTo see an example of getting started with Responsible AI with Vertex AI Gemini API,\n      run the \"Responsible AI with Vertex AI Gemini API: Safety ratings and thresholds\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nGoogle's generative AI models, like Gemini 2.0 Flash, are designed to\nprioritize safety. However, they can still generate harmful responses,\nespecially when they're explicitly prompted. To further enhance safety and\nminimize misuse, you can configure content filters to block potentially harmful\nresponses.\nThis page describes each of the safety and content filter types and outlines key safety concepts.\nFor configurable content filters, it shows you how to configure the blocking thresholds\nof each harm category to control how often prompts and responses are blocked.\nSafety and content filters act as a barrier, preventing harmful output, but they don't\ndirectly influence the model's behavior. To learn more about model steerability,\nseeSystem instructions for safety.\nThe Vertex AI Gemini API provides one of the followingenumcodes to explain why a prompt was rejected:\nTo learn more, seeBlockedReason.\nThe following is an example of Vertex AI Gemini API output when a prompt is\nblocked for containingPROHIBITED_CONTENT:\nThe following filters can detect and block potentially unsafe responses:\nNon-configurable safety filters, which block child sexual abuse material\n(CSAM) and personally identifiable information (PII).\nConfigurable content filters, which block unsafe content based on a list of\nharm categories and their user-configured blocking thresholds. You can\nconfigure blocking thresholds for each of these harms based on what is\nappropriate for your use case and business. To learn more, seeConfigurable content filters.\nCitation filters, which provide citations for source material.\nTo learn more, seeCitation filter.\nAn LLM generates responses in units of text called tokens. A model stops\ngenerating tokens because it reaches a natural stopping point or\nbecause one of the filters blocks the response. The Vertex AI Gemini API\nprovides one of the followingenumcodes to explain why token generation stopped:\nTo learn more, seeFinishReason.\nIf a filter blocks the response, it voids the response'sCandidate.contentfield. It does not provide any feedback to the model.\nContent filters assess content against a list\nof harms. For each harm category, the content filters assign one\n score based on theprobabilityof the content being harmful and another\n score based on theseverityof harmful content.\nThe configurable content filters don't have versioning independent of model\nversions. Google won't update the configurable content filter for a previously\nreleased version of a model. However, it may update the configurable content\nfilter for a future version of a model.\nContent filters assess content based on the following harm categories:\nTheprobabilitysafety score reflects the likelihood that a model response\nis associated with the respective harm. It has an\nassociated confidence score between0.0and1.0, rounded to one decimal place.\nThe confidence score is discretized into four confidence levels:NEGLIGIBLE,LOW,MEDIUM, andHIGH.\nTheseverityscore reflects the magnitude of how harmful a model\nresponse might be. It has an associated severity score ranging from0.0to1.0, rounded to one decimal place. The severity score is discretized\ninto four levels:NEGLIGIBLE,LOW,MEDIUM, andHIGH.\nContent can have a lowprobabilityscore and a highseverityscore, or a\nhighprobabilityscore and a lowseverityscore.\nYou can use the Vertex AI Gemini API or the Google Cloud console to configure\ncontent filters.\nThe Vertex AI Gemini API provides two \"harm block\" methods:\nSEVERITY: This method uses both probability and severity scores.\nPROBABILITY: This method uses the probability score only.\nThe default method isSEVERITY. For models older thangemini-1.5-flashandgemini-1.5-pro, the default method isPROBABILITY. To learn more, seeHarmBlockMethodAPI reference.\nThe Vertex AI Gemini API provides the following \"harm block\" thresholds:\nBLOCK_LOW_AND_ABOVE: Block when the probability score or the severity\nscore isLOW,MEDIUMorHIGH.\nBLOCK_MEDIUM_AND_ABOVE: Block when the probability score or the severity\nscore isMEDIUMorHIGH. Forgemini-2.0-flash-001and\nsubsequent models,BLOCK_MEDIUM_AND_ABOVEis the default\nvalue.\nBLOCK_ONLY_HIGH: Block when the probability score or the severity score\nisHIGH.\nHARM_BLOCK_THRESHOLD_UNSPECIFIED: Block using the default threshold.\nOFF: No automated response blocking and no metadata is returned.\nForgemini-2.0-flash-001and subsequent models,OFFis the\ndefault value.\nBLOCK_NONE: TheBLOCK_NONEsetting removes\nautomated response blocking. Instead, you can configure your own content\nguidelines with the returned scores. This is a restricted field that isn't\navailable to all users inGAmodel\nversions.\nFor example, the following Python code demonstrates how you can set the harm\nblock threshold toBLOCK_ONLY_HIGHfor the dangerous content category:\nThis will block most of the content that is classified as dangerous content.\nTo learn more, seeHarmBlockThresholdAPI reference.\nFor end-to-end examples in Python, Node.js, Java, Go, C# and REST, seeExamples of content filter configuration.\nThe Google Cloud console lets you configure a threshold for each content attribute.\nThe content filter uses only the probability scores. There is no option to use\nthe severity scores.\nThe Google Cloud console provides the following threshold values:\nOff(default): No automated response blocking.\nBlock few: Block when the probability score isHIGH.\nBlock some: Block when the probability score isMEDIUMorHIGH.\nBlock most: Block when the probability score isLOW,MEDIUMorHIGH.\nFor example, if you set the block setting toBlock fewfor theDangerous Content category, everything that has a high probability of\nbeing dangerous content is blocked. Anything with a lower probability is\nallowed. The default threshold isBlock some.\nTo set the thresholds, see the following steps:\nIn the Vertex AI section of the Google Cloud console, go to\ntheVertex AI Studiopage.Go to Vertex AI Studio\nIn the Vertex AI section of the Google Cloud console, go to\ntheVertex AI Studiopage.\nGo to Vertex AI Studio\nUnderCreate a new prompt, click any of the buttons to open the prompt\ndesign page.\nUnderCreate a new prompt, click any of the buttons to open the prompt\ndesign page.\nClickSafety settings.TheSafety settingsdialog window opens.\nClickSafety settings.\nTheSafety settingsdialog window opens.\nFor each harm category, configure the desired threshold value.\nFor each harm category, configure the desired threshold value.\nClickSave.\nClickSave.\nThe following is an example of Vertex AI Gemini API output when a response is\nblocked by the configurable content filter for containing dangerous content:\nThe following examples demonstrate how you can configure the content filter\nusing the Vertex AI Gemini API:\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nAfter youset up your environment,\n  you can use REST to test a text prompt. The following sample sends a request to the publisher\n  model endpoint.\nBefore using any of the request data,\n  make the following replacements:\nLOCATION: The region to process the request. Available\n  options include the following:Click to expand a partial list of available regionsus-central1us-west4northamerica-northeast1us-east4us-west1asia-northeast3asia-southeast1asia-northeast1\nClick to expand a partial list of available regions\nus-central1\nus-west4\nnorthamerica-northeast1\nus-east4\nus-west1\nasia-northeast3\nasia-southeast1\nasia-northeast1\nPROJECT_ID: Yourproject ID.\nMODEL_ID: The model ID of the multimodal model\n    that you want to use, likegemini-2.0-flash.\nROLE: \nThe role in a conversation associated with the content. Specifying a role is required even in\nsingleturn use cases.\n\nAcceptable values include the following:USER: Specifies content that's sent by you.MODEL: Specifies the model's response.TEXT: \nThe text instructions to include in the prompt.SAFETY_CATEGORY: \nThe safety category to configure a threshold for. Acceptable values include the following:Click to expand safety categoriesHARM_CATEGORY_SEXUALLY_EXPLICITHARM_CATEGORY_HATE_SPEECHHARM_CATEGORY_HARASSMENTHARM_CATEGORY_DANGEROUS_CONTENTTHRESHOLD: \nThe threshold for blocking responses that could belong to the specified safety category based on\nprobability. Acceptable values include the following:Click to expand blocking thresholdsBLOCK_NONEBLOCK_ONLY_HIGHBLOCK_MEDIUM_AND_ABOVE(default)BLOCK_LOW_AND_ABOVEBLOCK_LOW_AND_ABOVEblocks the most whileBLOCK_ONLY_HIGHblocks the least.\nUSER: Specifies content that's sent by you.\nMODEL: Specifies the model's response.\nTEXT: \nThe text instructions to include in the prompt.\nSAFETY_CATEGORY: \nThe safety category to configure a threshold for. Acceptable values include the following:Click to expand safety categoriesHARM_CATEGORY_SEXUALLY_EXPLICITHARM_CATEGORY_HATE_SPEECHHARM_CATEGORY_HARASSMENTHARM_CATEGORY_DANGEROUS_CONTENT\nClick to expand safety categories\nHARM_CATEGORY_SEXUALLY_EXPLICIT\nHARM_CATEGORY_HATE_SPEECH\nHARM_CATEGORY_HARASSMENT\nHARM_CATEGORY_DANGEROUS_CONTENT\nTHRESHOLD: \nThe threshold for blocking responses that could belong to the specified safety category based on\nprobability. Acceptable values include the following:Click to expand blocking thresholdsBLOCK_NONEBLOCK_ONLY_HIGHBLOCK_MEDIUM_AND_ABOVE(default)BLOCK_LOW_AND_ABOVEBLOCK_LOW_AND_ABOVEblocks the most whileBLOCK_ONLY_HIGHblocks the least.\nClick to expand blocking thresholds\nBLOCK_NONE\nBLOCK_ONLY_HIGH\nBLOCK_MEDIUM_AND_ABOVE(default)\nBLOCK_LOW_AND_ABOVE\nHTTP method and URL:\nRequest JSON body:\nTo send your request, choose one of these options:\nSave the request body in a file namedrequest.json,\n      and execute the following command:\nSave the request body in a file namedrequest.json,\n      and execute the following command:\nYou should receive a JSON response similar to the following.\nThe generative code features of Vertex AI are intended to produce\noriginal content. By design, Gemini limits the likelihood\nthat existing content is replicated at length. If a Gemini feature does\nmake an extensive quotation from a web page, Gemini cites that page.\nSometimes the same content can be found on multiple web pages. Gemini\nattempts to point you to a popular source. In the case of\ncitations to code repositories, the citation might also reference an applicable\nopen source license. Complying with any license requirements is your own\nresponsibility.\nTo learn about the metadata of the citation filter, see theCitation API reference.\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nThe civic integrity filter detects and blocks prompts that mention or relate to\npolitical elections and candidates. This filter is disabled by default. To turn\nit on, set the blocking threshold forCIVIC_INTEGRITYto any of the following\nvalues. It doesn't make a difference which value you specify.\nBLOCK_LOW_AND_ABOVE\nBLOCK_MEDIUM_AND_ABOVE\nBLOCK_ONLY_HIGH\nThe following Python code shows you how to turn on the civic integrity filter:\nFor more details about the civic integrity filter, contact your Google Cloud\nrepresentative.\nWhile content filters help prevent unsafe content, they might occasionally block\nbenign content or miss harmful content. Advanced models like\nGemini 2.0 Flash are designed to generate safe responses even without\nfilters. Test different filter settings to find the right balance between\nsafety and allowing appropriate content.\nLearn aboutsystem instructions for safety.\nLearn aboutabuse monitoring.\nLearn more aboutresponsible AI.\nLearn aboutdata governance.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-15 UTC."
  },
  {
    "url": "https://cloud.google.com/bigquery/docs/migration-intro",
    "title": "Introduction to BigQuery Migration Service",
    "content": "Home\nBigQuery\nDocumentation\nGuides\nThis document provides an overview of the BigQuery Migration Service.\nThe BigQuery Migration Service is a comprehensive solution for migrating your data\nwarehouse to BigQuery. It includes free-to-use tools that help you with\neach phase of migration, including assessment and planning, SQL\ntranslation for more than 10 dialects, data transfer, and data validation.\nTogether, these tools help you accelerate migrations and reduce risk, shortening\nthe time to value.\nIn the assessment and planning phase, you can use theBigQuery migration assessmentfeature to understand your existing data warehouse. Then, you can use thebatch SQL translatorand theinteractive SQL translatorto prepare your SQL queries and scripts to work in BigQuery. The\nbatch and interactive SQL translators support translation from a wide\nrange of SQL dialects.\nWhen you're ready to move your data, you can use theBigQuery Data Transfer Serviceto automate and manage the migration from your data warehouse to\nBigQuery. After you migrate your data, you can use theData Validation Toolto validate that the migration succeeded.\nQuotas and limits apply to the number of jobs as well as the size of files.\nFor more information on migration service quotas and limits, seeQuotas and limits.\nThere is no charge to use the BigQuery Migration API. However, storage used for\ninput and output files incurs the normal fees. For more information, seeStorage pricing.\nAdditionally, you can use thecost estimation functionality in Google Cloud Migration Centerto generate a cost estimate of running your data warehouse setup that you\nmigrate to BigQuery. For more information, seeStart a cost estimationandSpecify data warehousing requirements.\nFor more information on batch SQL translator, seeBatch SQL translator.\nFor more information on using the interactive SQL translator, seeInteractive SQL translator.\nFor more information on BigQuery migration assessment, seeBigQuery migration assessment.\nLearn about theData Validation Tool.\nFor information about quotas and limits for the BigQuery Migration Service, seeQuotas and limits.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-05 UTC."
  },
  {
    "url": "https://cloud.google.com/memorystore/docs/redis",
    "title": "Memorystore for Redis documentation",
    "content": "Home\nDocumentation\nMemorystore\nMemorystore for Redis\nMemorystore for Redis is a fully managed Redis service for Google Cloud.\n  Applications running on Google Cloud can achieve extreme performance by\n  leveraging the highly scalable, available, secure Redis service without the\n  burden of managing complex Redis deployments.\nMemorystore for Redis is based on and is compatible with open-source Redis\n    versions 7.2 and earlier and supports a subset of the total Redis command library.\nNot sure what database option is right for you? Learn more about ourdatabase services.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nQuickstart: Create a Memorystore for Redis instance by using the gcloud CLI\nQuickstart: Create a Memorystore for Redis instance by using the gcloud CLI\nQuickstart: Create a Memorystore for Redis instance by using the Google Cloud console\nQuickstart: Create a Memorystore for Redis instance by using the Google Cloud console\nOverview of Memorystore for Redis\nOverview of Memorystore for Redis\nConnect to a Redis instance\nConnect to a Redis instance\nCreate and manage Redis instances\nCreate and manage Redis instances\nMonitor Redis instances\nMonitor Redis instances\nConfigure a Redis instance\nConfigure a Redis instance\nExport data from a Redis instance\nExport data from a Redis instance\nScale Redis instances\nScale Redis instances\nSetting up client libraries\nSetting up client libraries\nREST API\nREST API\nSupported environments\nSupported environments\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nTroubleshoot issues\nTroubleshoot issues\nRelease notes\nRelease notes\nGetting support\nGetting support\nBilling questions\nBilling questions\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Redis is a trademark of Redis Ltd. All rights therein are reserved to Redis Ltd. Any use by Google is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and Google. Memorystore is based on and is compatible with open-source Redis versions 7.2 and earlier and supports a subset of the total Redis command library.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vpc/docs/flow-logs",
    "title": "VPC Flow Logs",
    "content": "Home\nVirtual Private Cloud\nDocumentation\nGuides\nVPC Flow Logs records a sample of packets sent from and received byvirtual machine (VM) instances, including\ninstances used asGoogle Kubernetes Engine nodes, and\npackets sent through VLAN attachments forCloud InterconnectandCloud VPNtunnels.\nFlow logs are aggregated by IP connection (5-tuple). These logs can be used\nfor network monitoring, forensics, security analysis, and expense optimization.\nYou can view flow logs inCloud Logging, and you\ncan export logs to any destination that Cloud Logging export supports.\nVPC Flow Logs provides you with visibility into network\nthroughput and performance. You can:\nMonitor the VPC network\nPerform network diagnosis\nFilter the flow logs by VMs, VLAN attachments, and\nCloud VPN tunnels to understand traffic changes\nUnderstand traffic growth for capacity forecasting\nYou can analyze network usage with VPC Flow Logs to\noptimize network traffic expenses. For example, you can\nanalyze the network flows for the following:\nTraffic between regions and zones\nTraffic to specific countries on the internet\nTraffic to on-premises and other cloud networks\nTop talkers in the network, including VMs, VLAN attachments, and\nCloud VPN tunnels\nYou can use VPC Flow Logs for network forensics. For example,\nif an incident occurs, you can examine the following:\nWhich IPs talked with whom and when\nAny compromised IPs by analyzing all the incoming and outgoing network flows\nVPC Flow Logs is part of Andromeda, the software that powers\nVPC networks. VPC Flow Logs introduces no delay\nor performance penalty when enabled.\nVPC Flow Logs works with VPC networks, not legacy\nnetworks. You enable or disable VPC Flow Logs per subnet,\nVLAN attachment for Cloud Interconnect, or Cloud VPN\ntunnel. If enabled for a subnet,\nVPC Flow Logs collects data from all VM instances, including\nGKE nodes, in that subnet.\nVPC Flow LogssamplesTCP, UDP, ICMP, ESP, and\nGRE flows. Both inbound and outbound flows are sampled. These flows can be\nwithin Google Cloud or between Google Cloud and other networks.\nIf a flow is captured by sampling,\nVPC Flow Logs generates a log for the flow. Each flow record\nincludes the information described in theRecord formatsection.\nVPC Flow Logs interacts with firewall rules in the following\nways:Egress packets are sampledbeforeegressfirewall rules. Even if an\negress firewall rule denies outbound packets, those packets can be\nsampled by VPC Flow Logs.Ingress packets are sampledafteringressfirewall rules. If an\ningress firewall rule denies inbound packets, those packets are not\nsampled by VPC Flow Logs.\nEgress packets are sampledbeforeegressfirewall rules. Even if an\negress firewall rule denies outbound packets, those packets can be\nsampled by VPC Flow Logs.\nIngress packets are sampledafteringressfirewall rules. If an\ningress firewall rule denies inbound packets, those packets are not\nsampled by VPC Flow Logs.\nYou can usefiltersin\nVPC Flow Logs to generate only certain logs.\nVPC Flow Logs supports VMs that have multiple network interfaces.\nYou need to enable VPC Flow Logs for each subnet, in each\nVPC, that contains a network interface.\nTo log flows between Pods on the same Google Kubernetes Engine (GKE) node, you\nmust enableIntranode\nvisibilityfor\nthe cluster.\nVPC Flow Logs are not reported from Cloud Run resources.\nPackets are sampled within an aggregation interval. All packets collected for\na given IP connection within the aggregation interval are aggregated into a\nsingle flow log entry. This data is then sent toLogging.\nLogs are stored in Logging for 30 days by default. If\nyou want to keep logs longer than that, you can eitherset a custom\nretention periodorexport themto a supported\ndestination.\nTo generate flow logs, VPC Flow Logs samples packets that\nleave and enter a VM or pass through a gateway such as a VLAN attachment\nor Cloud VPN tunnel. After the flow logs are generated,\nVPC Flow Logs processes them by following the procedure described\nin this section.\nVPC Flow Logs samples packets using aprimary sampling rate.\nThe primary sampling rate is dynamic and varies\ndepending on the load of the physical host running the VM or gateway at the\ntime of sampling. The probability of sampling any single IP connection increases\nwith the volume of packets. You can't control the primary flow log sampling\nprocess or adjust the primary sampling rate.\nAfter the flow logs are generated, VPC Flow Logs processes them\naccording to the following procedure:\nFiltering: You can specify that only logs that match specified criteria\nare generated. For example, you can filter so that only logs\nfor a particular VM or only logs with a particular metadata value\nare generated and the rest are discarded. For more information, seeLog filtering.\nAggregation: Information for sampled packets is aggregated over\na configurableaggregation intervalto produce aflow log entry.\nSecondary flow log sampling: This is a second sampling process. Flow log entries\nare further sampled according to a configurablesecondary sampling rateparameter.\nThe secondary sampling is performed on the flow logs generated by the\nprimary flow log sampling process. For example, if the secondary sampling\nrate is set to 1.0, or 100%, VPC Flow Logs samples 100% of\nthe flow logs generated by the primary flow log sampling.\nMetadata: If disabled, all metadata annotations are discarded. If you\nwant to keep metadata, you can specify that all fields or a\nspecified set of fields are retained. For more information, seeMetadata\nannotations.\nWrite to Logging: The final log entries are written to\nCloud Logging.\nBecause VPC Flow Logs does not capture every packet, it compensates\nfor missed packets by interpolating from the captured packets. This happens for\npackets missed because of initial and user-configurable sampling settings.\nEven though Google Cloud doesn't capture every packet, log record captures\ncan be quite large. You can balance your traffic visibility and storage cost\nneeds by adjusting the following aspects of logs collection:\nAggregation interval: Sampled packets for a time interval are aggregated\ninto a single log entry. This time interval can be 5 seconds\n(default), 30 seconds, 1 minute, 5 minutes, 10 minutes, or 15 minutes.\nSecondary sampling rate:For VMs, 50% of log entries are kept by default. You can set this\nparameter from1.0(100%, all log entries are kept)\nto0.0(0%, no logs are kept).For VLAN attachments and Cloud VPN tunnels, 100% of log entries\nare kept by default. You can set this\nparameter from1.0to greater than0.0.\nFor VMs, 50% of log entries are kept by default. You can set this\nparameter from1.0(100%, all log entries are kept)\nto0.0(0%, no logs are kept).\nFor VLAN attachments and Cloud VPN tunnels, 100% of log entries\nare kept by default. You can set this\nparameter from1.0to greater than0.0.\nMetadata annotations: By default, flow log entries are annotated with\nmetadata information, such as the names of the source and\ndestination within Google Cloud or the geographic region of external\nsources and destinations. Metadata annotations can be turned off, or you\ncan specify only certain annotations, to save storage space.\nFiltering: By default, logs are generated for every sampled flow.\nYou can set filters so that only logs that match certain criteria are\ngenerated.\nStandard pricing for Logging,\nBigQuery, or Pub/Sub apply.\nVPC Flow Logs pricing is described inNetwork Telemetry pricing.\nTo learn more about the VPC Flow Logs record format and which\nmetadata annotations are available, seeAbout VPC Flow Logs records.\nTo see examples of VPC Flow Logs that are collected for various\nuse cases, seeAbout traffic flows.\nTo start reporting flows for a subnet, seeConfigure VPC Flow Logs.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-30 UTC."
  },
  {
    "url": "https://cloud.google.com/bigquery/docs/dts-introduction",
    "title": "What is BigQuery Data Transfer Service?",
    "content": "Home\nBigQuery\nDocumentation\nGuides\nThe BigQuery Data Transfer Service automates data movement intoBigQueryon a scheduled, managed basis. Your analytics team can lay the foundation for a\nBigQuery data warehouse without writing a single line of code.\nYou can access the BigQuery Data Transfer Service using the:\nGoogle Cloud console\nbq command-line tool\nBigQuery Data Transfer Service API\nAfter you configure a data transfer, the BigQuery Data Transfer Service automatically\nloads data into BigQuery on a regular basis. You can also\ninitiate data backfills to recover from any outages or gaps. You\ncannot use the BigQuery Data Transfer Service to transfer data out of\nBigQuery.\nIn addition to loading data into BigQuery,\nBigQuery Data Transfer Service is used for two BigQuery operations:dataset copiesandscheduled queries.\nThe BigQuery Data Transfer Service supports loading data from the following data sources:\nAmazon S3\nAmazon Redshift\nAzure Blob Storage\nCampaign Manager\nCloud Storage\nComparison Shopping Service (CSS) Center(Preview)\nDisplay & Video 360\nFacebook Ads(Preview)\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics 4(Preview)\nGoogle Merchant Center(Preview)\nGoogle Play\nMySQL(Preview)\nOracle(Preview)\nPostgreSQL(Preview)\nSalesforce(Preview)\nSalesforce Marketing Cloud(Preview)\nSearch Ads 360\nServiceNow(Preview)\nTeradata\nYouTube Channel\nYouTube Content Owner\nLike BigQuery, the BigQuery Data Transfer Service is amulti-regional resource, with many additional single regions available.\nA BigQuery dataset's locality is specified when youcreate a destination datasetto store the data\ntransferred by the BigQuery Data Transfer Service. When you set up a transfer, the\ntransfer configuration itself is set to the same location as the destination\ndataset. The BigQuery Data Transfer Service processes and stages data in the same\nlocation as the destination dataset.\nThe data you want to transfer to BigQuery can also have a region. In most\ncases, the region where your data is stored and the location of\nthe destination dataset in BigQuery are irrelevant. In other kinds of\ntransfers, the dataset and the source data must becolocatedin the\nsame region, or a compatible region.\nFor detailed information about transfers and region compatibility for\nBigQuery Data Transfer Service, seeDataset locations and transfers.\nFor supported regions for BigQuery, seeDataset locations.\nFor information on BigQuery Data Transfer Service pricing, see thePricingpage.\nOnce data is transferred to BigQuery, standard\nBigQuerystorageandquerypricing applies.\nFor information on BigQuery Data Transfer Service quotas, see theQuotas and limitspage.\nTo learn how to create a transfer, see the documentation for yourdata source.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-05 UTC."
  },
  {
    "url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCompute Engine\nDocumentation\nGuides\nThis document describes the features, types, performance and benefits of Persistent Disk\nvolumes. If you need block storage for a virtual machine (VM) instance or container,\nsuch as for a boot disk or data disk, use Persistent Disk volumes if\nGoogle Cloud Hyperdisk isn't available for your compute instance. To learn about the\nother block storage options in Compute Engine, seeChoose a disk type.\nPersistent Disk volumes are durable network storage devices that your instances\ncan access like physical disks in a desktop or a server. Persistent Disk volumes\naren't attached to the physical machine hosting the instance. Instead, they are\nattached to the instance asnetwork block devices.\nWhen you read to or write from a Persistent Disk volume, data is transmitted over the network.\nThe data on each Persistent Disk volume is distributed across several physical disks.\nCompute Engine manages the physical disks and the data distribution for\nyou to ensure redundancy and optimal performance.\nYou can detach or move the volumes to keep your data even after you delete your\ninstances. Persistent Disk performance increases with size, so you can resize your\nexisting Persistent Disk volumes or add more Persistent Disk volumes to a\nVM to meet your performance and storage space requirements.\nAdd a non-boot disk to your instancewhen you need reliable and affordable storage with consistent performance\ncharacteristics.\nAdd a Persistent Disk to your instance\nWhen you create a Persistent Disk volume, you can select one of the following disk\ntypes:\nBalanced Persistent Disk(pd-balanced)An alternative to SSD (Performance) Persistent Disk.Balance of performance and cost. For most Compute Engine machine\ntypes, these disks have the same maximum IOPS as\nSSD Persistent Disk and lower IOPS per GiB. This disk type offers performance\nlevels suitable for most general-purpose applications at a price point\nbetween that of standard and SSD Persistent Disk.Backed by solid-state drives (SSD).\nAn alternative to SSD (Performance) Persistent Disk.\nBalance of performance and cost. For most Compute Engine machine\ntypes, these disks have the same maximum IOPS as\nSSD Persistent Disk and lower IOPS per GiB. This disk type offers performance\nlevels suitable for most general-purpose applications at a price point\nbetween that of standard and SSD Persistent Disk.\nBacked by solid-state drives (SSD).\nSSD (Performance) Persistent Disk(pd-ssd)Suitable for enterprise applications and high-performance databases\nthat require lower latency and more IOPS than standard Persistent Disk\nprovides.Backed by solid-state drives (SSD).\nSuitable for enterprise applications and high-performance databases\nthat require lower latency and more IOPS than standard Persistent Disk\nprovides.\nBacked by solid-state drives (SSD).\nStandard Persistent Disk(pd-standard)Suitable for large data processing workloads that primarily use\nsequential I/Os.Backed by standard hard disk drives (HDD).\nSuitable for large data processing workloads that primarily use\nsequential I/Os.\nBacked by standard hard disk drives (HDD).\nExtreme Persistent Disk(pd-extreme)Offers consistently high performance for both random access workloads and\nbulk throughput.Designed for high-end database workloads.Lets you provision the target IOPS.Backed by solid-state drives (SSD).Available with a limited number ofmachine types.\nOffers consistently high performance for both random access workloads and\nbulk throughput.\nDesigned for high-end database workloads.\nLets you provision the target IOPS.\nBacked by solid-state drives (SSD).\nAvailable with a limited number ofmachine types.\nIf you create a disk in the Google Cloud console, the default disk type ispd-balanced. If you create a disk using the gcloud CLI or the\nCompute Engine API, the default disk type ispd-standard.\nFor information about machine type support, refer to the following:\nZonal Persistent Disk\nRegional Persistent Disk\nDisk durability represents the probability of data loss, by design, for a\ntypical disk in a typical year, using a set of assumptions about hardware\nfailures, the likelihood of catastrophic events, isolation practices and\nengineering processes in Google data centers, and the internal encodings used\nby each disk type. Persistent Disk data loss events are extremely rare and have\nhistorically been the result of coordinated hardware failures, software bugs, or\na combination of the two. Google also takes many steps to mitigate the\nindustry-wide risk ofsilent data corruption.\nHuman error by a Google Cloud customer, such as when a customer\naccidentally deletes a disk, is outside the scope of Persistent Disk durability.\nThere is a very small risk of data loss occurring with a regional Persistent Disk\nvolume due to its internal data encodings and replication. Regional Persistent Disk\nprovideshigh availabilityand can be used for disaster recovery if an entire data center is lost and\ncan't be recovered.\nRegional Persistent Disk provides twice as many disk replicas as zonal Persistent Disk,\nwith each replica distributed between two zones in the same region. If a primary zone\nbecomes unavailable during an outage, the replica in the second zone can be\naccessed immediately.\nFor more information about region-specific considerations, seeGeography and regions.\nThe following table shows durability for each disk type's design. 99.999% durability\nmeans that with 1,000 disks, you would likely go a hundred years without\nlosing a single one.\nC4AC4C4D(Preview)C3C3DN4N2N2DN1T2DT2AE2Z3H3C2C2DX4M4M3M2M1N1+GPUA4A3 (H200)A3 (H100)A2G2\nSelect a machine series to see its supported Persistent Disk (PD) types.\nPersistent Disk volumes can be up to 64 TiB in size. You can add\nup to 127 secondary, non-boot zonal Persistent Disk volumes to a VM instance.\nHowever, the combined total capacity of all Persistent Disk volumes attached\nto a single VM can't exceed 257 TiB.\nYou can create single logical volumes of up to 257 TiB using logical volume\nmanagement inside your VM. For information about how to ensure maximum\nperformance with large volumes, seeLogical volume size.\nA zonal Persistent Disk is a Persistent Disk that's accessible only within one\nspecific zone, for example,europe-west-2.\nCompute Engine handles most disk management tasks for you so that\nyou don't need to deal with partitioning, redundant disk arrays, or subvolume\nmanagement.\nGenerally, you don't need to create larger logical volumes. However, you can extend\nyour secondary attached Persistent Disk capacity to 257 TiB per\nVM and apply these practices to your Persistent Disk volumes.\nYou can save time and get the best performance if youformat your Persistent Disk volumeswith a single file system and no partition tables.\nIf you need to separate your data into multiple unique volumes,create additional disksrather than dividing your existing disks into multiple partitions.\nWhen you require additional space on your Persistent Disk volumes,resize your disksrather than\nrepartitioning and formatting.\nPersistent Disk performance is predictable and scales linearly with\nprovisioned capacity until the limits for a VM's provisioned vCPUs are\nreached. For more information about performance scaling limits and optimization,\nseeConfigure disks to meet performance requirements.\nStandard Persistent Disk volumes are efficient and economical for handling\nsequential read/write operations, but they aren't optimized to handle high\nrates of random input/output operations per second (IOPS). If your apps require\nhigh rates of random IOPS, use SSD or extreme Persistent Disk. SSD Persistent Disk is\ndesigned for single-digit millisecond latencies. Observed latency is\napplication specific.\nCompute Engine optimizes performance and scaling on Persistent Disk\nvolumes automatically. You don't need to stripe multiple disks together or\npre-warm disks to get the best performance. When you need more disk space or\nbetter performance,resize your disksand possibly add more vCPUs to add more storage space, throughput, and IOPS.\nPersistent Disk performance is based on the total Persistent Disk capacity attached to a\nVM and the number of vCPUs that the VM has.\nFor boot devices, you can reduce costs by using a standard\nPersistent Disk. Small, 10 GiB Persistent Disk volumes can work for basic boot and\npackage management use cases. However, to ensure consistent performance for more\ngeneral use of the boot device, use a balanced Persistent Disk as your boot\ndisk.\nBecause Persistent Disk write operations contribute to the cumulative network\negress traffic for your VM, Persistent Disk write operations are capped by thenetwork egress capfor your VM.\nPersistent Disk has built-in redundancy to protect your data against\nequipment failure and to ensure data availability through data center\nmaintenance events. Checksums are calculated for all Persistent Disk operations,\nso we can ensure that what you read is what you wrote.\nAdditionally, you cancreate snapshots of Persistent Diskto\nprotect against data loss due to user error. Snapshots are incremental, and\ntake only minutes to create even if you snapshot disks that are attached\nto running VMs.\nRegional Persistent Disk volumes have storage qualities that are similar to zonal\nPersistent Disk. However, regional Persistent Disk volumes provide durable storage and\nreplication of data between two zones in the same region.\nWhen you create a new Persistent Disk, you can either\ncreate the disk in one zone, or replicate it across two zones within the\nsame region.\nFor example, if you create one disk in a zone, such as inus-west1-a, you\nhave one copy of the disk. A disk created in only one zone is referred to as a\nzonal disk. You can increase the disk's availability by storing another\ncopy of the disk in a different zone within the region, such as inus-west1-b.\nPersistent Disk replicated across two zones in the same region are called\nRegional Persistent Disk. You can also use Hyperdisk Balanced High Availability for cross-zonal synchronous\nreplication of Google Cloud Hyperdisk.\nIt's unlikely for a region to fail altogether, but zonal failures can happen.\nReplicating within the region to different zones, as shown in the following\nimage, helps with availability and reduces disk latency. If both replication\nzones fail, it's considered a region-wide failure.\nDisk is replicated in two zones.\nIn the replicated scenario, the data is available in the local zone\n(us-west1-a) which is the zone the virtual machine (VM) is running in. Then,\nthe data is replicated to another zone (us-west1-b). One of the zones must be\nthe same zone that the VM is running in.\nIf a zonal outage occurs, you can usually failover your workload\nrunning on Regional Persistent Disk to another zone. To learn more, seeRegional Persistent Disk failover.\nIf you'redesigning robust systemsorhigh availability serviceson\nCompute Engine, use Regional Persistent Disk combined with other best\npractices such asbacking up your data using snapshots.\nRegional Persistent Disk volumes are also designed to work withregional managed instance groups.\nRegional Persistent Disk volumes are designed for workloads that require a lowerRecovery Point Objective (RPO)andRecovery Time Objective (RTO)compared to using Persistent Disk snapshots.\nRegional Persistent Disk are an option when write performance is less critical\nthan data redundancy across multiple zones.\nLike zonal Persistent Disk, Regional Persistent Disk can achieve greater\nIOPS and throughput performance on VMs with a greater number of vCPUs.\nFor more information about this and other limitations, seeConfigure disks to meet performance requirements.\nWhen you need more disk space or better performance, you canresize your regional disksto add more storage space, throughput, and IOPS.\nCompute Engine replicates data of your regional Persistent Disk to the\nzones you selected when you created your disks. The data of each replica is\nspread across multiple physical machines within the zone to ensure redundancy.\nSimilar to zonal Persistent Disk, you cancreate snapshots of Persistent Diskto\nprotect against data loss due to user error. Snapshots are incremental, and\ntake only minutes to create even if you snapshot disks that are attached\nto running VMs.\nYou can attach regional Persistent Disk only to VMs that useE2,N1,N2, andN2Dmachine types.\nYou can attach Hyperdisk Balanced High Availability only tosupported\n      machine types.You can't create a regional Persistent Disk from anOS image, or from a disk that was created\n      from an OS image.You can't create a Hyperdisk Balanced High Availability disk by cloning a zonal disk. To create a Hyperdisk Balanced High Availability disk from an\n    zonal disk, complete the steps inChange a zonal disk to a\n     Hyperdisk Balanced High Availability disk.When using read-only mode, you can attach a regional balanced Persistent Disk to a maximum of 10\n      VM instances.The minimum size of a regional standard Persistent Disk is 200 GiB.You can only increase the size of a\n      regional Persistent Disk or\n      Hyperdisk Balanced High Availability volume; you can't decrease its size.Regional Persistent Disk and Hyperdisk Balanced High Availability volumes have different performance\n      characteristics than their corresponding zonal disks. For more information, seeAbout Persistent Disk performanceandHyperdisk Balanced High Availability performance limits.You can't use a Hyperdisk Balanced High Availability volume that's in multi-writer mode as a boot disk.If you create a replicated disk by cloning a zonal disk, then the two zonal replicas\n      aren't fully in sync at the time of creation. After creation, you can use the regional disk\n      clone within 3 minutes, on average. However, you might need to wait for tens of minutes\n      before the disk reaches a fully replicated state and therecovery point objective (RPO)is close to zero. Learn how tocheck if your replicated disk is fully replicated.\nYou can't create a regional Persistent Disk from anOS image, or from a disk that was created\n      from an OS image.\nYou can't create a Hyperdisk Balanced High Availability disk by cloning a zonal disk. To create a Hyperdisk Balanced High Availability disk from an\n    zonal disk, complete the steps inChange a zonal disk to a\n     Hyperdisk Balanced High Availability disk.\nWhen using read-only mode, you can attach a regional balanced Persistent Disk to a maximum of 10\n      VM instances.\nThe minimum size of a regional standard Persistent Disk is 200 GiB.\nYou can only increase the size of a\n      regional Persistent Disk or\n      Hyperdisk Balanced High Availability volume; you can't decrease its size.\nRegional Persistent Disk and Hyperdisk Balanced High Availability volumes have different performance\n      characteristics than their corresponding zonal disks. For more information, seeAbout Persistent Disk performanceandHyperdisk Balanced High Availability performance limits.\nYou can't use a Hyperdisk Balanced High Availability volume that's in multi-writer mode as a boot disk.\nIf you create a replicated disk by cloning a zonal disk, then the two zonal replicas\n      aren't fully in sync at the time of creation. After creation, you can use the regional disk\n      clone within 3 minutes, on average. However, you might need to wait for tens of minutes\n      before the disk reaches a fully replicated state and therecovery point objective (RPO)is close to zero. Learn how tocheck if your replicated disk is fully replicated.\nThe storage interface is chosen automatically for you when you create your\ninstance or add Persistent Disk volumes to a VM. Tau T2A and third generation VMs\n(such as M3) use theNVMeinterface for Persistent Disk.\nConfidential VMinstances also use NVMe Persistent Disk. All other Compute Engine machine\nseries use theSCSIdisk\ninterface for Persistent Disk.\nMost public images include both NVMe and SCSI drivers. Most images include a\nkernel with optimized drivers that allow your VM to achieve the best performance\nusing NVMe. Your imported Linux images achieve the best performance with NVMe if\nthey include kernel version4.14.68or later.\nTo determine if an operating system version supports NVMe, see theoperating system detailspage.\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nYou can attach an SSD Persistent Disk in multi-writer mode to up to two N2\nVMs simultaneously so that both VMs can read and write to the disk.\nPersistent Disk in multi-writer mode provides a shared block storage capability\nand presents an infrastructural foundation for building highly available\nshared file systems and databases. These specialized file systems and databases\nshould be designed to work with shared block storage and handle cache coherence\nbetween VMs by using tools such asSCSI Persistent Reservations.\nHowever, Persistent Disk with multi-writer mode should generally not be used\ndirectly. Many file systems such as EXT4, XFS,\nand NTFS aren't designed to be used with shared block storage. For more\ninformation about the best practices when sharing Persistent Disk between VMs,\nseeBest practices.\nIf you require a fully managed file storage, you canmount a Filestore\nfile share on your Compute Engine VMs.\nTo enable multi-writer mode for new Persistent Disk volumes, create a new\nPersistent Disk and specify the--multi-writerflag in the gcloud CLI\nor themultiWriterproperty in the Compute Engine API. For more information, seeShare Persistent Disk volumes between VMs.\nCompute Engine automatically encrypts your data before it travels\noutside of your VM to the Persistent Disk storage space. Each Persistent Disk\nremains encrypted either with system-defined keys or withcustomer-supplied keys.\nGoogle distributes Persistent Disk data across multiple physical\ndisks in a manner that users don't control.\nWhen you delete a Persistent Disk volume, Google discards the cipher keys,\nrendering the data irretrievable. This process is irreversible.\nIf you want to control the encryption keys that are used to encrypt your data,create your disks with your own encryption keys.\nYou can't attach a Persistent Disk volume to an VM in another project.\nYou can't attach a Persistent Disk volume to an VM in another project.\nYou can attach a balanced Persistent Disk to a maximum of 10 VMs in\nread-only mode.\nYou can attach a balanced Persistent Disk to a maximum of 10 VMs in\nread-only mode.\nForcustom machine typesor predefined machine types with a minimum of 1 vCPU, you can attach up to\n128 Persistent Disk volumes.\nForcustom machine typesor predefined machine types with a minimum of 1 vCPU, you can attach up to\n128 Persistent Disk volumes.\nEach Persistent Disk volume can be up to 64 TiB in size, so there is no\nneed to manage arrays of disks to create large logical volumes. Each VM can\nattach only a limited amount of total Persistent Disk space and a limited\nnumber of individual Persistent Disk volumes. Predefined machine types and\ncustom machine types have the same Persistent Disk limits.\nEach Persistent Disk volume can be up to 64 TiB in size, so there is no\nneed to manage arrays of disks to create large logical volumes. Each VM can\nattach only a limited amount of total Persistent Disk space and a limited\nnumber of individual Persistent Disk volumes. Predefined machine types and\ncustom machine types have the same Persistent Disk limits.\nMost VMs can have up to 128 Persistent Disk volumes and up to 257 TiB of\ntotal disk space attached. Total disk space for a VM includes the size\nof the boot disk.\nMost VMs can have up to 128 Persistent Disk volumes and up to 257 TiB of\ntotal disk space attached. Total disk space for a VM includes the size\nof the boot disk.\nShared-core machine typesare limited to 16 Persistent Disk volumes and 3 TiB of total Persistent Disk\nspace.\nShared-core machine typesare limited to 16 Persistent Disk volumes and 3 TiB of total Persistent Disk\nspace.\nCreating logical volumes larger than 64 TiB might require special\nconsideration. For more information about larger logical volume performance\nseelogical volume size.\nCreating logical volumes larger than 64 TiB might require special\nconsideration. For more information about larger logical volume performance\nseelogical volume size.\nPersistent Disk is designed to run in tandem with Google's file system,Colossus,\nwhich is a distributed block storage system. Persistent Disk drivers\nautomatically encrypt data on the VM before it's transmitted from the VM onto\nthe network. Then, Colossus persists the data. When Colossus reads the data, the\ndriver decrypts the incoming data.\nPersistent Disk volumes use Colossus for the storage backend.\nHaving disks as a service is useful in a number of cases, for example:\nResizing the disks while the instance is running becomes easier than\nstopping the instance first. You can increase the disk size without stopping\nthe instance.\nAttaching and detaching disks becomes easier when disks and VMs don't have\nto share the same lifecycle or be co-located. It's possible to stop a VM and\nuse its Persistent Disk boot disk to boot another VM.\nHigh availability features like replication become easier because the disk\ndriver can hide replication details and provide automatic write-time\nreplication.\nLearn how toadd a Persistent Disk volume to your VM.\nLearn how toadd a Persistent Disk volume to your VM.\nReviewdisk and image pricinginformation.\nReviewdisk and image pricinginformation.\nLearn how toclone a Persistent Disk volume.\nLearn how toclone a Persistent Disk volume.\nLearn how toshare Persistent Disk volumes between VMs.\nLearn how toshare Persistent Disk volumes between VMs.\nLearn how tooptimize Persistent Disk performance.\nLearn how tooptimize Persistent Disk performance.\nLearn how toview your Persistent Disk volumes' actual and forecasted usage.\nSeebest practices for disk snapshots.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/gitlab",
    "title": "GitLab on Google Cloud documentation",
    "content": "Home\nDocumentation\nGitLab on Google Cloud\nThe GitLab on Google Cloud integration simplifies deploying GitLab source code\n    to Google Cloud runtimes, and is available for the Free, Premium, and\n    Ultimate tier of the GitLab.com offering.\nTo get started,\n    try the end-to-end GitLab tutorial,Set up the GitLab on Google Cloud integration.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nSet up the GitLab on Google Cloud integration\nSet up the GitLab on Google Cloud integration\nOverview\nOverview\nGitLab on Google Cloud Identity and Access Management\nGitLab on Google Cloud Identity and Access Management\nGitLab on Google Cloud Artifact management\nGitLab on Google Cloud Artifact management\nProvisioning runners\nProvisioning runners\nCreate a GitLab pipeline to push to Google Artifact Registry\nCreate a GitLab pipeline to push to Google Artifact Registry\nCreate and deploy a web service with the Google Cloud Run component\nCreate and deploy a web service with the Google Cloud Run component\nGitLab Pricing\nGitLab Pricing\nArtifact Registry Pricing\nArtifact Registry Pricing\nGoogle Cloud Pricing\nGoogle Cloud Pricing\nRelease notes\nRelease notes\nArtifact Registry GitLab component\nArtifact Registry is a single place to manage container images. It is\n      fully integrated with Google Cloud’s tooling and runtimes. This makes\n      it simple to integrate it with your CI/CD tooling to set up automated\n      pipelines.Once you have connected GitLab to Artifact Registry and pushed a container image to your repository, you can view the\n      container image in GitLab orArtifact Registry, and you can access metadata\n      for each artifact inGoogle Cloud.Component\nCloud Deploy component\nThecreate-cloud-deploy-releaseGitLab Component\n      creates a Cloud Deploy release to manage the deployment of an\n      application to one or more Google Kubernetes Engine (GKE) Enterprise edition or Cloud Run targets.Component\nCloud Run component\nThedeploy-cloud-runGitLab Component automates the\n      deployment of your Cloud Run services within your GitLab\n      CI/CD pipeline. The component offers flexible deployment behavior,\n      creating a brand-new service if one doesn't already exist in your project\n      and region. Conversely, if a Cloud Run service with the same\n      name is already present, the component updates it to a new revision using\n      your inputs.Component\nGoogle Cloud SDK component\nTherun-gcloudGitLab component executes Google Cloud CLI commands. The component uses a customized Google Cloud CLI image\n  instead of thegoogle/cloud-sdkimage to reduce the image size\n  and avoid security vulnerabilities.Component\nCloud Storage component\nCloud Storage is a managed service for storing unstructured data.\n    Store any amount of data and retrieve it as often as you like. To upload to Cloud Storage, add the component to your CICD pipeline in GitLab.Component\nApp Engine component\nThedeploy-app-enginecomponent deploys container\n    images stored in Artifact Registry, or App Engine flexible environmentsource code, to App Engine as part of your GitLab CI/CD pipeline.Component\nGoogle Kubernetes Engine (GKE) component\nThedeploy-gkecomponent deploys a container image to a GKE cluster. It also performs horizontal pod autoscaling\n    up to 3 nodes and creates a Service if the application needs a port exposed.Component\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini",
    "title": "Text generationStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nTo see an example of getting started with Chat with the Gemini Pro model,\n      run the \"Getting Started with Chat with the Gemini Pro model\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nThis page shows you how to send chat prompts to a Gemini model by using\nthe Google Cloud console, REST API, and supported SDKs.\nTo learn how to add images and other media to your request, seeImage understanding.\nFor a list of languages supported by Gemini, seeLanguage support.\nTo explore\nthe generative AI models and APIs that are available on Vertex AI, go to\nModel Garden in the Google Cloud console.\nGo to Model Garden\nIf you're looking for a way to use Gemini directly from your mobile and\nweb apps, see theFirebase AI Logic client SDKsfor\nSwift, Android, Web, Flutter, and Unity apps.\nFor testing and iterating on chat prompts, we recommend using the\nGoogle Cloud console. To send prompts programmatically to the model, you can use the\nREST API, Google Gen AI SDK, Vertex AI SDK for Python, or one of the other supported libraries and\nSDKs.\nYou can use system instructions to steer the behavior of the model based on a\nspecific need or use case. For example, you can define a persona or role for a\nchatbot that responds to customer service requests. For more information, see\nthesystem instructions code samples.\nYou can use theGoogle Gen AI SDKto send requests if\nyou're usingGemini 2.0 Flash.\nHere is a simple text generation example.\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nLearn how to install or update theGen AI SDK for Go.\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nLearn how to install or update theGen AI SDK for Java.\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nYou can choose whether the model generatesstreamingresponses ornon-streamingresponses. For streaming responses, you receive each response\nas soon as its output token is generated. For non-streaming responses, you receive\nall responses after all of the output tokens are generated.\nHere is a streaming text generation example.\nBefore trying this sample, follow thePythonsetup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AIPythonAPI\n    reference documentation.\nTo authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.\nLearn how to send multimodal prompt requests:Image understandingVideo understandingAudio understandingDocument understanding\nLearn how to send multimodal prompt requests:\nImage understanding\nVideo understanding\nAudio understanding\nDocument understanding\nLearn aboutresponsible AI best practices and Vertex AI's safety filters.\nLearn aboutresponsible AI best practices and Vertex AI's safety filters.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/samples",
    "title": "Google Cloud samples",
    "content": "Home\nDocumentation\nSearch for samples demonstrating the usage of Google Cloud\n  products.\nFor Terraform samples, seeResource samplesandBlueprints."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments",
    "title": "Introduction to Vertex AI ExperimentsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nTo see an example of getting started with Vertex AI Experiments,\n      run the \"Get started with Vertex AI Experiments\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nVertex AI Experiments is a tool that helps you track and analyze different\nmodel architectures, hyperparameters, and training environments,\nletting you track the steps, inputs, and outputs of\nan experiment run. Vertex AI Experiments can also evaluate how your model\n performed in aggregate,\nagainst test datasets, and during the training run. You can then use this\ninformation to select the best model for your particular use case.\nExperiment runs don't incur additional charges. You're only charged for\nresources that you use during your experiment as described inVertex AI pricing.\nVertex AI Experiments lets you track:\nsteps of anexperiment run,\nfor example, preprocessing, training,\ninputs, for example, algorithm, parameters, datasets,\noutputs of those steps, for example, models, checkpoints, metrics.\nYou can then figure out what worked and what didn't, and identify further\navenues for experimentation.\nFor user journey examples, check out:\nModel training\nCompare models\nVertex AI Experiments lets you track and evaluate how\nthe model performed in aggregate, against test datasets, and during\nthe training run. This ability helps to understand the performance\ncharacteristics of the models -- how well a particular model works overall,\nwhere it fails, and where the model excels.\nFor user journey examples, check out:\nCompare pipeline runs\nCompare models\nVertex AI Experiments lets you group and compare multiple models\nacrossexperiment runs.\nEach model has its own specified parameters, modeling techniques, architectures,\nand input. This approach helps select the best model.\nFor user journey examples, check out:\nCompare pipeline runs\nCompare models\nThe Google Cloud console provides a centralized view of experiments,\na cross-sectional view of the experiment runs, and the details for each run.\nThe Vertex AI SDK for Python provides APIs to consume experiments, experiment runs,\nexperiment run parameters, metrics, and artifacts.\nVertex AI Experiments, along withVertex ML Metadata, provides a way\nto find the artifacts tracked in an experiment. This lets you quickly view the\nartifact's lineage and the artifacts consumed and produced by steps in a run.\nVertex AI Experiments supports development of models using\nVertex AI custom training, Vertex AI Workbench\nnotebooks, Notebooks, and all Python ML Frameworks across most ML Frameworks.\nFor some ML frameworks, such as TensorFlow, Vertex AI Experiments\nprovides deep integrations into the framework that makes the user experience\nautomagical. For other ML frameworks, Vertex AI Experiments provides\na framework neutral Vertex AI SDK for Python that you can use.\n(see:Prebuilt containersfor\nTensorFlow, scikit-learn, PyTorch, XGBoost).\nVertex AI Experiments is acontextinVertex ML Metadatawhere an experiment\ncan containnexperiment runs in addition tonpipeline runs. An experiment\nrun consists of parameters, summary metrics, time series metrics, andPipelineJob,Artifact, \nandExecutionVertex AI resources.Vertex AI TensorBoard, a\nmanaged version of open source TensorBoard, is used for time-series metrics\nstorage. Executions andartifactsof a pipeline run are viewable\nin theGoogle Cloud console.\nexperimentAn experiment is a context that can contain a set of n experiment runs in addition to  pipeline runs where a user can investigate, as a group, different configurations such as input artifacts or hyperparameters.SeeCreate an experiment.\nAn experiment is a context that can contain a set of n experiment runs in addition to  pipeline runs where a user can investigate, as a group, different configurations such as input artifacts or hyperparameters.\nexperiment runAn experiment run can contain user-defined metrics, parameters, executions, artifacts, and Vertex resources (for example, PipelineJob).SeeCreate and manage experiment runs.\nAn experiment run can contain user-defined metrics, parameters, executions, artifacts, and Vertex resources (for example, PipelineJob).\npipeline runOne or more Vertex PipelineJobs can be associated with an experiment where each PipelineJob is represented as a single run. In this context, the parameters of the run are inferred by the parameters of the PipelineJob. The metrics are inferred from the system.Metric artifacts produced by that PipelineJob. The artifacts of the run are inferred from artifacts produced by that PipelineJob.One or more Vertex AIPipelineJobresource\ncan be associated with anExperimentRunresource.\nIn this context, the parameters, metrics, and artifacts are not inferred.\nOne or more Vertex PipelineJobs can be associated with an experiment where each PipelineJob is represented as a single run. In this context, the parameters of the run are inferred by the parameters of the PipelineJob. The metrics are inferred from the system.Metric artifacts produced by that PipelineJob. The artifacts of the run are inferred from artifacts produced by that PipelineJob.\nSeeAssociate a pipeline with an experiment.\nSeeLog parameters.\nsummary metricsSummary metrics are a single value for each metric key in an experiment run. For example, the test accuracy of an experiment is the accuracy calculated against a test dataset at the end of training that can be captured as a single value summary metric.\nSummary metrics are a single value for each metric key in an experiment run. For example, the test accuracy of an experiment is the accuracy calculated against a test dataset at the end of training that can be captured as a single value summary metric.\nSeeLog summary metrics.\ntime series metricsTime series metrics are longitudinal metric values where each value represents a step in the training routine portion of a run. Time series metrics are stored in Vertex AI TensorBoard. Vertex AI Experiments stores a reference to the Vertex TensorBoard resource.\nTime series metrics are longitudinal metric values where each value represents a step in the training routine portion of a run. Time series metrics are stored in Vertex AI TensorBoard. Vertex AI Experiments stores a reference to the Vertex TensorBoard resource.\nSeeLog time series metrics.\npipeline jobA pipeline job or a pipeline run corresponds to the PipelineJob resource in the Vertex AI API. It's an execution instance of your ML pipeline definition, which is defined as a set of ML tasks interconnected by input-output dependencies.\nA pipeline job or a pipeline run corresponds to the PipelineJob resource in the Vertex AI API. It's an execution instance of your ML pipeline definition, which is defined as a set of ML tasks interconnected by input-output dependencies.\nartifactAn artifact is a discrete entity or piece of data produced and consumed by a machine learning workflow. Examples of artifacts include datasets, models, input files, and training logs.\nAn artifact is a discrete entity or piece of data produced and consumed by a machine learning workflow. Examples of artifacts include datasets, models, input files, and training logs.\nVertex AI Experiments lets you use a schema to define the type of\nartifact. For example, supported schema types includesystem.Dataset,system.Model, andsystem.Artifact. For more information, seeSystem schemas.\nGet started with Vertex AI Experiments\nSet up to get started with Vertex AI Experiments\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/devtools",
    "title": "Google Cloud SDK, languages, frameworks, and tools",
    "content": "Home\nDocumentation\nUse Google Cloud SDK, languages, frameworks, and tools effectively in cloud development.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/iac",
    "title": "Infrastructure as code",
    "content": "Home\nDocumentation\nConfigure your infrastructure using code instead of graphical interfaces or command-line scripts.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview",
    "title": "Grounding overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nTo see an example of grounding,\n      run the \"Intro to grounding\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nIn generative AI, grounding is the ability to connect model output to verifiable\nsources of information. If you provide models with access to specific data\nsources, then grounding tethers their output to these data and reduces the\nchances of inventing content. This is particularly important in situations where\naccuracy and reliability are significant.\nGrounding provides the following benefits:\nReduces model hallucinations, which are instances where the model generates\ncontent that isn't factual.\nAnchors model responses to your data sources.\nProvides auditability by\nproviding grounding support, which are links to sources.\nYou can ground supported-model output in Vertex AI in the following ways:\nFor language support, seeSupported languages for prompts.\nTo learn more about responsible AI best practices and Vertex AI's\nsafety filters, seeResponsible AI.\nTo ground with your Google Search API, seeGrounding with\nGoogle Search\nAPI.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-20 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/buildpacks",
    "title": "Buildpacks documentation",
    "content": "Home\nDocumentation\nGoogle Cloud's buildpacks\nUse  Google Cloud's Buildpacks to create and run containers on Google Cloud.Learn more.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nGuideBuild an application with buildpacks\nBuild an application with buildpacks\nGuideBuild a function with buildpacks\nBuild a function with buildpacks\nReferenceLanguages supported by buildpacks\nLanguages supported by buildpacks\nReferenceBuildpacks configurations\nBuildpacks configurations\nReferenceBuildpacks build and run image configurations\nBuildpacks build and run image configurations\nGitHubGoogle Cloud's buildpacks\nGoogle Cloud's buildpacks\nSample apps\nFind samples to build your functions and applications with buildpacks.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/generative-ai",
    "title": "No title",
    "content": "Home\nDocumentation\nGoogle Cloud provides a set of state-of-the-art foundation models through Vertex AI, including Gemini. You can also deploy a third-party model to either Vertex AI Model Garden or self-host on GKE or Compute Engine.\nPrompt design is the process of authoring prompt and response pairs to give language models additional context and instructions. After you author prompts, you feed them to the model as a prompt dataset for pretraining. When a model serves predictions, it responds with your instructions built in.\nGroundingconnects AI models to data sources to improve the accuracy of responses and reduce  hallucinations.RAG, a common grounding technique, searches for relevant information and adds it to the model's prompt, ensuring output is based on facts and up-to-date information.\nAgents make it easy to design and integrate a conversational user interface into your mobile app, while function calling extends the capabilities of a model.\nSpecialized tasks, such as training a language model on specific terminology, might require more training than you can do with prompt design or grounding alone. In that scenario, you can use model tuning to improve performance, or train your own model.\nC# and .NET\nC++\nGo\nJava\nJavaScript and Node.js\nPython\nRuby\nPython (LangChain)\nJavaScript (LangChain.js)\nJava (LangChain4j)\nGo (LangChainGo)\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-17 UTC."
  },
  {
    "url": "https://cloud.google.com/gemini/docs/bigquery/overview",
    "title": "Gemini in BigQuery overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGemini for Google Cloud\nDocumentation\nGuides\nThis document describes how Gemini in BigQuery, which is part\nof theGemini for Google Cloudproduct suite,\nprovides AI-powered assistance to help you work with your data.\nGemini in BigQuery provides AI assistance to help\nyou do the following:\nExplore and understand your data with data insights. Data insights offers an automated,\n    intuitive way to uncover patterns and perform statistical analysis by using insightful queries\n    that are generated from the metadata of your tables. This feature is especially helpful in\n    addressing the cold-start challenges of early data exploration. For more information, seeGenerate data insights in BigQuery.\nDiscover, transform, query, and visualize data with BigQuery data canvas. You can use\n      natural language with Gemini in BigQuery, to find, join, and\n      query table assets, visualize results, and seamlessly collaborate with others throughout the\n      entire process. For more information, seeAnalyze with\n    data canvas.\nGet assisted SQL and Python data analysis. You can use Gemini in\n        BigQuery to generate or suggest code in either SQL or Python, and to explain\n        an existing SQL query. You can also use natural language queries to begin data analysis. To\n        learn how to generate, complete, and summarize code, see the following documentation:SQL code assistUse the SQL generation toolPrompt to generate SQL queriesGenerate SQL queries with Gemini Cloud Assist(Preview)Complete a SQL query(Preview)Explain a SQL queryPython code assistGenerate Python code with the code generation toolGenerate Python code with Gemini Cloud Assist(Preview)Python code completionGenerate BigQuery DataFrames Python code(Preview)Prepare data for analysis. Data preparation in BigQuery gives you context\n    aware, AI-generated transformation recommendations to cleanse data for analysis. For more\n    information, seePrepare data with Gemini.Customize your SQL translations with translation rules. (Preview)\n    Create Gemini-enhanced translation rules to customize your SQL translations when\n    using theinteractive SQL translator.\n    You can describe changes to the SQL translation output using natural language prompts or specify\n    SQL patterns to find and replace. For more information, seeCreate a translation\n    rule.\nSQL code assistUse the SQL generation toolPrompt to generate SQL queriesGenerate SQL queries with Gemini Cloud Assist(Preview)Complete a SQL query(Preview)Explain a SQL query\nUse the SQL generation tool\nPrompt to generate SQL queries\nGenerate SQL queries with Gemini Cloud Assist(Preview)\nComplete a SQL query(Preview)\nExplain a SQL query\nPython code assistGenerate Python code with the code generation toolGenerate Python code with Gemini Cloud Assist(Preview)Python code completionGenerate BigQuery DataFrames Python code(Preview)\nGenerate Python code with the code generation tool\nGenerate Python code with Gemini Cloud Assist(Preview)\nPython code completion\nGenerate BigQuery DataFrames Python code(Preview)\nPrepare data for analysis. Data preparation in BigQuery gives you context\n    aware, AI-generated transformation recommendations to cleanse data for analysis. For more\n    information, seePrepare data with Gemini.\nCustomize your SQL translations with translation rules. (Preview)\n    Create Gemini-enhanced translation rules to customize your SQL translations when\n    using theinteractive SQL translator.\n    You can describe changes to the SQL translation output using natural language prompts or specify\n    SQL patterns to find and replace. For more information, seeCreate a translation\n    rule.\nGemini for Google Cloud doesn't use your prompts or its\nresponses as data to train its models without your express permission. For more\ninformation about how Google uses your data, seeHow Gemini for Google Cloud uses your data.\nLearn how and when Gemini\n     for Google Cloud uses your data.\n  As an early-stage technology, Gemini for Google Cloud products can\n  generate output that seems plausible but is factually incorrect. We recommend that you validate\n  all output from Gemini for Google Cloud products before you use it. For more\n  information, seeGemini for Google Cloud and responsible AI.\nSeeGemini for Google Cloud pricing.\nFor quotas and limits that apply to Gemini in BigQuery,\nseeGemini for Google Cloud quotas and limits.\nAfter youset up Gemini in BigQuery,\nyou can use Gemini in BigQuery to do the following\nin BigQuery Studio:\nTogenerate data insights,\ngo to theInsightstab for a table entry,\nwhere you can identify patterns, assess quality, and run statistical\nanalysis across your BigQuery data.\nTo use data canvas,create a data canvas or use data canvasfrom a table or query to explore data assets with natural language and\nshare your canvases.\nTo use natural language to generate SQL or Python code, or receive\nsuggestions with autocomplete while typing,\nuse theSQL generation toolfor yourSQL queriesorPython code.\nGemini in BigQuery can also\nexplain your SQL code in natural language.\nTo prepare data for analysis, in theCreate newlist, selectData preparation. For more information, seeOpen the data preparation editor in BigQuery.\nFor detailed setup steps, seeSet up Gemini in BigQuery.\nIn order to provide accurate results, Gemini in\nBigQuery requires access to both your Customer Data and metadata\nin BigQuery for enhanced features. Enabling Gemini\nin BigQuery grants Gemini permission to access\nthis data, which includes your tables and query history. Gemini\nin BigQuery doesn't use your data to train or fine-tune its\nmodels. For more information on how Gemini uses your data, seehow Gemini for Google Cloud uses your data.\nEnhanced features in Gemini in BigQuery are the following:\nSQL generation tool\nPrompt to generate SQL queries\nComplete a SQL query\nExplain a SQL query\nGenerate python code\nPython code completion\nData canvas\nData preparation\nData insights\nFor information about where Gemini processes your data, seeGemini serving locations.\nSee the latest enhancements and fixes inrelease notes.\nLearn how toset up Gemini in BigQuery.\nLearn how towrite queries with Gemini assistance.\nLearn more aboutGoogle Cloud compliance.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-14 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/predictions/overview",
    "title": "Overview of getting predictions on Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nA prediction is the output of a trained machine learning model. This page\nprovides an overview of the workflow for getting predictions from your models on\nVertex AI.\nVertex AI offers two methods for getting prediction:\nOnline predictionsare synchronous requests made to\n    a model that is deployed to anEndpoint.\n    Therefore, before sending a request, you must first deploy theModelresource to an endpoint. This associatescompute resourceswith\n    the model so that the model can serve online predictions with\n    low latency. Use online predictions when you are making requests in\n    response to application input or in situations that require timely\n    inference.\nBatch\n    predictionsare asynchronous requests made to a model\n    that isn't deployed to an endpoint. You send the request (as aBatchPredictionJobresource) directly to theModelresource. Use\n    batch predictions when you don't require an immediate response and\n    want to process accumulated data by using a single request.\nTo get predictions, you must firstimport your\nmodel. After it's imported, it becomes aModelresource that is visible inVertex AI Model Registry.\nThen, read the following documentation to learn how to get predictions:\nGet batch predictionsOr\nGet batch predictions\nOr\nDeploy model to endpointandget online predictions.\nDeploy model to endpointandget online predictions.\nUnlike custom trained models, AutoML models are automatically imported into the\nVertex AI Model Registry after training.\nOther than that, the workflow for AutoML models is similar, but varies slightly\nbased on your data type and model objective. The documentation for getting\nAutoML predictions is located alongside the other AutoML documentation. Here are links\nto the documentation:\nLearn how to get predictions from the following types of image AutoML models:\nImage classification models\nImage object detection models\nLearn how to get predictions from the following types of tabular AutoML models:\nTabular classification and regression modelsOnline predictionsBatch predictions\nTabular classification and regression models\nOnline predictions\nBatch predictions\nTabular forecasting models(batch predictions only)\nTabular forecasting models(batch predictions only)\nLearn how to get predictions from the following types of text AutoML models:\nText classification models\nText entity extraction models\nText sentiment analysis models\nLearn how to get predictions from the following types of video AutoML models:\nVideo action recognition models(batch predictions only)\nVideo classification models(batch predictions only)\nVideo object tracking models(batch predictions only)\nThere are two ways to get predictions from BigQuery ML models:\nYou can request batch predictions directly from the model in\nBigQuery ML.\nYou can register the models directly with the\nModel Registry, without exporting them from\nBigQuery ML or importing them into the\nModel Registry.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction",
    "title": "Introduction to Vertex AI TensorBoardStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI TensorBoard is an enterprise-ready managed\nversion ofOpen Source TensorBoard(TB), which is a Google Open Source project for machine learning experiment\nvisualization.\nWith Vertex AI TensorBoard, you can track, visualize, and compare\nML experiments and share them with your team.\nVertex AI TensorBoard provides various detailed visualizations, that\nincludes:\ntracking and visualizing metrics such as loss and accuracy over time,\nvisualizing model computational graphs (ops and layers),\nviewing histograms of weights, biases, or other tensors as they change over\ntime,\nprojecting embeddings to a lower dimensional space,\nand displaying image, text, and audio samples.\nIn addition to the powerful visualizations from\nTensorBoard, Vertex AI TensorBoard provides:\na persistent, shareable link to your experiment's,\nVertex AI TensorBoard experiment,\ntight integrations with Vertex AI services for model training,\nenterprise-grade security, privacy, and compliance.\nIntegration with Vertex AI Experiments lets you:\nuse a searchable and compare list of all experiments in a project,\nview time series metrics in the Google Cloud console,\ncompare scalars across experiments and experiment runs,\nhave direct access to Vertex AI TensorBoard.\nThe Google Cloud console is used to:\ncreate or delete Vertex AI TensorBoard instances,\ncreateVertex AI Experiments,\nview Vertex AI TensorBoard instance storage size → associated costs,\nview associatedVertex AI Experiments, Custom Jobs, and\nPipeline Runs,\nvisualize sometime series metrics.\nSetup Vertex AI TensorBoard\nCheck out theTensorBoard API documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/application-development",
    "title": "Application development",
    "content": "Home\nDocumentation\nCreate applications with a comprehensive set of tools and services.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nDesign, build, productize, and manage APIs for your internal and external customers.\nAutomate code deployment using CI/CD processes and products that assist in efficient, seamless, and secure deployment of code.\nWrite, deploy, and debug your applications faster with powerful developer tools.\nOrchestrate decoupled services and build message-based and event-driven solutions.\nExpand this section to see relevant products and documentation.\nAutomate your business workflows with integrations that connect to enterprise applications, databases, and much more.\nTrack modifications to source code.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGemini for Google Cloud\nDocumentation\nGuides\nPreview\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA products and features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nGemini in Looker is a product in theGemini for Google Cloudportfolio that provides generative AI-powered assistance to help you analyze and gain valuable insights from your data. Gemini in Looker can provide assistance for tasks in Looker (original) instances, Looker (Google Cloud core) instances, and in Looker Studio. For more information about available features, see theGemini in Looker overviewdocumentation page.\nLearnhow and when Gemini\n     for Google Cloud uses your data.\nWhen Gemini in Looker is enabled,\nLooker users can perform the following tasks in a\nLooker (Google Cloud core) instance or a Looker (original) instance:\nAsk questions about and converse with your data using Conversational Analytics. Gemini in Looker lets you ask questions about your data source by using natural language. Gemini returns Looker Studio charts or data tables that are based on your query. You can learn more about how your response was generated and save your conversation for future reference.\nGenerate custom Looker visualizationsGemini in Looker lets you customize formatting\noptions for Looker visualizations by using natural language. Gemini generates JSON formatting options from text-based prompts, which you can apply to your visualization. You can also use prompts as a starting point for creating templates and patterns for more complex customizations and thenmanually update the visualization formatting options.\nGenerate LookML. Gemini in Looker assists you in generating LookML parameters. Gemini suggests LookML parameters based on a natural\nlanguage prompt, which you can add to your project files.\nTo access these features in a Looker (original) instance, a Looker admin mustenable Gemini in Lookerin the Looker (original) instance settings. The instance must be on Looker 25.2 or later and be Looker hosted. Conversational Analytics is available on Looker instances on 25.0 or later.\nTo access these features in a Looker (Google Cloud core) instance, a user with theLooker Admin(roles/looker.admin) IAM role mustenable Gemini in Lookerin the Looker (Google Cloud core) instance settings in the Google Cloud console.\nTo use any of the aforementioned Gemini in Looker features in a Looker instance, users must be granted a Looker role that contains thegemini_in_lookerpermissionfor the models that they're applying Gemini assistance to. This permission is available as part of the defaultGemini role.\nThe following Gemini in Looker features require additional permissions:\nTocreate custom visualizations with Gemini assistance, you must be assigned a Looker role that contains thecan_override_vis_configpermission.\nTowrite LookML with Gemini assistance, you must be assigned a Looker role that contains thedeveloppermission for at least one model in a LookML project.\nTo query data or create a data agent withConversational Analytics, you must be assigned a Looker role that contains theaccess_datapermission for the model that you are querying.\nWhen Gemini in Looker is enabled for\nLooker Studio, Looker Studio users can perform the\nfollowing tasks in Looker Studio:\nAsk questions about and converse with your data using Conversational Analytics. Gemini in Looker lets you ask questions about your data source by using natural language. Gemini returns Looker Studio charts or data tables that are based on your query. You can learn more about how your response was generated and save your conversation for future reference.\nCreate calculated fields by using natural language. Gemini in Looker lets you createcalculated fieldsin Looker Studio by prompting you to describe the kinds of fields that you'd like to create. Based on your input, Gemini suggests a formula for a calculated field by using fields from your data source along with Looker Studio functions and operators.\nAdd Looker Studio content to your Slides presentation. Gemini in Looker lets you importcomponentsfrom your Looker Studio Pro reports into your Slides presentations. Gemini inserts report charts as images, generates a textual summary of each image, and inserts the summary as a text element. You can generate a new Slides presentation by using all or selected visualizations in a Looker Studio report, or you can add or update Looker Studio content to an existing Slides presentation. You can also update the Looker Studio data that has been imported in a Slides presentation.\nTo access these features, a user with the appropriate IAM or Google Workspace role mustenable Gemini in Lookerin Looker Studio.\nTo use any of the aforementioned Gemini in Looker features in Looker Studio, users must be granted the following roles or privileges:\nTo create calculated fields, users must be assigned anEditorrole.\nTo add Looker Studio content to your Slides presentation, users must be assigned a Viewer or Editor role in Looker Studio and have theEditor or Ownerpermission level for the Slides presentation.\nTo useConversational Analytics with a Looker data sourcein Looker Studio users must be granted thegemini_in_lookerpermissionfor the Looker models that they're applying Gemini assistance to.\nLooker users can also interact with the Gemini in Looker features that appear in Looker Studioas part of a Looker Studio Pro subscription.\nIf your Looker admin has accepted the complimentary Looker Studio Pro licenses for yourLooker (Google Cloud core) instanceorLooker (original) instance, as a Looker user, you can access the Gemini in Looker features that appear in Looker Studio when Gemini in Looker is enabled for your Looker Studio Pro subscription.\nAfter you enable Gemini in Looker for the assistants that appear in your Looker product, you can seek Gemini assistance in the places that are described in the following sections.\nTo access Conversational Analytics, follow these steps:\nNavigate to themain navigation menu.\nSelectchat_sparkConversations.\nTo access Conversational Analytics, follow these steps:\nNavigate to the Looker instance homepage.\nSelect theCreatemenu.\nSelectchat_sparkConversation.\nTo access Conversational Analytics, follow these steps:\nNavigate to theLooker Explorethat contains the data you would like to chat with.\nSelectStart a conversation.\nTheVisualization Assistantis available for visualizations that use the\nHighCharts API, which includes mostCartesian charts,\nsuch as thecolumn chart,bar chart,\nandline chart.\nTo access theVisualization Assistant, follow these steps:\nView a supported visualization in an Explore, or edit a visualization in a Look or dashboard.\nOpen theEditmenu in the visualization.\nClickVisualization Assistantto open the prompt menu.\nFor more information, seeCreate visualizations with Gemini assistance.\nTo use Gemini to create LookML in your\nLooker project, follow these steps:\nOn your Looker instance, enableDevelopment Mode.\nOpen your project in the Looker IDE.\nUse the IDEfile browserto open a LookML view file in which you want to insert LookML.\nSelect theHelp Me Codeicon from the side panel selector.\nFor more information, seeWrite LookML with Gemini assistance.\nTo use Gemini assistance to query your data in natural language,\nselectConversational Analyticsfrom the left navigation in\nLooker Studio.\nTo create a custom data agent, selectConversational Analyticsfrom the left\nnavigation in Looker Studio, and then selectManage agents.\nIf you want to query data within yourpersonal sandbox,\n follow these steps:\nFrom the left navigation, select theSandboxproject.\nClickCreate.\nSelectConversation.\nFor more information about querying your data in natural language, seeConversational Analytics: Query your data in natural language with Gemini assistance. For more information about creating and managing a custom data agent, seeConversational Analytics: Data Agents.\nTo use Gemini assistance to write formulas for calculated fields,\nfollow these steps from a Looker Studio report:\nEdit the data source.\nClickAdd a field.\nSelectAdd calculated field.\nClick theHelp me writeicon.\nFor more information, seeCreate calculated fields with Gemini assistance.\nTo use Gemini assistance to create a Slides\npresentation that includes all or selected visualizations in a\nLooker Studio report, follow these steps:\nOpen a Looker Studio report in either view or edit mode.\nSelect the Gemini panel in the panel manager.\nSelectGenerate Slides.\nFor more information, seeAdding Looker Studio content to your Slides presentation with Gemini assistance.\nTo use Gemini assistance to add Looker Studio\ncontent to an existing Slides presentation, follow these steps:\nOpen a Slides presentation.\nClick the Looker Studio icon on the right-hand toolbar to open the Looker Studio Pro panel.\nFor more information, seeAdding Looker Studio content to your Slides presentation with Gemini assistance.\nSee the latest enhancements and fixes inrelease notes.\nAssign the Gemini role to Gemini in Looker users\nLearn howGemini for Google Cloud uses your data.\nLearn more aboutGoogle Cloud compliance.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/spanner/docs/graph",
    "title": "Spanner Graph documentation",
    "content": "Documentation\nSpanner Graph unites purpose-built graph database capabilities withSpanner, which offers industry-leading scalability,\n  availability, and consistency.\nSpanner Graph supports a graph query interface compatible with the ISO GQL\n  (Graph Query Language) standards. It also supports interoperability between relational and graph\n  models and combines the well-established SQL capabilities with the expressiveness of graph pattern\n  matching from GQL. To learn more, see theSpanner Graph overview.\nNot sure what database option is right for you? Learn more about ourdatabase services.\nSpanner Graph overview\nSpanner Graph overview\nSet up and query Spanner Graph\nSet up and query Spanner Graph\nSpanner Graph schema overview\nSpanner Graph schema overview\nInsert, update, or delete Spanner Graph data\nInsert, update, or delete Spanner Graph data\nSpanner Graph queries overview\nSpanner Graph queries overview\nMigrate to Spanner Graph\nMigrate to Spanner Graph\nSpanner Graph reference for openCypher users\nSpanner Graph reference for openCypher users\nTroubleshoot Spanner Graph\nTroubleshoot Spanner Graph\nGraph Query Language\nGraph Query Language\nSchema statements\nSchema statements\nQuery statements\nQuery statements\nGQL within SQL\nGQL within SQL\ngcloud command-line tool\ngcloud command-line tool\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nRelease notes\nRelease notes\nGet support\nGet support\nGet started with the Spanner Graph codelab\nIn this Codelab, learn how to set up a Spanner Graph database using a pre-populated dataset, query the graph using GQL, and access both graph and relational data together by combining GQL and SQL.\nSpanner Graph reference for openCypher users\nLearn the differences between Spanner Graph and openCypher.MigrationOpenCypherApache\nCreate database with a property graph\nCreate a Spanner database using a property graph.\nInsert graph data\nInsert data into a Spanner Graph database.\nQuery data in a graph\nQuery data in a Spanner Graph database.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nWith Vertex AI Neural Architecture Search, you can search for optimal\nneural architectures in terms of accuracy, latency, memory, a\ncombination of these, or a custom metric.\nVertex AI Neural Architecture Search is a high-end optimization tool used to find best\nneural architectures in terms of accuracy with or without\nconstraints such as latency, memory, or a custom metric. The\nsearch space of possible neural architecture choices\ncan be as large as 10^20. It is based on a technique, which has\nsuccessfully generated several state\nof the art computer vision models in the past years, includingNasnet,MNasnet,EfficientNet,NAS-FPN,\nandSpineNet.\nNeural Architecture Search isn't a solution where you can just bring your\ndata and expect a good result without experimentation. It is an\nexperimentation tool.\nNeural Architecture Search isn't for hyperparameter tuning\nsuch as for tuning the learning rate or optimizer settings. It is only meant\nfor an architecture search. You shouldn't combine\nhyper-parameter tuning with Neural Architecture Search.\nNeural Architecture Search is not recommended with limited training data or\nfor highly imbalanced datasets where some classes are very rare.\nIf you are already using heavy augmentations for your baseline\ntraining due to lack of data, then Neural Architecture Search\nis not recommended.\nYou should first try other traditional and conventional machine\nlearning methods and techniques such as hyperparameter tuning.\nYou should use Neural Architecture Search only if you don't see further\ngain with such traditional methods.\nYou should have an in-house team for model tuning, which has some basic idea\nabout architecture parameters to modify and try. These architecture\nparameters can include the kernel size, number of channels or connections among\nmany other possibilities. If you have a search space in mind to explore,\nthen Neural Architecture Search is highly valuable and can\nreduce at least approximately six months\nof engineering time in exploring a large search space: up to 10^20\narchitecture choices.\nNeural Architecture Search is meant for enterprise customers who can spend\nseveral thousand dollars on an experiment.\nNeural Architecture Search isn't limited to vision only use case.\nCurrently, only vision-based prebuilt search spaces and\nprebuilt trainers are provided, but\ncustomers can bring their own non-vision search spaces and trainers as well.\nNeural Architecture Search doesn't use asupernet(oneshot-NAS or weight-sharing based NAS) approach where you just\nbring your own data, and use it as a solution. It is non-trivial\n(months of effort) to customize a supernet. Unlike a supernet,\nNeural Architecture Search is highly customizable to define custom search spaces\nand rewards. The customization can be done in approximately one to two days.\nNeural Architecture Search is supported in 8 regions across the world.\nCheck theavailability in your region.\nYou should also read the following section on expected cost, result gains,\nand GPU quota requirements before using Neural Architecture Search.\nThe figure shows a typical Neural Architecture Search curve.\nTheY-axisshows the trial rewards, and theX-axisshows the number of trials launched.\nAs the number of trials increase, the controller starts finding better\nmodels. Therefore, the reward starts increasing, and later, the\nreward variance and the reward growth start decreasing\nand show the convergence. At the point of convergence, the number of trials\ncan vary based on the search-space size, but it is of the order of\napproximately 2000 trials.\nEach trial is designed to be a smaller version of\nfull training calledproxy-taskwhich runs for approximately one to two hours on two Nvidia\nV100 GPUs. The customer\ncan stop the search manually at any point and might find higher\nreward models compared to their baseline\nbefore the point of convergence occurs.\nIt might be better to wait until the point of convergence occurs\nto choose the better results.\nAfter the search, the next stage is to pick\ntop 10 trials (models) and run a full training on them.\nIn this mode, observe the search curve\nor a few trials, approximately 25, and do a test drive with a prebuilt\nMNasNet search space and trainer.\nIn the figure, the best stage-1 reward starts to climb up from ~0.30 at trial-1\nto ~0.37 at trial-17. Your exact run may look slightly different\ndue to sampling randomness but you should see some small increase in the best\nreward.Note that this is still a toy run\nand doesn't represent any proof-of-concept or a public\nbenchmark validation.\nThe cost for this run is detailed as follows:\nStage-1:Number of trials: 25Number of GPUs per trial: 2GPU type: TESLA_T4Number of CPUs per trial: 1CPU type: n1-highmem-16Avg single trial training time: 3 hoursNumber of parallel trials: 6GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 12 GPUs.\nUseus-central1 regionfor the test drive and host training data\nin the same region.No extra quota needed.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials) = 12 hoursGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 150 T4 GPU hoursCPU hours: (total-trials * training-time-per-trial * num-cpus-per-trial) = 75 n1-highmem-16 hoursCost: Approximately $185. You can stop the job earlier to reduce the cost.\nRefer to thepricing pageto calculate\nexact price.\nNumber of trials: 25\nNumber of GPUs per trial: 2\nGPU type: TESLA_T4\nNumber of CPUs per trial: 1\nCPU type: n1-highmem-16\nAvg single trial training time: 3 hours\nNumber of parallel trials: 6\nGPU quota used: (num-gpus-per-trial * num-parallel-trials) = 12 GPUs.\nUseus-central1 regionfor the test drive and host training data\nin the same region.No extra quota needed.\nTime to run: (total-trials * training-time-per-trial)/(num-parallel-trials) = 12 hours\nGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 150 T4 GPU hours\nCPU hours: (total-trials * training-time-per-trial * num-cpus-per-trial) = 75 n1-highmem-16 hours\nCost: Approximately $185. You can stop the job earlier to reduce the cost.\nRefer to thepricing pageto calculate\nexact price.\nBecause this is a toy run, there is no need to run a\nfull stage-2 training for models from stage-1. \nTo learn more about running stage-2, seetutorial 3.\nTheMnasNet notebookis used for this run.\nIn case you are interested in almost replicating a publishedMNasnetresult, you can use this mode. According to the paper, MnasNet achieves\n75.2% top-1 accuracy with 78 ms latency on a Pixel phone, which is 1.8x\nfaster than the MobileNetV2 with 0.5% higher accuracy and 2.3x\nfaster than NASNet with 1.2% higher accuracy. However, this example uses\nGPUs instead of TPUs for training and uses cloud-CPU (n1-highmem-8)\nto evaluate latency. With this example, the expected\nStage2 top-1 accuracy on MNasNet is 75.2% with 50ms\nlatency on cloud-CPU (n1-highmem-8).\nThe cost for this run is detailed as follows:\nStage-1 search:Number of trials: 2000Number of GPUs per trial: 2GPU type: TESLA_T4Avg single trial training time: 3 hoursNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Since this number is above the default quota,\ncreate a quota request from your project UI. \nFor more information, seesetting_up_path.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 25 days.\nNote: The job terminates after 14 days. After that time, you canresume the search jobeasily with one command for another 14 days. If you have higher GPU\nquota, then the runtime decreases proportionately.GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 12000 T4 GPU hours.Cost: ~$15,000\nStage-1 search:\nNumber of trials: 2000\nNumber of GPUs per trial: 2\nGPU type: TESLA_T4\nAvg single trial training time: 3 hours\nNumber of parallel trials: 10\nGPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Since this number is above the default quota,\ncreate a quota request from your project UI. \nFor more information, seesetting_up_path.\nTime to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 25 days.\nNote: The job terminates after 14 days. After that time, you canresume the search jobeasily with one command for another 14 days. If you have higher GPU\nquota, then the runtime decreases proportionately.\nGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 12000 T4 GPU hours.\nCost: ~$15,000\nStage-2 full-training with top 10 models:Number of trials: 10Number of GPUs per trial: 4GPU type: TESLA_T4Avg single trial training time: ~9 daysNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 40 T4 GPUs.Because this number is above the default quota,\ncreate a quota request from your project UI. \nFor more information, seesetting_up_path.You can also run this with 20 T4 GPUs by running the job twice\nwith five models at a time instead of all 10 in parallel.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = ~9 daysGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 8960 T4 GPU hours.Cost: ~$8,000\nStage-2 full-training with top 10 models:\nNumber of trials: 10\nNumber of GPUs per trial: 4\nGPU type: TESLA_T4\nAvg single trial training time: ~9 days\nNumber of parallel trials: 10\nGPU quota used: (num-gpus-per-trial * num-parallel-trials) = 40 T4 GPUs.Because this number is above the default quota,\ncreate a quota request from your project UI. \nFor more information, seesetting_up_path.You can also run this with 20 T4 GPUs by running the job twice\nwith five models at a time instead of all 10 in parallel.\nTime to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = ~9 days\nGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 8960 T4 GPU hours.\nCost: ~$8,000\nTotal cost: Approximately $23,000.  Refer to thepricing pageto calculate exact price. Note: This example isn't an average regular training\njob. The full training runs for\napproximately nine days on four TESLA_T4 GPUs.\nTheMnasNet notebookis used for this run.\nWe provide an approximate cost for an average custom user.\nYour needs can vary depending on your training task and GPUs\nand CPUs used. You need at least 20 GPUs quota for an end-to-end runas documented here.\nNote: The performance gain is completely dependent on your task.\nWe can only provide examples like MNasnet as\nreferenced examples for performance gain.\nThe cost for this hypothetical custom run is detailed as follows:\nStage-1 search:Number of trials: 2,000Number of GPUs per trial: 2GPU type: TESLA_T4Avg single trial training time: 1.5 hoursNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Because this number is above the default quota, you need to\ncreate a quota request from your project UI. For more information, seeRequest additional device quota for the project.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 12.5 daysGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 6000 T4 GPU hours.Cost: approximately $7,400\nStage-1 search:\nNumber of trials: 2,000\nNumber of GPUs per trial: 2\nGPU type: TESLA_T4\nAvg single trial training time: 1.5 hours\nNumber of parallel trials: 10\nGPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Because this number is above the default quota, you need to\ncreate a quota request from your project UI. For more information, seeRequest additional device quota for the project.\nTime to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 12.5 days\nGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 6000 T4 GPU hours.\nCost: approximately $7,400\nStage-2 full training with top 10 models:Number of trials: 10Number of GPUs per trial: 2GPU type: TESLA_T4Average single trial training time: approximately 4 daysNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.\n**Since this number is above the default quota, you need to\ncreate a quota request from your project UI. For more information,\nseeRequest additional device quota for the project. Refer\nto the same documentation for custom quota needs.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = approximately 4 daysGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 1920 T4 GPU hours.Cost: approximately $2,400\nStage-2 full training with top 10 models:\nNumber of trials: 10\nNumber of GPUs per trial: 2\nGPU type: TESLA_T4\nAverage single trial training time: approximately 4 days\nNumber of parallel trials: 10\nGPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.\n**Since this number is above the default quota, you need to\ncreate a quota request from your project UI. For more information,\nseeRequest additional device quota for the project. Refer\nto the same documentation for custom quota needs.\nTime to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = approximately 4 days\nGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 1920 T4 GPU hours.\nCost: approximately $2,400\nFor more information on proxy-task design cost, seeProxy task designThe cost is similar to training 12 models\n(stage-2 in the figure uses 10 models):GPU quota used: Same as stage-2 run in the figure.Cost: (12/10) * stage-2-cost-for-10-models = ~$2,880\nFor more information on proxy-task design cost, seeProxy task designThe cost is similar to training 12 models\n(stage-2 in the figure uses 10 models):\nGPU quota used: Same as stage-2 run in the figure.\nCost: (12/10) * stage-2-cost-for-10-models = ~$2,880\nTotal cost: approximately $12,680.  Refer to thepricing pageto calculate exact price.\nThese stage-1 search cost are for the search until the convergence\npoint is reached and for maximum\nperformance gain. However, don't wait until the search converges.\nYou can expect to see a smaller amount\nof performance gain with a smaller search cost by running stage-2 full training\nwith the best model so far if the search-reward\ncurve has started growing.\nFor example, for thesearch-plot shown earlier,\ndon't wait until the 2,000 trials for convergence are reached.\nYou might have found better\nmodels at 700 or 1,200 trials and can run stage-2 full training for those.\nYou can always stop the search earlier to reduce the cost. You might also\ndo stage-2 full training in parallel while the search is running, but make sure\nyou have GPU quota to support an extra parallel job.\nThe following table summarizes some data points with different use cases\nand associated performance and cost.\nNeural Architecture Search features are both flexible and easy to use. A\nnovice user can use prebuilt search spaces, prebuilt-trainer,\nand notebooks without any further setup to start exploring\nVertex AI Neural Architecture Search for their dataset.\nAt the same time, an expert user can use Neural Architecture Search\nwith their custom trainer,\ncustom search space, and custom inference device and even extend\narchitecture-search for non-vision use cases as well.\nNeural Architecture Search offers prebuilt trainers and search spaces to be\nrun on GPUs for the following use cases:\nTensorflow trainers with public dataset based results published in a notebookImage Object Detection with end to end (SpineNet) search spacesClassification with prebuilt backbone (MnasNet) search spacesLiDAR 3D Point Cloud Object Detection with prebuilt end to end search spacesLatency and memory constrained search for targeting devices\nImage Object Detection with end to end (SpineNet) search spaces\nClassification with prebuilt backbone (MnasNet) search spaces\nLiDAR 3D Point Cloud Object Detection with prebuilt end to end search spaces\nLatency and memory constrained search for targeting devices\nPyTorch trainers to be used only as a tutorial examplePyTorch 3D medical image segmentation search space examplePyTorch-based MNasNet classificationLatency and memory constrained search for targeting devices\nPyTorch 3D medical image segmentation search space example\nPyTorch-based MNasNet classification\nLatency and memory constrained search for targeting devices\nAdditional Tensorflow based prebuilt state-of-the-art search spaces with codeModel ScalingData Augmentation\nModel Scaling\nData Augmentation\nThe full set of features that Neural Architecture Search offers can be used easily\nfor customized architectures and use cases as well:\nA Neural Architecture Search language to define a custom search space over possible\nneural-architectures and integrate this search space with\ncustom trainer code.\nReady-to-use prebuilt state-of-the-art search spaces with code.\nReady-to-use prebuilt Trainer, with code, which runs on GPU.\nA Managed Service for architecture-search includingA Neural Architecture Search controller which samples the search space to find the best architecture.Prebuilt docker/libraries, with code, to calculate latency/FLOPs/Memory\non custom hardware.\nA Neural Architecture Search controller which samples the search space to find the best architecture.\nPrebuilt docker/libraries, with code, to calculate latency/FLOPs/Memory\non custom hardware.\nTutorials to teach NAS usage.\nA set of tools to design proxy-tasks.\nGuidance and example for efficient PyTorch training with Vertex AI.\nLibrary support for custom metrics reporting and analysis.\nGoogle Cloud console UI to monitor and manage jobs.\nEasy to use notebooks to kick-start the search.\nLibrary support for GPU/CPU resource usage management on per project or\nper job level of granularity.\nPython-based Nas-client to build dockers, launch NAS jobs, and resume a previous search job.\nGoogle Cloud console UI-based customer support.\nNeural Architecture Searchis a technique for automating the design ofneural networks. It\nhas successfully generated several state of the art computer vision models in\nthe past years, including:\nNasnet,\nMNasnet,\nEfficientNet,\nNAS-FPN,\nSpineNet\nThese resulting models are leading the way in all 3 key classes of computer\nvision problems: image classification, object detection, and segmentation.\nWith Neural Architecture Search, engineers can optimize models foraccuracy,latency, andmemoryin the same trial, reducing the time needed to deploy\nmodels. Neural Architecture Search explores many different types of models: thecontrollerproposes ML models, then trains and evaluates models and iterates\n1k+ times to find the best solutionswith latency and/or memory constraint on\ntargeting devices. The following figure shows the key components of the architecture\nsearch framework:\nModel: A neural architecture with operations and connections.\nSearch space: The space of possible models (operations and connections)\nthat can be designed and optimized.\nTrainer docker: User customizable trainer code to train and\nevaluate a model and compute accuracy of the model.\nInference device: A hardware device such as CPU/GPU on which the model\nlatency and memory usage is computed.\nReward: A combination of model metrics such as the accuracy,\nlatency, and memory used for ranking the models as better or worse.\nNeural Architecture Search Controller: The orchestrating algorithm that (a) samples the\nmodels from the search space, (b) receives the model-rewards, and\n(c) provides next set of model suggestions to evaluate to find the\nmost optimal models.\nNeural Architecture Search offers prebuilt trainer integrated with\nprebuilt search spaces which can be easily used with provided notebooks\nwithout any further setup.\nHowever, most users need to use their custom trainer, custom search spaces,\ncustom metrics (memory, latency, and training time, for examples), and custom reward\n(combination of things such as accuracy and latency).\nFor this, you need to:\nDefine a custom search space using the provided Neural Architecture Search language.\nIntegrate the search space definition into the trainer code.\nAdd custom metrics reporting to the trainer code.\nAdd custom reward to the trainer code.\nBuild training container and use it to start Neural Architecture Search jobs.\nThe following diagram illustrates this:\nAfter you set up the training container to use, the Neural Architecture Search service\nthen launches multiple training-containers in parallel on multiple\nGPU devices. You can control how many trials to use in parallel\nfor training and how many total trials to launch. Each\ntraining-container is provided a suggested architecture from the search space.\nThe training-container builds the suggested model, does train/eval,\nand then reports rewards back to the Neural Architecture Search service. As this process\nprogresses, the Neural Architecture Search service uses the reward feedback to find better and better\nmodel-architectures. After the search, you have access to the reported\nmetrics for further analysis.\nThe high level steps for performing an Neural Architecture Search experiment are as\nfollows:\nSetups and definitions:Identify the labeled dataset and specify the task type\n(detection or segmentation, for example).Customize trainer code:Use a prebuilt search space or define a custom search space using the Neural Architecture Search language.Integrate the search-space definition into the trainer code.Add custom metrics reporting to the trainer code.Add custom reward to the trainer code.Build a trainer container.Set up search trial parameters for partial training (proxy task). The\nsearch training should ideally finish fast (for example, 30-60 minutes)\nto partially train the models:Minimum epochs needed for sampled models to gather\nreward (the minimum epochsdon'tneed to ensure model convergence).Hyperparameters (for example, learning rate).\nSetups and definitions:\nIdentify the labeled dataset and specify the task type\n(detection or segmentation, for example).\nCustomize trainer code:Use a prebuilt search space or define a custom search space using the Neural Architecture Search language.Integrate the search-space definition into the trainer code.Add custom metrics reporting to the trainer code.Add custom reward to the trainer code.\nUse a prebuilt search space or define a custom search space using the Neural Architecture Search language.\nIntegrate the search-space definition into the trainer code.\nAdd custom metrics reporting to the trainer code.\nAdd custom reward to the trainer code.\nBuild a trainer container.\nSet up search trial parameters for partial training (proxy task). The\nsearch training should ideally finish fast (for example, 30-60 minutes)\nto partially train the models:Minimum epochs needed for sampled models to gather\nreward (the minimum epochsdon'tneed to ensure model convergence).Hyperparameters (for example, learning rate).\nMinimum epochs needed for sampled models to gather\nreward (the minimum epochsdon'tneed to ensure model convergence).\nHyperparameters (for example, learning rate).\nRun search locally to ensure the search space integrated container can run properly.\nRun search locally to ensure the search space integrated container can run properly.\nStart the Google Cloud search (stage-1) job with fivetest trialsand\nverify that the search trials meet the runtime and accuracy goals.\nStart the Google Cloud search (stage-1) job with fivetest trialsand\nverify that the search trials meet the runtime and accuracy goals.\nStart the Google Cloud search (stage-1) job with +1k trials.As part of the search, also set a regular interval to train\n(stage-2) top N models:Hyperparameters and algorithm for hyperparameter search.\nstage-2 normally uses the similar configuration as stage-1,\nbut with higher settings for certain parameters,\nsuch as training steps/epochs, and number of channels.Stop criteria (the number of epochs).\nStart the Google Cloud search (stage-1) job with +1k trials.\nAs part of the search, also set a regular interval to train\n(stage-2) top N models:Hyperparameters and algorithm for hyperparameter search.\nstage-2 normally uses the similar configuration as stage-1,\nbut with higher settings for certain parameters,\nsuch as training steps/epochs, and number of channels.Stop criteria (the number of epochs).\nAs part of the search, also set a regular interval to train\n(stage-2) top N models:\nHyperparameters and algorithm for hyperparameter search.\nstage-2 normally uses the similar configuration as stage-1,\nbut with higher settings for certain parameters,\nsuch as training steps/epochs, and number of channels.\nStop criteria (the number of epochs).\nAnalyze the reported metrics and/or visualize architectures for insights.\nAnalyze the reported metrics and/or visualize architectures for insights.\nAn architecture-search experiment can be followed up by a\nscaling-search experiment, followed up by an augmentation search experiment\nas well.\n(Required)Set up your environment\n(Required)Tutorials\n(Required only for PyTorch customers)PyTorch efficient training with cloud data\n(Required)Best practices and suggested workflow\n(Required)Proxy task design\n(Required only when using prebuilt trainers)How to use prebuilt search spaces and a prebuilt trainer\nUsing Machine Learning to Explore Neural Network Architecture\nMnasNet: Towards Automating the Design of Mobile Machine Learning Models\nEfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling\nNAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection\nSpineNet: Learning Scale-Permuted Backbone for Recognition and Localization\nRandAugment\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/workbench/user-managed",
    "title": "Vertex AI Workbench: User-managed notebooks\n documentation",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI Workbench\nVertex AI Workbench user-managed notebooks isdeprecated. On\n    April 14, 2025, support for\n    user-managed notebooks will end and the ability to create user-managed notebooks instances\n    will be removed. Existing instances will continue to function\n    but patches, updates, and upgrades won't be available. To continue using\n    Vertex AI Workbench, we recommend that youmigrate\n    your user-managed notebooks instances to Vertex AI Workbench instances.\nUser-managed notebooks instances offer an integrated and secure\n    JupyterLab environment for data scientists and machine learning developers\n    to experiment, develop, and deploy models into production. User-managed notebooks\n    instances come preinstalled with the latest data science\n    and machine learning frameworks.Learn more.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCreate a user-managed notebooks instance\nCreate a user-managed notebooks instance\nIntroduction to user-managed notebooks\nIntroduction to user-managed notebooks\nInstall dependencies\nInstall dependencies\nChange machine type and configure GPUs of a user-managed notebooks instance\nChange machine type and configure GPUs of a user-managed notebooks instance\nUse R with BigQuery\nUse R with BigQuery\nSave a notebook to GitHub\nSave a notebook to GitHub\nPricing\nPricing\nRelease notes\nRelease notes\nGet support\nGet support\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/dhm-cloud",
    "title": "Distributed, hybrid, and multicloud",
    "content": "Home\nDocumentation\nExtend your Google Cloud topology to the edge, on-premises, and other cloud platforms.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nExtend Google Cloud infrastructure and services to the edge and into your data centers.\nCreate and manage Kubernetes clusters from Google Cloud in both AWS and Azure cloud environments.\nSimplify managing multi-cluster deployments, including  clusters outside Google Cloud.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/migration",
    "title": "Migration",
    "content": "Home\nDocumentation\nUse tools and information to help you on your journey to Google Cloud.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nTools and information to guide your migration to Google Cloud.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke",
    "title": "Backup for GKE documentation",
    "content": "Home\nGoogle Kubernetes Engine (GKE)\nDocumentation\nDocumentation\nBackup for GKE lets you protect, manage, and restore your containerized\napplications and data for workloads running on Google Kubernetes Engine clusters.Learn more.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nInstall Backup for GKE\nInstall Backup for GKE\nUsing VPC service controls\nUsing VPC service controls\nDefine custom backup and restore logic\nDefine custom backup and restore logic\nPlan a set of backups\nPlan a set of backups\nBack up your workloads\nBack up your workloads\nPlan a set of restores\nPlan a set of restores\nModify resources during restoration\nModify resources during restoration\nRestore a backup\nRestore a backup\ngcloud CLI commands\ngcloud CLI commands\nREST API\nREST API\nIAM roles and permissions\nIAM roles and permissions\nPricing\nPricing\nQuotas and limits\nQuotas and limits\nRelease notes\nRelease notes\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCompute Engine\nDocumentation\nGuides\nIf your workloads need high performance, low latency, temporary storage, consider\nusing Local solid-state drive (Local SSD) disks when you create your compute\ninstance. Local SSD disks are always-encrypted temporary\nsolid-state storage for Compute Engine. To learn about the other disks\navailable in Compute Engine, seeChoose a disk type.\nLocal SSD disks are ideal when you need storage for any of\nthe following use cases:\nCaches or storage for transient data\nScratch processing space for high performance computing or data analytics\nTemporary data storage like for thetempdbsystem database for Microsoft\nSQL Server\nLocal SSD disks offer superior I/O operations per second (IOPS), and very low\nlatency compared to the persistent storage provided byGoogle Cloud HyperdiskandPersistent Disk. This low latency is\nbecause Local SSD disks are physically attached to the server that hosts your\ninstance. For this same reason, Local SSD disks canprovide only temporary storage.\nBecause Local SSD is suitable only for temporary storage, you must store data\nthat isn't temporary or ephemeral in nature on a Hyperdisk\nor Persistent Disk volume.\nTo use Local SSD disks with a compute instance,add Local SSD disks when you create the instance.\nYou can't add Local SSD disks to an instance after you create it.\nLocal SSD disks come in two types:\nTitanium SSD: Titanium SSD is a custom-designed local SSD disk\nthat usesTitanium I/O offload processingand offers\nenhanced SSD security, performance, and management. Titanium offers\nhigher storage IOPS, throughput, and lower latency than the previous\ngeneration of Local SSD. The following machine series offer local SSD storage\nusing Titanium SSD:Storage-optimizedZ3 machine seriesGeneral-purposeC4AandC4D(Preview) machine seriesTitanium SSD disks are directly attached to the compute instances\ninside their host server.\nTitanium SSD: Titanium SSD is a custom-designed local SSD disk\nthat usesTitanium I/O offload processingand offers\nenhanced SSD security, performance, and management. Titanium offers\nhigher storage IOPS, throughput, and lower latency than the previous\ngeneration of Local SSD. The following machine series offer local SSD storage\nusing Titanium SSD:\nStorage-optimizedZ3 machine series\nGeneral-purposeC4AandC4D(Preview) machine series\nTitanium SSD disks are directly attached to the compute instances\ninside their host server.\nLocal SSD: Local SSD is the original local SSD feature for Google Cloud.\nEach Local SSD disk attached to an instance provides 375 GiB of capacity.\nThese disks provide higher performance than Hyperdisk or\nPersistent Disk. You can use either the NVMe or SCSI interface to mount Local\nSSD disks.Local SSD disks are directly attached to the instances inside their host\nserver.\nLocal SSD: Local SSD is the original local SSD feature for Google Cloud.\nEach Local SSD disk attached to an instance provides 375 GiB of capacity.\nThese disks provide higher performance than Hyperdisk or\nPersistent Disk. You can use either the NVMe or SCSI interface to mount Local\nSSD disks.\nLocal SSD disks are directly attached to the instances inside their host\nserver.\nUnless Titanium SSD is specifically mentioned, the term\n\"Local SSD\" applies to both Local SSD and Titanium SSD when describing\nfeatures of local SSD disks.\nLocal SSD performance depends on several factors, including the number of\nattached Local SSD disks, the selected disk interface\n(NVMeorSCSI), and the instance's\nmachine type. The available performance increases as you attach more Local SSD\ndisks to your instance.\nThe following tables list the maximum IOPS and throughput for NVMe- and\nSCSI-attached Local SSD disks. The metrics are listed by the total capacity of\nLocal SSD disks attached to the instance.\nThe following table lists the performance limits for Titanium SSD disks on\nC4A,C4D(Preview), andZ3instances.\nThe following table lists the performance limits for Local SSD disks that are\nattached to instances using NVMe.\nThe following table lists the performance limits for Local SSD disks that are\nattached to instances using SCSI.\nTo reach the stated performance levels, you must configure your compute instance\nas follows:\nAttach the Local SSD disks with the NVMe interface. Disks attached\nwith the SCSI interface have lower performance.\nAttach the Local SSD disks with the NVMe interface. Disks attached\nwith the SCSI interface have lower performance.\nThe following machine types also require a minimum number of vCPUs to reach\nthese maximums:N2,N2D,\norA2machine types\nrequire at least 24 vCPUs.N1machine types\nrequire at least 32 vCPUs.\nThe following machine types also require a minimum number of vCPUs to reach\nthese maximums:\nN2,N2D,\norA2machine types\nrequire at least 24 vCPUs.\nN1machine types\nrequire at least 32 vCPUs.\nIf your instance uses a custom Linux image, the image must use version 4.14.68\nor later of the Linux kernel. If you use the public images provided by\nCompute Engine, you don't have to take any further action.\nIf your instance uses a custom Linux image, the image must use version 4.14.68\nor later of the Linux kernel. If you use the public images provided by\nCompute Engine, you don't have to take any further action.\nFor additional instance and disk configuration settings that can improve Local\nSSD performance, seeOptimizing Local SSD performance.\nFor more information about selecting a disk interface,\nseeChoose a disk interface.\nCompute Engine preserves the data on Local SSD disks in certain scenarios,\nand in other cases, Compute Engine does not guarantee Local SSD data\npersistence.\nThe following information describes these scenarios and applies to each Local\nSSD disk attached to an instance.\nData on Local SSD disks persist only through the following events:\nIf you reboot the guest operating system.\nIf you configure your instance forlive migrationand the instance goes through a host maintenance event.\nIf youopt to preserve the Local SSD datawhen you stop or\nsuspend the instance. This feature is inPreview.\nData on Local SSD disks might be lost if ahost erroroccurs on the instance and\nCompute Engine can't reconnect the instance to the Local SSD disk within a\nspecified time.\nYou can control how much time, if any, is spent attempting to recover\nthe data with the Local SSD recovery timeout. If Compute Engine can't\nreconnect to the disk before the timeout expires, the instance is restarted.\nWhen the instance restarts, the Local SSD data is unrecoverable.\nCompute Engine attaches a blank Local SSD disk to the restarted\ninstance.\nThe Local SSD recovery timeout is part of an instance's host maintenance policy.\nFor more information, seeLocal SSD recovery timeout.\nData on Local SSD disks does not persist through the following events:\nIf you shut down the guest operating system and force the instance to\nstop.\nIf you create aSpot VMorpreemptible VMand the VM goes\nthrough the preemption process.\nIf you configure the instance tostop on host maintenance eventsand the instance goes through a host maintenance event.\nIf you misconfigure the Local SSD so that it becomes unreachable.\nIf you disable project billing, causing the instance to stop.\nIf Compute Engine was unable to recover an instance's Local SSD data,\nCompute Engine restarts the instance with a mounted and attached Local SSD\ndisk for each previously attached Local SSD disk.\nYou can use Local SSD disks with the following machine series.\nC4AC4C4D(Preview)C3C3DN4N2N2DN1T2DT2AE2Z3H3C2C2DX4M4M3M2M1N1+GPUA4A3 (H200)A3 (H100)A2G2\nSelect a machine series to display its support for Local SSD.\nHowever, there are constraints around how many Local SSD disks you can\nattach based on each machine type. For more information, seeChoose a valid number of Local SSD disks.\nLocal SSD has the following limitations:\nYou can't use Local SSD disks with VMs withshared-coremachine\ntypes.\nYou can't use Local SSD disks with VMs withshared-coremachine\ntypes.\nYou can't attach Local SSD disks to instances that use N4, H3, M4, M2, E2,\nTau T2A, or Tau T2D machine types.\nYou can't attach Local SSD disks to instances that use N4, H3, M4, M2, E2,\nTau T2A, or Tau T2D machine types.\nYou can't use customer-supplied encryption keys or customer-managed encryption\nkeys with Local SSD disks. Compute Engine automatically encrypts your data\nwhen it's written to Local SSD storage.\nYou can't use customer-supplied encryption keys or customer-managed encryption\nkeys with Local SSD disks. Compute Engine automatically encrypts your data\nwhen it's written to Local SSD storage.\nYou can't back up Local SSD disks with snapshots, clones, machine images, or\nimages. Store important data on Hyperdisk or Persistent Disk volumes.\nYou can't back up Local SSD disks with snapshots, clones, machine images, or\nimages. Store important data on Hyperdisk or Persistent Disk volumes.\nCompute Engine automatically encrypts your data when it is written to\nLocal SSD disks. You can't usecustomer-supplied encryption keyswith Local SSD disks.\nSince you can't back up Local SSD data with disk images, standard snapshots, or\ndisk clones, Google recommends that you always store valuable data on adurable storage option.\nIf you need to preserve the data on a Local SSD disk, attach a Persistent Disk or\nGoogle Cloud Hyperdisk to the instance. After you mount the Persistent Disk or\nHyperdisk copy the data from the Local SSD disk to the newly\nattached disk.\nTo achieve the highest Local SSD performance, you must attach your disks to the\ninstance with the NVMe interface. Performance is lower if you use the SCSI\ninterface.\nThe disk interface you choose also depends on the machine type and OS that your\ninstance uses. Some of the available machine types in Compute Engine allow\nyou to choose between NVMe and SCSI interfaces, while others support either\nonly NVMe or only SCSI. Similarly, some of the public OS images provided by\nCompute Engine might support both NVMe and SCSI, or only one of the two.\nThe following pages provide more information about available machine types and\nsupported public images, as well as performance details.\nSupported interfaces by machine types: SeeMachine series comparison.\nIn theChoose VM properties to comparelist, selectDisk interface type.\nSupported interfaces by machine types: SeeMachine series comparison.\nIn theChoose VM properties to comparelist, selectDisk interface type.\nOS image: For a list of which public OS images provided by Compute Engine\nsupport SCSI or NVMe, see theInterfacestab for each table\nin the operating system details documentation.\nOS image: For a list of which public OS images provided by Compute Engine\nsupport SCSI or NVMe, see theInterfacestab for each table\nin the operating system details documentation.\nIf your instance uses a custom Linux image,\nyou must use version 4.14.68 or later of the Linux kernel for optimal NVMe\nperformance.\nIf you have an existing setup that requires using a SCSI interface, consider\nusing multi-queue SCSI to achieve better performance over the standard SCSI\ninterface.\nIf you are using a custom image that you imported, seeEnable multi-queue SCSI.\nMost machine types available on Compute Engine support Local SSD disks.\nSome machine types always include a fixed number of Local SSD disks by default,\nwhile others allow you to add specific numbers of disks. You can only add Local\nSSD disks when you create the instance. You can't add Local SSD disks to an\ninstance after you create it.\nFor instances created using a storage-optimized Z3 machine type, each attached\nTitanium SSD disk has 3,000 GiB of capacity. For all other machine\nseries, each Local SSD disk that you attach has 375 GiB of capacity.\nThe following table lists the machine types that come with Local SSD disks by\ndefault. The table also shows the number of these disks that are attached when\nyou create the instance.\nThe machine types listed in the following table don't attach Local SSD disks to\na newly created instance unless you specify how many disks to attach. Because\nyou can add Local SSD disks only during instance creation, use the information\nin this section to determine how many Local SSD disks to attach when you create\nan instance.\nFor each Local SSD disk you create, you are billed for the total capacity of\nthe disk for the lifetime of the instance that it is attached to.\nFor detailed information about Local SSD pricing and available discounts,\nseeLocal SSD pricing.\nIf you start aSpot VMor preemptible VM\nwith a Local SSD disk, Compute Engine charges discountedspot pricesfor the Local SSD usage. Local SSD disks that are attached to Spot VMs or\npreemptible VMs work like normal Local SSD disks, retain the samedata persistence characteristics,\nand remain attached for the life of the VM.\nCompute Engine doesn't charge you for Local SSD disk usage on a\nSpot VM or preemptible VM if the VM is preempted within a minute after it\nstarts running.\nTo reserve Local SSD resources in a specific zone, seeChoose a reservation type.\nTo receive committed use discounts for Local SSD disks in a specific zone, you\nmust purchase resource-based commitments for the Local SSD resources and also\nattach reservations that specify matching Local SSD resources to your\ncommitments. For more information, seeAttach reservations to resource-based commitments.\nTo use a Local SSD disk with a compute instance, you must complete the\nfollowing steps:\nAdd Local SSD disks when you create an instance.\nFormat and mount Local SSD disksthat you added to your instance.\nThe Linux device names for the disks attached to your instance depend on the\ninterface that you choose when creating the disks. When you use thelsblkoperating system command to view your disk devices, it displays the prefixnvmefor disks attached with the NVMe interface, and the prefixsdfor\ndisks attached with the SCSI interface.\nThe ordering of the disk numbers or NVMe controllers is not predictable or\nconsistent across instance restarts. On the first boot, a disk might benvme0n1(orsdafor SCSI). On the second boot, the device name for the same\ndisk might benvme2n1ornvme0n3(orsdcfor SCSI).\nWhen accessing attached disks, you should use the symbolic links created in/dev/disk/by-id/instead. These names persist across reboots.\nFor more information about symlinks, seeSymbolic links for disks attached to an instance.\nFor more information about device names, seeDevice naming on Linux instances.\nWhen youstoporsuspend a VM,\nCompute Engine discards the data of any\nLocal SSD disks attached to the VM by default. When you resume the VM,\nall Local SSD disks attached to the VM are blank.\nPreview\n      \n        — Preserving Local SSD data\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nWhen you stop or suspend a VM, you can optionally preserve the data on the Local\nSSD disks attached to the VM.\nWhen the stop or suspend operation starts, Compute Engine\nperforms a managed migration of the Local SSD disk data to durable storage.\nWhen you resume or restart the VM, Compute Engine copies the preserved data\nto Local SSD disks attached to the VM. After you resume or restart the VM,\nyou might have toremount the Local SSD disk into the file system.\nYou're billed for the storage space used to preserve the Local SSD data until\nyou restart or resume the VM. The used storage space consumes your project'sPersistent disk standard GBquota.\nPreserving Local SSD data is inPreviewonly and is not covered under the GA terms for Compute Engine.\nYou can't use this feature with Z3 instances.\nYou can't preserve the Local SSD data if you stop or suspend a VM that\nhas more than 32 Local SSD disks attached.\nYou can't preserve Local SSD data if you stop or suspend a VM from the\nGoogle Cloud console.\nSaving the Local SSD data is a slow process and begins only after the\nsuspend or stop operation starts.\nIf you're using Spot VMs or preemptible VMs and you opt to preserve\nLocal SSD data during a suspend or stop operation,\nthen theLocal SSD data is lostif Compute Engine preempts the VM\nduring the stop or suspend operation.\nTo learn how to preserve Local SSD data when you stop or suspend a VM, seeStop an instance with Local SSD disksandSuspend an instance with Local SSD disks,\nrespectively.\nTo remove or delete Local SSD disks, you must delete the VM the disks are attached\nto. You can't delete Local SSD disks unless you delete the VM.\nBefore youdelete a VMthat has Local SSD disks attached, make sure that you migrate any critical data\non the Local SSD disks to a Persistent Disk, Hyperdisk, or to\nanother VM. Otherwise, the data on the Local SSD disks is\npermanently lost.\nLearn how toCreate a VM with Local SSD disks.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nHex-LLM, a high-efficiency large language model (LLM) serving with XLA, is the\nVertex AI LLM serving framework that's designed and optimized forCloud TPUhardware. Hex-LLM combines LLM serving technologies such as\ncontinuous batching andPagedAttentionwith\nVertex AI optimizations that are tailored forXLAand Cloud TPU. It's a high-efficiency\nand low-cost LLM serving on Cloud TPU for open source models.\nHex-LLM is available inModel Gardenthrough model\nplayground, one-click deployment, and notebook.\nHex-LLM is based on open source projects with Google's own optimizations for XLA\nand Cloud TPU. Hex-LLM achieves high throughput and low latency when serving\nfrequently used LLMs.\nHex-LLM includes the following optimizations:\nToken-based continuous batching algorithm to help ensure models are fully\nutilizing the hardware with a large number of concurrent requests.\nA complete rewrite of the attention kernels that are optimized for XLA.\nFlexible and composable data parallelism and tensor parallelism strategies\nwith highly optimized weight sharding methods to efficiently run LLMs on\nmultiple Cloud TPU chips.\nHex-LLM supports a wide range of dense and sparse LLMs:\nGemma 2B and 7B\nGemma 2 9B and 27B\nLlama 2 7B, 13B and 70B\nLlama 3 8B and 70B\nLlama 3.1 8B and 70B\nLlama 3.2 1B and 3B\nLlama Guard 3 1B and 8B\nMistral 7B\nMixtral 8x7B and 8x22B\nPhi-3 mini and medium\nQwen2 0.5B, 1.5B and 7B\nQwen2.5 0.5B, 1.5B, 7B, 14B and 32B AWQ\nHex-LLM also provides a variety of features, such as the following:\nHex-LLM is included in a single container. Hex-LLM packages the API server,\ninference engine, and supported models into a single Docker image to be\ndeployed.\nCompatible with theHugging Face modelsformat. Hex-LLM can load a Hugging Face model from local disk, the Hugging\nFace Hub, and a Cloud Storage bucket.\nQuantization usingbitsandbytesandAWQ.\nDynamicLoRAloading. Hex-LLM is able to\nload the LoRA weights through reading the request argument during serving.\nHex-LLM supports the following advanced features:\nMulti-host serving\nDisaggregated serving [experimental]\nPrefix caching\n4-bit quantization support\nHex-LLM now supports serving models with amulti-host TPU slice.\nThis feature lets you serve large models that can't be loaded\ninto a single host TPU VM, which contains at most eight v5e cores.\nTo enable this feature, set--num_hostsin the Hex-LLM container arguments and\nset--tpu_topologyin the Vertex AI SDK model upload request. The\nfollowing example shows how to deploy the Hex-LLM container with a TPU 4x4 v5e\ntopology that serves the Llama 3.1 70B bfloat16 model:\nFor an end-to-end tutorial for deploying the Hex-LLM container with a multi-host\nTPU topology, see theVertex AI Model Garden - Llama 3.1 (Deployment) notebook.\nIn general, the only changes needed to enable multi-host serving are:\nSet argument--tensor_parallel_sizeto the total number of cores within the\nTPU topology.\nSet argument--num_hoststo the number of hosts within the TPU topology.\nSet--tpu_topologywith the Vertex AI SDK model upload API.\nHex-LLM now supports disaggregated serving as an experimental feature. It can\nonly be enabled on the single host setup and the performance is under tuning.\nDisaggregated serving is an effective method for balancing Time to First Token\n(TTFT) and Time Per Output Token (TPOT) for each request, and the overall\nserving throughput. It separates the prefill phase and the decode phase into\ndifferent workloads so that they don't interfere with each other. This method\nis especially useful for scenarios that set strict latency requirements.\nTo enable this feature, set--disagg_topoin the Hex-LLM container arguments.\nThe following is an example that shows how to deploy the Hex-LLM container on\nTPU v5e-8 that serves the Llama 3.1 8B bfloat16 model:\nThe--disagg_topoargument accepts a string in the format\"number_of_prefill_workers,number_of_decode_workers\".\nIn the earlier example, it is set to\"3,1\"to configure three prefill workers\nand 1 decode worker. Each worker uses two TPU v5e cores.\nPrefix caching reduces Time to First Token (TTFT) for prompts that have\nidentical content at the beginning of the prompt, such as company-wide preambles,\ncommon system instructions, and multi-turn conversation history. Instead of\nprocessing the same input tokens repeatedly, Hex-LLM can retain a temporary\ncache of the processed input token computations to improve TTFT.\nTo enable this feature, set--enable_prefix_cache_hbmin the Hex-LLM container\narguments. The following is an example that shows how to deploy the Hex-LLM\ncontainer on TPU v5e-8 that serves the Llama 3.1 8B bfloat16 model:\nHex-LLM employs prefix caching to optimize performance for prompts exceeding a\ncertain length (512 tokens by default, configurable usingprefill_len_padding).\nCache hits occur in increments of this value, ensuring the cached token count is\nalways a multiple ofprefill_len_padding. Thecached_tokensfield ofusage.prompt_tokens_detailsin the chat completion API response indicates how\nmany of the prompt tokens were a cache hit.\nQuantization is a technique for reducing the computational and memory costs of\nrunning inference by representing the weights or activations with low-precision\ndata types like INT8 or INT4 instead of the usual BF16 or FP32.\nHex-LLM supports INT8 weight-only quantization. Extended support includes models\nwith INT4 weights quantized using AWQ zero-point quantization. Hex-LLM supports\nINT4 variants of Mistral, Mixtral and Llama model families.\nThere is no additional flag required for serving quantized models.\nThe Hex-LLM Cloud TPU serving container is integrated into\nModel Garden. You can access this serving technology through the\nplayground, one-click deployment, and Colab Enterprise notebook\nexamples for a variety of models.\nModel Garden playground is a pre-deployed Vertex AI\nendpoint that is reachable by sending requests in the model card.\nEnter a prompt and, optionally, include arguments for your request.\nEnter a prompt and, optionally, include arguments for your request.\nClickSUBMITto get the model response quickly.\nClickSUBMITto get the model response quickly.\nTry it out with\nGemma!\nYou can deploy a custom Vertex AI endpoint with Hex-LLM by using\na model card.\nNavigate to themodel card pageand clickDeploy.\nNavigate to themodel card pageand clickDeploy.\nFor the model variation that you want to use, select theCloud TPU\nv5e machine typefor deployment.\nFor the model variation that you want to use, select theCloud TPU\nv5e machine typefor deployment.\nClickDeployat the bottom to begin the deployment process. You receive\ntwo email notifications; one when the model is uploaded and another when the\nendpoint is ready.\nClickDeployat the bottom to begin the deployment process. You receive\ntwo email notifications; one when the model is uploaded and another when the\nendpoint is ready.\nFor flexibility and customization, you can use Colab Enterprise\nnotebook examples to deploy a Vertex AI endpoint with Hex-LLM by\nusing the Vertex AI SDK for Python.\nNavigate to the model card page and clickOpen notebook.\nNavigate to the model card page and clickOpen notebook.\nSelect the Vertex Serving notebook. The notebook is opened in\nColab Enterprise.\nSelect the Vertex Serving notebook. The notebook is opened in\nColab Enterprise.\nRun through the notebook to deploy a model by using Hex-LLM and send\nprediction requests to the endpoint. The code snippet for the deployment is\nas follows:\nRun through the notebook to deploy a model by using Hex-LLM and send\nprediction requests to the endpoint. The code snippet for the deployment is\nas follows:\nExample Colab Enterprise notebooks include:\nGemma 2 deployment\nCodeGemma deployment\nLlama 3.2 deployment\nLlama 3.1 deployment\nPhi-3 deployment\nQwen2 deployment\nYou can set the following arguments to launch the Hex-LLM server. You can tailor\nthe arguments to best fit your intended use case and requirements. Note that the\narguments are predefined for one-click deployment for enabling the easiest\ndeployment experience. To customize the arguments, you can build off of the\nnotebook examples for reference and set the arguments accordingly.\nModel\n--model: The model to load. You can specify a Hugging Face model ID, a\nCloud Storage bucket path (gs://my-bucket/my-model), or a local path.\nThe model artifacts are expected to follow the Hugging Face format and usesafetensorsfiles for\nthe model weights.BitsAndBytesint8 andAWQquantized model artifacts are supported for Llama, Gemma 2 and\nMistral/Mixtral.\n--tokenizer: Thetokenizerto load. This can be a Hugging Face model ID, aCloud Storagebucket path (gs://my-bucket/my-model), or a local path. If this argument\nis not set, it defaults to the value for--model.\n--tokenizer_mode: The tokenizer mode. Possible choices are[\"auto\", \"slow\"]. The default value is\"auto\". If this is set to\"auto\", the fast tokenizer is used if available. The slow tokenizers are\nwritten in Python and provided in the Transformers library, while the fast\ntokenizers offering performance improvement are written in Rust and provided\nin the Tokenizers library. For more information, see theHugging Face documentation.\n--trust_remote_code: Whether to allow remote code files defined in the\nHugging Face model repositories. The default value isFalse.\n--load_format: Format of model checkpoints to load. Possible choices are[\"auto\", \"dummy\"]. The default value is\"auto\". If this is set to\"auto\", the model weights are loaded in safetensors format. If this is set\nto\"dummy\", the model weights are randomly initialized. Setting this to\"dummy\"is useful for experimentation.\n--max_model_len: The maximum context length (input length plus the output\nlength) to serve for the model. The default value is read from the model\nconfiguration file in Hugging Face format:config.json. A larger maximum\ncontext length requires more TPU memory.\n--sliding_window: If set, this argument overrides the model's window size\nforsliding window attention. Setting\nthis argument to a larger value makes the attention mechanism include more\ntokens and approaches the effect of standard self attention. This argument\nis meant for experimental usage only. In general use cases, we recommend\nusing the model's original window size.\n--seed: The seed for initializing all random number generators. Changing\nthis argument might affect the generated output for the same prompt through\nchanging the tokens that are sampled as next tokens. The default value is0.\nInference engine\n--num_hosts: The number of hosts to run. The default value is1. For\nmore details, refer to the documentation onTPU v5e configuration.\n--disagg_topo: Defines the number of prefill workers and decode workers\nwith the experimental feature disaggregated serving. The default value isNone. The argument follows the format:\"number_of_prefill_workers,number_of_decode_workers\".\n--data_parallel_size: The number of data parallel replicas. The default\nvalue is1. Setting this toNfrom1approximately improves the\nthroughput byN, while maintaining the same latency.\n--tensor_parallel_size: The number of tensor parallel replicas. The\ndefault value is1. Increasing the number of tensor parallel replicas\ngenerally improves latency, because it speeds up matrix multiplication by\nreducing the matrix size.\n--worker_distributed_method: The distributed method to launch the worker.\nUsempfor themultiprocessingmodule orrayfor theRaylibrary. The default\nvalue ismp.\n--enable_jit: Whether to enableJIT (Just-in-Time Compilation)mode. The default value isTrue. Setting--no-enable_jitdisables it.\nEnabling JIT mode improves inference performance at the cost of requiring\nadditional time spent on initial compilation. In general, the inference\nperformance benefits overweigh the overhead.\n--warmup: Whether to warm up the server with sample requests during\ninitialization. The default value isTrue. Setting--no-warmupdisables\nit. Warmup is recommended, because initial requests trigger heavier\ncompilation and therefore will be slower.\n--max_prefill_seqs: The maximum number of sequences that can be scheduled\nfor prefilling per iteration. The default value is1. The larger this\nvalue is, the higher throughput the server can achieve, but with potential\nadverse effects on latency.\n--prefill_seqs_padding: The server pads the prefill batch size to a\nmultiple of this value. The default value is8. Increasing this value\nreduces model recompilation times, but increases wasted computation and\ninference overhead. The optimal setting depends on the request traffic.\n--prefill_len_padding: The server pads the sequence length to a multiple\nof this value. The default value is512. Increasing this value reduces\nmodel recompilation times, but increases wasted computation and inference\noverhead. The optimal setting depends on the data distribution of the\nrequests.\n--max_decode_seqs/--max_running_seqs: The maximum number of sequences\nthat can be scheduled for decoding per iteration. The default value is256.\nThe larger this value is, the higher throughput the server can achieve, but\nwith potential adverse effects on latency.\n--decode_seqs_padding: The server pads the decode batch size to a multiple\nof this value. The default value is8. Increasing this value reduces model\nrecompilation times, but increases wasted computation and inference overhead.\nThe optimal setting depends on the request traffic.\n--decode_blocks_padding: The server pads the number of memory blocks used\nfor a sequence's Key-Value cache (KV cache) to a multiple of this value\nduring decoding. The default value is128. Increasing this value reduces\nmodel recompilation times, but increases wasted computation and inference\noverhead. The optimal setting depends on the data distribution of the\nrequests.\n--enable_prefix_cache_hbm: Whether to enableprefix cachingin HBM. The default value isFalse. Setting this argument can improve\nperformance by reusing the computations of shared prefixes of prior requests.\nMemory management\n--hbm_utilization_factor: The percentage of freeCloud TPU High Bandwidth Memory (HBM)that can be allocated for KV cache after model weights are loaded. The\ndefault value is0.9. Setting this argument to a higher value increases\nthe KV cache size and can improve throughput, but it increases the risk of\nrunning out of Cloud TPU HBM during initialization and at runtime.\n--num_blocks: Number of device blocks to allocate for KV cache. If this\nargument is set, the server ignores--hbm_utilization_factor. If this\nargument is not set, the server profiles HBM usage and computes the number\nof device blocks to allocate based on--hbm_utilization_factor. Setting\nthis argument to a higher value increases the KV cache size and can improve\nthroughput, but it increases the risk of running out of Cloud TPU HBM during\ninitialization and at runtime.\n--block_size: Number of tokens stored in a block. Possible choices are[8, 16, 32, 2048, 8192]. The default value is32. Setting this argument\nto a larger value reduces overhead in block management, at the cost of more\nmemory waste. The exact performance impact needs to be determined\nempirically.\nDynamic LoRA\n--enable_lora: Whether to enable dynamicLoRA adaptersloading from Cloud Storage. The default value isFalse. This is\nsupported for the Llama model family.\n--max_lora_rank: The maximum LoRA rank supported for LoRA adapters defined\nin requests. The default value is16. Setting this argument to a higher\nvalue allows for greater flexibility in the LoRA adapters that can be used\nwith the server, but increases the amount of Cloud TPU HBM allocated for\nLoRA weights and decreases throughput.\n--enable_lora_cache: Whether to enable caching of dynamic LoRA adapters.\nThe default value isTrue. Setting--no-enable_lora_cachedisables it.\nCaching improves performance because it removes the need to re-download\npreviously used LoRA adapter files.\n--max_num_mem_cached_lora: The maximum number of LoRA adapters stored in\nTPU memory cache.The default value is16. Setting this argument to a\nlarger value improves the chance of a cache hit, but it increases the amount\nof Cloud TPU HBM usage.\nYou can also configure the server using the following environment variables:\nHEX_LLM_LOG_LEVEL: Controls the amount of logging information generated.\nThe default value isINFO. Set this to one of the standard Python logging\nlevels defined in thelogging module.\nHEX_LLM_VERBOSE_LOG: Whether to enable detailed logging output. Allowed\nvalues aretrueorfalse. Default value isfalse.\nThe server arguments are interrelated and have a collective effect on the\nserving performance. For example, a larger setting of--max_model_len=4096leads to higher TPU memory usage, and therefore requires larger memory\nallocation and less batching. In addition, some arguments are determined by the\nuse case, while others can be tuned. Here is a workflow for configuring the\nHex-LLM server.\nDetermine the model family and model variant of interest. For example, Llama\n3.1 8B Instruct.\nEstimate the lower bound of TPU memory needed based on the model size and\nprecision:model_size * (num_bits / 8). For an 8B model and bfloat16\nprecision, the lower bound of TPU memory needed would be8 * (16 / 8) = 16 GB.\nEstimate the number of TPU v5e chips needed, where each v5e chip offers 16GB:tpu_memory / 16. For an 8B model and bfloat16 precision, you need more\nthan 1 chip. Among the1-chip, 4-chip and 8-chip configurations,\nthe smallest configuration that offers more than 1 chip is the 4-chip\nconfiguration:ct5lp-hightpu-4t. You can subsequently set--tensor_parallel_size=4.\nDetermine the maximum context length (input length + output length) for the\nintended use case. For example, 4096. You can subsequently set--max_model_len=4096.\nTune the amount of free TPU memory allocated for KV cache to the maximum\nvalue achievable given the model, hardware and server configurations\n(--hbm_utilization_factor). Start with0.95. Deploy the Hex-LLM server\nand test the server with long prompts and high concurrency. If the server\nruns out-of-memory, reduce the utilization factor accordingly.\nA sample set of arguments for deploying Llama 3.1 8B Instruct is:\nA sample set of arguments for deploying Llama 3.1 70B Instruct AWQ onct5lp-hightpu-4tis:\nIn Model Garden, your default quota is 4 Cloud TPU v5e\nchips in theus-west1region. This quotas applies to one-click deployments and\nColab Enterprise notebook deployments. To request additional quotas,\nseeRequest a higher\nquota.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
    "title": "Google modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\n2.5 Flashpreview\nA preview of the latest version of our Flash line of models\nInput audio, images, video, and text, get text responses\nSee the model's thinking process as part of the response\nBalances price and performance\n2.0 Flashspark\nOur newest multimodal model, with next generation features and improved\n      capabilities\nInput audio, images, video, and text, get text responses\nGenerate code and images, extract data, analyze files, generate graphs, and more\nLow latency, enhanced performance, built to power agentic experiences\n2.0 Flash-Lite\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency\nInput audio, images, video, and text, get text responses\nOutperforms 1.5 Flash on the majority of benchmarks\nA 1 million token context window and multimodal input, like Flash 2.0\nAll the Gemini models can understand and respond in the\n      following languages:\nArabic (ar), Bengali (bn), Bulgarian (bg),\n      Chinese (Simplified and Traditional) (zh), Croatian (hr), Czech (cs),\n      Danish (da), Dutch (nl), English (en), Estonian (et), Finnish (fi),\n      French (fr), German (de), Greek (el), Hebrew (iw), Hindi (hi), Hungarian (hu),\n      Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Latvian (lv),\n      Lithuanian (lt), Norwegian (no), Polish (pl), Portuguese (pt), Romanian (ro),\n      Russian (ru), Serbian (sr), Slovak (sk), Slovenian (sl), Spanish (es),\n      Swahili (sw), Swedish (sv), Thai (th), Turkish (tr), Ukrainian (uk),\n      Vietnamese (vi)Gemini 2.0 Flash, Gemini 1.5 Pro and\n      Gemini 1.5 Flash models can understand and respond in the\n      following additional languages:Afrikaans (af), Amharic (am), Assamese (as), Azerbaijani (az),\n      Belarusian (be), Bosnian (bs), Catalan (ca), Cebuano (ceb), Corsican (co),\n      Welsh (cy), Dhivehi (dv), Esperanto (eo), Basque (eu), Persian (fa),\n      Filipino (Tagalog) (fil), Frisian (fy), Irish (ga), Scots Gaelic (gd),\n      Galician (gl), Gujarati (gu), Hausa (ha), Hawaiian (haw), Hmong (hmn),\n      Haitian Creole (ht), Armenian (hy), Igbo (ig), Icelandic (is), Javanese (jv),\n      Georgian (ka), Kazakh (kk), Khmer (km), Kannada (kn), Krio (kri),\n      Kurdish (ku), Kyrgyz (ky), Latin (la), Luxembourgish (lb), Lao (lo),\n      Malagasy (mg), Maori (mi), Macedonian (mk), Malayalam (ml), Mongolian (mn),\n      Meiteilon (Manipuri) (mni-Mtei), Marathi (mr), Malay (ms), Maltese (mt),\n      Myanmar (Burmese) (my), Nepali (ne), Nyanja (Chichewa) (ny),\n      Odia (Oriya) (or), Punjabi (pa), Pashto (ps), Sindhi (sd),\n      Sinhala (Sinhalese) (si), Samoan (sm), Shona (sn), Somali (so), Albanian (sq),\n      Sesotho (st), Sundanese (su), Tamil (ta), Telugu (te), Tajik (tg),\n      Uyghur (ug), Urdu (ur), Uzbek (uz), Xhosa (xh), Yiddish (yi), Yoruba (yo),\n      Zulu (zu)\nGemini 2.0 Flash, Gemini 1.5 Pro and\n      Gemini 1.5 Flash models can understand and respond in the\n      following additional languages:Afrikaans (af), Amharic (am), Assamese (as), Azerbaijani (az),\n      Belarusian (be), Bosnian (bs), Catalan (ca), Cebuano (ceb), Corsican (co),\n      Welsh (cy), Dhivehi (dv), Esperanto (eo), Basque (eu), Persian (fa),\n      Filipino (Tagalog) (fil), Frisian (fy), Irish (ga), Scots Gaelic (gd),\n      Galician (gl), Gujarati (gu), Hausa (ha), Hawaiian (haw), Hmong (hmn),\n      Haitian Creole (ht), Armenian (hy), Igbo (ig), Icelandic (is), Javanese (jv),\n      Georgian (ka), Kazakh (kk), Khmer (km), Kannada (kn), Krio (kri),\n      Kurdish (ku), Kyrgyz (ky), Latin (la), Luxembourgish (lb), Lao (lo),\n      Malagasy (mg), Maori (mi), Macedonian (mk), Malayalam (ml), Mongolian (mn),\n      Meiteilon (Manipuri) (mni-Mtei), Marathi (mr), Malay (ms), Maltese (mt),\n      Myanmar (Burmese) (my), Nepali (ne), Nyanja (Chichewa) (ny),\n      Odia (Oriya) (or), Punjabi (pa), Pashto (ps), Sindhi (sd),\n      Sinhala (Sinhalese) (si), Samoan (sm), Shona (sn), Somali (so), Albanian (sq),\n      Sesotho (st), Sundanese (su), Tamil (ta), Telugu (te), Tajik (tg),\n      Uyghur (ug), Urdu (ur), Uzbek (uz), Xhosa (xh), Yiddish (yi), Yoruba (yo),\n      Zulu (zu)\nGemini 2.0 Flash, Gemini 1.5 Pro and\n      Gemini 1.5 Flash models can understand and respond in the\n      following additional languages:\nAfrikaans (af), Amharic (am), Assamese (as), Azerbaijani (az),\n      Belarusian (be), Bosnian (bs), Catalan (ca), Cebuano (ceb), Corsican (co),\n      Welsh (cy), Dhivehi (dv), Esperanto (eo), Basque (eu), Persian (fa),\n      Filipino (Tagalog) (fil), Frisian (fy), Irish (ga), Scots Gaelic (gd),\n      Galician (gl), Gujarati (gu), Hausa (ha), Hawaiian (haw), Hmong (hmn),\n      Haitian Creole (ht), Armenian (hy), Igbo (ig), Icelandic (is), Javanese (jv),\n      Georgian (ka), Kazakh (kk), Khmer (km), Kannada (kn), Krio (kri),\n      Kurdish (ku), Kyrgyz (ky), Latin (la), Luxembourgish (lb), Lao (lo),\n      Malagasy (mg), Maori (mi), Macedonian (mk), Malayalam (ml), Mongolian (mn),\n      Meiteilon (Manipuri) (mni-Mtei), Marathi (mr), Malay (ms), Maltese (mt),\n      Myanmar (Burmese) (my), Nepali (ne), Nyanja (Chichewa) (ny),\n      Odia (Oriya) (or), Punjabi (pa), Pashto (ps), Sindhi (sd),\n      Sinhala (Sinhalese) (si), Samoan (sm), Shona (sn), Somali (so), Albanian (sq),\n      Sesotho (st), Sundanese (su), Tamil (ta), Telugu (te), Tajik (tg),\n      Uyghur (ug), Urdu (ur), Uzbek (uz), Xhosa (xh), Yiddish (yi), Yoruba (yo),\n      Zulu (zu)\nGemma supports only the English language.\nMultilingual text embedding models support the following languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque,\n    Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese,\n    Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino,\n    Finnish, French, Galician, Georgian, German, Greek, Gujarati,\n    Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian,\n    Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada,\n    Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian,\n    Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori,\n    Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish,\n    Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic,\n    Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho,\n    Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai,\n    Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian,\n    Xhosa, Yiddish, Yoruba, Zulu.\nImagen 3 supports the following languages:\nEnglish, Chinese, Hindi, Japanese, Korean, Portuguese, and Spanish.\nThe MedLM model supports the English language.\nModel Garden is a platform that helps you discover, test, customize,\nand deploy Google proprietary and select OSS models and assets. To explore\nthe generative AI models and APIs that are available on Vertex AI, go to\nModel Garden in the Google Cloud console.\nGo to Model Garden\nTo learn more about Model Garden, including available models and\ncapabilities, seeExplore AI models in Model Garden.\nTo see all model versions, including legacy and retired models, seeModel versions and lifecycle.\nTry a quickstart tutorial usingVertex AI Studioor\ntheVertex AI API.\nExplore pretrained models inModel Garden.\nLearn how to control access to specific models in Model Garden by\nusing aModel Garden organization\npolicy.\nLearn aboutpricing.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-21 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview",
    "title": "Imagen on Vertex AI | AI Image GeneratorStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nAPI reference overview: To view an overview of the API options for image generation and\nediting, see theimagegenerationmodel API reference.\nImagen on Vertex AI brings Google's state of the art image generative AI\ncapabilities to application developers. With Imagen on Vertex AI,\napplication developers can build next-generation AI products that transform\ntheir user's imagination into high quality visual assets using AI generation,\nin seconds.\nTry image generation (Vertex AI Studio)\nTry Imagen in a Colab\nWith Imagen, you can do the following:\nGenerate novel images using only a text prompt (text-to-image AI\ngeneration).\nEdit or expand an uploaded or generated image using a mask area you define.\nUpscale existing, generated, or edited images.\nThese images are generated using the general Imagen 3\n      image generation model (imagen-3.0-generate-002) and the\n      following prompts:\nClaymation scene. A medium wide shot of an elderly woman. She is\n         wearing flowing clothing. She is standing in a lush garden watering\n         the plants with an orange watering can\nShot in the style of DSLR camera with the polarizing filter. A\n         photo of two hot air balloons over the unique rock formations in\n         Cappadocia, Turkey. The colors and patterns on these balloons contrast\n         beautifully against the earthy tones of the landscape below. This shot\n         captures the sense of adventure that comes with enjoying such an\n         experience.\nA weathered, wooden mech robot covered in flowering vines stands\n         peacefully in a field of tall wildflowers, with a a small blue bird\n         resting on its outstrecteched hand. Digital Cartoon, with warm colors\n         and soft lines. A large cliff with a  waterfall looms behind.\nA view of a person's hand as they hold a little clay figurine of\n         a bird in their hand and sculpt it with a modeling tool in their other\n         hand. You can see the sculptor's scarf. Their hands are covered in\n         clay dust. A macro DSLR image highlighting the texture and\n         craftsmanship.\nA large, colorful bouquet of flowers in an old blue glass vase\n         on the table. In front is one beautiful peony flower surrounded by\n         various other blossoms like roses, lilies, daisies, orchids, fruits,\n         berries, green leaves. The background is dark gray. Oil painting in\n         the style of the Dutch Golden Age.\nA single comic book panel of a boy and his father on a grassy\n         hill, staring at the sunset. A speech bubble points from the boy's\n         mouth and says: The sun will rise again. Muted, late 1990s coloring\n         style\nYou can generate novel images using only descriptive text as an input. The\nfollowing samples show a simplified case for generating images, but you can useadditional\nparametersto\ntailor the generated images to your needs.\nSign in to your Google Cloud account. If you're new to\n        Google Cloud,create an accountto evaluate how our products perform in\n        real-world scenarios. New customers also get $300 in free credits to\n        run, test, and deploy workloads.\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.Note: If you don't plan to keep the\n    resources that you create in this procedure, create a project instead of\n    selecting an existing project. After you finish these steps, you can\n    delete the project, removing all resources associated with the project.Go to project selector\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.\nGo to project selector\nMake sure that billing is enabled for your Google Cloud project.\nMake sure that billing is enabled for your Google Cloud project.\nEnable the Vertex AI API.Enable the API\nEnable the Vertex AI API.\nEnable the API\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.Note: If you don't plan to keep the\n    resources that you create in this procedure, create a project instead of\n    selecting an existing project. After you finish these steps, you can\n    delete the project, removing all resources associated with the project.Go to project selector\nIn the Google Cloud console, on the project selector page,\n        select or create a Google Cloud project.\nGo to project selector\nMake sure that billing is enabled for your Google Cloud project.\nMake sure that billing is enabled for your Google Cloud project.\nEnable the Vertex AI API.Enable the API\nEnable the Vertex AI API.\nEnable the API\nSet up authentication for your environment.Select the tab for how you plan to use the samples on this page:PythonTo use the Python samples on this page in a local\n    development environment, install and initialize the gcloud CLI, and\n    then set up Application Default Credentials with your user credentials.Installthe Google Cloud CLI.\nSet up authentication for your environment.\nSelect the tab for how you plan to use the samples on this page:\nTo use the Python samples on this page in a local\n    development environment, install and initialize the gcloud CLI, and\n    then set up Application Default Credentials with your user credentials.\nInstallthe Google Cloud CLI.\nInstallthe Google Cloud CLI.\nIf you're using an external identity provider (IdP), you must firstsign in to the gcloud CLI with your federated identity.Toinitializethe gcloud CLI, run the following command:gcloudinitIf you're using a local shell, then create local authentication credentials for your user\n        account:gcloudauthapplication-defaultloginYou don't need to do this if you're using Cloud Shell.If an authentication error is returned, and you are using an external identity provider\n        (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity.For more information, seeSet up ADC for a local development environmentin the Google Cloud authentication documentation.\nIf you're using an external identity provider (IdP), you must firstsign in to the gcloud CLI with your federated identity.\nToinitializethe gcloud CLI, run the following command:gcloudinitIf you're using a local shell, then create local authentication credentials for your user\n        account:gcloudauthapplication-defaultloginYou don't need to do this if you're using Cloud Shell.If an authentication error is returned, and you are using an external identity provider\n        (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity.For more information, seeSet up ADC for a local development environmentin the Google Cloud authentication documentation.\nToinitializethe gcloud CLI, run the following command:\nIf you're using a local shell, then create local authentication credentials for your user\n        account:gcloudauthapplication-defaultloginYou don't need to do this if you're using Cloud Shell.If an authentication error is returned, and you are using an external identity provider\n        (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity.\nIf you're using a local shell, then create local authentication credentials for your user\n        account:\nYou don't need to do this if you're using Cloud Shell.\nIf an authentication error is returned, and you are using an external identity provider\n        (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity.\nFor more information, seeSet up ADC for a local development environmentin the Google Cloud authentication documentation."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nTo see an example of function calling,\n      run the \"Intro to Function Calling with the Gemini API\" Jupyter notebook in one of the following\n      environments:\nOpen\nin Colab|Open\nin Colab Enterprise|Open\nin Vertex AI Workbench user-managed notebooks|View on GitHub\nLarge Language Models (LLMs) are powerful at solving many types of problems.\nHowever, they are constrained by the following limitations:\nThey are frozen after training, leading to stale knowledge.\nThey can't query or modify external data.\nFunction calling can address these shortcomings. Function calling is sometimes\nreferred to astool usebecause it allows the model to use external tools such\nas APIs and functions.\nWhen submitting a prompt to the LLM, you also\nprovide the model with a set of tools that it can use to respond to the user's prompt. For\nexample, you could provide a functionget_weatherthat takes a location\nparameter and returns information about the weather conditions at that location.\nWhile processing a prompt, the model can choose to delegate certain\ndata processing tasks to the functions that you identify. The model does not\ncall the functions directly. Instead, the model provides structured data output\nthat includes the function to call and parameter values to use. For example, for\na promptWhat is the weather like in Boston?, the model can delegate processing\nto theget_weatherfunction and provide the location parameter valueBoston, MA.\nYou can use the structured output from the model to invoke external APIs. For\nexample, you could connect to a weather service API, provide the locationBoston, MA, and receive information about temperature, cloud cover, and wind\nconditions.\nYou can then provide the API output back to the model, allowing it to complete\nits response to the prompt. For the weather example, the model may provide the\nfollowing response:It is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies.\nThe following models provide support for function calling:\nVertex AI Model Optimizer\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nGemini 2.0 Flash-Lite\nYou can use function calling for the following tasks:\nHere are some more use cases:\nInterpret voice commands: Create functions that correspond with\nin-vehicle tasks. For example, you can create functions that turn on the\nradio or activate the air conditioning. Send audio files of the user's voice\ncommands to the model, and ask the model to convert the audio into text and\nidentify the function that the user wants to call.\nInterpret voice commands: Create functions that correspond with\nin-vehicle tasks. For example, you can create functions that turn on the\nradio or activate the air conditioning. Send audio files of the user's voice\ncommands to the model, and ask the model to convert the audio into text and\nidentify the function that the user wants to call.\nAutomate workflows based on environmental triggers: Create functions to\nrepresent processes that can be automated. Provide the model with data from\nenvironmental sensors and ask it to parse and process the data to determine\nwhether one or more of the workflows should be activated. For example, a\nmodel could process temperature data in a warehouse and choose to activate a\nsprinkler function.\nAutomate workflows based on environmental triggers: Create functions to\nrepresent processes that can be automated. Provide the model with data from\nenvironmental sensors and ask it to parse and process the data to determine\nwhether one or more of the workflows should be activated. For example, a\nmodel could process temperature data in a warehouse and choose to activate a\nsprinkler function.\nAutomate the assignment of support tickets: Provide the model with\nsupport tickets, logs, and context-aware rules. Ask the model to process all\nof this information to determine who the ticket should be assigned to. Call\na function to assign the ticket to the person suggested by the model.\nAutomate the assignment of support tickets: Provide the model with\nsupport tickets, logs, and context-aware rules. Ask the model to process all\nof this information to determine who the ticket should be assigned to. Call\na function to assign the ticket to the person suggested by the model.\nRetrieve information from a knowledge base: Create functions that\nretrieve academic articles on a given subject and summarize them. Enable the\nmodel to answer questions about academic subjects and provide citations for\nits answers.\nRetrieve information from a knowledge base: Create functions that\nretrieve academic articles on a given subject and summarize them. Enable the\nmodel to answer questions about academic subjects and provide citations for\nits answers.\nTo enable a user to interface with the model and use function calling, you must\ncreate code that performs the following tasks:\nSet up your environment.\nDefine and describe a set of available functionsusingfunction declarations.\nSubmit a user's prompt and the function declarations to the model.\nInvoke a functionusing the structured data output from the model.\nProvide the function output to the model.\nYou can create an application that manages all of these tasks. This application\ncan be a text chatbot, a voice agent, an automated workflow, or any other program.\nYou can use function calling to generate a single text response or to support\na chat session. Ad hoc text responses are useful for specific business tasks,\nincluding code generation. Chat sessions are useful in freeform, conversational\nscenarios, where a user is likely to ask follow-up questions.\nIf you use function calling to generate a single response, you must provide the\nmodel with the full context of the interaction. On the other hand, if you use\nfunction calling in the context of a chat session, the session stores the\ncontext for you and includes it in every model request. In both cases,\nVertex AI stores the history of the interaction on the client side.\nThis guide demonstrates how you can use function calling to generate a single\ntext response. For an end-to-end sample, seeText examples.\nTo learn how to use function calling to support a chat session, seeChat examples.\nImport the required modules and initialize the model:\nDeclare aToolthat contains up to 128FunctionDeclarations. You will later pass this tool to the model when submitting the prompt. The model can use the functions in the tool to process the prompt. At most one tool can be provided with the request.\nYou must provide function declarations in a schema format that's compatible with theOpenAPI schema. Vertex AI offers limited support of the OpenAPI schema. The following attributes are supported from the OpenAPI schema:type,nullable,required,format,description,properties,items,enum,anyOf. Remaining attributes are not supported. For best practices related to the function declarations, including tips for names and descriptions, seeBest practices.\nIf you use the REST API, specify the schema using JSON. If you use the\nVertex AI SDK for Python, you can specify the schema either manually using a Python dictionary or automatically with thefrom_funchelper function.\nThe following function declaration takes a singlestringparameter:\nThe following function declaration takes both object and array parameters:\nThe following function declaration takes an integerenum:\nThe following code sample declares a function that multiplies an array of numbers and usesfrom_functo generate theFunctionDeclarationschema.\nWhen the user provides a prompt, the application must provide the model with the\nuser prompt and thefunction declarations. To configure how the model\ngenerates results, the application can provide the model with ageneration configuration. To configure how the model uses the function declarations,\nthe application can provide the model with atool configuration.\nThe following is an example of a user prompt: \"What is the weather like in Boston?\"\nThe following is an example of how you can define the user prompt:\nFor best practices related to the user prompt, seeBest practices - User prompt.\nThe model can generate different results for different parameter values. The\ntemperature parameter controls the degree of randomness in this generation.\nLower temperatures are good for functions that require deterministic parameter\nvalues, while higher temperatures are good for functions with parameters that\naccept more diverse or creative parameter values. A temperature of0is\ndeterministic. In this case, responses for a given prompt are mostly\ndeterministic, but a small amount of variation is still possible. To learn\nmore, seeGemini API.\nTo set this parameter, submit a generation configuration (generation_config)\nalong with the prompt and the function declarations. You can update thetemperatureparameter during a chat conversation using the Vertex AI\nAPI and an updatedgeneration_config. For an example of setting thetemperatureparameter, seeHow to submit the prompt and the function declarations.\nFor best practices related to the generation configuration, seeBest practices - Generation configuration.\nYou can place some constraints on how the model should use the function\ndeclarations that you provide it with. For example, instead of allowing the\nmodel to choose between a natural language response and a function call, you can\nforce it to only predict function calls. This is known as \"forced function\ncalling\". You can also choose to provide the model with a full set of\nfunction declarations, but restrict its responses to a subset of these\nfunctions.\nTo place these constraints, submit a tool configuration (tool_config) along\nwith the prompt and the function declarations. In the configuration, you can\nspecify one of the following modes:\nFor a list of models that support theANYmode (\"forced function calling\"),\nseesupported models.\nTo learn more, seeFunction Calling API.\nThe following is an example of how can you submit the prompt and the function\ndeclarations to the model, and constrain the model to predict onlyget_current_weatherfunction calls.\nIf the model determines that it needs the output of a particular function, the\nresponse that the application receives from the model contains the function name\nand the parameter values that the function should be called with.\nThe following is an example of a model response to the user prompt \"What is the weather like in Boston?\". The model proposes calling\ntheget_current_weatherfunction with the parameterBoston, MA.\nFor prompts such as \"Get weather details in New Delhi and San Francisco?\",\nthe model may propose several parallel function calls. To\nlearn more, seeParallel function calling example.\nIf the application receives a function name and parameter values from the\nmodel, the application must connect to an external API and call the function.\nThe following example uses synthetic data to simulate a response payload from\nan external API:\nFor best practices related to API invocation, seeBest practices - API invocation.\nAfter an application receives a response from an external API, the application must\nprovide this response to the model. The following is an example of how you\ncan do this using Python:\nIf the model had proposed several parallel function calls, the application must\nprovide all of the responses back to the model. To learn more, seeParallel function calling example.\nThe model may determine that the\noutput of another function is necessary for responding to the prompt. In this case,\nthe response that the application receives from the model contains another\nfunction name and another set of parameter values.\nIf the model determines that the API response is sufficient for responding to\nthe user's prompt, it creates a natural language response and returns it to the\napplication. In this case, the application must pass the response back to the\nuser. The following is an example of a response:\nYou can use function calling to generate a single text response. Ad hoc text\nresponses are useful for specific business tasks, including code generation.\nIf you use function calling to generate a single response, you must provide the\nmodel with the full context of the interaction. Vertex AI stores\nthe history of the interaction on the client side.\nThis example demonstrates a text scenario with one function and one\nprompt. It uses theGenerativeModelclass and its methods. For more\ninformation about using the Vertex AI SDK for Python with Gemini multimodal\nmodels, seeIntroduction to multimodal classes in the Vertex AI SDK for Python.\nGen AI SDK for PythonInstallpip install --upgrade google-genaiTo learn more, see theSDK reference documentation.Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=Truefromgoogleimportgenaifromgoogle.genai.typesimport(FunctionDeclaration,GenerateContentConfig,HttpOptions,Tool,)client=genai.Client(http_options=HttpOptions(api_version=\"v1\"))model_id=\"gemini-2.0-flash-001\"get_album_sales=FunctionDeclaration(name=\"get_album_sales\",description=\"Gets the number of albums sold\",# Function parameters are specified in JSON schema formatparameters={\"type\":\"OBJECT\",\"properties\":{\"albums\":{\"type\":\"ARRAY\",\"description\":\"List of albums\",\"items\":{\"description\":\"Album and its sales\",\"type\":\"OBJECT\",\"properties\":{\"album_name\":{\"type\":\"STRING\",\"description\":\"Name of the music album\",},\"copies_sold\":{\"type\":\"INTEGER\",\"description\":\"Number of copies sold\",},},},},},},)sales_tool=Tool(function_declarations=[get_album_sales],)response=client.models.generate_content(model=model_id,contents='At Stellar Sounds, a music label, 2024 was a rollercoaster. \"Echoes of the Night,\" a debut synth-pop album, ''surprisingly sold 350,000 copies, while veteran rock band \"Crimson Tide\\'s\" latest, \"Reckless Hearts,\" ''lagged at 120,000. Their up-and-coming indie artist, \"Luna Bloom\\'s\" EP, \"Whispers of Dawn,\" ''secured 75,000 sales. The biggest disappointment was the highly-anticipated rap album \"Street Symphony\" '\"only reaching 100,000 units. Overall, Stellar Sounds moved over 645,000 units this year, revealing unexpected \"\"trends in music consumption.\",config=GenerateContentConfig(tools=[sales_tool],temperature=0,),)print(response.function_calls)# Example response:# [FunctionCall(#     id=None,#     name=\"get_album_sales\",#     args={#         \"albums\": [#             {\"album_name\": \"Echoes of the Night\", \"copies_sold\": 350000},#             {\"copies_sold\": 120000, \"album_name\": \"Reckless Hearts\"},#             {\"copies_sold\": 75000, \"album_name\": \"Whispers of Dawn\"},#             {\"copies_sold\": 100000, \"album_name\": \"Street Symphony\"},#         ]#     },# )]\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nThis example demonstrates a text scenario with one function and one\nprompt.\nNode.jsBefore trying this sample, follow theNode.jssetup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AINode.jsAPI\n    reference documentation.To authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.const{VertexAI,FunctionDeclarationSchemaType,}=require('@google-cloud/vertexai');constfunctionDeclarations=[{function_declarations:[{name:'get_current_weather',description:'get weather in a given location',parameters:{type:FunctionDeclarationSchemaType.OBJECT,properties:{location:{type:FunctionDeclarationSchemaType.STRING},unit:{type:FunctionDeclarationSchemaType.STRING,enum:['celsius','fahrenheit'],},},required:['location'],},},],},];constfunctionResponseParts=[{functionResponse:{name:'get_current_weather',response:{name:'get_current_weather',content:{weather:'super nice'}},},},];/*** TODO(developer): Update these variables before running the sample.*/asyncfunctionfunctionCallingStreamContent(projectId='PROJECT_ID',location='us-central1',model='gemini-2.0-flash-001'){// Initialize Vertex with your Cloud project and locationconstvertexAI=newVertexAI({project:projectId,location:location});// Instantiate the modelconstgenerativeModel=vertexAI.getGenerativeModel({model:model,});constrequest={contents:[{role:'user',parts:[{text:'What is the weather in Boston?'}]},{role:'ASSISTANT',parts:[{functionCall:{name:'get_current_weather',args:{location:'Boston'},},},],},{role:'USER',parts:functionResponseParts},],tools:functionDeclarations,};conststreamingResp=awaitgenerativeModel.generateContentStream(request);forawait(constitemofstreamingResp.stream){console.log(item.candidates[0].content.parts[0].text);}}\nBefore trying this sample, follow theNode.jssetup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AINode.jsAPI\n    reference documentation.\nTo authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.\nThis example demonstrates a text scenario with one function and one prompt.\nLearn how to install or update theGen AI SDK for Go.\nTo learn more, see theSDK reference documentation.\nSet environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True\nThis example demonstrates a text scenario with one function and one prompt.\nBefore trying this sample, follow theC#setup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AIC#API\n    reference documentation.\nTo authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.\nThis example demonstrates a text scenario with three functions and\none prompt.\nIn this example, you call the generative AI model twice.\nIn thefirst call, you provide the model\nwith the prompt and the function declarations.\nIn thesecond call, you provide the\nmodel with the API response.\nThe request must define a prompt in thetextparameter. This example defines\nthe following prompt: \"Which theaters in Mountain View show the Barbie movie?\".\nThe request must also define a tool (tools) with a set of function\ndeclarations (functionDeclarations). These function declarations must be\nspecified in a format that's compatible with theOpenAPI schema. This example\ndefines the following functions:\nfind_moviesfinds movie titles playing in theaters.\nfind_theatresfinds theaters based on location.\nget_showtimesfinds the start times for movies playing in a specific theater.\nTo learn more about the parameters of the model request, seeGemini API.\nReplacemy-projectwith the name of your Google Cloud project.\nFor the prompt \"Which theaters in Mountain View show the Barbie movie?\", the model\nmight return the functionfind_theatreswith parametersBarbieandMountain View, CA.\nThis example uses synthetic data instead of calling the external API.\nThere are two results, each with two parameters (nameandaddress):\nname:AMC Mountain View 16,address:2000 W El Camino Real, Mountain View, CA 94040\nname:Regal Edwards 14,address:245 Castro St, Mountain View, CA 94040\nReplacemy-projectwith the name of your Google Cloud project.\nThe model's response might be similar to the following:\nYou can use function calling to support a chat session. Chat sessions are useful\nin freeform, conversational scenarios, where a user is likely to ask follow-up\nquestions.\nIf you use function calling in the context of a chat session, the session stores\nthe context for you and includes it in every model request.\nVertex AI stores the history of the interaction on the client side.\nBefore trying this sample, follow theJavasetup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AIJavaAPI\n    reference documentation.\nTo authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.\nBefore trying this sample, follow theGosetup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AIGoAPI\n    reference documentation.\nTo authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.\nBefore trying this sample, follow theNode.jssetup instructions in theVertex AI quickstart using\n            client libraries.\n        \n      \n      \n  For more information, see theVertex AINode.jsAPI\n    reference documentation.\nTo authenticate to Vertex AI, set up Application Default Credentials.\n      For more information, seeSet up authentication for a local development environment.\nFor prompts such as \"Get weather details in New Delhi and San Francisco?\",\nthe model may propose several parallel function calls. For a list of models that\nsupport parallel function calling, seeSupported models.\nThis example demonstrates a scenario with oneget_current_weatherfunction.\nThe user prompt is \"Get weather details in New Delhi and San Francisco?\". The\nmodel proposes two parallelget_current_weatherfunction calls: one with the\nparameterNew Delhiand the other with the parameterSan Francisco.\nTo learn more about the parameters of the model request, seeGemini API.\nThe following command demonstrates how you can provide the function output to\nthe model. Replacemy-projectwith the name of your Google Cloud project.\nThe natural language response created by the model is similar to the following:\nFunction name should start with a letter or an underscore and contains only characters a-z, A-Z, 0-9, underscores, dots or dashes with a maximum length of 64.\nWrite function descriptions clearly and verbosely. For example, for abook_flight_ticketfunction:\nThe following is an example of a good function description:book flight tickets after confirming users' specific requirements, such as time, departure, destination, party size and preferred airline\nThe following is an example of a bad function description:book flight ticket\nFunction parameter and nested attribute names should start with a letter or an underscore and contains only characters a-z, A-Z, 0-9, or underscores with a maximum length of 64. Don't use period (.), dash (-), or space characters in the function parameter names and nested attributes.\nInstead, use underscore (_) characters or any other characters.\nWrite clear and verbose parameter descriptions, including details such as your\npreferred format or values. For example, for\nabook_flight_ticketfunction:\nThe following is a good example of adepartureparameter description:Use the 3 char airport code to represent the airport. For example, SJC or SFO. Don't use the city name.\nThe following is a bad example of adepartureparameter description:the departure airport\nIf possible, use strongly typed parameters to reduce model hallucinations. For\nexample, if the parameter values are from a finite set, add anenumfield\ninstead of putting the set of values into the description. If the parameter\nvalue is always an integer, set the type tointegerrather thannumber.\nWhen using functions with date, time, or location parameters, include the\ncurrent date, time, or relevant location information (for example, city and\ncountry) in the system instruction. This ensures the model has the necessary\ncontext to process the request accurately, even if the user's prompt lacks\ndetails.\nFor best results, prepend the user prompt with the following details:\nAdditional context for the model-for example,You are a flight API assistant to help with searching flights based on user preferences.\nDetails or instructions on how and when to use the functions-for example,Don't make assumptions on the departure or destination airports. Always use a future date for the departure or destination time.\nInstructions to ask clarifying questions if user queries are ambiguous-for example,Ask clarifying questions if not enough information is available.\nFor the temperature parameter, use0or another low value. This instructs\nthe model to generate more confident results and reduces hallucinations.\nIf the model proposes the invocation of a function that would send an order,\nupdate a database, or otherwise have significant consequences, validate the\nfunction call with the user before executing it.\nThe pricing for function calling is based on the number of characters within the\ntext inputs and outputs. To learn more, seeVertex AI pricing.\nHere, text input (prompt)\nrefers to the user prompt for the current conversation turn, the function\ndeclarations for the current conversation turn, and the history of the\nconversation. The history of the conversation includes the queries, the function\ncalls, and the function responses of previous conversation turns.\nVertex AI truncates the history of the conversation at 32,000 characters.\nText output (response) refers to the function calls and the text responses\nfor the current conversation turn.\nSee theAPI reference for function calling.\nSee theAPI reference for function calling.\nLearn aboutVertex AI extensions.\nLearn aboutVertex AI extensions.\nLearn aboutLangChain on Vertex AI.\nLearn aboutLangChain on Vertex AI.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-21 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nMachine learning models are often seen as \"black boxes\", where even its\ndesigners can't explain how or why a model produced a specific prediction.\nVertex Explainable AI offersFeature-basedandExample-basedexplanations to provide\nbetter understanding of model decision making.\nKnowing how a model behaves, and how it is influenced by its training dataset,\ngives anyone who builds or uses ML new abilities to improve models, build\nconfidence in their predictions, and understand when and why things go awry.\nWith example-based explanations, Vertex AI usesnearest neighbor searchto return a list of examples (typically from the training set) that are most\nsimilar to the input. Because we generally expect similar inputs to yield\nsimilar predictions, we can use these explanations to explore and explain our\nmodel's behavior.\nExample-based explanations can be useful in several scenarios:\nImprove your data or model: One of the core use cases for example-based\nexplanations is helping you understand why your model made certain mistakes\nin its predictions, and using those insights to improve your data or model.\nTo do so, first select test data that is of interest to you. This could be\neither driven by business needs or heuristics like data where the model made\nthe most egregious mistakes.For example, suppose we have a model that classifies images as either a bird\nor a plane, and that it is misclassifying the following bird as a plane with\nhigh confidence. You can use Example-based explanations to retrieve similar\nimages from the training set to figure out what is happening.Since all of its explanations are dark silhouettes from the plane class,\nit's a signal to get more bird silhouettes.However, if the explanations were mainly from the bird class, it's a signal\nthat our model is unable to learn relationships even when the data is rich,\nand we should consider increasing model complexity (for example, adding more\nlayers).\nImprove your data or model: One of the core use cases for example-based\nexplanations is helping you understand why your model made certain mistakes\nin its predictions, and using those insights to improve your data or model.\nTo do so, first select test data that is of interest to you. This could be\neither driven by business needs or heuristics like data where the model made\nthe most egregious mistakes.\nFor example, suppose we have a model that classifies images as either a bird\nor a plane, and that it is misclassifying the following bird as a plane with\nhigh confidence. You can use Example-based explanations to retrieve similar\nimages from the training set to figure out what is happening.\nSince all of its explanations are dark silhouettes from the plane class,\nit's a signal to get more bird silhouettes.\nHowever, if the explanations were mainly from the bird class, it's a signal\nthat our model is unable to learn relationships even when the data is rich,\nand we should consider increasing model complexity (for example, adding more\nlayers).\nInterpret novel data: Assume, for the moment, that your model was\ntrained to classify birds\nand planes, but in the real world, the model also encounters images of\nkites, drones, and helicopters. If your nearest neighbor dataset includes\nsome labeled images of kites, drones, and helicopters, you can use\nexample-based explanations to classify novel images by applying the most\nfrequently occurring label of its nearest neighbors. This is possible\nbecause we expect the latent representation of kites to be different from\nthat of birds or planes and more similar to the labeled kites in the nearest\nneighbor dataset.\nInterpret novel data: Assume, for the moment, that your model was\ntrained to classify birds\nand planes, but in the real world, the model also encounters images of\nkites, drones, and helicopters. If your nearest neighbor dataset includes\nsome labeled images of kites, drones, and helicopters, you can use\nexample-based explanations to classify novel images by applying the most\nfrequently occurring label of its nearest neighbors. This is possible\nbecause we expect the latent representation of kites to be different from\nthat of birds or planes and more similar to the labeled kites in the nearest\nneighbor dataset.\nDetect anomalies: Intuitively, if an instance is far away from all of\nthe data in the training set, then it is likely an outlier. Neural networks\nare known to be overconfident in their mistakes, thus masking their errors.\nMonitoring your models using example-based explanations helps identify the\nmost serious outliers.\nDetect anomalies: Intuitively, if an instance is far away from all of\nthe data in the training set, then it is likely an outlier. Neural networks\nare known to be overconfident in their mistakes, thus masking their errors.\nMonitoring your models using example-based explanations helps identify the\nmost serious outliers.\nActive learning: Example-based explanations can help you identify the\ninstances that might benefit from human labeling. This is particularly\nuseful if labeling is slow or expensive, ensuring that you get the richest\npossible dataset from limited labeling resources.For example, suppose we have a model that classifies a medical patient as\neither having a cold or a flu. If a patient is classified as having the flu,\nand all of her example-based explanations are from the flu class, then the\ndoctor can be more confident in the model's prediction without having to\ntake a closer look. However, if some of the explanations are from the flu\nclass, and some others from cold class, it would be worthwhile to get a\ndoctor's opinion. This will lead to a dataset where difficult instances have\nmore labels, making it easier for downstream models to learn complex\nrelationships.\nActive learning: Example-based explanations can help you identify the\ninstances that might benefit from human labeling. This is particularly\nuseful if labeling is slow or expensive, ensuring that you get the richest\npossible dataset from limited labeling resources.\nFor example, suppose we have a model that classifies a medical patient as\neither having a cold or a flu. If a patient is classified as having the flu,\nand all of her example-based explanations are from the flu class, then the\ndoctor can be more confident in the model's prediction without having to\ntake a closer look. However, if some of the explanations are from the flu\nclass, and some others from cold class, it would be worthwhile to get a\ndoctor's opinion. This will lead to a dataset where difficult instances have\nmore labels, making it easier for downstream models to learn complex\nrelationships.\nTo create a Model that supports example-based explanations, seeConfiguring\nexample-based\nexplanations.\nAny TensorFlow model that can provide an embedding (latent representation) for\ninputs is supported. Tree-based models, such as decision trees, are not\nsupported. Models from other frameworks, such as PyTorch or XGBoost, are not\nsupported yet.\nFor deep neural networks, we generally assume that the higher layers (closer to\nthe output layer) have learned something \"meaningful\", and thus, the penultimate\nlayer is often chosen for embeddings. You can experiment with a few different\nlayers, investigate the examples you are getting, and choose one based on some\nquantitative (class match) or qualitative (looks sensible) measures.\nFor a demonstration on how to extract embeddings from a TensorFlow model and\nperform nearest neighbor search, see theexample-based explanation\nnotebook.\nVertex Explainable AI integratesfeature attributionsinto Vertex AI. This\nsection provides a brief conceptual overview of the feature attribution methods\navailable with Vertex AI.\nFeature attributions indicate how much each feature in your model contributed to\nthe predictions for each given instance. When you request predictions, you get\npredicted values as appropriate for your model. When you requestexplanations,\nyou get the predictions along with feature attribution information.\nFeature attributions work on tabular data, and include built-in visualization\ncapabilities for image data. Consider the following examples:\nA deep neural network is trained to predict the duration of a bike ride,\nbased on weather data and previous ride sharing data. If you request only\npredictions from this model, you get predicted durations of bike rides in\nnumber of minutes. If you requestexplanations, you get the predicted bike\ntrip duration, along with an attribution score for each feature in your\nexplanations request. The attribution scores show how much the feature\naffected the change in prediction value, relative to the baseline value that\nyou specify. Choose a meaningful baseline that makes sense for your model -\nin this case, the median bike ride duration. You can plot the feature\nattribution scores to see which features contributed most strongly to the\nresulting prediction:\nA deep neural network is trained to predict the duration of a bike ride,\nbased on weather data and previous ride sharing data. If you request only\npredictions from this model, you get predicted durations of bike rides in\nnumber of minutes. If you requestexplanations, you get the predicted bike\ntrip duration, along with an attribution score for each feature in your\nexplanations request. The attribution scores show how much the feature\naffected the change in prediction value, relative to the baseline value that\nyou specify. Choose a meaningful baseline that makes sense for your model -\nin this case, the median bike ride duration. You can plot the feature\nattribution scores to see which features contributed most strongly to the\nresulting prediction:\nAn image classification model is trained to predict whether a given image\ncontains a dog or a cat. If you request predictions from this model on a new\nset of images, then you receive a prediction for each image (\"dog\" or\n\"cat\"). If you requestexplanations, you get the predicted class along\nwith an overlay for the image, showing which pixels in the image contributed\nmost strongly to the resulting prediction:A photo of a cat with feature attribution overlayA photo of a dog with feature attribution overlay\nAn image classification model is trained to predict whether a given image\ncontains a dog or a cat. If you request predictions from this model on a new\nset of images, then you receive a prediction for each image (\"dog\" or\n\"cat\"). If you requestexplanations, you get the predicted class along\nwith an overlay for the image, showing which pixels in the image contributed\nmost strongly to the resulting prediction:\nAn image classification model is trained to predict the species of a flower\nin the image. If you request predictions from this model on a new set of\nimages, then you receive a prediction for each image (\"daisy\" or\n\"dandelion\"). If you requestexplanations, you get the predicted class\nalong with an overlay for the image, showing which areas in the image\ncontributed most strongly to the resulting prediction:A photo of a daisy with feature attribution overlay\nAn image classification model is trained to predict the species of a flower\nin the image. If you request predictions from this model on a new set of\nimages, then you receive a prediction for each image (\"daisy\" or\n\"dandelion\"). If you requestexplanations, you get the predicted class\nalong with an overlay for the image, showing which areas in the image\ncontributed most strongly to the resulting prediction:\nFeature attribution is supported for all types of models (both AutoML and\ncustom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML\nmodels, and modalities (images, text, tabular, video).\nTo use feature attribution,configure your model for feature\nattributionwhen you upload or register the model to the Vertex AI Model Registry.\nAdditionally, for the following types of AutoML models, feature attribution is\nintegrated into the Google Cloud console:\nAutoML image models (classification models only)\nAutoML tabular models (classification and regression models only)\nFor AutoML model types that are integrated, you can enable feature attribution\nin the Google Cloud console during training and seemodel feature importancefor\nthe model overall, andlocal feature importancefor bothonlineandbatchpredictions.\nFor AutoML model types that are not integrated, you can still enable feature\nattribution by exporting the model artifacts and configuring feature attribution\nwhen you upload the model artifacts to the Vertex AI Model Registry.\nIf you inspect specific instances, and also aggregate feature attributions\nacross your training dataset, you can get deeper insight into how your model\nworks. Consider the following advantages:\nDebugging models: Feature attributions can help detect issues in the\ndata that standard model evaluation techniques would usually miss.For example, an image pathology model achieved suspiciously good results on\na test dataset of chest X-Ray images. Feature attributions revealed that the\nmodel's high accuracy depended on the radiologist's pen marks in the image.\nFor more details about this example, see theAI Explanations Whitepaper.\nDebugging models: Feature attributions can help detect issues in the\ndata that standard model evaluation techniques would usually miss.\nFor example, an image pathology model achieved suspiciously good results on\na test dataset of chest X-Ray images. Feature attributions revealed that the\nmodel's high accuracy depended on the radiologist's pen marks in the image.\nFor more details about this example, see theAI Explanations Whitepaper.\nOptimizing models: You can identify and remove features that are less\nimportant, which can result in more efficient models.\nOptimizing models: You can identify and remove features that are less\nimportant, which can result in more efficient models.\nEach feature attribution method is based onShapley values- a cooperative\ngame theory algorithm that assigns credit to each player in a game for a\nparticular outcome. Applied to machine learning models, this means that each\nmodel feature is treated as a \"player\" in the game. Vertex Explainable AI assigns\nproportional credit to each feature for the outcome of a particular prediction.\nThesampled Shapleymethod provides a sampling approximation of exact Shapley\nvalues. AutoML tabular models use the sampled Shapley method for feature\nimportance. Sampled Shapley works well for these models, which are\nmeta-ensembles of trees and neural networks.\nFor in-depth information about how the sampled Shapley method works, read the\npaperBounding the Estimation Error of Sampling-based Shapley Value\nApproximation.\nIn theintegrated gradientsmethod, the gradient of the prediction output is\ncalculated with respect to the features of the input, along an integral path.\nThe gradients are calculated at different intervals of a scaling parameter.\nThe size of each interval is determined by using theGaussian\nquadraturerule. (For\nimage data, imagine this scaling parameter as a \"slider\" that is scaling all\npixels of the image to black.)\nThe gradients are integrated as follows:The integral is approximated by using a weighted average.The element-wise product of the averaged gradients and the original\ninput is calculated.\nThe integral is approximated by using a weighted average.\nThe element-wise product of the averaged gradients and the original\ninput is calculated.\nFor an intuitive explanation of this process as applied to images, refer to the\nblog post,\"Attributing a deep network's prediction to its input features\".\nThe authors of the original paper about integrated gradients (Axiomatic\nAttribution for Deep Networks) show in the preceding blog post what the images\nlook like at each step of the process.\nTheXRAImethod combines the integrated gradients method with additional steps\nto determine whichregionsof the image contribute the most to a given class\nprediction.\nPixel-level attribution: XRAI performs pixel-level attribution for the input\nimage. In this step, XRAI uses the integrated gradients method with a black\nbaseline and a white baseline.\nOversegmentation: Independently of pixel-level attribution, XRAI\noversegments the image to create a patchwork of small regions. XRAI usesFelzenswalb's graph-based methodto create the\nimage segments.\nRegion selection: XRAI aggregates the pixel-level attribution within each\nsegment to determine its attribution density. Using these values, XRAI ranks\neach segment and then orders the segments from most to least positive. This\ndetermines which areas of the image are most salient, or contribute most\nstrongly to a given class prediction.\nVertex Explainable AI offers three methods to use for feature attributions:sampled\nShapley,integrated gradients, andXRAI.\nClassification and regression on tabular data\nAny custom-trained model (running in any prediction container)\nAutoML tabular models\nClassification and regression on tabular data\nClassification on image data\nCustom-trained TensorFlow models that use aTensorFlow\n            prebuilt containerto serve predictions\nAutoML image models\nClassification on image data\nCustom-trained TensorFlow models that use aTensorFlow\n          prebuilt containerto serve predictions\nAutoML image models\nFor a more thorough comparison of attribution methods, see theAI Explanations\nWhitepaper.\nIndifferentiablemodels, you can calculate the derivative of all the\noperations in your TensorFlow graph. This property helps to make backpropagation\npossible in such models. For example, neural networks are differentiable. To get\nfeature attributions for differentiable models, use the integrated gradients\nmethod.\nThe integrated gradients method doesnotwork for non-differentiable models.\nLearn more aboutencoding non-differentiable\ninputsto work\nwith the integrated gradients method.\nNon-differentiablemodels include non-differentiable operations in the\nTensorFlow graph, such as operations that perform decoding and rounding tasks.\nFor example, a model built as an ensemble of trees and neural networks is\nnon-differentiable. To get feature attributions for non-differentiable models,\nuse the sampled Shapley method. Sampled Shapley also works on differentiable\nmodels, but in that case, it is more computationally expensive than necessary.\nConsider the following limitations of feature attributions:\nFeature attributions, including local feature importance for AutoML, are\nspecific to individual predictions. Inspecting the feature attributions for\nan individual prediction may provide good insight, but the insight may not\nbe generalizable to the entire class for that individual instance, or the\nentire model.To get more generalizable insight for AutoML models, refer to the model\nfeature importance. To get more generalizable insight for other models,\naggregate attributions over subsets over your dataset, or the entire\ndataset.\nFeature attributions, including local feature importance for AutoML, are\nspecific to individual predictions. Inspecting the feature attributions for\nan individual prediction may provide good insight, but the insight may not\nbe generalizable to the entire class for that individual instance, or the\nentire model.\nTo get more generalizable insight for AutoML models, refer to the model\nfeature importance. To get more generalizable insight for other models,\naggregate attributions over subsets over your dataset, or the entire\ndataset.\nAlthough feature attributions can help with model debugging, they do not\nalways indicate clearly whether an issue arises from the model or from the\ndata that the model is trained on. Use your best judgment, and diagnose\ncommon data issues to narrow the space of potential causes.\nAlthough feature attributions can help with model debugging, they do not\nalways indicate clearly whether an issue arises from the model or from the\ndata that the model is trained on. Use your best judgment, and diagnose\ncommon data issues to narrow the space of potential causes.\nFeature attributions are subject to similar adversarial attacks as\npredictions in complex models.\nFeature attributions are subject to similar adversarial attacks as\npredictions in complex models.\nFor more information about limitations, refer to thehigh-level limitations\nlistand theAI Explanations Whitepaper.\nFor feature attribution, the implementations of sampled Shapley, integrated\ngradients, and XRAI are based on the following references, respectively:\nBounding the Estimation Error of Sampling-based Shapley Value\nApproximation\nAxiomatic Attribution for Deep Networks\nXRAI: Better Attributions Through Regions\nLearn more about the implementation of Vertex Explainable AI by reading theAI\nExplanations Whitepaper.\nTo get started using Vertex Explainable AI, use these notebooks:\nThe following resources provide further useful educational material:\nExplainable AI for Practitioners\nInterpretable Machine Learning: Shapley values\nAnkur Taly'sIntegrated Gradients GitHub repository.\nIntroduction to Shapley values\nConfigure your model for feature-based explanations\nConfigure your model for example-based explanations\nViewfeature importance for AutoML tabular\nmodels.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nDocument AI Warehouse\nDocumentation\nGuides\nCaution: Document AI Warehouse is deprecated and will no longer be available on Google Cloud\n  after January 16, 2025. To safeguard your data, migrate any documents currently saved in Document AI Warehouse to an alternative like Cloud Storage.\n  Verify that your data migration is completed before the discontinuation date to prevent any data loss. SeeDeprecationsfor details.\nDocument AI Warehouse is an integrated, cloud-based platform to store, search,\norganize, govern and analyze documents and their structured metadata (called\nProperties). Documents include structured (e.g. forms, invoices) and\nunstructured (e.g. contracts, research papers) and their Properties (metadata)\nincludes AI-extracted data from documents and manually or AI-assigned tags (for example,\naccount number, loan ID, document type).\nDocument AI Warehouse offers several advantages over legacy repositories. Following\nare some features and benefits:\nAPI-first: single integrated APIto manage documents and their\nproperties (extracted or tagged metadata), that integrates into your\nworkflows and applications.\nMetadata Management: to manage extracted and tagged metadata.\nGovernance: integrated with IAM and corporate directoriesFine-grained Access Control (permissions) at the document and folder\nlevels can be assigned to users and groups to view, edit, manage (share,\ndelete) documents.Document AI Warehouse is integrated with IAM (Cloud Identity), so that users\nand groups can be provisioned into Cloud IdentityUsers/groups can also be federated/synced into Cloud Identity from an\nenterprise LDAP / identity provider such Azure AD, Active Directory and\nKeycloak.\nFine-grained Access Control (permissions) at the document and folder\nlevels can be assigned to users and groups to view, edit, manage (share,\ndelete) documents.\nDocument AI Warehouse is integrated with IAM (Cloud Identity), so that users\nand groups can be provisioned into Cloud Identity\nUsers/groups can also be federated/synced into Cloud Identity from an\nenterprise LDAP / identity provider such Azure AD, Active Directory and\nKeycloak.\nSearch: the product supports rich semantic search, including the\nfollowing features:Full-text searchFiltering search results by Properties (date, numeric, enum, text).\nFilters can be combined withANDandORoperatorsSemantic search - support common synonyms and misspellings, stemmings.\nQuotes (\" \") may be used in the query to specify exact matching keywordsCustom synonyms - industry-specific or company-specific terms, for\nexample.Search within a root-folder hierarchyOperators for search keywords:\"\"exact match,|or,+and,-exclude\nFull-text search\nFiltering search results by Properties (date, numeric, enum, text).\nFilters can be combined withANDandORoperators\nSemantic search - support common synonyms and misspellings, stemmings.\nQuotes (\" \") may be used in the query to specify exact matching keywords\nCustom synonyms - industry-specific or company-specific terms, for\nexample.\nSearch within a root-folder hierarchy\nOperators for search keywords:\"\"exact match,|or,+and,-exclude\nOrganization:Flexible Folder managementDocuments can be cataloged into one or more folders, based on\napplication (for example, an ID card is placed in a KYC folder, Loan\nfolder, Bank Account folder), without replicating the document.These folders have their own Properties and Access Control, independent\nfrom Document properties and access control.The folders can be nested in one or more hierarchies [for example,\nAllLoans->State->Branch->Loans or LoanTypes->Loans].Users can search for documents within a folder hierarchy e.g. search\nwithin AllLoans->State\nDocuments can be cataloged into one or more folders, based on\napplication (for example, an ID card is placed in a KYC folder, Loan\nfolder, Bank Account folder), without replicating the document.\nThese folders have their own Properties and Access Control, independent\nfrom Document properties and access control.\nThe folders can be nested in one or more hierarchies [for example,\nAllLoans->State->Branch->Loans or LoanTypes->Loans].\nUsers can search for documents within a folder hierarchy e.g. search\nwithin AllLoans->State\nUI*- the product includes Web-accessible\nUI with the following features:Doc Explorer: search documents, filter search results, select documents\nto bulk-update properties or deleteDoc Viewer: view documents, view/update its properties,assign ACLs, add\nto foldersUpload: upload documents and run them through a\nDocAI**extractor (either OCR or a\nsupported specialized parser such as Invoice DocAI).Folder Explorer: add documents to one or more folders, explore folder\nhierarchy.Embeddable UI: the Doc Explorer and the Doc Viewer (for PDFs) components\ncan be integrated in customer's applications\nDoc Explorer: search documents, filter search results, select documents\nto bulk-update properties or delete\nDoc Viewer: view documents, view/update its properties,assign ACLs, add\nto folders\nUpload: upload documents and run them through a\nDocAI**extractor (either OCR or a\nsupported specialized parser such as Invoice DocAI).\nFolder Explorer: add documents to one or more folders, explore folder\nhierarchy.\nEmbeddable UI: the Doc Explorer and the Doc Viewer (for PDFs) components\ncan be integrated in customer's applications\nConnectors***to common on-premise\nand cloud repositories: We provide a Cloud Storage to Document AI Warehouse\nconnector (as a separate template based onGoogle Workflows) that can be\ncustomized/extended to other repositories. We also work with partners to\nprovide out-of-box connectors to repositories such as Sharepoint, Amazon S3,\nIBM FileNet and others, to ingest and index documents.\nMigrate vs Federate flexibility: The product supports a flexible\narchitecture such that your document content can be migrated to\nDocument AI Warehouse or stay-in-place if there are constraints in migrating\ncontent (we simply index the content and metadata)\nIntegrated with Document Workflows- this integrates with Google\nWorkflows and other document processing workflows by supporting:Properties- that represent the state of a document in a workflow\nand APIs that workflows can use to update the state of documentsDoc Explorer interface- to track the progress of documents through\na workflow pipeline, enabling a human to inspect, manage failures and\nstalled documents in the workflow pipeline.Conditional Notifications- where documents meeting a certain\nconditions can trigger/notify a workflow via a Pub/Sub topic or\na Web API call: for example, Trigger: OnUpdate; Condition:\n(DocType=Invoice and TotalAmount>$1000) -> send Pub/Sub\nNotification\nProperties- that represent the state of a document in a workflow\nand APIs that workflows can use to update the state of documents\nDoc Explorer interface- to track the progress of documents through\na workflow pipeline, enabling a human to inspect, manage failures and\nstalled documents in the workflow pipeline.\nConditional Notifications- where documents meeting a certain\nconditions can trigger/notify a workflow via a Pub/Sub topic or\na Web API call: for example, Trigger: OnUpdate; Condition:\n(DocType=Invoice and TotalAmount>$1000) -> send Pub/Sub\nNotification\nPolicy Management and Compliance Enforcement: conditional notifications\nand scheduled notifications can be used to trigger workflows that enforce\npolicies (for example, records management, retention and disposition, legal\nholds) on specific documents in Document AI Warehouse.\nFiles supported- Text PDFs, Images (scanned PDFs, TIFF files, JPEG\nfiles), Office (DOCX, PPTX, XLSX) files - run through OCR and indexed.Note - while the product focus is documents, it is also used to manage\nassociated images (e.g. in verticals such as Insurance, Engg,\nConstruction, Research, etc).\nNote - while the product focus is documents, it is also used to manage\nassociated images (e.g. in verticals such as Insurance, Engg,\nConstruction, Research, etc).\nIntegrated with DocAI: Document AI Warehouse is integrated with Document AI\nprocessors at several levels:Document AI processing in UI: Document AI Warehouse UI enables\nusers to upload either scanned PDFs/TIFFs or special document types,\nboth of which are automatically extracted by Document AI OCR or\nspecialized processors respectively before the document is indexed into\nDocument AI Warehouse.Managing batch Document AI\npipelines***: Document AI Warehouse\nintegrates with Workflows to provide templates that process\nbatch pipelines of documents through Document AI extraction and\nclassification. This is non-trivial because it entails long-running\n(LRO) operations and asynchronous API calls that need to be managed for\nfailures and retries. The Workflows template orchestrates such\npipelines. Document AI Warehouse UI may be used to search and track the\ndocument flow through such pipelines, visualize the Document AI\noutput for failures in each step of the pipeline and take action on\nstalled/failed documents.\nIntegrated with DocAI: Document AI Warehouse is integrated with Document AI\nprocessors at several levels:\nDocument AI processing in UI: Document AI Warehouse UI enables\nusers to upload either scanned PDFs/TIFFs or special document types,\nboth of which are automatically extracted by Document AI OCR or\nspecialized processors respectively before the document is indexed into\nDocument AI Warehouse.\nManaging batch Document AI\npipelines***: Document AI Warehouse\nintegrates with Workflows to provide templates that process\nbatch pipelines of documents through Document AI extraction and\nclassification. This is non-trivial because it entails long-running\n(LRO) operations and asynchronous API calls that need to be managed for\nfailures and retries. The Workflows template orchestrates such\npipelines. Document AI Warehouse UI may be used to search and track the\ndocument flow through such pipelines, visualize the Document AI\noutput for failures in each step of the pipeline and take action on\nstalled/failed documents.\n*The UI is in Preview and expected to go GA soon.\n**OCR and other document extractors are available in\nDocument AI products but not included in Document AI Warehouse.\n***These features are not part of\nDocument AI Warehouse. These features are enabled by external open source components\nand scripts that customers can deploy or customize and are not implemented\nwithin Document AI Warehouse.\nFor more information about Disclaimers and Known Limitations, seeDisclaimers and Known Limitations\nFollowing are terms used in Document AI Warehouse.\n[Images stored in Document AI Warehouse are also referred to as \"Documents\"]\nDocument AI Specialized parsers (for Procurement forms, Lending forms, others)OCR, AutoML, Forms parser (for images such as TIFF/PNG/etc.)Other custom modelsText extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents.\nOCR, AutoML, Forms parser (for images such as TIFF/PNG/etc.)Other custom modelsText extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents.\nOther custom modelsText extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents.\nText extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents.\nNote that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents.\nA user needs Edit permission to the Folder and View permission to the Document), in order to add a Document to a Folder\nA Trigger, for example, upon DocUpdate/DocCreateCondition, for example, Invoice.Amount <$1000Action, for example, Update Doc Metadata, Return Condition Evaluation, Add Doc to Folder, etc.A policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later)\nCondition, for example, Invoice.Amount <$1000Action, for example, Update Doc Metadata, Return Condition Evaluation, Add Doc to Folder, etc.A policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later)\nAction, for example, Update Doc Metadata, Return Condition Evaluation, Add Doc to Folder, etc.A policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later)\nA policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later)\nIt is expressed in a low-code Common Expression Language (JSON format, specified later)\nAPI: Admin API used to create/update/read/delete policies.\nFacet is typically an enumerated field.. We will support Date and Numeric facets in future releases.Facets for a Document type are specified in the Document Schema by Admins (via Admin API)\nFacets for a Document type are specified in the Document Schema by Admins (via Admin API)\nUniversal access - any user can access any document in the project.  The API is access-controlled to user accounts or service accounts but no document-level permissionsDoc-level ACL - users are granted document-level permissions. Each document has R/U/D permissions assigned to users/groups.\nDoc-level ACL - users are granted document-level permissions. Each document has R/U/D permissions assigned to users/groups.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nApigee\nDocumentation\nGuides\nSupported versions:\nv1.14 (latest)\nv1.13\nList of supported versions\nUnsupported versions:\nv1.12\nv1.11\nv1.10\nv1.9\nv1.8\nv1.7\nv1.6\nv1.5\nv1.4\nv1.3\nv1.2\nv1.1\nApigee hybrid is a platform for developing and managing API proxies that features a hybrid deployment model.\n  The hybrid model includes a management plane hosted by Apigee in the Cloud and a runtime plane that you install\n  and manage on one of thesupported Kubernetes platforms.\nApigee hybrid helps you manage internal and external APIs with\n          Google Cloud.\nWith unified API management, you can provide your developers, partners, and\n              customers a consistent API program experience.\nIf your compliance and security considerations make on-premises deployment a must for\n            your applications, with an enterprise-grade hybrid gateway, you can host and manage the\n            Apigee hybrid runtime plane on your premises.\nYou manage and control the runtime, enabling you to\n            leverage your existing compliance, governance, and security infrastructure.\nBalancing cost and performance may lead you to a hybrid strategy.\nWhether you are just exploring different cloud providers or have already chosen a\n              hybrid strategy, your API management platform should give you the flexibility you need.\n              Host and manage enterprise-grade hybrid gateways across your data center,\n              Google Cloud, or both.\nTolearn moreabout hybrid:\nKeep Reading\nToinstallhybrid:\nLet's go!\nApigee hybrid consists of a management plane maintained by Google and a runtime plane\n  that you install on asupported Kubernetes platform. Both planes use Google Cloud\n  Platform services, as the following image shows:\nAs you can see, hybrid consists of the following primary components:\nApigee-runmanagement plane: A set\n    of services hosted in the cloud and maintained by Google. These services include the UI,\n    management API, and analytics.\nCustomer-managedruntime plane: A\n    set of containerized runtime services that you set up and maintain in your own Kubernetes\n    cluster. All API traffic passes through and is processed within the runtime plane.You manage the containerized runtime on your Kubernetes cluster for greater agility with\n    staged rollouts, auto-scaling, and other operational benefits of containers.\nCustomer-managedruntime plane: A\n    set of containerized runtime services that you set up and maintain in your own Kubernetes\n    cluster. All API traffic passes through and is processed within the runtime plane.\nYou manage the containerized runtime on your Kubernetes cluster for greater agility with\n    staged rollouts, auto-scaling, and other operational benefits of containers.\nGoogle Cloud: A suite of Cloud services hosted\n    by Google.\nOne key thing to know about hybrid is that all API traffic is processed within the boundaries of\nyour network and under your control, while management services such as the UI and API analytics run\nin the cloud and are maintained by Google. For more information, seeWhere is\nyour data stored?\nThe following video provides a deep dive into the hybrid architecture:\nThe runtime plane is a set of containerized runtime services that you set up and maintain in your\nown Kubernetes cluster running on asupported Kubernetes platform.\n  All API traffic passes through and is processed within the runtime\nplane. The runtime plane includes the following major components:\nMessage Processors\nSynchronizer\nCassandra\nMART\nUDCA\nThe runtime plane runs in a Kubernetes cluster running on asupported Kubernetes platformthat you maintain.\nThe following image shows the primary services that execute on the runtime plane:\nFor general information about the runtime components, see the sections that follow. In addition,\nseeRuntime service configuration overview.\nThe following sections describe each of these primary runtime plane services in more detail.\nHybrid Message Processors (MPs) provide API request processing and policy execution on the\nruntime plane. MPs load all of the deployed proxies, resources, target servers, certificates, and\nkeystores from local storage. You configure an Apigee ingress gateway to expose the MPs to\nrequests that come from outside the cluster.\nThe Synchronizer fetches configuration data about an API environment from the management plane\nand propagates it across the runtime plane. This downloaded data is also called thecontractand is stored on the local file system.\nThe Synchronizer periodically polls the Management Server for\nchanges and downloads a new configuration whenever changes are\ndetected. The configuration data is retrieved and stored locally as\na JSON file on the local file system, where the Message Processors can access it.\nThe downloaded configuration data allows the runtime plane to\nfunction independently from the management plane. With the contract,\nMessage Processors on the runtime plane use the locally\nstored data as their configuration. If the connection between the\nmanagement and runtime plane goes down, services on the runtime\nplane continue to function.\nThe configuration data downloaded by the Synchronizer includes:\nProxy bundles and shared flow deployments\nFlow hooks\nEnvironment information\nShared API resources\nTarget server\n    definitions\nTLS settings\nKey Value Map (KVM) names\nData masks\nApache Cassandrais the runtime datastore that provides data persistence for the runtime plane.\nCassandra is a distributed data system that provides data persistence on the runtime plane.\nYou deploy the Cassandra database as aStatefulSetnode pool on your Kubernetes cluster.\nLocating these entities close to the runtime processing services helps support requirements\nfor security and high scalability.\nThe Cassandra database stores information about the following entities:\nKey management system (KMS)\nKey Value Map (KVM)\nOAuth\nManagement API for RunTime data (MART)\nMonetization data\nQuotas\nResponse cache\nData that belongs to your organization and is accessed during runtime API calls are\nstored by Cassandra in the runtime plane.This data includes:Application configurationsKey Management System (KMS) dataCacheKey Value Maps (KVMs)API productsDeveloper appsTo access and update that data—for example, to add a\nnew KVM or to remove an environment—you can use the Apigee hybrid UI or theApigee APIs. The MART\nserver (Management API for Runtime data) processes the API calls against the runtime\ndatastore.This section describes the role that MART plays when you call the Apigee APIs to access the\nruntime datastore.What MART isTo call an Apigee API, you send an authenticated request to the Management Server\n          (MS) on the management plane. The MS authenticates and authorizes the request, and then\n          forwards the request to MART on the runtime plane. Attached to that request is a token\n          that the MS generated using a pre-configured service account.MART receives the request, authenticates and authorizes it, and then performs business\n          validation on it. (For example, if the app is part of an API product, MART ensures it's a\n          valid request.) After determining  that a request is valid, MART then processes it.Cassandra stores the runtime data that MART processes (it is, after all, aruntime datastore). MART might read data from the Cassandra or it might update\n          that data, depending on the type of request.Like most hybrid services, MART is stateless: it does not persist its own state at\n          runtime.What MART is notThe Management plane communicates with MART via the Apigee Connect agent, which uses\n          a service account with the Apigee Connect Agent role (the MART service account in most\n          installations). You do not call MART directly.Furthermore, MART does not receive API proxy requests; those calls are handled by the\n            Runtime ingress controller and are routed to your\n          cluster's Message Processors.It's worth pointing out that both MART and the Message Processors have access to the\n  same runtime datastore (Cassandra), which is how data such as KMS, KVMs, and caches are\n  shared.The following image shows the flow of an Apigee API call:UDCAThe Universal Data Collection Agent (UDCA) is a service running within the data collection pod\nin the runtime plane that extracts analytics, debug, and deployment status data and sends it to the\nUAP.For more information, seeDebug, analytics, and deployment status\ndata collection.About the management planeThe management plane runs on Google Cloud. It includes administrative services such\nas:Apigee hybrid UI:Provides a UI for developers to create and deploy API\n    proxies, configure policies, create API products, and create developer apps. Administrators\n    can use the Apigee hybrid UI to monitor deployment status.Apigee APIs:Provide a programmatic interface for managing your\n    organization and environments.Unified Analytics Platform (UAP):Receives and processes analytics and\n    deployment status data from the runtime plane.The following image shows the primary services that execute on the management plane:Management plane data and data residencyIf your installation is usingdata residencyyou can specify the region in which this data is stored. SeeUsing data residency with Apigee hybrid.About the Google Cloud servicesThe following table describes the key Google Cloud services that hybrid leverages:Google Cloud ServiceDescriptionIdentityUser account authentication uses Google Cloud accounts. Authorization uses\n     Google Cloud service accounts.RolesAccess management for hybrid uses Google's roles engine, IAM, and supports default Apigee\n     roles.Resource HierarchyResources are organized in Google Cloud projects (linked to Apigee organizations).Cloud OperationsProvides logging and metrics data analysis.Types of usersApigee has identified the following primary types of hybrid users:RoleTypical responsibilities/tasksAreas of interestSystem administrators/operatorsInstall and configure services on hybrid's runtime planeSet up Google Cloud, Apigee, and service accountsCreate Google Cloud services and projectsManage the Kubernetes clusterMaintain all of the aboveTroubleshoot the API proxiesCreate and administer a Kubernetes clusterDownload the Apigee Helm chartsUpgrade Apigee hybridConfigure Google Cloud services and Apigee hybrid UIInstall hybrid overviewAdminister hybridData collection on hybridHybrid service configurationKubernetesDevelopersBuild API proxies using the Apigee hybrid UI or Apigee APIsDeploy API proxies to the runtime planeTroubleshoot the API proxiesTest the API proxiesCreate and deploy API proxiesData collection on hybridApigee PoliciesAdvantagesApigee hybrid has the following advantages:Increased agilityBecause hybrid is delivered and runs in containers, you can achieve staged rollouts,\n  auto-scaling, and other operational benefits of a containerized system.Reduced latencyAll communication with the hybrid management plane is asynchronous and does not happen as\n  part of processing client API requests.Increased API adoptionAlthough it is possible to process internal APIs\n    using Apigee, the reduced latency and efficiency you can achieve with hybrid makes processing internal\n    APIs with hybrid an attractive option. Part of this efficiency is achieved because your API gateway\n    runs on-premises, in close proximity to your backend services.\n    Also, if you are on Apigee, you can increase your adoption of Apigee by\n  processing internal APIs through hybrid.Greater controlMany enterprises are embarking on a hybrid strategy. The ability to\n  manage API runtimes deployed in private\n  data centers is a key requirement for large enterprises.\n  Currently, the hybrid runtime plane can be deployed to Google Cloud\n  or in your own data center.Next stepSeethe Big Picture—an overview of the hybrid installation\n  process.\nThis data includes:\nApplication configurations\nKey Management System (KMS) data\nCache\nKey Value Maps (KVMs)\nAPI products\nDeveloper apps\nTo access and update that data—for example, to add a\nnew KVM or to remove an environment—you can use the Apigee hybrid UI or theApigee APIs. The MART\nserver (Management API for Runtime data) processes the API calls against the runtime\ndatastore.\nThis section describes the role that MART plays when you call the Apigee APIs to access the\nruntime datastore.\nTo call an Apigee API, you send an authenticated request to the Management Server\n          (MS) on the management plane. The MS authenticates and authorizes the request, and then\n          forwards the request to MART on the runtime plane. Attached to that request is a token\n          that the MS generated using a pre-configured service account.MART receives the request, authenticates and authorizes it, and then performs business\n          validation on it. (For example, if the app is part of an API product, MART ensures it's a\n          valid request.) After determining  that a request is valid, MART then processes it.Cassandra stores the runtime data that MART processes (it is, after all, aruntime datastore). MART might read data from the Cassandra or it might update\n          that data, depending on the type of request.Like most hybrid services, MART is stateless: it does not persist its own state at\n          runtime.\nMART receives the request, authenticates and authorizes it, and then performs business\n          validation on it. (For example, if the app is part of an API product, MART ensures it's a\n          valid request.) After determining  that a request is valid, MART then processes it.\nCassandra stores the runtime data that MART processes (it is, after all, aruntime datastore). MART might read data from the Cassandra or it might update\n          that data, depending on the type of request.\nLike most hybrid services, MART is stateless: it does not persist its own state at\n          runtime.\nThe Management plane communicates with MART via the Apigee Connect agent, which uses\n          a service account with the Apigee Connect Agent role (the MART service account in most\n          installations). You do not call MART directly.\nFurthermore, MART does not receive API proxy requests; those calls are handled by the\n            Runtime ingress controller and are routed to your\n          cluster's Message Processors.\nIt's worth pointing out that both MART and the Message Processors have access to the\n  same runtime datastore (Cassandra), which is how data such as KMS, KVMs, and caches are\n  shared.\nThe following image shows the flow of an Apigee API call:\nThe Universal Data Collection Agent (UDCA) is a service running within the data collection pod\nin the runtime plane that extracts analytics, debug, and deployment status data and sends it to the\nUAP.\nFor more information, seeDebug, analytics, and deployment status\ndata collection.\nThe management plane runs on Google Cloud. It includes administrative services such\nas:\nApigee hybrid UI:Provides a UI for developers to create and deploy API\n    proxies, configure policies, create API products, and create developer apps. Administrators\n    can use the Apigee hybrid UI to monitor deployment status.\nApigee APIs:Provide a programmatic interface for managing your\n    organization and environments.\nUnified Analytics Platform (UAP):Receives and processes analytics and\n    deployment status data from the runtime plane.\nThe following image shows the primary services that execute on the management plane:\nIf your installation is usingdata residencyyou can specify the region in which this data is stored. SeeUsing data residency with Apigee hybrid.\nThe following table describes the key Google Cloud services that hybrid leverages:\nApigee has identified the following primary types of hybrid users:\nInstall and configure services on hybrid's runtime plane\nSet up Google Cloud, Apigee, and service accounts\nCreate Google Cloud services and projects\nManage the Kubernetes cluster\nMaintain all of the above\nTroubleshoot the API proxies\nCreate and administer a Kubernetes cluster\nDownload the Apigee Helm charts\nUpgrade Apigee hybrid\nConfigure Google Cloud services and Apigee hybrid UI\nInstall hybrid overview\nAdminister hybrid\nData collection on hybrid\nHybrid service configuration\nKubernetes\nBuild API proxies using the Apigee hybrid UI or Apigee APIs\nDeploy API proxies to the runtime plane\nTroubleshoot the API proxies\nTest the API proxies\nCreate and deploy API proxies\nData collection on hybrid\nApigee Policies\nApigee hybrid has the following advantages:\nSeethe Big Picture—an overview of the hybrid installation\n  process.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/attached",
    "title": "GKE attached clusters documentation",
    "content": "Home\nGoogle Kubernetes Engine (GKE)\nGKE Enterprise\nClusters\nDocumentation\nGKE Multi-Cloud\nGKE attached clusters\nGKE attached clusters\nWith GKE attached clusters you can manage any standard,CNCF-compliantKubernetes\ninstallation, including clusters already in production. Then addGoogle Kubernetes Engine (GKE) Enterprise editionfeatures to standardize and secure your clusters across multiple cloud\nenvironments and Kubernetes vendors.\nGKE attached clusters can automatically installConfig SyncandPolicy Controlleron your clusters, so your platform team can maintain consistent configurations\n  and security policies across all your Kubernetes environments. App teams are\n  free to use their continuous delivery tool of choice with policies in place to\n  provide necessary safeguards.\nYou can usePolicy ControllerandCloud Service Meshto apply security and\n  network configurations across all your cluster configurations. Additionally,\n  you can enable valuable intra-cluster telemetry.\nUse theConnect gatewayto connect to clusters across environments without proxies, inbound firewall\nrules, or bastion hosts.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nAttach your EKS cluster\nAttach your EKS cluster\nConnect to your EKS cluster\nConnect to your EKS cluster\nInstall Config Management\nInstall Config Management\nInstall Cloud Service Mesh\nInstall Cloud Service Mesh\nInstall logging for EKS attached clusters\nInstall logging for EKS attached clusters\nInstall monitoring for EKS attached clusters\nInstall monitoring for EKS attached clusters\nEnable Binary Authorization for EKS attached clusters\nEnable Binary Authorization for EKS attached clusters\nGet support\nGet support\nAttach your AKS cluster\nAttach your AKS cluster\nConnect to your AKS cluster\nConnect to your AKS cluster\nInstall Config Management\nInstall Config Management\nInstall Cloud Service Mesh\nInstall Cloud Service Mesh\nInstall logging for AKS attached clusters\nInstall logging for AKS attached clusters\nInstall monitoring for AKS attached clusters\nInstall monitoring for AKS attached clusters\nEnable Binary Authorization for AKS attached clusters\nEnable Binary Authorization for AKS attached clusters\nGet support\nGet support\nAttach other Kubernetes clusters\nAttach other Kubernetes clusters\nConnect to your cluster\nConnect to your cluster\nInstall Config Management\nInstall Config Management\nInstall Cloud Service Mesh\nInstall Cloud Service Mesh\nInstall logging for attached clusters\nInstall logging for attached clusters\nInstall monitoring for attached clusters\nInstall monitoring for attached clusters\nGet support\nGet support\nImpact of temporary disconnection from Google Cloud\nImpact of temporary disconnection from Google Cloud\nPricing\nPricing\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models",
    "title": "Introduction to tuningStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nModel tuningis a crucial process in adapting Gemini to perform specific tasks\nwith greater precision and accuracy. Model tuning works by providing a model\nwith a training dataset that contains a set of examples of specific downstream\ntasks.\nThis page provides an overview of model tuning for Gemini, describes\nthe tuning options available for Gemini, and helps you determine when\neach tuning option should be used.\nModel tuning is an effective way to customize large models to your tasks. It's a\nkey step to improve the model's quality and efficiency. Model tuning provides the\nfollowing benefits:\nHigher quality for your specific tasks\nIncreased model robustness\nLower inference latency and cost due to shorter prompts\nPrompting with pre-trained Gemini models: Prompting is the art of crafting effective\ninstructions to guide AI models like Gemini in generating the outputs you want.\nIt involves designing prompts that clearly convey the task, format you want,\nand any relevant context. You can use Gemini's capabilities with minimal setup.\nIt's best suited for:Limited labeled data: If you have a small amount of labeled data or can't afford a\nlengthy fine-tuning process.Rapid prototyping: When you need to quickly test a concept or get a baseline\nperformance without heavy investment in fine-tuning.\nLimited labeled data: If you have a small amount of labeled data or can't afford a\nlengthy fine-tuning process.\nRapid prototyping: When you need to quickly test a concept or get a baseline\nperformance without heavy investment in fine-tuning.\nCustomized fine-tuning of Gemini models: For more tailored results, Gemini lets you fine-tune its models on your specific datasets. To create an AI model that excels in your specific domain, consider fine-tuning. This involves retraining the base model on your own labeled dataset, adapting its weights to your task and data. You can adapt Gemini to your use cases. Fine-tuning is most effective when:You have labeled data: A sizable dataset to train on (think 100 examples or more),\nwhich allows the model to deeply learn your task's specifics.Complex or unique tasks: For scenarios where advanced prompting strategies are\nnot sufficient, and a model tailored to your data is essential.\nYou have labeled data: A sizable dataset to train on (think 100 examples or more),\nwhich allows the model to deeply learn your task's specifics.\nComplex or unique tasks: For scenarios where advanced prompting strategies are\nnot sufficient, and a model tailored to your data is essential.\nWe recommend starting with prompting to find the optimal prompt. Then, move on to\nfine-tuning (if required) to further boost performances or fix recurrent errors.\nWhile adding more examples might be beneficial, it is important to evaluate where\nthe model makes mistakes before adding more data. High-quality, well-labeled data\nis crucial for good performance and better than quantity. Also, the data you use\nfor fine-tuning should reflect the prompt distribution, format and context the\nmodel will encounter in production.\nTuning provides the following benefits over prompt design:\nAllows deep customization on the model and results in better performance on\nspecific tasks.\nAlign the model with custom syntax, instructions, domain specific semantic\nrules.\nOffers more consistent and reliable results.\nCapable of handling more examples at once.\nSave cost at inference by removing few-shot examples, long instructions\nin the prompts\nParameter-efficient tuning and full fine-tuning are two approaches to\ncustomizing large models. Both methods have their advantages and implications in\nterms of model quality and resource efficiency.\nParameter-efficient tuning, also called adapter tuning, enables efficient\nadaptation of large models to your specific tasks or domain. Parameter-efficient tuning\nupdates a relatively small subset of the model's parameters during the tuning\nprocess.\nTo understand how Vertex AI supports adapter tuning and serving, you\ncan find more details in the following whitepaper,Adaptation of Large Foundation Models.\nFull fine-tuning updates all parameters of the model, making it suitable for\nadapting the model to highly complex tasks, with the potential of achieving higher\nquality. However full fine tuning demands higher computational resources for both\ntuning and serving, leading to higher overall costs.\nParameter-efficient tuning is more resource efficient and cost effective compared\nto full fine-tuning. It uses significantly lower computational resources to train.\nIt's able to adapt the model faster with a smaller dataset. The flexibility of\nparameter-efficient tuning offers a solution for multi-task learning without the need\nfor extensive retraining.\nVertex AI supports supervised fine-tuning to customize foundational models.\nSupervised fine-tuning improves the performance of the model by teaching it a new\nskill. Data that contains hundreds of labeled examples is used to teach the\nmodel to mimic a desired behavior or task. Each labeled example demonstrates\nwhat you want the model to output during inference.\nWhen you run a supervised fine-tuning job, the model learns additional parameters\nthat help it encode the necessary information to perform the desired task or\nlearn the desired behavior. These parameters are used during inference. The\noutput of the tuning job is a new model that combines the newly learned\nparameters with the original model.\nSupervised fine-tuning of a text model is a good option when the output of your model\nisn't complex and is relatively easy to define. Supervised fine-tuning is recommended\nfor classification, sentiment analysis, entity extraction, summarization of\ncontent that's not complex, and writing domain-specific queries. For code\nmodels, supervised tuning is the only option.\ngemini-2.0-flash-001\ngemini-2.0-flash-lite-001\nFor more information on using supervised fine-tuning with each respective model,\nsee the following pages: Tunetext,image,audio, anddocumentdata types.\nTo learn more about the document understanding capability of Gemini models, see theDocument understandingoverview.\nTo start tuning, seeTune Gemini models by using supervised fine-tuning\nTo learn how supervised fine-tuning can be used in a solution that builds a\ngenerative AI knowledge base, seeJump Start Solution: Generative AI\nknowledge base.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nCloud Key Management Service\nDocumentation\nGuides\nThis page provides an overview of Cloud External Key Manager (Cloud EKM).TerminologyExternal key manager (EKM)The key manager used outside of Google Cloud to manage your keys.Cloud External Key Manager (Cloud EKM)A Google Cloud service for using your external keys that are\n    managed within a supported EKM.Cloud EKM through the\n      internetA version of Cloud EKM where Google Cloud communicates\n      with your external key manager over the internet.Cloud EKM through a\n      VPCA version of Cloud EKM where Google Cloud communicates\n      with your external key manager over a Virtual Private Cloud (VPC). For more\n      information, seeVPC network overview.EKM key management from\nCloud KMSWhen using Cloud EKM through a VPC with an\n      external key management partner that supports the Cloud EKM\n      control plane, you can use theCloud KMSEKM\n      management mode to simplify the process of maintaining external keys in\n      your external key management partner and in Cloud EKM. For more information, seeCoordinated external keysandEKM key management from\nCloud KMSon this page.Crypto spaceA container for your resources within your external key management partner. Your crypto\n      space is identified by a unique crypto space path. The format of the\n      crypto space path varies by external key management partner—for example,v0/cryptospaces/YOUR_UNIQUE_PATH.Partner-managed EKMAn arrangement where your EKM is managed for you by a trusted partner.\n    For more information, seePartner-managed EKMon this page.Key Access JustificationsWhen you use Cloud EKM with Key Access Justifications, each request to\n      your external key management partner includes a field that identifies the reason for each\n      request. You can configure your external key management partner to allow or deny requests\n      based on the Key Access Justifications code provided. For more information about\n      Key Access Justifications, seeKey Access Justifications overview.OverviewWith Cloud EKM, you can use keys that you manage within asupported external key management partnerto protect data within\nGoogle Cloud. You can protect data at rest insupported CMEK integration\nservices, or by calling the Cloud Key Management Service API directly.Cloud EKM provides several benefits:Key provenance:You control the location and distribution of your\nexternally managed keys. Externally managed keys are never cached or stored\nwithin Google Cloud. Instead, Cloud EKM communicates directly\nwith the external key management partner for each request.Access control:You manage access to your externally managed keys in\nyour external key manager. You can't use an externally managed key in\nGoogle Cloud without first granting the Google Cloud project\naccess to the key in your external key manager. You can revoke this access\nat any time.Centralized key management:You can manage your keys and access policies\nfrom a single user interface, whether the data they protect resides in the\ncloud or on your premises.In all cases, the key resides on the external system, and is never sent to\nGoogle.You can communicate with your external key managerover the\ninternetorover a\nVirtual Private Cloud (VPC).\nThis page provides an overview of Cloud External Key Manager (Cloud EKM).\nExternal key manager (EKM)The key manager used outside of Google Cloud to manage your keys.\nExternal key manager (EKM)\nThe key manager used outside of Google Cloud to manage your keys.\nCloud External Key Manager (Cloud EKM)A Google Cloud service for using your external keys that are\n    managed within a supported EKM.\nCloud External Key Manager (Cloud EKM)\nA Google Cloud service for using your external keys that are\n    managed within a supported EKM.\nCloud EKM through the\n      internetA version of Cloud EKM where Google Cloud communicates\n      with your external key manager over the internet.\nCloud EKM through the\n      internet\nA version of Cloud EKM where Google Cloud communicates\n      with your external key manager over the internet.\nCloud EKM through a\n      VPCA version of Cloud EKM where Google Cloud communicates\n      with your external key manager over a Virtual Private Cloud (VPC). For more\n      information, seeVPC network overview.\nCloud EKM through a\n      VPC\nA version of Cloud EKM where Google Cloud communicates\n      with your external key manager over a Virtual Private Cloud (VPC). For more\n      information, seeVPC network overview.\nEKM key management from\nCloud KMSWhen using Cloud EKM through a VPC with an\n      external key management partner that supports the Cloud EKM\n      control plane, you can use theCloud KMSEKM\n      management mode to simplify the process of maintaining external keys in\n      your external key management partner and in Cloud EKM. For more information, seeCoordinated external keysandEKM key management from\nCloud KMSon this page.\nEKM key management from\nCloud KMS\nWhen using Cloud EKM through a VPC with an\n      external key management partner that supports the Cloud EKM\n      control plane, you can use theCloud KMSEKM\n      management mode to simplify the process of maintaining external keys in\n      your external key management partner and in Cloud EKM. For more information, seeCoordinated external keysandEKM key management from\nCloud KMSon this page.\nCrypto spaceA container for your resources within your external key management partner. Your crypto\n      space is identified by a unique crypto space path. The format of the\n      crypto space path varies by external key management partner—for example,v0/cryptospaces/YOUR_UNIQUE_PATH.\nCrypto space\nA container for your resources within your external key management partner. Your crypto\n      space is identified by a unique crypto space path. The format of the\n      crypto space path varies by external key management partner—for example,v0/cryptospaces/YOUR_UNIQUE_PATH.\nPartner-managed EKMAn arrangement where your EKM is managed for you by a trusted partner.\n    For more information, seePartner-managed EKMon this page.\nPartner-managed EKM\nAn arrangement where your EKM is managed for you by a trusted partner.\n    For more information, seePartner-managed EKMon this page.\nKey Access JustificationsWhen you use Cloud EKM with Key Access Justifications, each request to\n      your external key management partner includes a field that identifies the reason for each\n      request. You can configure your external key management partner to allow or deny requests\n      based on the Key Access Justifications code provided. For more information about\n      Key Access Justifications, seeKey Access Justifications overview.\nKey Access Justifications\nWhen you use Cloud EKM with Key Access Justifications, each request to\n      your external key management partner includes a field that identifies the reason for each\n      request. You can configure your external key management partner to allow or deny requests\n      based on the Key Access Justifications code provided. For more information about\n      Key Access Justifications, seeKey Access Justifications overview.\nWith Cloud EKM, you can use keys that you manage within asupported external key management partnerto protect data within\nGoogle Cloud. You can protect data at rest insupported CMEK integration\nservices, or by calling the Cloud Key Management Service API directly.\nCloud EKM provides several benefits:\nKey provenance:You control the location and distribution of your\nexternally managed keys. Externally managed keys are never cached or stored\nwithin Google Cloud. Instead, Cloud EKM communicates directly\nwith the external key management partner for each request.\nKey provenance:You control the location and distribution of your\nexternally managed keys. Externally managed keys are never cached or stored\nwithin Google Cloud. Instead, Cloud EKM communicates directly\nwith the external key management partner for each request.\nAccess control:You manage access to your externally managed keys in\nyour external key manager. You can't use an externally managed key in\nGoogle Cloud without first granting the Google Cloud project\naccess to the key in your external key manager. You can revoke this access\nat any time.\nAccess control:You manage access to your externally managed keys in\nyour external key manager. You can't use an externally managed key in\nGoogle Cloud without first granting the Google Cloud project\naccess to the key in your external key manager. You can revoke this access\nat any time.\nCentralized key management:You can manage your keys and access policies\nfrom a single user interface, whether the data they protect resides in the\ncloud or on your premises.\nCentralized key management:You can manage your keys and access policies\nfrom a single user interface, whether the data they protect resides in the\ncloud or on your premises.\nIn all cases, the key resides on the external system, and is never sent to\nGoogle.\nCloud EKM key versions consist of these parts:\nExternal key material: The external key material of a Cloud EKM\nkey is cryptographic material created and stored in your EKM. This material\ndoes not leave your EKM and it is never shared with Google.\nKey reference: Each Cloud EKM key version contains either a key\nURI or a key path. This is a unique identifier for the external key material\nthat Cloud EKM uses when requesting cryptographic operations using\nthe key.\nInternal key material: When a symmetric Cloud EKM key is\ncreated, Cloud KMS creates additional key material in\nCloud KMS, which never leaves Cloud KMS. This key material\nis used as an extra layer of encryption when communicating with your EKM.\nThis internal key material does not apply to asymmetric signing keys.\nTo use your Cloud EKM keys, Cloud EKM sends requests for\ncryptographic operations to your EKM. For example, to encrypt data with a\nsymmetric encryption key, Cloud EKM first encrypts the data using the\ninternal key material. The encrypted data is included in a request to the EKM.\nThe EKM wraps the encrypted data in another layer of encryption using the\nexternal key material, and then returns the resulting ciphertext. Data encrypted\nusing a Cloud EKM key can't be decrypted without both the external key\nmaterial and the internal key material.\nCreating and managing Cloud EKM keys requires corresponding changes in\nboth Cloud KMS and the EKM.\n\nThese corresponding changes are handled differently formanually managed\nexternal keysand forcoordinated external keys. All external keys accessed over the\ninternet are manually managed. External keys accessed over a VPC network can be\nmanually managed or coordinated, depending on the EKM management mode of the EKM\nconnection. TheManualEKM management mode is used for manually\nmanaged keys. TheCloud KMSEKM management mode is used for coordinated external keys.\nFor more information about EKM management modes, seeManually managed external\nkeysandCoordinated external keyson this page.\nThe following diagram shows how Cloud KMS fits into the key management\nmodel. This diagram uses Compute Engine and BigQuery as two examples;\nyou can also seethe full list of services that support Cloud EKM\nkeys.\nYou can learn about theconsiderationsandrestrictionswhen using Cloud EKM.\nThis section provides a broad overview of how Cloud EKM works with a\nmanually managed external key.\nYou create or use an existing key in asupported external key management partner\nsystem. This key has a unique URI or key path.\nYou grant your Google Cloud project access to use the key,\nin the external key management partner system.\nIn your Google Cloud project, you create a Cloud EKM key\nversion, using the URI or key path for the externally managed key.\nMaintenance operations like key rotation must be manually managed between\nyour EKM and Cloud EKM. For example, key version rotation or\nkey version destruction operations need to be completed both directly in\nyour EKM and in Cloud KMS.\nWithin Google Cloud, the key appears alongside your other\nCloud KMS and Cloud HSM keys, with protection levelEXTERNALorEXTERNAL_VPC. The Cloud EKM key and the\nexternal key management partner key work together to protect your data. The external key\nmaterial is never exposed to Google.\nThis section provides an overview of how Cloud EKM works with\ncoordinated external keys.\nYouset up an EKM connection,\nsetting theEKM management modetoCloud KMS. During setup, you\nmust authorize your EKM to access your VPC network and authorize your\nGoogle Cloud project service account to access yourcrypto spacein\nyour EKM. Your EKM connection uses the hostname of your EKM and acrypto\nspace paththat identifies your resources within your EKM.\nYouset up an EKM connection,\nsetting theEKM management modetoCloud KMS. During setup, you\nmust authorize your EKM to access your VPC network and authorize your\nGoogle Cloud project service account to access yourcrypto spacein\nyour EKM. Your EKM connection uses the hostname of your EKM and acrypto\nspace paththat identifies your resources within your EKM.\nYoucreate an external keyin\nCloud KMS. When you create a Cloud EKM key using an EKM\nover VPC connection with theCloud KMSEKM management mode enabled, the\nfollowing steps take place automatically:Cloud EKM sends a key creation request to your EKM.Your EKM creates the requested key material. This external key material\nremains in the EKM and is never sent to Google.Your EKM returns a key path to Cloud EKM.Cloud EKM creates your Cloud EKM key version using the\nkey path provided by your EKM.\nYoucreate an external keyin\nCloud KMS. When you create a Cloud EKM key using an EKM\nover VPC connection with theCloud KMSEKM management mode enabled, the\nfollowing steps take place automatically:\nCloud EKM sends a key creation request to your EKM.\nYour EKM creates the requested key material. This external key material\nremains in the EKM and is never sent to Google.\nYour EKM returns a key path to Cloud EKM.\nCloud EKM creates your Cloud EKM key version using the\nkey path provided by your EKM.\nMaintenance operations on coordinated external keys can be initiated from\nCloud KMS. For example, coordinated external keys used for\nsymmetric encryption can be automatically rotated on a set schedule. The\ncreation of new key versions is coordinated in your EKM by\nCloud EKM. You can also trigger the creation or destruction of\nkey versions in your EKM from Cloud KMS using the\nGoogle Cloud console, the gcloud CLI, the Cloud KMS\nAPI, or Cloud KMS client libraries.\nMaintenance operations on coordinated external keys can be initiated from\nCloud KMS. For example, coordinated external keys used for\nsymmetric encryption can be automatically rotated on a set schedule. The\ncreation of new key versions is coordinated in your EKM by\nCloud EKM. You can also trigger the creation or destruction of\nkey versions in your EKM from Cloud KMS using the\nGoogle Cloud console, the gcloud CLI, the Cloud KMS\nAPI, or Cloud KMS client libraries.\nWithin Google Cloud, the key appears alongside your other\nCloud KMS and Cloud HSM keys, with protection levelEXTERNAL_VPC. The Cloud EKM key and the external key management partner key work\ntogether to protect your data. The external key material is never exposed to\nGoogle.\nCoordinated external keys are made possible by EKM connections that use EKM key management from\nCloud KMS.\nIf your EKM supports the Cloud EKM\ncontrol plane, then you can enable EKM key management from\nCloud KMS for your\nEKM connections to create coordinated external keys. With EKM key management from\nCloud KMS enabled,\nCloud EKM can request the following changes in your EKM:\nCreate a key: When you create an externally managed key in\nCloud KMS using a compatible EKM connection,\nCloud EKM sends your key creation request to your EKM. When\nsuccessful, your EKM creates the new key and key material and returns the\nkey path for Cloud EKM to use to access the key.\nCreate a key: When you create an externally managed key in\nCloud KMS using a compatible EKM connection,\nCloud EKM sends your key creation request to your EKM. When\nsuccessful, your EKM creates the new key and key material and returns the\nkey path for Cloud EKM to use to access the key.\nRotate a key: When you rotate an externally-managed key in\nCloud KMS using a compatible EKM connection,\nCloud EKM sends your rotation request to your EKM. When successful,\nyour EKM creates new key material and returns the key path for\nCloud EKM to use to access the new key version.\nRotate a key: When you rotate an externally-managed key in\nCloud KMS using a compatible EKM connection,\nCloud EKM sends your rotation request to your EKM. When successful,\nyour EKM creates new key material and returns the key path for\nCloud EKM to use to access the new key version.\nDestroy a key: When you destroy a key version for an externally-managed key\nin Cloud KMS using a compatible EKM connection,\nCloud KMS schedules the key version for destruction in\nCloud KMS. If the key version is not restored before thescheduled\nfor destructionperiod ends, Cloud EKM destroys its part of the\nkey's cryptographic material and sends a destruction request to your EKM.Data encrypted with this key version cannot be decrypted after the key\nversion is destroyed in Cloud KMS, even if the EKM has not yet\ndestroyed the key version. You can see whether the EKM has successfully\ndestroyed the key version by viewing the key's details in\nCloud KMS.\nDestroy a key: When you destroy a key version for an externally-managed key\nin Cloud KMS using a compatible EKM connection,\nCloud KMS schedules the key version for destruction in\nCloud KMS. If the key version is not restored before thescheduled\nfor destructionperiod ends, Cloud EKM destroys its part of the\nkey's cryptographic material and sends a destruction request to your EKM.\nData encrypted with this key version cannot be decrypted after the key\nversion is destroyed in Cloud KMS, even if the EKM has not yet\ndestroyed the key version. You can see whether the EKM has successfully\ndestroyed the key version by viewing the key's details in\nCloud KMS.\nWhen keys in your EKM are managed from Cloud KMS, the key material\nstill resides in your EKM. Google can't make any key management requests to your\nEKM without explicit permission.\n\nGoogle can't change permissions or Key Access Justifications policies in your\nexternal key management partner system.\n\nIf you revoke Google's permissions in your EKM, key management operations\nattempted in Cloud KMS fail.\nYou can store external keys in the following external key management partner systems:\nSupported today:FortanixFuturexThales\nFortanix\nFuturex\nThales\nThe following services support integration with Cloud KMS for\nexternal (Cloud EKM) keys:\nAgent Assist\nAlloyDB for PostgreSQL\nApigee API hub\nApplication Integration\nArtifact Registry\nBackup for GKE\nBigQuery\nBigtable\nCloud Composer\nCloud Data Fusion\nCloud Healthcare API\nCloud Logging:Data in the Log RouterandData in Logging storage\nCloud Run\nCloud Run functions\nCloud SQL\nCloud Storage\nCloud Tasks\nCloud Workstations\nCompute Engine:Persistent disks,Snapshots,Custom images,\n        \n      \n        \n          \n          andMachine images\nConversational Insights\nDatabase Migration Service:MySQL migrations - data written to databases,PostgreSQL migrations - Data written to databases,PostgreSQL to AlloyDB migrations - Data written to databases,SQL Server migrations - Data written to databases,\n        \n      \n        \n          \n          andOracle to PostgreSQL data at rest\nDataflow\nDataform\nDataplex\nDataproc:Dataproc clusters data on VM disksandDataproc serverless data on VM disks\nDataproc Metastore\nDialogflow CX\nDocument AI\nEventarc Standard\nFilestore\nFirestore\nGoogle Cloud Managed Service for Apache Kafka\nGoogle Distributed Cloud\nGoogle Kubernetes Engine:Data on VM disksandApplication-layer secrets\nIntegration Connectors(Preview)\nLooker (Google Cloud core)\nMemorystore for Redis\nMigrate to Virtual Machines:Data migrated from VMware, AWS, and Azure VM sourcesandData migrated from disk and machine image sources\nParameter Manager\nPub/Sub\nSecret Manager\nSecure Source Manager\nSpanner\nSpeaker ID\n    \n    \n  \n            (Restricted GA)\nSpeech-to-Text\nVertex AI\nVertex AI Workbench instances\nWorkflows\nWhen you use a Cloud EKM key, Google has no control over the\navailability of your externally managed key in the external key management partner system.\nIf you lose keys that you manage outside of Google Cloud, Google can't\nrecover your data.\nWhen you use a Cloud EKM key, Google has no control over the\navailability of your externally managed key in the external key management partner system.\nIf you lose keys that you manage outside of Google Cloud, Google can't\nrecover your data.\nReview the guidelines aboutexternal key management partners and regionswhen\nchoosing the locations for your Cloud EKM keys.\nReview the guidelines aboutexternal key management partners and regionswhen\nchoosing the locations for your Cloud EKM keys.\nReview theCloud EKM Service Level Agreement\n(SLA).\nReview theCloud EKM Service Level Agreement\n(SLA).\nCommunicating with an external service over the internet can lead to\nproblems with reliability, availability, and latency. For applications with\nlow tolerance for these types of risks, consider using Cloud HSM or\nCloud KMS to store your key material.If an external key is unavailable, Cloud KMS returns aFAILED_PRECONDITIONerror and provides details in thePreconditionFailureerror detail.Enable data audit loggingto maintain a record of all\nerrors related to Cloud EKM. Error messages contain detailed\ninformation to help pinpoint the source of the error. An example of a\ncommon error is when an external key management partner does not respond to a request\nwithin a reasonable timeframe.You need a support contract with the external key management partner.\nGoogle Cloud support can only help with issues in\nGoogle Cloud services and cannot directly assist with issues on\nexternal systems. Sometimes, you must work with support on both sides to\ntroubleshoot interoperability issues.\nCommunicating with an external service over the internet can lead to\nproblems with reliability, availability, and latency. For applications with\nlow tolerance for these types of risks, consider using Cloud HSM or\nCloud KMS to store your key material.\nIf an external key is unavailable, Cloud KMS returns aFAILED_PRECONDITIONerror and provides details in thePreconditionFailureerror detail.Enable data audit loggingto maintain a record of all\nerrors related to Cloud EKM. Error messages contain detailed\ninformation to help pinpoint the source of the error. An example of a\ncommon error is when an external key management partner does not respond to a request\nwithin a reasonable timeframe.\nIf an external key is unavailable, Cloud KMS returns aFAILED_PRECONDITIONerror and provides details in thePreconditionFailureerror detail.\nEnable data audit loggingto maintain a record of all\nerrors related to Cloud EKM. Error messages contain detailed\ninformation to help pinpoint the source of the error. An example of a\ncommon error is when an external key management partner does not respond to a request\nwithin a reasonable timeframe.\nYou need a support contract with the external key management partner.\nGoogle Cloud support can only help with issues in\nGoogle Cloud services and cannot directly assist with issues on\nexternal systems. Sometimes, you must work with support on both sides to\ntroubleshoot interoperability issues.\nYou need a support contract with the external key management partner.\nGoogle Cloud support can only help with issues in\nGoogle Cloud services and cannot directly assist with issues on\nexternal systems. Sometimes, you must work with support on both sides to\ntroubleshoot interoperability issues.\nCloud EKM can be used withBare Metal Rack HSMto create a single-tenant\nHSM solution integrated with Cloud KMS. To learn more, choose a\nCloud EKM partner that supports single-tenant HSMs and review therequirements for Bare Metal Rack HSMs.\nCloud EKM can be used withBare Metal Rack HSMto create a single-tenant\nHSM solution integrated with Cloud KMS. To learn more, choose a\nCloud EKM partner that supports single-tenant HSMs and review therequirements for Bare Metal Rack HSMs.\nEnable audit logging in your external key manager to capture access and\nusage to your EKM keys.\nEnable audit logging in your external key manager to capture access and\nusage to your EKM keys.\nWhen you create a Cloud EKM key using the API or the\nGoogle Cloud CLI, it must not have an initial key version. This does not\napply to Cloud EKM keys created using the\nGoogle Cloud console.\nAutomatic rotation is not supported for manually-managed external\nkeys.\nCloud EKM operations are subject tospecific\nquotasin addition to the quotas on Cloud KMS operations.\nSymmetric encryption keys are only supported for the following:Customer managed encryption keys (CMEK) insupported integration\nservices.Symmetric encryption and decryption using Cloud KMS\ndirectly.\nCustomer managed encryption keys (CMEK) insupported integration\nservices.\nSymmetric encryption and decryption using Cloud KMS\ndirectly.\nData that is encrypted by Cloud EKM using an externally managed key\ncannot be decrypted without using Cloud EKM.\nAsymmetric signing keys are limited to a subset of Cloud KMS\nalgorithms.\nAsymmetric signing keys are only supported for the following use cases:Asymmetric signing using Cloud KMS\ndirectly.Custom signing key with\nAccess Approval.\nAsymmetric signing using Cloud KMS\ndirectly.\nCustom signing key with\nAccess Approval.\nOnce an asymmetric signing algorithm is set on a Cloud EKM key, it\ncannot be modified.\nSigning must be done on thedatafield.\nCloud EKM needs to be able to reach your keys quickly to\navoid an error. When creating a Cloud EKM key, choose a\nGoogle Cloud location that is geographically near the location of the\nexternal key management partner key. See your external key management partner's documentation to determine\nwhich locations they support.\nCloud EKM over the internet: available in most Google Cloud\nlocations where Cloud KMS is available, including regional and\nmulti-regional locations.\nCloud EKM over a VPC: available in mostregional\nlocationswhere Cloud KMS is\navailable. Cloud EKM over a VPC isn't available in\nmulti-regional locations.\nSome locations includingglobalandnam-eur-asia1aren't available for\nCloud EKM. To learn which locations support Cloud EKM, seeCloud KMS locations.\nWhen you use an externally managed key with a multi-region, the metadata of the\nkey is available in multiple data centers within the multi-region. This metadata\nincludes the information needed to communicate with the external key management partner. If your\napplication fails over from one data center to another within the multi-region,\nthe new data center initiates key requests. The new data center may have\ndifferent network characteristics from the previous data center, including\ndistance from the external key management partner and the likelihood of timeouts. We recommend\nonly using a multi-region with Cloud EKM if your chosen external key\nmanager provides low latency to all areas of that multi-region.\nPartner-managed EKM lets you use Cloud EKM through a trusted sovereign\npartner that manages your EKM system for you. With partner-managed EKM, your\npartner creates and manages the keys that you\nuse in Cloud EKM. The partner ensures that your EKM complies with\nsovereignty requirements.\nWhen you onboard with your sovereign partner, the partner provisions resources\nin the Google Cloud and your EKM. These resources include a\nCloud KMS project to manage your Cloud EKM keys and an EKM\nconnection configured for EKM key management from\nCloud KMS. Your partner creates resources\nin Google Cloud locations according to your data residency requirements.\nEach Cloud EKM key includes\nCloud KMS metadata, which lets Cloud EKM send requests to your\nEKM to perform cryptographic operations using the external key material that\nnever leaves your EKM. Symmetric Cloud EKM keys also include\nCloud KMS internal key material that never leaves Google Cloud.\nFor more information about the internal and external sides of Cloud EKM\nkeys, seeHow Cloud EKM workson this page.\nFor more information about partner-managed EKM, seeConfigure partner-managed\nCloud KMS.\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nYou can use Cloud Monitoring to monitor your EKM connection. The following\nmetrics can help you understand your EKM usage:\ncloudkms.googleapis.com/ekm/external/request_latencies\ncloudkms.googleapis.com/ekm/external/request_count\nFor more information about these metrics, seecloudkms metrics. You can create a\ndashboard to track these metrics. To learn how to set up a dashboard to monitor\nyour EKM connection, seeMonitor EKM usage.\nIf you experience an issue with Cloud EKM, contactSupport.\nStartusing the\nAPI.\nStartusing the\nAPI.\nSet up Cloud EKM over the\ninternet.\nSet up Cloud EKM over the\ninternet.\nCreate an EKM connectionto use EKM over\nVPC.\nCreate an EKM connectionto use EKM over\nVPC.\nRead through theCloud KMS API\nReference.\nRead through theCloud KMS API\nReference.\nLearn aboutLoggingin Cloud KMS. Logging is based\non operations, and applies to keys with both HSM and software protection\nlevels.\nLearn aboutLoggingin Cloud KMS. Logging is based\non operations, and applies to keys with both HSM and software protection\nlevels.\nSeeReference architectures for reliable deployment of Cloud EKM\nservicesfor recommendations on configuring an External Key\nManager (EKM) service deployment integrated\nwith Cloud EKM.\nSeeReference architectures for reliable deployment of Cloud EKM\nservicesfor recommendations on configuring an External Key\nManager (EKM) service deployment integrated\nwith Cloud EKM.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nDocumentation\nThis document provides information about committed use discounts (CUDs) for\nGoogle Cloud and how they apply for various services.\nCUDs provide discounted prices for eligible Google Cloud resources in\nexchange for purchasing committed use contracts (also known as commitments).\nWhen you purchase Google Cloud commitments, you commit to either using a\nminimum level of resources or spending a minimum amount, for a term duration of\none or three years.\nCUDs for Google Cloud are broadly available as spend-based or\nresource-based CUDs and cover a wide range of services and resources.\nYou can choose the type of commitment that you want to purchase depending on\nthe service that you use and whether you have predictable or unpredictable\nresource needs.\nSpend-based CUDs provide a discount in exchange for your\ncommitment to spend a minimum amount for any of the services listed in this\nsection. The discount applies to the set of eligible resources for each service.\nWith spend-based commitments, you commit to spending on eligible resources or\nservices that are worth a specified minimum amount of on-demand price, per hour,\nthroughout the commitment's term of one or three years. In return, you receive\ndiscounted rate on the minimum spend amount that you commit to. Any overage\nusage that takes your hourly spend amount over your committed amount is\ncharged at the on-demand rate.\nSpend-based CUDs apply to eligible usage in any projects that the\nCloud Billing account pays for.\nDepending on the Google Cloud service that you use, you can purchase\nspend-based commitments in one of the following ways:\nCompute flexible commitments\nService-specific spend-based commitments\nCompute flexible commitments are ideal for scenarios where you have predictable\nGoogle Cloud spend needs that span usage beyond a single service. Compute\nFlexible CUDs apply to your spend across one or more of the following services:\nCompute Engine\nGoogle Kubernetes Engine\nCloud Run\nYou can purchase a single flexible commitment to cover your eligible spend\nacross all three services. For more information about how Compute flexible CUDs\napply to each service, see the service-specific CUDs documentation:\nCUDs documentation for Compute Engine\nCUDs documentation for GKE\nCUDs documentation for Cloud Run\nThese commitments are ideal for scenarios where you have predictable spend needs\nwithin a Google Cloud service. You must purchase these commitments\nseparately for each Google Cloud service and CUDs from these commitments\napply only to spend within that service. You can get spend-based CUDs for the\nfollowing Google Cloud services:\nAlloyDB for PostgreSQL\nBackup and DR Service\nBackup for GKE\nBigQuery\nBigtable\nCloud Run\nDataflow\nFirestore\nSpanner\nCloud SQL\nGoogle Cloud Managed Service for Apache Kafka\nGoogle Cloud NetApp Volumes\nGoogle Cloud VMware Engine\nGoogle Kubernetes Engine (Autopilot)\nMemorystore\nAlloyDB for PostgreSQL committed use discounts are spend-based CUDs that apply\nto all AlloyDB for PostgreSQL instance vCPU and memory usage. The discount applies\nto AlloyDB for PostgreSQL instances in any project or region that is associated\nwith a single Cloud Billing account.\nAlloyDB for PostgreSQL commitmentsdo not applyto storage, backup, and network\negress.\nFor current rates, seeAlloyDB for PostgreSQL pricing.\nBackup and DR (for VMware Engine) committed use discountsapply to the combined node usage in a region. This gives you low, predictable costs,\nwithout the need to make any manual changes or updates yourself. They apply to\nVMware Engine backups in the regions where the service is\navailable and to which you have committed.\nBackup and DR (for protecting Oracle databases into a backup vault) committed\nuse discountsapply to\naggregate Oracle database protection into backup vault usage. This gives you\nlow, predictable costs, without the need to make any manual changes or updates\nyourself. This flexibility saves you time and helps you to save more by\nachieving high utilization rates across your commitments.\nFor current rates, seeBackup and DR pricing.\nBackup for GKE committed use discountsare spend-based CUDs that apply to Backup for GKE backup management, that is\nPods per plan in a region. Committed use discounts don't apply to backup storage\n(GiB) or inter-region data transfer (GiB).\nFor current rates, seeBackup for GKE CUDs pricing.\nBigQuery CUDsprovide discounted\nprices in exchange for your commitment to use BigQuery PAYG\ncompute capacity for a one- or three-year term, with the commitment fee billed\nmonthly. BigQuery CUDs apply to all compute types.\nFor current rates, seeBigQuery pricing.\nAs a Bigtable customer, you can purchase a commitment to receive a\ncommitted use discount on the price of Bigtable nodes. The\ndiscount applies to nodes in any project or region that is associated with a\nsingle Cloud Billing account. The discount does not apply to the cost of\nstorage, backup storage, or network egress. For details, seeBigtable committed use\ndiscounts. For current rates, seeBigtable pricing.\nCloud Run committed use discountsapply to all aggregated\nCloud Run CPU, memory, and request usage in a region, giving you\nlow, predictable costs when your code is running in one of the supported\ncontainer ecosystems. Cloud Run commitmentsdo not applyto\nnetworking changes.\nFor current rates, see theCloud Run pricing details.\nDataflow committed use discountsapply to your\nspending on the Dataflow compute capacity used by streaming jobs across\nprojects. The discount applies to any eligible usage in Dataflow\nprojects associated with the Cloud Billing account used to purchase the\ncommitment, regardless of instance configuration or region. All CUDs apply to\nboth regional and multi-region configurations.\nThe discount doesn't apply to the cost of worker CPU and memory for batch and\nFlexRS jobs, Dataflow Shuffle data processed, Data Compute Units (DCUs)\nfor batch jobs, Persistent Disk storage, GPUs, snapshots, and confidential VMs.\nFor current rates and other details, seeDataflow pricing.\nAs a Firestore customer, you can purchase a commitment to receive a\ncommitted use discount on the price of Firestore\nRead/Write/Delete operations. The discount applies to Read/Write/Delete\noperations in any project or region that is associated with a single\nCloud Billing account. The discount doesn't apply to any other\nFirestore resource. For details, seeFirestore committed use discounts.\nFor current rates, seeFirestore pricing.\nSpanner committed use discountsapply to\nall Spanner compute capacity associated with a single\nCloud Billing account, regardless of region. This includes all\ninstances in all projects, whether configured as single-region or\nmulti-region instances.\nSpanner CUDs do not apply to Spanner storage, backup,\nor network egress.\nFor current rates and other details, seeSpanner\npricing.\nCloud SQL committed use discountsprovide you the flexibility to use\nany machine shapes with the supported Cloud SQL database engines,\nwithout having to modify your commitments. They\napply to all Cloud SQL database instance vCPU\nand memory usage for the service in the region you purchased the\ncommitments,exceptshared CPU machine types (such as db-f1-micro and\ndb-g1-small). The commitments apply to usage from all supported database\nengines, such as MySQL, PostgreSQL, and SQL Server.\nCloud SQL commitmentsdo not applyto persistent disk snapshots,\nstorage, IP addresses, network egress, or licensing.\nFor current rates, seeCloud SQL pricing.\nManaged Service for Apache Kafka committed use discounts (CUDs)are discounts that apply to the Kafka compute (vCPU and RAM) costs for running\nManaged Service for Apache Kafka clusters across projects. You can apply\nthe CUDs to any Managed Service for Apache Kafka project that is\nassociated with the Cloud Billing account used to purchase the commitment.\nYou can apply CUDs to any available region. You cannot apply the\nManaged Service for Apache Kafka CUDs to the cost of storage, networking or\nPrivate Service Connect.\nFor current rates and other details, seeManaged Service for Apache Kafka\npricing.\nGoogle Cloud NetApp Volumes committed use discountsapply to the aggregate storage capacity on Flex, Standard, Premium, and Extreme\nservice levels and regions at your billing account level. CUDs can keep your\nstorage costs low when you have predictable storage needs.\nNetApp Volumes CUDs are'nt available for volume replications and\nbackups.\nFor current rates, seeNetApp Volumes pricing.\nVMware Engine committed use discountsapply to aggregate\nVMware Engine node usage in a region, giving you low,\npredictable costs, without the need to make any manual changes or updates\nyourself. They apply to VMware Engine node CPU and memory\nusage in the regions where the service is available and you have committed.\nCurrent rates and supported regions for Google Cloud VMware Engine\nCUDs are detailed on theVMware Engine pricing page.\nGoogle Kubernetes Engine (Autopilot Mode) committed use discountsapply to all Autopilot Pod workload vCPU, memory, and ephemeral storage\nusage in the region in which you have purchased commitments. Google Kubernetes Engine\n(Autopilot Mode) CUDs don't apply to the cluster management fee or to\nGKE Standard mode compute nodes.\nFor current rates, see theGoogle Kubernetes Engine pricing details.\nMemorystore CUDs apply to Memorystore for Valkey, Memorystore for Redis Cluster,\nMemorystore for Redis, and Memorystore for Memcached usage. A Memorystore\nCUD gives you the flexibility to use Valkey, Redis Cluster, Redis, or\nMemcached instance spending toward a commitment on a single Cloud Billing\naccount.\nMemorystore commitmentsdon't applyto Cloud Storage\nstorage for backups, persistence, network egress, or Memorystore for Redis M1\ncapacity tier\ninstances (less than 5 GB).\nFor current rates, see the following pages:\nMemorystore for Valkey committed use discounts\nMemorystore for Redis Cluster committed use discounts\nMemorystore for Redis committed use discounts\nMemorystore for Memcached committed use discounts\nResource-based CUDs are available only for Compute Engine.Compute Engine resource-based CUDsprovide discounts\nin exchange for committing to using a minimum amount of Compute Engine\nresources in a specific region and a project. These CUDs are ideal when you\nhave predictable resource needs. You can purchase Compute Engine\nresource-based commitments for the following hardware and software resources\nfor a term duration of one or three years:\nvCPUs\nMemory\nGPUs\nLocal SSD disks\nSole-tenant nodes\nOperating system (OS) licenses\nCommitments for hardware resources are separate from the ones for OS licenses.\nYou can purchase both categories of commitments for the same virtual machine\n(VM) instance, but you cannot purchase a single commitment that covers both\nhardware resources and licenses.\nFor current rates, seeVM instance pricing.\nThe following table demonstrates the differences betweenspend-basedandresource-basedCUDs, using Compute Engine as the example service.\nResource-basedcommitments\n        are purchased in the context of an individual project, rather than that\n        of a Cloud Billing account.\nYou can enablediscount sharingso that Compute Engine resource-based CUDs are shared\n      across  all projects that are paid for by the same Cloud Billing\n      account.\nYou can change the\n      Cloud Billing account that pays for the project\n      where you purchased the resource-based commitments.Learn about changing the Cloud Billing account for projects.\nThe following table summarizes the differences between resource-based CUDs and\nspend-based CUDs.\nCloud Run\nCompute Engine\nGoogle Kubernetes Engine\nAlloyDB for PostgreSQL\nBackup and DR Service\nBackup for GKE\nBigtable\nCloud Run\nCloud SQL\nDataflow\nFirestore\nGoogle Cloud NetApp Volumes\nGoogle Cloud VMware Engine\nGoogle Kubernetes Engine (Autopilot)\nMemorystore\nSpanner\nResource-based (available for vCPUs, memory, Local SSD disks and GPUs).\nYou commit to purchasing a minimum amount of eligible resources.\nSpend-based (for example, $100/hour).\nYou commit to spending a minimum dollar amount per hour of equivalent on-demand spend on eligible services.\nThese CUDs apply to a specific project by default, but you canshare themacross all projects linked to the same billing account.\nYou can buy spend-based commitments measured in dollars per hour of equivalent on-demand spend. You buy these commitments at the billing account level and they apply to eligible usage in any project linked to that billing account, across all regions.\nCompute Engine\nAlloyDB for PostgreSQL\nBackup and DR Service\nBackup for GKE\nBigtable\nCloud Run\nCloud SQL\nCompute Engine\nDataflow\nFirestore\nGoogle Cloud NetApp Volumes\nGoogle Cloud VMware Engine\nGoogle Kubernetes Engine (Autopilot)\nMemorystore\nSpanner\nTo view your spend-based and resource-based CUDs in the dashboard, complete the\nfollowing steps:\nIn the Google Cloud console, open theCommitted use discounts (CUDs)page forBilling.Go to Committed use discounts (CUDs)\nIn the Google Cloud console, open theCommitted use discounts (CUDs)page forBilling.\nGo to Committed use discounts (CUDs)\nAt the prompt, choose the Cloud Billing account for which you want\nto view commitments.\nAt the prompt, choose the Cloud Billing account for which you want\nto view commitments.\nTheCommitted use discounts (CUDs)dashboard displays a list of all the\ncommitments, across services, that are associated with your Cloud Billing\naccount. You can view which of your commitments are expiring in the next 30 days.\nYou can also specify yourresource-basedcommitments to automatically renew\nat the end of their ongoing terms by changing the setting in theAuto-renewcolumn.\nFor information about viewingspend-basedCUDs, seeViewing spend-based commitments.\nFor information about viewingresource-basedCUDs, seeViewing resource-based commitments.\nYou are billed a monthly fee for the commitments you purchase. This fee is\ncalculated when you purchase the commitments, based on the list\nprice on the date you made the purchase. This monthly fee applies\nto your purchased commitments for the entire duration of the commitment period.\nFuture changes to the list prices don't affect your commitment fee during\nthe commitment period.\nTo understand how your commitment fees and credits are applied to your\nCloud Billing account and projects, seeAttribution of committed use discount fees and credits.\nPricing for CUDs is unique for each Google Cloud\nproduct:\nBackup and DR Service\nBackup for GKE\nBigtable CUD pricing\nCloud Run pricing details\nCloud SQL pricing\nFirestore CUD pricing\nSpanner CUD pricing\nCompute Engine pricing\nGoogle Cloud VMware Engine pricing\nGoogle Kubernetes Engine (Autopilot Mode) pricing details\nMemorystore for Memcached CUD pricing\nMemorystore for Redis CUD pricing\nTo purchasespend-basedcommitments, seePurchasing spend-based commitments.\nTo purchaseresource-basedcommitments for Compute Engine,\nsee one of the following, depending on your use case:\nPurchase commitments without attached reservations\nPurchase commitments with attached reservations\nPurchase commitments for software licenses\nLearn how topurchase spend-based CUDs.\nLearn more aboutresource-based CUDs for Compute Engine.\nLearn how toview your CUDs reports.\nLearn how toview your Cloud Billing reports and cost trends.\nUnderstand your savings with cost breakdown reports.\nLearn how toexport Cloud Billing data to BigQuery.\nLearn how toview your cost and payment history.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-30 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/databases",
    "title": "Databases",
    "content": "Home\nDocumentation\nMigrate and manage enterprise data with security, reliability, high availability, and fully-managed data services.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExpand this section to see relevant products and documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/aws",
    "title": "GKE on AWS documentation",
    "content": "Home\nGoogle Kubernetes Engine (GKE)\nGKE Enterprise\nClusters\nDocumentation\nGKE Multi-Cloud\nGKE on AWS\nGKE on AWS lets you manage\n    GKE clusters running on AWS infrastructure through the GKE Multi-Cloud API.\n    Combined withConnect,\n   GKE on AWS lets you manage\n    GKE clusters on both Google Cloud and AWS from the\n    Google Cloud console.Learn more about how GKE on AWS works.\nWhen you create a cluster with GKE on AWS, Google creates the AWS\n    resources you need and brings up a cluster on your behalf. You can then deploy your\n   workloads with thegcloudandkubectlcommand-line tools.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nGKE on AWS architecture\nGKE on AWS architecture\nQuickstart: Deploy an application\nQuickstart: Deploy an application\nPrerequisites\nPrerequisites\nCreate a cluster\nCreate a cluster\nCreate a cluster with Terraform\nCreate a cluster with Terraform\nCreate AWS IAM roles\nCreate AWS IAM roles\nSupported AWS instance types\nSupported AWS instance types\nSupported AWS regions\nSupported AWS regions\ngcloud commands\ngcloud commands\nAWS IAM role list\nAWS IAM role list\nTroubleshooting GKE on AWS\nTroubleshooting GKE on AWS\nGet support\nGet support\nRelease notes\nRelease notes\nQuotas and limits\nQuotas and limits\nContact us for GKE on AWS pricing information\nContact us for GKE on AWS pricing information\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models",
    "title": "Vertex AI partner models for MaaSStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nVertex AI supports a curated list of models developed by Google partners.\nPartner models can be used withVertex AIas a model\nas a service (MaaS) and are offered as a managed API. When you use a partner\nmodel, you continue to send your requests to Vertex AI endpoints. Partner\nmodels are serverless so there's no need to provision or manage infrastructure.\nPartner models can be discovered using Model Garden. You can also\ndeploy models using Model Garden. For more information, seeExplore AI models in\nModel Garden.\nWhile information about each available partner model can be found on its model\ncard in Model Garden, only third-party models that perform as a\nMaaS with Vertex AI are documented in this guide.\nAnthropic's Claude and Mistral models are examples of third-party managed models\nthat are available to use on Vertex AI.\nGoogle offers provisioned throughput for some partner models that reserves\nthroughput capacity for your models for a fixed fee. You decide on the\nthroughput capacity and in which regions to reserve that capacity. Because\nprovisioned throughput requests are prioritized over the standard pay-as-you-go\nrequests, provisioned throughput provides increased availability. When the\nsystem is overloaded, your requests can still be completed as long as the\nthroughput remains under your reserved throughput capacity. For more information\nor to subscribe to the service,Contact sales.\nFor regional endpoints, requests are served from your specified region. In cases\nwhere you have data residency requirements or if a model doesn't support the\nglobal endpoint, use the regional endpoints.\nWhen you use the global endpoint (Preview),\nGoogle can process and serve your requests from any region that is supported by\nthe model that you are using, which might result in higher latency in some\ncases. The global endpoint helps improve overall availability and helps reduce\nerrors.\nThere is no price difference with the regional endpoints when you use the global\nendpoint. However, the global endpoint quotas and supported model capabilities\ncan differ from the regional endpoints. For more information, view the\nrelated third-party model page.\nTo use the global endpoint, set the region toglobal.\nFor example, the request URL for a curl command uses the following format:https://aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/global/publishers/PUBLISHER_NAME/models/MODEL_NAME\nFor the Vertex AI SDK, a regional endpoint is the default. Set the\nregion toGLOBALto use the global endpoint.\nThe global endpoint is available for the following models:\nClaude Sonnet 4\nClaude 3.7 Sonnet\nClaude 3.5 Sonnet v2\nFor you to enable partner models and make a prompt request, a Google Cloud\nadministrator mustset the required permissionsandverify\nthe organization policy allows the use of required\nAPIs.\nThe following roles and permissions are required to use partner models:\nYou must have the Consumer Procurement Entitlement Manager\nIdentity and Access Management (IAM) role. Anyone who's been granted this role can\nenable partner models in Model Garden.\nYou must have the Consumer Procurement Entitlement Manager\nIdentity and Access Management (IAM) role. Anyone who's been granted this role can\nenable partner models in Model Garden.\nYou must have theaiplatform.endpoints.predictpermission. This\npermission is included in the Vertex AI User IAM role.\nFor more information, seeVertex AI\nUserandAccess\ncontrol.\nYou must have theaiplatform.endpoints.predictpermission. This\npermission is included in the Vertex AI User IAM role.\nFor more information, seeVertex AI\nUserandAccess\ncontrol.\nTo grant the Consumer Procurement Entitlement Manager IAM\nroles to a user, go to theIAMpage.Go to IAM\nTo grant the Consumer Procurement Entitlement Manager IAM\nroles to a user, go to theIAMpage.\nGo to IAM\nIn thePrincipalcolumn, find the userprincipalfor which you\nwant to enable access to partner models, and then clickeditEdit principalin that row.\nIn thePrincipalcolumn, find the userprincipalfor which you\nwant to enable access to partner models, and then clickeditEdit principalin that row.\nIn theEdit accesspane, clickaddAdd another role.\nIn theEdit accesspane, clickaddAdd another role.\nInSelect a role, selectConsumer Procurement Entitlement Manager.\nInSelect a role, selectConsumer Procurement Entitlement Manager.\nIn theEdit accesspane, clickaddAdd another role.\nIn theEdit accesspane, clickaddAdd another role.\nInSelect a role, selectVertex AI User.\nInSelect a role, selectVertex AI User.\nClickSave.\nClickSave.\nIn the Google Cloud console, activate Cloud Shell.Activate Cloud Shell\nIn the Google Cloud console, activate Cloud Shell.\nActivate Cloud Shell\nGrant the Consumer Procurement Entitlement Manager role that's required\nto enable partner models in Model Gardengcloudprojectsadd-iam-policy-bindingPROJECT_ID\\--member=PRINCIPAL--role=roles/consumerprocurement.entitlementManager\nGrant the Consumer Procurement Entitlement Manager role that's required\nto enable partner models in Model Garden\nGrant the Vertex AI User role that includes theaiplatform.endpoints.predictpermission which is required to make\nprompt requests:gcloudprojectsadd-iam-policy-bindingPROJECT_ID\\--member=PRINCIPAL--role=roles/aiplatform.userReplacePRINCIPALwith the identifier for\nthe principal. The identifier takes the formuser|group|serviceAccount:emailordomain:domain—for\nexample,user:cloudysanfrancisco@gmail.com,group:admins@example.com,serviceAccount:test123@example.domain.com, ordomain:example.domain.com.The output is a list of policy bindings that includes the following:- members:\n  - user:PRINCIPALrole: roles/roles/consumerprocurement.entitlementManagerFor more information, seeGrant a single roleandgcloud projects add-iam-policy-binding.\nGrant the Vertex AI User role that includes theaiplatform.endpoints.predictpermission which is required to make\nprompt requests:\nReplacePRINCIPALwith the identifier for\nthe principal. The identifier takes the formuser|group|serviceAccount:emailordomain:domain—for\nexample,user:cloudysanfrancisco@gmail.com,group:admins@example.com,serviceAccount:test123@example.domain.com, ordomain:example.domain.com.\nThe output is a list of policy bindings that includes the following:\nFor more information, seeGrant a single roleandgcloud projects add-iam-policy-binding.\nTo enable partner models, your organization policy must allow the following\nAPI: Cloud Commerce Consumer Procurement API -cloudcommerceconsumerprocurement.googleapis.com\nIf your organization sets an organization policy torestrict service usage,\nthen an organization administrator must verify thatcloudcommerceconsumerprocurement.googleapis.comis allowed bysetting the organization policy.\nAlso, if you have an organization policy that restricts model usage in\nModel Garden, the policy must allow access to partner models. For more\ninformation, seeControl model\naccess.\nThecertificationsforGenerative AI on Vertex AIcontinue to\napply when partner models are used as a managed API using Vertex AI.\nIf you need details about the models themselves, additional information can be\nfound in the respective Model Card, or you can contact the respective model\npublisher.\nYour data is stored at rest within the selected region or multi-region for\npartner models on Vertex AI, but the regionalization of data\nprocessing may vary. For a detailed list of partner models' data processing\ncommitments, seeData residency for partner\nmodels.\nCustomer prompts and model responses are not shared with third-parties when\nusing the Vertex AI API, including partner models. Google only processes\nCustomer Data as instructed by the Customer, which is further described in ourCloud Data Processing Addendum.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-22 UTC."
  },
  {
    "url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "content": "Home\nDocument AI\nDocumentation\nGuides\nThis page contains detailed information on all processors offered by\nDocument AI. You can see a list of all processors by solution type.\nAll Document AI processors adhere to theData Processing and Security Terms.\nRefer to theManaging processor versionsdocumentation for more details. Also, specific processor limits apply in addition\nto overall productquotas and limits.\nIdentify and extract text in different types of documents.\nThis processor allows you to identify and extract text, including handwritten text, from documents in more than 200 languages. The processor also uses machine learning to perform a quality assessment of a document based on the readability of its content.\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nFor more information, seeManaging processor versions.\nasia-south1\nasia-southeast1\naustralia-southeast1\neu\neurope-west2\neurope-west3\nnorthamerica-northeast1\nus\nRefer toSample datasetsfor sample labeled and unlabeled datasets to use for training.\nExtract fields from documents using generative AI or custom models; fine tune models to accurately extract data from your documents.\nIf using generative AI for extraction, then:Only the English language is officially supported.Region availability is in theUS,EU,northamerica-northeast1andasia-southeast1.\nIf using generative AI for extraction, then:Only the English language is officially supported.Region availability is in theUS,EU,northamerica-northeast1andasia-southeast1.\nOnly the English language is officially supported.\nRegion availability is in theUS,EU,northamerica-northeast1andasia-southeast1.\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can find more information in theEnrichment & normalization, andCreate datasetpages.\ndateTime asSTRING\ncurrency asSTRING\nmoney asgoogle.type.Money\nnumber asFLOATorINTEGER\nasia-south1\nasia-southeast1\naustralia-southeast1\neu\neurope-west2\neurope-west3\nnorthamerica-northeast1\nus\nExtract general key-value pairs (entity and checkbox), tables, and generic entities from documents in addition to OCR text.\nThis processor applies advanced machine learning technologies to extract key-value pairs, checkboxes, and tables from documents more than 200 languages. This processor also leverages deep learning models to extract 11 generic entities that are common in various document types.\nNone\nNone\nemail\nphone\nurl\ndate_time\naddress\nperson\norganization\nquantity\nprice\nid\npage_number\nNone\nNone\nNone\nFor more information, seeManaging processor versions.\nasia-south1\nasia-southeast1\naustralia-southeast1\neu\neurope-west2\neurope-west3\nnorthamerica-northeast1\nus\nExtracts document content elements (text, tables, and lists) and creates context-aware chunks.\nLayout Parser extracts document content elements like text, tables, and lists, and creates context-aware chunks that facilitate information retrieval in generative AI and discovery applications.\nThis parser supports PDF, HTML and DOCX files.\nNone\nNone\nFor more information, seeManaging processor versions.\neu\nus\nExtract from bank statements including name, account, transactions, etc.\nIf a page of a multi-page input file is the correct document type and one of the supported versions, the processor performs entity extraction on the first supported document. If the processor doesn't find any applicable documents in the input file, the processor returns an error message.\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\naccount_number\naccount_type\nbank_address\nbank_name\nclient_address\nclient_name\nending_balance\nstarting_balance\nstatement_date\nstatement_end_date\nstatement_start_date\ntable_item\ntable_item/transaction_deposit\ntable_item/transaction_deposit_date\ntable_item/transaction_deposit_description\ntable_item/transaction_withdrawal\ntable_item/transaction_withdrawal_date\ntable_item/transaction_withdrawal_description\nYou can find more information in theEnrichment & normalizationpage.\nbank_address\nbank_name\nYou can find more information in theEnrichment & normalizationpage.\nending_balance\nstarting_balance\nstatement_date\nstatement_end_date\nstatement_start_date\ntable_item/transaction_deposit\ntable_item/transaction_deposit_date\ntable_item/transaction_withdrawal\ntable_item/transaction_withdrawal_date\neu\nus\nExtract from Form W2, including employee, employer, wages, etc.\nIf a page of a multi-page input file is the correct document type and one of the supported versions, the processor performs entity extraction on the first supported document. If the processor doesn't find any applicable documents in the input file, the processor returns an error message.\n2020 (standard and customized versions)\n2019 (standard and customized versions)\n2018 (standard and customized versions)\nNone\nNone\nNone\nNone\nAllocatedTips\nControlNumber\nDependentCareBenefits\nEIN\nEmployeeAddress\nEmployeeName\nEmployerNameAndAddress\nEmployerStateIdNumber_Line1\nFederalIncomeTaxWithheld\nFormYear\nLocalIncomeTax_Line1\nLocalityName_Line1\nLocalWagesTipsEtc_Line1\nMedicareTaxWithheld\nMedicareWagesAndTips\nNonqualifiedPlans\nSocialSecurityTaxWithheld\nSocialSecurityTips\nSocialSecurityWages\nSSN\nState_Line1\nStateIncomeTax_Line1\nStateWagesTipsEtc_Line1\nWagesTipsOtherCompensation\nNone\nQuality improvements and supporting new fields; does not include splitter.\nAllocatedTips\nControlNumber\nDependentCareBenefits\nEIN\nEmployeeAddress_AdditionalStreetAddressOrPostalBox\nEmployeeAddress_City\nEmployeeAddress_State\nEmployeeAddress_StreetAddressOrPostalBox\nEmployeeAddress_Zip\nEmployeeName_FirstName\nEmployeeName_LastName\nEmployeeName_MiddleNameOrInitial\nEmployerAddress_AdditionalStreetAddressOrPostalBox\nEmployerAddress_City\nEmployerAddress_State\nEmployerAddress_StreetAddressOrPostalBox\nEmployerAddress_Zip\nEmployerName\nEmployerStateIdNumber_Line1\nFederalIncomeTaxWithheld\nFormYear\nLocalIncomeTax_Line1\nLocalWagesTipsEtc_Line1\nLocalityName_Line1\nMedicareTaxWithheld\nMedicareWagesAndTips\nNonqualifiedPlans\nSSN\nSocialSecurityTaxWithheld\nSocialSecurityTips\nSocialSecurityWages\nStateIncomeTax_Line1\nStateWagesTipsEtc_Line1\nState_Line1\nWagesTipsOtherCompensation\na_Code\na_Value\nb_Code\nb_Value\nc_Code\nc_Value\nd_Code\nd_Value\nNone\nQuality improvements and support for box 12 fields and fine-grained predictions ofEmployeeName,EmployeeAddress, andEmployerNameAndAddress, all of which are no longer part of the output and are replaced with additional fields.\nAllocatedTips\nControlNumber\nDependentCareBenefits\nEIN\nEmployeeAddress_AdditionalStreetAddressOrPostalBox\nEmployeeAddress_City\nEmployeeAddress_State\nEmployeeAddress_StreetAddressOrPostalBox\nEmployeeAddress_Zip\nEmployeeName_FirstName\nEmployeeName_LastName\nEmployeeName_MiddleNameOrInitial\nEmployeeName_Suffix\nEmployerAddress_AdditionalStreetAddressOrPostalBox\nEmployerAddress_City\nEmployerAddress_State\nEmployerAddress_StreetAddressOrPostalBox\nEmployerAddress_Zip\nEmployerName\nEmployerStateIdNumber_Line1\nFederalIncomeTaxWithheld\nFormYear\nLocalIncomeTax_Line1\nLocalWagesTipsEtc_Line1\nLocalityName_Line1\nMedicareTaxWithheld\nMedicareWagesAndTips\nNonqualifiedPlans\nSSN\nSocialSecurityTaxWithheld\nSocialSecurityTips\nSocialSecurityWages\nStateIncomeTax_Line1\nStateWagesTipsEtc_Line1\nState_Line1\nWagesTipsOtherCompensation\na_Code\na_Value\nb_Code\nb_Value\nc_Code\nc_Value\nd_Code\nd_Value\nNone\nSimilar to versionpretrained-w2-v2.0-2022-03-30with further quality enhancements and introducing one more entityEmployeeName_Suffix.\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\nControlNumber\nEIN\nEmployeeAddress\nEmployeeName\nEmployerNameAndAddress\nFederalIncomeTaxWithheld\nMedicareTaxWithheld\nMedicareWagesAndTips\nSSN\nSocialSecurityTaxWithheld\nSocialSecurityWages\nWagesTipsOtherCompensation\nYou can find more information in theEnrichment & normalizationpage.\nEmployerNameAndAddress\nEIN\neu\nus\nExtract fields such as names, document ID, date of birth, etc.\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\nFamily Name\nGiven Names\nDocument Id\nExpiration Date\nDate Of Birth\nIssue Date\nMRZ Code\nPortrait\nYou can find more information in theEnrichment & normalizationpage.\nDate Of Birth\nExpiration Date\nIssue Date\neu\nus\nExtract text and values from utility bills such as supplier name and previous paid amount.\nNone\nNone\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\nadjusted_amount\namount_due\nbalance_transfer_amount\ncarrier\ncurrency\ncurrency_exchange_rate\ndelivery_date\ndeposit_credited_amount\ndue_date\nfreight_amount\ninvoice_date\ninvoice_id\nlate_fee_amount\nline_item\nline_item/amount\nline_item/description\nline_item/frequency\nline_item/product_code\nline_item/purchase_order\nline_item/quantity\nline_item/service_address\nline_item/service_end_date\nline_item/service_id_1\nline_item/service_id_2\nline_item/service_start_date\nline_item/supplier_account_number\nline_item/tax_amount\nline_item/unit_number\nline_item/unit_of_measure\nline_item/unit_price\nline_item/usage\nnet_amount\npayment_terms\nprior_amount_due\nprior_paid_amount\npurchase_order\nreceiver_address\nreceiver_email\nreceiver_name\nreceiver_phone\nreceiver_tax_id\nreceiver_website\nreclaimed_water\nremit_to_address\nremit_to_name\nservice\nservice/service_end_date\nservice/service_id\nservice/service_start_date\nservice/unit_of_measure\nservice/usage\nservice_address\nservice_end_date\nservice_id\nservice_start_date\nship_from_address\nship_from_name\nship_to_address\nship_to_name\nsupplier_account_number\nsupplier_address\nsupplier_email\nsupplier_iban\nsupplier_name\nsupplier_payment_ref\nsupplier_phone\nsupplier_registration\nsupplier_tax_id\nsupplier_website\ntampering\ntotal_amount\ntotal_tax_amount\nusage\nvat\nvat/amount\nvat/category_code\nvat/tax_amount\nvat/tax_rate\nYou can find more information in theEnrichment & normalizationpage.\nadjusted_amount\namount_due\nbalance_transfer_amount\ncurrency\ncurrency_exchange_rate\ndelivery_date\ndue_date\ninvoice_date\nlate_fee_amount\nline_item/amount\nline_item/quantity\nline_item/tax_amount\nline_item/unit_price\nnet_amount\nprior_amount_due\nprior_paid_amount\ntotal_amount\ntotal_tax_amount\neu\nus\nPredict the validity of ID documents using multiple signals.\nIdentity Document Proofing Processor is designed to help predict the validity of ID documents with four different signals.The processor currently returns information from the following signals:fraud_signals_is_identity_documentdetection: Predicts whether an image contains a recognized identity document.fraud_signals_suspicious_wordsdetection: Predicts whether words are present that aren't typical on IDs.fraud_signals_image_manipulationdetection: Predicts whether the image was altered or tampered with an image editing tool.fraud_signals_online_duplicatedetection: Predicts whether the image can be found online (US only).\nIdentity Document Proofing Processor is designed to help predict the validity of ID documents with four different signals.\nThe processor currently returns information from the following signals:\nfraud_signals_is_identity_documentdetection: Predicts whether an image contains a recognized identity document.\nfraud_signals_suspicious_wordsdetection: Predicts whether words are present that aren't typical on IDs.\nfraud_signals_image_manipulationdetection: Predicts whether the image was altered or tampered with an image editing tool.\nfraud_signals_online_duplicatedetection: Predicts whether the image can be found online (US only).\nThe Online Duplicate Detection feature is currently processed in US data centers. Regional and multi-regional support is unavailable for this feature outside of the US.\nThis processor is supported by algorithms that are updated more frequently than new processor versions are released. For this reason, the processor might return different outputs over time even when using the same processor version. For example, the Online Duplicate Detection system monitors images present on the web. The system's behavior can then change more quickly than can be tracked in processor versions.\nRefer to notes on Responsible AI[†]and Human review.[‡]\nSupport for US passports, passcards and driver's licenses.\nNone\nNone\nfraud_signals_photocopy_detection\nNone\nAdditional photocopy detection signal\nfraud_signals_photocopy_detection\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\nfraud_signals_is_identity_document\nfraud_signals_suspicious_words\nevidence_suspicious_word\nevidence_inconclusive_suspicious_word\nfraud_signals_image_manipulation\nfraud_signals_online_duplicate (US only)\nfraud_signals_photocopy_detection\nevidence_hostname (US only)\nevidence_thumbnail_url (US only)\nYou can find more information in theEnrichment & normalizationpage.\nfraud_signals_image_manipulation\nfraud_signals_online_duplicate (US only)\nfraud_signals_is_identity_document\nfraud_signals_suspicious_words\neu\nus\nExtract from pay slips, including name, business, amounts, etc.\nIf the multi-page input document contains more than one valid pay slips, the processor extracts entities from only the first valid pay slip. If no pay slips are found in the input file, the processor returns an error message.\nNone\nNone\nnet_pay\nnet_pay_ytd\nemployee_account_number\nNone\nNone\nNone\ndeduction_item\ndeduction_item/deduction_type\ndeduction_item/deduction_this_period\ndeduction_item/deduction_ytd\ndirect_deposit_item\ndirect_deposit_item/direct_deposit\ndirect_deposit_item/employee_account_number\nearning_item\nearning_item/earning_type\nearning_item/earning_rate\nearning_item/earning_hours\nearning_item/earning_this_period\nearning_item/earning_ytd\npage_number\ntax_item\ntax_item/tax_type\ntax_item/tax_this_period\ntax_item/tax_ytd\nfederal_additional_tax\nfederal_allowance\nfederal_marital_status\nstate_additional_tax\nstate_allowance\nstate_marital_status\nNone\nThis version assumes that the input file contains a single pay slip. Unlike the default version, this version does not check the input file for pay slips and will not return an error if no pay slips are found.Quality improvement, new fields support and new schema. Bonus, Commissions, Holiday, Overtime, Regular Pay and Vacation are now part of earning_item/earning_this_period, and their year-to-date versions are in earning_item/earning_ytd. Direct Deposit and Employee Account Number are now nested under direct_deposit_item.Async page limit is 10.\nQuality improvement, new fields support and new schema. Bonus, Commissions, Holiday, Overtime, Regular Pay and Vacation are now part of earning_item/earning_this_period, and their year-to-date versions are in earning_item/earning_ytd. Direct Deposit and Employee Account Number are now nested under direct_deposit_item.\nAsync page limit is 10.\nNone\nNone\nQuality improvement and uptraining enhancements.\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\nbonus\nbonus_ytd\ncommissions\ncommissions_ytd\ndirect_deposit\nemployee_account_number (Added in \"pretrained-paystub-v1.1-2021-08-13\")\nemployee_address\nemployee_name\nemployer_address\nemployer_name\nend_date\ngross_earnings\ngross_earnings_ytd\nholiday\nholiday_ytd\nnet_pay (Added in \"pretrained-paystub-v1.1-2021-08-13\")\nnet_pay_ytd (Added in \"pretrained-paystub-v1.1-2021-08-13\")\novertime\novertime_ytd\npay_date\nregular_pay\nregular_pay_ytd\nssn\nstart_date\nvacation\nvacation_ytd\nYou can find more information in theEnrichment & normalizationpage.\nemployer_address\nemployer_name\nYou can find more information in theEnrichment & normalizationpage.\nbonus\nbonus_ytd\ncommissions\ncommissions_ytd\ndirect_deposit\nend_date\ngross_earnings\ngross_earnings_ytd\nholiday\nholiday_ytd\nnet_pay\nnet_pay_ytd\novertime\novertime_ytd\npay_date\nregular_pay\nregular_pay_ytd\nstart_date\nvacation\nvacation_ytd\neu\nus\nExtract fields such as names, document ID, date of birth, etc.\nSupports all 50 States and D.C.\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\nFamily Name\nGiven Names\nDocument Id\nExpiration Date\nDate Of Birth\nIssue Date\nAddress\nPortrait\nYou can find more information in theEnrichment & normalizationpage.\nDate Of Birth\nExpiration Date\nIssue Date\neu\nus\nExtract text and values from expense documents such as expense date, supplier name, total amount, and currency.\nNone\nNone\ncredit_card_last_four_digits\nline_item/quantity\npayment_type\nja: Japanese\ntraveler_name\nreservation_id\nline_item/transaction_date\nja: Japanese\nit: Italian\npt: Portuguese (Portugal & Brazil)\ntraveler_name\nreservation_id\nline_item/transaction_date\nja: Japanese\nit: Italian\npt: Portuguese (Portugal & Brazil)\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\ncredit_card_last_four_digits\ncurrency\nend_date\nnet_amount\npayment_type\npurchase_time\nreceipt_date\nstart_date\nsupplier_address\nsupplier_city\nsupplier_name\ntip_amount\ntotal_amount\ntotal_tax_amount\nline_item\nline_item/amount\nline_item/description\nline_item/product_code\nYou can find more information in theEnrichment & normalizationpage.\nsupplier_address\nsupplier_name\nsupplier_phone\nYou can find more information in theEnrichment & normalizationpage.\ncurrency\ntotal_amount\ntotal_tax_amount\nnet_amount\nreceipt_date\npurchase_time\nstart_date\nend_date\nline_item/amount\nline_item/payment_date\nline_item/payment_amount\nasia-southeast1\naustralia-southeast1\neu\nnorthamerica-northeast1\nus\nExtract text and values from invoices such as invoice number, supplier name, invoice amount, tax amount, invoice date, due date.\nThe invoice Parser extracts both header and line item fields, such as invoice number, supplier name, invoice amount, tax amount, invoice date, due date, and line item amounts.\nNone\nNone\nNone\nNone\nNone\nit: Italian\npt: Portuguese (Portugal & Brazil)\nro: Romanian\nsv: Swedish\net: Estonian\nlv: Latvian\nlt: Lithuanian\nNone\nNone\nNone\nNone\nNone\nNone\nFor more information, seeManaging processor versions.\nYou can also find this information in theField detectedpage.\namount_paid_since_last_invoice\ncarrier\ncurrency\ncurrency_exchange_rate\ndelivery_date\ndue_date\nfreight_amount\ninvoice_date\ninvoice_id\nline_item\nline_item/amount\nline_item/description\nline_item/product_code\nline_item/purchase_order\nline_item/quantity\nline_item/unit\nline_item/unit_price\nnet_amount\npayment_terms\npurchase_order\nreceiver_address\nreceiver_email\nreceiver_name\nreceiver_phone\nreceiver_tax_id\nreceiver_website\nremit_to_address\nremit_to_name\nship_from_address\nship_from_name\nship_to_address\nship_to_name\nsupplier_address\nsupplier_email\nsupplier_iban\nsupplier_name\nsupplier_payment_ref\nsupplier_phone\nsupplier_registration\nsupplier_tax_id\nsupplier_website\ntotal_amount\ntotal_tax_amount\nvat\nvat/amount\nvat/category_code\nvat/tax_amount\nvat/tax_rate\nYou can find more information in theEnrichment & normalizationpage.\nsupplier_address\nsupplier_name\nsupplier_phone\nYou can find more information in theEnrichment & normalizationpage.\namount_paid_since_last_invoice\ncurrency\ncurrency_exchange_rate\ndelivery_date\ndue_date\nfreight_amount\ninvoice_date\nnet_amount\ntotal_amount\ntotal_tax_amount\nline_item/amount\nline_item/quantity\nline_item/unit_price\nvat/amount\nvat/tax_amount\nvat/tax_rate\nasia-south1\nasia-southeast1\naustralia-southeast1\neu\nnorthamerica-northeast1\nus\nTrain a model to classify a document type from a set of classes.\nasia-south1\nasia-southeast1\naustralia-southeast1\neu\neurope-west2\neurope-west3\nnorthamerica-northeast1\nus\nTrain a model to split a file containing multiple documents into individual, classified documents.\ni18n can be supported through custom training options only.\nasia-south1\nasia-southeast1\naustralia-southeast1\neu\neurope-west2\neurope-west3\nnorthamerica-northeast1\nus\nGet abstract and bullet point summaries for short and long documents.\nNone\nNone\nFor more information, seeManaging processor versions.\nus\nTo request API access, fill out and submit theDocument AI limited\n    access customer request form.\n  The form requests information about you, your company, and your use case.\n  Note that a Google Cloud Project ID is required for access.\n  To create a new Google Cloud project, or identify your existing project's\n    Project ID see the followinginstructions.\nAfter you submit the form, the Document AI team will\n  review your request to ensure you meet the criteria for access.\n  If approved, you will receive an email with instructions on how to access\n  and use this feature.\n[†]Identity Document Proofing works to extract and evaluate information from ID documents that contributes to identifying whether the input image represents an authentic ID.At Google Cloud, we prioritize helping customers safely develop and implement AI solutions, and Identity Proofing has been developed in accordance with Google's AI Principles.Based on Google's AI Principles and current product design, we strongly recommend using caution and carefully evaluating the potential benefits and risks of using Identity Document Proofing for the following:\nDecision-making without a human in the loop for predictions that can impact human rights.\nIn sensitive domains including but not limited to employment, access to public services, healthcare, and safety-critical contexts.\n[‡]Always use Identity Proofing as part of your broader identity-detection process and workflow.\n    It is important that you have a human reviewer in your workflow to verify whether the predicted signals are accurate. The Identity Proofing processor isn't meant to replace human review of IDs in a workflow, but rather to assist human reviewers in validating ID documents. The Identity Proofing processor shouldn't be used as an automated decision tool to determine whether an ID is valid. With human review, customers can achieve higher document processing accuracy and help businesses evaluate predictions using purpose-built tools to enable those reviews.Make sure that you review regulations in the region where you are implementing this technology, and research existing industry guidance to learn about policy guidelines and common fairness issues. Read about fairness in machine learning, including ways to mitigate bias in training datasets, evaluate your custom models for disparities in performance, and other considerations as you use your custom model.We encourage customers to keep fairness, interpretability, and privacy and security best practices in mind when implementing Identity Proofing. To learn more about how to implement responsible AI, read Google'srecommendations for Responsible AI practices.Refer to the blog postAutomate identity document processing with Document AI]for more information on use cases and a sample application code repository.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-13 UTC."
  },
  {
    "url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "content": "Home\nVirtual Private Cloud\nDocumentation\nGuides\nThis page is an overview of Packet Mirroring.\nPacket Mirroring clones the traffic of specified instances in your\nVirtual Private Cloud (VPC) network and forwards it for examination.\nPacket Mirroring captures all traffic and packet data, including\npayloads and headers. The capture can be configured for both egress and ingress\ntraffic, only ingress traffic, or only egress traffic.\nThe mirroring happens on the virtual machine (VM) instances, not on the network.\nConsequently, Packet Mirroring consumes additional bandwidth on the\nVMs.\nPacket Mirroring is useful when you need to monitor and analyze your security\nstatus. It exports all traffic, not only the traffic between sampling periods.\nFor example, you can use security software that analyzes mirrored traffic to\ndetect all threats or anomalies. Additionally, you can inspect the full traffic\nflow to detect application performance issues. For more information, see the\nexampleuse cases.\nPacket Mirroring copies traffic frommirrored sourcesand sends it to acollector destination. To configure Packet Mirroring, you create apacket\nmirroring policythat specifies the source and destination.\nMirrored sourcesare Compute Engine VM instances that you can select by\nspecifying subnets, network tags, or instance names. If you specify a subnet,\nall existing and future instances in that subnet are mirrored. You can specify\none or more source types—if an instance matches at least one of them, it's\nmirrored.Packet Mirroring collects traffic from an instance's network interface\nin the network where the packet mirroring policy applies. In cases where an instance\nhas multiple network interfaces, the other interfaces aren't mirrored unless\nanother policy has been configured to do so.\nMirrored sourcesare Compute Engine VM instances that you can select by\nspecifying subnets, network tags, or instance names. If you specify a subnet,\nall existing and future instances in that subnet are mirrored. You can specify\none or more source types—if an instance matches at least one of them, it's\nmirrored.\nPacket Mirroring collects traffic from an instance's network interface\nin the network where the packet mirroring policy applies. In cases where an instance\nhas multiple network interfaces, the other interfaces aren't mirrored unless\nanother policy has been configured to do so.\nAcollector destinationis an instance group that is behind an internal load\nbalancer. Instances in the instance group are referred to ascollector\ninstances.When you specify the collector destination, you enter the name of a forwarding\nrule that is associated with the internal passthrough Network Load Balancer. Google Cloud\nthen forwards the mirrored traffic to the collector instances. An internal\nload balancer for Packet Mirroring is similar to other internal load\nbalancers except that the forwarding rule must be configured for\nPacket Mirroring. Any non-mirrored traffic that is sent to the load\nbalancer is dropped.\nAcollector destinationis an instance group that is behind an internal load\nbalancer. Instances in the instance group are referred to ascollector\ninstances.\nWhen you specify the collector destination, you enter the name of a forwarding\nrule that is associated with the internal passthrough Network Load Balancer. Google Cloud\nthen forwards the mirrored traffic to the collector instances. An internal\nload balancer for Packet Mirroring is similar to other internal load\nbalancers except that the forwarding rule must be configured for\nPacket Mirroring. Any non-mirrored traffic that is sent to the load\nbalancer is dropped.\nBy default, Packet Mirroring collects all IPv4 traffic of mirrored\ninstances. Instead of collecting all IPv4 traffic, you can use filters to expand\nthe traffic that's collected to include all or some IPv6 traffic. You can also\nuse filters to narrow the traffic that's mirrored, which can help you limit\nthe bandwidth that's used by mirrored instances.\nYou can configure filters to collect traffic based on protocol, CIDR ranges\n(IPv4, IPv6, or both), direction of traffic (ingress-only, egress-only, or\nboth), or a combination.\nMultiple packet mirroring policies can apply to an instance. The priority of a\npacket mirroring policy is always1000and cannot be\nchanged. Identical policies are not supported. Google Cloud can send\ntraffic to any of the load balancers that have been configured with identical\npacket mirroring policies. To predictably and consistently send mirrored traffic\nto a single load balancer, create policies that have filters with\nnon-overlapping address ranges. If ranges overlap, set unique filter protocols.\nDepending on each policy's filter, Google Cloud chooses a policy for each\nflow. If you have distinct policies, Google Cloud uses the\ncorresponding policy that matches the mirrored traffic. For example, you might\nhave one policy that has the filter198.51.100.3/24:TCPand another policy\nthat has the filter2001:db8::/64:TCP:UDP. Because the policies are distinct,\nthere's no ambiguity about which policy Google Cloud uses.\nHowever, if you have overlapping policies, Google Cloud evaluates\ntheir filters to choose which policy to use. For example, you might have two\npolicies, one that has a filter for10.0.0.0/24:TCPand another for10.0.0.0/16:TCP. These policies overlap because their CIDR ranges overlap.\nWhen choosing a policy, Google Cloud prioritizes policies by\ncomparing their filter's CIDR range size.\nGoogle Cloud chooses a policy based on a filter:\nIf policies have different but overlapping CIDR ranges and the same exact\nprotocols, Google Cloud chooses the policy that uses the most specific\nCIDR range. Suppose the destination for a TCP packet leaving\na mirrored instance is10.240.1.4, and there are two policies with the\nfollowing filters:10.240.1.0/24:ALLand10.240.0.0/16:TCP. Because the\nmost specific match for10.240.1.4is10.240.1.0/24:ALL,\nGoogle Cloud uses the policy that has the filter10.240.1.0/24:ALL.\nIf policies have different but overlapping CIDR ranges and the same exact\nprotocols, Google Cloud chooses the policy that uses the most specific\nCIDR range. Suppose the destination for a TCP packet leaving\na mirrored instance is10.240.1.4, and there are two policies with the\nfollowing filters:10.240.1.0/24:ALLand10.240.0.0/16:TCP. Because the\nmost specific match for10.240.1.4is10.240.1.0/24:ALL,\nGoogle Cloud uses the policy that has the filter10.240.1.0/24:ALL.\nIf policies specify the same exact CIDR range with overlapping protocols,\nGoogle Cloud chooses a policy with the most specific protocol.\nFor example, the following filters have the same range but overlapping\nprotocols:10.240.1.0/24:TCPand10.240.1.0/24:ALL. For matching TCP\ntraffic, Google Cloud uses the10.240.1.0/24:TCPpolicy.\nThe10.240.1.0/24:ALLpolicy applies to matching traffic for all other\nprotocols.\nIf policies specify the same exact CIDR range with overlapping protocols,\nGoogle Cloud chooses a policy with the most specific protocol.\nFor example, the following filters have the same range but overlapping\nprotocols:10.240.1.0/24:TCPand10.240.1.0/24:ALL. For matching TCP\ntraffic, Google Cloud uses the10.240.1.0/24:TCPpolicy.\nThe10.240.1.0/24:ALLpolicy applies to matching traffic for all other\nprotocols.\nIf policies have the same exact CIDR range but distinct protocols, these\npolicies don't overlap. Google Cloud uses the policy that corresponds\nto the mirrored traffic's protocol. For example, you might have a policy for2001:db8::/64:TCPand another for2001:db8::/64:UDP. Depending on the\nmirrored traffic's protocol, Google Cloud uses either the TCP or UDP\npolicy.\nIf policies have the same exact CIDR range but distinct protocols, these\npolicies don't overlap. Google Cloud uses the policy that corresponds\nto the mirrored traffic's protocol. For example, you might have a policy for2001:db8::/64:TCPand another for2001:db8::/64:UDP. Depending on the\nmirrored traffic's protocol, Google Cloud uses either the TCP or UDP\npolicy.\nIf overlapping policies have the same exact filter, they are identical. In\nthis case, Google Cloud might choose the same policy or a different\npolicy each time that matching traffic is re-evaluated against these\npolicies. We recommend that you avoid creating identical packet mirroring\npolicies.\nIf overlapping policies have the same exact filter, they are identical. In\nthis case, Google Cloud might choose the same policy or a different\npolicy each time that matching traffic is re-evaluated against these\npolicies. We recommend that you avoid creating identical packet mirroring\npolicies.\nVPC Flow Logs doesn't log mirrored packets. If a collector instance is on a\nsubnet that has VPC Flow Logs enabled, traffic that is sent directly to the\ncollector instance is logged, including traffic from mirrored instances. That\nis, if the original destination IPv4 or IPv6 address matches the IPv4 or IPv6\naddress of the collector instance, the flow is logged.\nFor more information about VPC Flow Logs, seeUsing VPC Flow Logs.\nThe following list describes constraints or behaviors with\nPacket Mirroring that are important to understand before you use it:\nEach packet mirroring policy definesmirrored sourcesand acollector\ndestination. You must adhere to the following rules:All mirrored sources must be in the same project, VPC\nnetwork, and Google Cloud region.A collector destination must be in the same region as the mirrored sources.\nA collector destination can be located in either the same VPC\nnetwork as the mirrored sources or a VPC network connected\nto the mirrored sources' network using VPC Network Peering.Each mirroring policy can only reference a single collector destination.\nHowever, a single collector destination can be referenced by multiple\nmirroring policies.\nEach packet mirroring policy definesmirrored sourcesand acollector\ndestination. You must adhere to the following rules:\nAll mirrored sources must be in the same project, VPC\nnetwork, and Google Cloud region.\nA collector destination must be in the same region as the mirrored sources.\nA collector destination can be located in either the same VPC\nnetwork as the mirrored sources or a VPC network connected\nto the mirrored sources' network using VPC Network Peering.\nEach mirroring policy can only reference a single collector destination.\nHowever, a single collector destination can be referenced by multiple\nmirroring policies.\nAlllayer 4 protocolsare supported by Packet Mirroring.\nAlllayer 4 protocolsare supported by Packet Mirroring.\nYou cannot mirror and collect traffic on the same network interface of a VM\ninstance because doing this would cause a mirroring loop.\nYou cannot mirror and collect traffic on the same network interface of a VM\ninstance because doing this would cause a mirroring loop.\nTo mirror traffic passing between Pods on the same Google Kubernetes Engine (GKE)\nnode, you must enableIntranode\nvisibilityfor\nthe cluster.\nTo mirror traffic passing between Pods on the same Google Kubernetes Engine (GKE)\nnode, you must enableIntranode\nvisibilityfor\nthe cluster.\nTo mirror IPv6 traffic, use filters to specify the IPv6 CIDR ranges\nof the IPv6 traffic that you want to mirror. You can mirror all IPv6 traffic\nby using a CIDR range filter of::/0. You can mirror all IPv4 and IPv6\ntraffic by using the following comma-separated CIDR range filter:0.0.0.0/0,::/0.\nTo mirror IPv6 traffic, use filters to specify the IPv6 CIDR ranges\nof the IPv6 traffic that you want to mirror. You can mirror all IPv6 traffic\nby using a CIDR range filter of::/0. You can mirror all IPv4 and IPv6\ntraffic by using the following comma-separated CIDR range filter:0.0.0.0/0,::/0.\nMirroring traffic consumes bandwidth on the mirrored instance. For example,\nif a mirrored instance experiences 1 Gbps of ingress traffic and 1 Gbps of\negress traffic, the total traffic on the instances is 1 Gbps of ingress and 3\nGbps of egress (1 Gbps of normal egress traffic and 2 Gbps of mirrored egress\ntraffic). To limit what traffic is collected, you can use filters.\nMirroring traffic consumes bandwidth on the mirrored instance. For example,\nif a mirrored instance experiences 1 Gbps of ingress traffic and 1 Gbps of\negress traffic, the total traffic on the instances is 1 Gbps of ingress and 3\nGbps of egress (1 Gbps of normal egress traffic and 2 Gbps of mirrored egress\ntraffic). To limit what traffic is collected, you can use filters.\nThe cost of Packet Mirroring varies depending on the amount of egress\ntraffic traveling from a mirrored instance to an instance group and whether the\ntraffic travels between zones.\nThe cost of Packet Mirroring varies depending on the amount of egress\ntraffic traveling from a mirrored instance to an instance group and whether the\ntraffic travels between zones.\nPacket Mirroring applies to both ingress and egress direction. If two\nVM instances that are being mirrored send traffic to each other,\nGoogle Cloud collects two versions of the same packet. You can alter\nthis behaviour by specifying that only ingress or only egress packets are\nmirrored.\nPacket Mirroring applies to both ingress and egress direction. If two\nVM instances that are being mirrored send traffic to each other,\nGoogle Cloud collects two versions of the same packet. You can alter\nthis behaviour by specifying that only ingress or only egress packets are\nmirrored.\nThere is a maximum number of packet mirroring policies that you can create for\na project. For more information, see the per-project quotas on thequotaspage.\nThere is a maximum number of packet mirroring policies that you can create for\na project. For more information, see the per-project quotas on thequotaspage.\nFor each packet mirroring policy, the maximum number of mirrored sources that\nyou can specify depends on the source type:5 subnets5 tags50 instances\nFor each packet mirroring policy, the maximum number of mirrored sources that\nyou can specify depends on the source type:\n5 subnets\n5 tags\n50 instances\nThe maximum number of packet mirroring filters is 30, which is the number of\nIPv4 and IPv6 address ranges multiplied by the number of protocols. For\nexample, you can specify 30 ranges and 1 protocol, which would be 30 filters.\nHowever, you cannot specify 30 ranges and 2 protocols, which would be 60\nfilters and greater than the maximum.\nThe maximum number of packet mirroring filters is 30, which is the number of\nIPv4 and IPv6 address ranges multiplied by the number of protocols. For\nexample, you can specify 30 ranges and 1 protocol, which would be 30 filters.\nHowever, you cannot specify 30 ranges and 2 protocols, which would be 60\nfilters and greater than the maximum.\nMirrored traffic is encrypted only if the VM encrypts that traffic at the\napplication layer. While VM-to-VM connections within VPC networks\nand peered VPC networks areencrypted,\nthe encryption and decryption happens in the hypervisors. From the perspective\nof the VM, this traffic is not encrypted.\nMirrored traffic is encrypted only if the VM encrypts that traffic at the\napplication layer. While VM-to-VM connections within VPC networks\nand peered VPC networks areencrypted,\nthe encryption and decryption happens in the hypervisors. From the perspective\nof the VM, this traffic is not encrypted.\nThe following sections describe real-world scenarios that demonstrate why you\nmight use Packet Mirroring.\nSecurity and network engineering teams must ensure that they are catching all\nanomalies and threats that might indicate security breaches and intrusions. They\nmirror all traffic so that they can complete a comprehensive inspection of\nsuspicious flows. Because attacks can span multiple packets, security teams must\nbe able to get all packets for each flow.\nFor example, the following security tools require you to capture multiple\npackets:\nIntrusion detection system\n(IDS) tools require\nmultiple packets of a single flow to match a\nsignature so that the tools can detect persistent threats.\nIntrusion detection system\n(IDS) tools require\nmultiple packets of a single flow to match a\nsignature so that the tools can detect persistent threats.\nDeep Packet Inspection engines inspect packet payloads to detect protocol\nanomalies.\nDeep Packet Inspection engines inspect packet payloads to detect protocol\nanomalies.\nNetwork forensics for PCI compliance and other regulatory use cases require\nthat most packets be examined. Packet Mirroring\nprovides a solution for capturing different attack vectors, such as\ninfrequent communication or attempted but unsuccessful communication.\nNetwork forensics for PCI compliance and other regulatory use cases require\nthat most packets be examined. Packet Mirroring\nprovides a solution for capturing different attack vectors, such as\ninfrequent communication or attempted but unsuccessful communication.\nNetwork engineers can use mirrored traffic to troubleshoot performance issues\nreported by application and database teams. To check for networking issues,\nnetwork engineers can view what's going over the wire rather than relying on\napplication logs.\nFor example, network engineers can use data from Packet Mirroring to complete\nthe following tasks:\nAnalyze protocols and behaviors so that they can find and fix issues, such as\npacket loss or TCP resets.\nAnalyze protocols and behaviors so that they can find and fix issues, such as\npacket loss or TCP resets.\nAnalyze (in real time) traffic patterns from remote desktop, VoIP, and other\ninteractive applications. Network engineers can search for issues that affect\nthe application's user experience, such as multiple packet resends or more\nthan expected reconnections.\nAnalyze (in real time) traffic patterns from remote desktop, VoIP, and other\ninteractive applications. Network engineers can search for issues that affect\nthe application's user experience, such as multiple packet resends or more\nthan expected reconnections.\nYou can use Packet Mirroring in various setups. The following examples\nshow the location of collector destinations and their policies for different\npacket mirroring configurations, such as VPC Network Peering and\nShared VPC.\nThe following example shows a packet mirroring configuration where the mirrored\nsource and collector destination are in the same VPC network.\nIn the preceding diagram, the packet mirroring policy is configured to mirrormirrored-subnetand send mirrored traffic to the internal passthrough Network Load Balancer.\nGoogle Cloud mirrors the traffic on existing and future instances in the\nsubnet. All traffic to and from the internet, on-premises hosts, or Google\nservices is mirrored.\nYou can build a centralized collector model, where instances in different\nVPC networks send mirrored traffic to a collector destination in\na central VPC network. That way, you can use a single\ndestination collector.\nIn the following example, thecollector-load-balancerinternal passthrough Network Load Balancer is\nin theus-central1region in thenetwork-aVPC network inproject-a. This destination collector can be used by two packet mirroring\npolicies:\npolicy-1collects packets from mirrored sources in theus-central1region\nin thenetwork-aVPC network inproject-aand sends them\nto thecollector-load-balancerdestination.\npolicy-1collects packets from mirrored sources in theus-central1region\nin thenetwork-aVPC network inproject-aand sends them\nto thecollector-load-balancerdestination.\npolicy-2collects packets from mirrored sources in theus-central1region\nin thenetwork-bVPC network inproject-band sends them\nto the samecollector-load-balancerdestination.\npolicy-2collects packets from mirrored sources in theus-central1region\nin thenetwork-bVPC network inproject-band sends them\nto the samecollector-load-balancerdestination.\nTwo mirroring policies are required because mirrored sources exist in different\nVPC networks.\nIn the preceding diagram, the collector destination collects mirrored traffic\nfrom subnets in two different networks. All resources (the source and\ndestination) must be in the same region. The setup innetwork-ais similar to\nthe example where themirrored source and collector destination are in the same\nVPC network.policy-1is configured to collect\ntraffic fromsubnet-aand send it tocollector-ilb.\npolicy-2is configured inproject-abut specifiessubnet-bas a mirrored\nsource. Becausenetwork-aandnetwork-bare peered, the destination\ncollector can collect traffic fromsubnet-b.\nThe networks are in different projects and might have different owners. It's\npossible for either owner to create the packet mirroring policy if they have the\nright permissions:\nIf theowners ofproject-acreate the packet mirroring policy, they must\nhave thecompute.packetMirroringAdminrole on the network, subnet, or\ninstances to mirror inproject-b.\nIf theowners ofproject-acreate the packet mirroring policy, they must\nhave thecompute.packetMirroringAdminrole on the network, subnet, or\ninstances to mirror inproject-b.\nIf theowners ofproject-bcreate the packet mirroring policy, they must\nhavecompute.packetMirroringUserrole inproject-a.\nIf theowners ofproject-bcreate the packet mirroring policy, they must\nhavecompute.packetMirroringUserrole inproject-a.\nFor more information about enabling private connectivity across two\nVPC networks, seeVPC Network Peering.\nIn the following Shared VPC scenarios, the mirrored instances for the\ncollector destination are all in the same Shared VPC network. Even though the\nresources are all in the same network, they can be in different projects, such\nas the host project or several different service projects. The following\nexamples show where packet mirroring policies must be created and who can create\nthem.\nIf both the mirrored sources and collector destination are in the same project,\neither in a host project or service project, the setup is similar to having\neverything in thesame VPC network. The project\nowner can create all the resources and set the required permissions in that\nproject.\nFor more information, seeShared VPC overiew.\nIn the following example, the collector destination is in a service project that\nuses a subnet in the host project. In this case, the policy is also in the\nservice project. The policy could also be in the host project.\nIn the preceding diagram, the service project contains the collector instances\nthat use the collector subnet in the Shared VPC network. The packet\nmirroring policy was created in the service project and is configured to mirror\ninstances that have a network interface insubnet-mirrored.\nService or host project users can create the packet mirroring policy. To do so,\nusers must have thecompute.packetMirroringUserrole in the service project\nwhere the collector destination is located. Users must also have thecompute.packetMirroringAdminrole on the mirrored sources.\nIn the following example, the collector destination is in the host project and\nmirrored instances are in the service projects.\nThis example might apply to scenarios where developers deploy applications in\nservice projects and use the Shared VPC network. They don't have to\nmanage the networking infrastructure or Packet Mirroring. Instead, a\ncentralized networking or security team, who have control over the host project\nand Shared VPC network, are responsible for provisioning packet\nmirroring policies.\nIn the preceding diagram, the packet mirroring policy is created in the host\nproject, where the collector destination is located. The policy is configured to\nmirror instances in the mirrored subnet. VM instances in service projects can\nuse the mirrored subnet, and their traffic is mirrored.\nService or host project users can create the packet mirroring policy. To do so,\nusers in the service project must have thecompute.packetMirroringUserrole in\nthe host project. Users in the host project require thecompute.packetMirroringAdminrole for mirrored sources in the service\nprojects.\nYou can include VM instances that have multiple network interfaces in a packet\nmirroring policy. Because a policy can mirror resources from a single network,\nyou cannot create one policy to mirror traffic for all network interfaces of an\ninstance. If you need to mirror more than one network interface of a multiple\nnetwork interface instance, you must create one packet mirroring policy for each\ninterface because each interface connects to a unique VPC network.\nYou are charged for the amount of data processed by Packet Mirroring.\nFor details, seePacket Mirroringpricing.\nYou are also charged for all the prerequisite components and egress traffic\nthat are related to Packet Mirroring. For example, the instances\nthat collect traffic are charged at the regular rate. Also, if packet\nmirroring traffic travels between zones, you are charged for the egress\ntraffic. For pricing details, see the relatedpricing page.\nTo create and manage packet mirroring policies, seeUse Packet Mirroring.\nTo view metrics and check your existing packet mirroring policies, seeMonitor Packet Mirroring.\nFor information about internal passthrough Network Load Balancers, seeInternal passthrough Network Load Balancer overview.\nFor a list of partner providers, seePacket Mirroring partner\nproviders.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-04-30 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma",
    "title": "Use Gemma open modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nGenerative AI on Vertex AI\nDocumentation\nGemma is a set of lightweight, generative artificial intelligence (AI)\nopen models. Gemma models are available to run in your\napplications and on your hardware, mobile devices, or hosted services. You can\nalso customize these models using tuning techniques so that they excel at\nperforming tasks that matter to you and your users. Gemma models are\nbased onGeminimodels and are intended\nfor the AI development community to extend and take further.\nFine-tuning can help improve a model's performance in specific tasks. Because\nmodels in the Gemma model family are open weight, you can tune any of\nthem using the AI framework of your choice and the Vertex AI SDK.\nYou can open a notebook example to fine-tune the Gemma model using\na link available on the Gemma model card in Model Garden.\nThe following Gemma models are available to use with Vertex AI.\nTo learn more about and test the Gemma models, see their\nModel Garden model cards.\nThe following are some options for where you can use Gemma:\nVertex AI offers a managed platform for rapidly building and scaling\nmachine learning projects without needing in-house MLOps expertise. You can use\nVertex AI as the downstream application that serves the\nGemma models. For example, you might port weights from the Keras\nimplementation of Gemma. Next, you can use Vertex AI to\nserve that version of Gemma to get predictions. We recommend using\nVertex AI if you want end-to-end MLOps capabilities, value-added ML\nfeatures, and a serverless experience for streamlined development.\nTo get started with Gemma, see the following notebooks:\nServe Gemma 3 in Vertex AI\nServe Gemma 3 in Vertex AI\nServe Gemma 2 in Vertex AI\nServe Gemma 2 in Vertex AI\nServe Gemma in Vertex AI\nServe Gemma in Vertex AI\nFine-tune Gemma 3 using PEFT and then deploy to Vertex AI from Vertex\nFine-tune Gemma 3 using PEFT and then deploy to Vertex AI from Vertex\nFine-tune Gemma 2 using PEFT and then deploy to Vertex AI from Vertex\nFine-tune Gemma 2 using PEFT and then deploy to Vertex AI from Vertex\nFine-tune Gemma using PEFT and then deploy to Vertex AI from Vertex\nFine-tune Gemma using PEFT and then deploy to Vertex AI from Vertex\nFine-tune Gemma using PEFT and then deploy to Vertex AI from Huggingface\nFine-tune Gemma using PEFT and then deploy to Vertex AI from Huggingface\nFine-tune Gemma using KerasNLP and then deploy to Vertex AI\nFine-tune Gemma using KerasNLP and then deploy to Vertex AI\nFine-tune Gemma with Ray on Vertex AI and then deploy to Vertex AI\nFine-tune Gemma with Ray on Vertex AI and then deploy to Vertex AI\nRun local inference with ShieldGemma 2 with Hugging Face transformers\nRun local inference with ShieldGemma 2 with Hugging Face transformers\nYou can use Gemma with other Google Cloud products, such as\nGoogle Kubernetes Engine and Dataflow.\nGoogle Kubernetes Engine (GKE) is the Google Cloud solution\nfor managed Kubernetes that provides scalability, security, resilience, and cost\neffectiveness. We recommend this option if you have existing Kubernetes\ninvestments, your organization has in-house MLOps expertise, or if you need\ngranular control over complex AI/ML workloads with unique security, data\npipeline, and resource management requirements. To learn more, see the following\ntutorials in the GKE documentation:\nServe Gemma with vLLM\nServe Gemma with TGI\nServe Gemma with Triton and TensorRT-LLM\nServe Gemma with JetStream\nYou can use Gemma models with Dataflow forsentiment analysis.\nUse Dataflow to run inference pipelines that use the\nGemma models. To learn more, seeRun inference pipelines with Gemma open models.\nYou can use Gemma with Colaboratory to create your Gemma\nsolution. In Colab, you can use Gemma with framework\noptions such as PyTorch and JAX. To learn more, see:\nGet started with Gemma using Keras.\nGet started with Gemma using PyTorch.\nBasic tuning with Gemma using Keras.\nDistributed tuning with Gemma using Keras.\nGemma models are available in several sizes so you can build\ngenerative AI solutions based on your available computing resources, the\ncapabilities you need, and where you want to run them. Each model is available\nin a tuned and an untuned version:\nPretrained- This version of the model wasn't trained on any specific tasks\nor instructions beyond the Gemma core data training set. We don't\nrecommend using this model without performing some tuning.\nPretrained- This version of the model wasn't trained on any specific tasks\nor instructions beyond the Gemma core data training set. We don't\nrecommend using this model without performing some tuning.\nInstruction-tuned- This version of the model was trained with human language\ninteractions so that it can participate in a conversation, similar to a basic\nchat bot.\nInstruction-tuned- This version of the model was trained with human language\ninteractions so that it can participate in a conversation, similar to a basic\nchat bot.\nMix fine-tuned- This version of the model is fine-tuned on a mixture of\nacademic datasets and accepts natural language prompts.\nMix fine-tuned- This version of the model is fine-tuned on a mixture of\nacademic datasets and accepts natural language prompts.\nLower parameter sizes means lower resource requirements and more deployment\nflexibility.\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nPretrained\nMix fine-tuned\nPretrained\nMix fine-tuned\nPretrained\nMix fine-tuned\nPretrained\nMix fine-tuned\nFine-tuned\nPretrained\nInstruction-tuned\nPretrained\nInstruction-tuned\nPretrained\nGemma has been tested using Google's purpose built v5e TPU\nhardware and NVIDIA's L4(G2 Standard), A100(A2 Standard),\nH100(A3 High) GPU hardware.\nSeeGemma documentation.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "content": "Home\nBigQuery\nDocumentation\nGuides\nBigQuery ML lets youcreate and run machine learning (ML) modelsby\nusing GoogleSQL queries. BigQuery ML models are stored\nin BigQuery datasets, similar to tables and views.\nBigQuery ML also lets you accessVertex AI modelsandCloud AI APIsto perform artificial\nintelligence (AI) tasks like text generation or machine\ntranslation. Gemini for Google Cloud also provides AI-powered\nassistance for BigQuery tasks. To see a list of AI-powered\nfeatures in BigQuery, seeGemini in BigQuery overview.\nUsually, performing ML or AI on large datasets requires extensive programming\nand knowledge of ML frameworks. These requirements restrict solution development\nto a very small set of people within each company, and they exclude data\nanalysts who understand the data but have limited ML knowledge and programming\nexpertise. However, with BigQuery ML, SQL practitioners can use\nexisting SQL tools and skills to build and evaluate models, and to generate\nresults from LLMs and Cloud AI APIs.\nYou can work with BigQuery ML capabilities by using the\nfollowing:\nThe Google Cloud console\nThe bq command-line tool\nThe BigQuery REST API\nIntegratedColab Enterprise notebooks in BigQuery\nExternal tools such as a Jupyter notebook or business intelligence platform\nBigQuery ML offers several advantages over other approaches to\nusing ML or AI with a cloud-based data warehouse:\nBigQuery ML democratizes the use of ML and AI by empowering data\nanalysts, the primary data warehouse users, to build and run models using\nexisting business intelligence tools and spreadsheets. Predictive analytics\ncan guide business decision-making across the organization.\nYou don't need to program an ML or AI solution using Python or Java. You\ntrain models and access AI resources by using SQL—a language that's\nfamiliar to data analysts.\nBigQuery ML increases the speed of model development and\ninnovation by removing the need to move data from the data warehouse.\nInstead, BigQuery ML brings ML to the data, which offers the\nfollowing advantages:Reduced complexity because fewer tools are required.Increased speed to production because moving and formatting large amounts\nof data for Python-based ML frameworks isn't required to train a model in\nBigQuery.For more information, watch the videoHow to accelerate machine learning development with BigQuery ML.\nBigQuery ML increases the speed of model development and\ninnovation by removing the need to move data from the data warehouse.\nInstead, BigQuery ML brings ML to the data, which offers the\nfollowing advantages:\nReduced complexity because fewer tools are required.\nIncreased speed to production because moving and formatting large amounts\nof data for Python-based ML frameworks isn't required to train a model in\nBigQuery.\nFor more information, watch the videoHow to accelerate machine learning development with BigQuery ML.\nBy using the default settings in theCREATE MODELstatements and the\ninference functions, you can create and use BigQuery ML models\neven without much ML knowledge. However, having basic knowledge about the\nML development lifecycle, such as feature engineering and model training,\nhelps you optimize both your data and your model to\ndeliver better results. We recommend using the following resources to develop\nfamiliarity with ML techniques and processes:\nMachine Learning Crash Course\nIntro to Machine Learning\nData Cleaning\nFeature Engineering\nIntermediate Machine Learning\nYou can use BigQuery ML capabilities to perform a range of\ngenerative AI tasks.\nUseremote models,\nwhich are BigQuery ML models over Vertex AI\nmodels, to perform the following tasks:Text generationby\nusing Vertex AI text or multimodalmodels.Text or multimodal embeddingby using Vertex AI embedding models.\nUseremote models,\nwhich are BigQuery ML models over Vertex AI\nmodels, to perform the following tasks:\nText generationby\nusing Vertex AI text or multimodalmodels.\nText or multimodal embeddingby using Vertex AI embedding models.\nUse BigQuery ML functions to perform the following tasks:Generate values of a specific typeby using functions over Vertex AI hosted models.Forecastingby using a function over BigQuery ML's built-inTimesFM time series model.\nUse BigQuery ML functions to perform the following tasks:\nGenerate values of a specific typeby using functions over Vertex AI hosted models.\nForecastingby using a function over BigQuery ML's built-inTimesFM time series model.\nUseremote modelsover Cloud AI APIs to perform the following tasks:Natural language processingby using theCloud Natural Language API.Machine translationby using theCloud Translation API.Document processingby using theDocument AI API.Audio transcriptionby using theSpeech-to-Text API.Computer vision\nUseremote modelsover Cloud AI APIs to perform the following tasks:\nNatural language processingby using theCloud Natural Language API.\nMachine translationby using theCloud Translation API.\nDocument processingby using theDocument AI API.\nAudio transcriptionby using theSpeech-to-Text API.\nComputer vision\nAmodelin\nBigQuery ML represents what an ML system has\nlearned from training data. The following sections describe the types of models\nthat BigQuery ML supports. For more information about\ncreating reservation assignments for the different types of models, seeAssign slots to BigQuery ML workloads.\nThe following models are built in to BigQuery ML:\nContribution analysis(Preview) is for determining the effect of\none or more dimensions on the value for a given metric. For example, seeing the\neffect of store location and sales date on store revenue. For more\ninformation, seeContribution analysis overview.\nLinear regressionis for predicting the value of a numerical metric for new data by using a\nmodel trained on similar remote data. Labels are real-valued, meaning\nthey cannot be positive infinity or negative infinity or a NaN (Not a Number).\nLogistic regressionis for the classification of two or more possible values such as whether an\ninput islow-value,medium-value, orhigh-value. Labels can have up to 50 unique values.\nK-means clusteringis for data segmentation. For example, this model identifies customer\nsegments. K-means is an unsupervised learning technique, so model training\ndoesn't require labels or split data for training or evaluation.\nMatrix factorizationis for creating product recommendation systems. You can create product\nrecommendations using historical customer behavior, transactions, and product\nratings, and then use those recommendations for personalized customer\nexperiences.\nPrincipal component analysis (PCA)is the process of computing the principal components and using them to\nperform a change of basis on the data. It's commonly used for dimensionality\nreduction by projecting each data point onto only the first few principal\ncomponents to obtain lower-dimensional data while preserving as much of the\ndata's variation as possible.\nTime series is for performing time series forecasts. You can use this feature\nto create millions of time series models and use them for forecasting. TheARIMA_PLUSandARIMA_PLUS_XREGtime series models offer multiple tuning options, and automatically handle\nanomalies, seasonality, and holidays.If you don't want to manage your own time series forecasting model, you can\nuse theAI.FORECASTfunctionwith BigQuery ML's built-inTimesFM time series model(Preview) to perform forecasting.\nTime series is for performing time series forecasts. You can use this feature\nto create millions of time series models and use them for forecasting. TheARIMA_PLUSandARIMA_PLUS_XREGtime series models offer multiple tuning options, and automatically handle\nanomalies, seasonality, and holidays.\nIf you don't want to manage your own time series forecasting model, you can\nuse theAI.FORECASTfunctionwith BigQuery ML's built-inTimesFM time series model(Preview) to perform forecasting.\nYou can perform adry runon theCREATE MODELstatements for internally trained models to get an estimate of\nhow much data they will process if you run them.\nThe following models are external to BigQuery ML and trained in\nVertex AI:\nDeep neural network (DNN)is for creating TensorFlow-based deep neural networks for\nclassification and regression models.\nWide & Deepis useful for generic large-scale regression and classification problems\nwith sparse inputs\n(categorical featureswith a large number of possible feature values), such as recommender\nsystems, search, and ranking problems.\nAutoencoderis for creating TensorFlow-based models\nwith the support of sparse data representations. You can use the models in\nBigQuery ML for tasks such as unsupervised anomaly detection\nand non-linear dimensionality reduction.\nBoosted Treeis for creating classification and regression models that are based onXGBoost.\nRandom forestis for constructing multiple learning method decision trees for\nclassification, regression, and other tasks at training time.\nAutoMLis a supervised ML service that builds and deploys classification and\nregression models on tabular data at high speed and scale.\nYou can't perform adry runon theCREATE MODELstatements for externally trained models to get an estimate of\nhow much data they will process if you run them.\nYou can createremote modelsin BigQuery that use models deployed toVertex AI.\nYou reference the deployed model by specifying the model'sHTTPS endpointin the remote model'sCREATE MODELstatement.\nTheCREATE MODELstatements for remote models don't process any bytes and\ndon't incur BigQuery charges.\nBigQuery ML lets you import custom models that are trained outside\nof BigQuery and then perform prediction within\nBigQuery. You can import the following models into\nBigQuery fromCloud Storage:\nOpen Neural Network Exchange (ONNX)is an open standard format for representing ML models. Using\nONNX, you can make models that are trained with popular ML frameworks\nlike PyTorch and scikit-learn available in BigQuery ML.\nTensorFlowis a free, open source software library\nfor ML and artificial intelligence. You can use TensorFlow\nacross a range of tasks, but it has a particular focus on training and\ninference of deep neural networks. You can load previously trained\nTensorFlow models into BigQuery as\nBigQuery ML models and then perform prediction in\nBigQuery ML.\nTensorFlow Liteis a light version of TensorFlow for deployment on mobile\ndevices, microcontrollers, and other edge devices. TensorFlow\noptimizes existing TensorFlow models for reduced model size and\nfaster inference.\nXGBoostis an optimized distributed gradient boosting library designed to be highly\nefficient, flexible, and portable. It implements ML algorithms\nunder thegradient boostingframework.\nTheCREATE MODELstatements for imported models don't process any bytes and\ndon't incur BigQuery charges.\nIn BigQuery ML, you can use a model with data from multiple\nBigQuery Datasets for training and for prediction.\nDownload the model selection decision tree.\nBigQuery ML integrates with Vertex AI, which is the\nend-to-end platform for AI and ML in Google Cloud. You can register your\nBigQuery ML models to Model Registry in\norder to deploy these models to endpoints for online prediction. For more\ninformation, see the following:\nTo learn more about using your BigQuery ML\nmodels with Vertex AI, seeManage BigQuery ML models with Vertex AI.\nIf you aren't familiar with Vertex AI and want to learn more\nabout how it integrates with BigQuery ML, seeVertex AI for BigQuery users.\nWatch the videoHow to simplify AI models with Vertex AI and BigQuery ML.\nYou can now use Colab Enterprise notebooks to perform ML\nworkflows in BigQuery. Notebooks let you use SQL, Python,\nand other popular libraries and languages to accomplish your ML tasks.\nFor more information, seeCreate notebooks.\nBigQuery ML is supported in the same regions as\nBigQuery. For more information, seeBigQuery ML locations.\nYou are charged for the compute resources that you use to train models and to\nrun queries against models. The type of model that you create affects where the\nmodel is trained and the pricing that applies to that operation. Queries\nagainst models always run in BigQuery and useBigQuery compute pricing.\nBecauseremote modelsmake calls to Vertex AI\nmodels, queries against remote models also incur charges from\nVertex AI.\nYou are charged for the storage used by trained models, usingBigQuery storage pricing.\nFor more information, seeBigQuery ML pricing.\nIn addition toBigQuery ML-specific limits,\nqueries that use BigQuery ML functions andCREATE MODELstatements are subject to the quotas and limits on BigQueryquery jobs.\nBigQuery ML isn't available in theStandard edition.\nTo get started using BigQuery ML, seeCreate machine learning models in BigQuery ML.\nTo learn more about machine learning and BigQuery ML, see the\nfollowing resources:Applying Machine Learning to your data with Google Cloudcourse at CourseraSmart analytics and data managementtraining programMachine learning crash courseMachine learning glossary\nApplying Machine Learning to your data with Google Cloudcourse at Coursera\nSmart analytics and data managementtraining program\nMachine learning crash course\nMachine learning glossary\nTo learn about MLOps with Model Registry, seeManage BigQuery ML models in Vertex AI.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-05 UTC."
  },
  {
    "url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nVertex AI\nDocumentation\nVertex AI lets you perform machine learning with tabular data\nusing simple processes and interfaces. You can create the following model types\nfor your tabular data problems:\nBinary classificationmodels predict a binary outcome (one of\ntwo classes). Use this model type for yes or no questions. For example, you might want\nto build a binary classification model to predict whether a customer would\nbuy a subscription. Generally, a binary classification\nproblem requires less data than other model types.\nMulti-class classificationmodels predict one class from three\nor more discrete classes. Use this model type for categorization. For example, as a\nretailer, you might want to build a multi-class classification model to segment\ncustomers into different personas.\nRegressionmodels predict a continuous value. For example, as a retailer,\nyou might want to build a regression model to predict how much a\ncustomer will spend next month.\nForecastingmodels predict a sequence of values. For example,\nas a retailer, you might want to forecast daily demand of your products\nfor the next 3 months so that you can appropriately stock product\ninventories in advance.\nFor an introduction to machine learning with tabular data, seeIntroduction to Tabular Data. For further\ninformation about Vertex AI solutions, seeVertex AI solutions for classification and regressionandVertex AI solutions for forecasting.\nGoogle is committed to making progress in followingresponsible AI practices.\nTo this end, our ML products, including AutoML, are designed around core\nprinciples such asfairnessandhuman-centered machine learning.\nFor more information about best practices for mitigating bias when building\nyour own ML system, seeInclusive ML guide - AutoML.\nVertex AI offers the following solutions for classification and regression:\nTabular Workflow for End-to-End AutoML\nTabular Workflow for TabNet\nTabular Workflow for Wide & Deep\nClassification and regression with AutoML\nTabular Workflow for End-to-End AutoML is a complete AutoML\npipeline for classification and regression tasks. It is similar to theAutoML API,\nbut allows you to choose what to control and what to automate. Instead of having\ncontrols for thewholepipeline, you have controls forevery stepin the\npipeline. These pipeline controls include:\nData splitting\nFeature engineering\nArchitecture search\nModel training\nModel ensembling\nModel distillation\nSupportslarge datasetsthat are multiple TB in size and have up to 1000 columns.Allows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search.Allows you toimprove training speedby manually selecting the hardware used for training and architecture search.Allows you toreduce model size and improve latencywith distillation or by changing the ensemble size.Each AutoML component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures, and many more details.Each AutoML component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs, and more.\nSupportslarge datasetsthat are multiple TB in size and have up to 1000 columns.\nAllows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search.\nAllows you toimprove training speedby manually selecting the hardware used for training and architecture search.\nAllows you toreduce model size and improve latencywith distillation or by changing the ensemble size.\nEach AutoML component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures, and many more details.\nEach AutoML component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs, and more.\nTo learn more about Tabular Workflows, seeTabular Workflows on Vertex AI.\nTo learn more about Tabular Workflow for End-to-End AutoML, seeTabular Workflow for End-to-End AutoML.\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nTabular Workflow for TabNet is a pipeline that you can use to train\nclassification or regression models.TabNetusessequential attentionto choose which\nfeatures to reason from at each decision step. This promotes interpretability and\nmore efficient learning because the learning capacity is used for the most salient features.\nAutomatically selects the appropriate hyperparameter search space based on the dataset size, prediction type, and training budget.Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.Provides inherent model interpretability. You can get insight into which features TabNet used to make its decision.Supports GPU training.\nAutomatically selects the appropriate hyperparameter search space based on the dataset size, prediction type, and training budget.\nIntegrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.\nProvides inherent model interpretability. You can get insight into which features TabNet used to make its decision.\nSupports GPU training.\nTo learn more about Tabular Workflows, seeTabular Workflows on Vertex AI.\nTo learn more about Tabular Workflow for TabNet, seeTabular Workflow for TabNet.\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nTabular Workflow for Wide & Deep is a pipeline that you can use to train\nclassification or regression models.Wide & Deepjointly trains wide linear models and\ndeep neural networks. It combines the benefits of memorization and\ngeneralization. In some online experiments, the results showed that Wide & Deep\nsignificantly increased Google store application acquisitions compared with wide-only and deep-only models.\nIntegrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.\nIntegrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.\nTo learn more about Tabular Workflows, seeTabular Workflows on Vertex AI.\nTo learn more about Tabular Workflow for Wide & Deep, seeTabular Workflow for Wide & Deep.\nVertex AI offers integrated, fully managed pipelines for end-to-end\nclassification or regression tasks. Vertex AI searches for the\noptimal set of hyperparameters, trains multiple models with multiple sets of\nhyperparameters and then creates a single, final model from an ensemble of the\ntop models. Vertex AI considersneural networksand boosted trees for the model types.\nEasy to use: model type, model parameters, and hardware are chosen for you.\nFor further information, seeClassification and Regression Overview.\nVertex AI offers the following solutions for forecasting:\nTabular Workflow for Forecasting\nForecasting with AutoML\nForecasting with BigQuery ML ARIMA_PLUS\nForecasting with Prophet\nPreview\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\n        of theService Specific Terms.\n        \n        Pre-GA features are available \"as is\" and might have limited support.\n      \n      For more information, see thelaunch stage descriptions.\nTabular Workflow for Forecasting is the complete pipeline for\nforecasting tasks. It is similar to theAutoML API,\nbut allows you to choose what to control and what to automate. Instead of having\ncontrols for thewholepipeline, you have controls forevery stepin the\npipeline. These pipeline controls include:\nData splitting\nFeature engineering\nArchitecture search\nModel training\nModel ensembling\nSupportslarge datasetsthat are up to 1TB in size and have up to 200 columns.Allows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search.Allows you toimprove training speedby manually selecting the hardware used for training and architecture search.For some model training methods, allows you toreduce model size and improve latencyby changing the ensemble size.Each component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures and many more details.Each component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs and more.\nSupportslarge datasetsthat are up to 1TB in size and have up to 200 columns.\nAllows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search.\nAllows you toimprove training speedby manually selecting the hardware used for training and architecture search.\nFor some model training methods, allows you toreduce model size and improve latencyby changing the ensemble size.\nEach component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures and many more details.\nEach component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs and more.\nTo learn more about Tabular Workflows, seeTabular Workflows on Vertex AI.\nTo learn more about Tabular Workflow for Forecasting, seeTabular Workflow for Forecasting.\nVertex AI offers an integrated, fully managed pipeline for end-to-end\nforecasting tasks. Vertex AI searches for the optimal set of\nhyperparameters, trains multiple models with multiple sets of hyperparameters,\nand then creates a single, final model from an ensemble of the top models. You\ncan choose betweenTime series Dense Encoder (TiDE),Temporal Fusion Transformer (TFT),AutoML (L2L),\nand Seq2Seq+ for your model training method. Vertex AI considers\nonlyneural networksfor the model type.\nEasy to use: model parameters and hardware are chosen for you.\nFor further information, seeForecasting Overview.\nBigQuery ML ARIMA_PLUSis a univariate forecasting model. As\na statistical model, it is faster to train than amodel based on neural networks.\nWe recommend training a BigQuery ML ARIMA_PLUS model if you need to\nperform many quick iterations of model training or if you need an inexpensive\nbaseline to measure other models against.\nLikeProphet,\nBigQuery ML ARIMA_PLUS attempts to decompose each time series into\ntrends, seasons, and holidays, producing a forecast using the aggregation of\nthese models' predictions. One of the many differences, however, is that\nBQML ARIMA+ uses ARIMA to model the trend component, while Prophet attempts to\nfit a curve using a piecewise logistic or linear model.\nGoogle Cloud offers a pipeline for training a BigQuery ML ARIMA_PLUS\nmodel and a pipeline for getting batch predictions from a BigQuery ML ARIMA_PLUS model.\nBoth pipelines are instances ofVertex AI PipelinesfromGoogle Cloud Pipeline Components(GCPC).\nEasy to use: model parameters and hardware are chosen for you.\nFast: model training gives a low-cost baseline to compare other models against.\nFor further information, seeForecasting with ARIMA+.\nProphet is a forecasting model maintained by Meta. See theProphet paperfor algorithm details and thedocumentationfor more information about the library.\nLikeBigQuery ML ARIMA_PLUS,\nProphet attempts to decompose each time series into trends, seasons, and holidays,\nproducing a forecast using the aggregation of these models' predictions. An\nimportant difference, however, is that BQML ARIMA+ uses ARIMA to model the trend\ncomponent, while Prophet attempts to fit a curve using a piecewise logistic or\nlinear model.\nGoogle Cloud offers a pipeline for training a Prophet model and a pipeline for\ngetting batch predictions from a Prophet model. Both pipelines are instances ofVertex AI PipelinesfromGoogle Cloud Pipeline Components(GCPC).\nIntegration of Prophet with Vertex AI means that you can do the following:\nUse Vertex AIdata splittingandwindowing strategies.\nRead data from either BigQuery tables or CSVs stored in\nCloud Storage. Vertex AI expects each row to have the same\nformat asVertex AI Forecasting.\nAlthough Prophet is a multivariate model, Vertex AI supports\nonly a univariate version of it.\nFlexible: you can improve training speed by selecting the hardware used for training\nFor further information, seeForecasting with Prophet.\nLearn aboutmachine learning with tabular data.\nLearn aboutclassification and regression with AutoML.\nLearn aboutforecasting with AutoML.\nLearn aboutforecasting with Prophet.\nLearn aboutforecasting with BigQuery ML ARIMA_PLUS.\nLearn aboutTabular Workflows.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-08 UTC."
  },
  {
    "url": "https://cloud.google.com/docs/data",
    "title": "Data analytics",
    "content": "Home\nDocumentation\nUnlock your data's potential. Transform data into actionable AI insights with data analytics.\nGet access to Gemini 2.0 Flash Thinking\nFree monthly usage of popular products, including AI APIs and BigQuery\nNo automatic charges, no commitment\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nUse a serverless, cost-effective, and multi-cloud data and AI platform designed to help you turn big data into valuable business insights powered by Gemini.\nGet business intelligence in real time, built on governed data, that offers repeatable analysis and inspires in-depth understanding of the data.\nManage the end-to-end data lifecycle and make it easier to manage, discover, govern, and share data and AI assets.\nIngest, transform, and load data from disparate data sources in a scalable and secure fashion, and build end-to-end orchestration for the enterprise.\nMigrate your lakehouse or warehouse to BigQuery with easy to use tools powered by Gemini that assist in every phase of migration.\nEmpower your data journey, from robust batch processing using managed Apache Spark and Apache Hadoop, to dynamic real-time stream processing with serverless, scalable pipelines using Apache Beam.\nIngest, process, and analyze event streams in real time, and generate actionable, real-time insights.\nSeamlessly integrate the power of generative AI and machine learning directly within your data to unlock deeper insights.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-09 UTC."
  },
  {
    "url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "content": "Home\nDocumentation\nTransfer Appliance\nGuides\nTransfer Appliance is a high-capacity storage device that enables you\nto transfer and securely ship your data to a Google upload facility, where we\nupload your data to Cloud Storage. For Transfer Appliance\ncapacities and requirements, refer to theSpecifications page.\nYour data and network security is important. Transfer Appliance helps\nensure that you're connecting a trusted device to your equipment and\nnetwork, and secures your data from end to end so that it is read by people\nyou trust.\nTo ensure Transfer Appliance is trusted and safe to connect to your\ndevices, Transfer Appliance offers the following features:\nTamper resistant: Bad actors cannot easily open\nTransfer Appliance's physical case. We also apply\ntamper-evident tags to the shipping case, so that you can visually inspect\neach appliance's integrity prior to opening the package.\nTamper resistant: Bad actors cannot easily open\nTransfer Appliance's physical case. We also apply\ntamper-evident tags to the shipping case, so that you can visually inspect\neach appliance's integrity prior to opening the package.\nRuggedized: Transfer Appliance's shipping container is\nruggedized, ensuring your data arrives safely.\nRuggedized: Transfer Appliance's shipping container is\nruggedized, ensuring your data arrives safely.\nTrusted Platform Module (TPM) chip: We validate the TPM's Platform\nConfiguration Registers to ensure that the immutable root filesystem and\nsoftware components haven't been tampered with.\nTrusted Platform Module (TPM) chip: We validate the TPM's Platform\nConfiguration Registers to ensure that the immutable root filesystem and\nsoftware components haven't been tampered with.\nHardware attestation: We use a remote attestation process to validate\nthe appliance before you can connect it to your device and copy data to it.\nIf anything is amiss, we work with you to quickly send you a new appliance.\nHardware attestation: We use a remote attestation process to validate\nthe appliance before you can connect it to your device and copy data to it.\nIf anything is amiss, we work with you to quickly send you a new appliance.\nTo ensure your data is safe during and after transit,\nTransfer Appliance uses the following features to protect you:\nAES 256 encryption: Your data is encrypted with industry-standard\nencryption to keep it safe.\nAES 256 encryption: Your data is encrypted with industry-standard\nencryption to keep it safe.\nCustomer-managed encryption keys: We use encryption keys that you manage\nusing Cloud Key Management Service (Cloud KMS), enabling you to control and secure\nyour data prior to shipping an appliance back to us.\nCustomer-managed encryption keys: We use encryption keys that you manage\nusing Cloud Key Management Service (Cloud KMS), enabling you to control and secure\nyour data prior to shipping an appliance back to us.\nNIST 800-88 compliant data erasure: We securely erase your data from\nTransfer Appliance after uploading your data to\nCloud Storage. You can request a wipe certificate to verify that we've\nwiped your data.\nNIST 800-88 compliant data erasure: We securely erase your data from\nTransfer Appliance after uploading your data to\nCloud Storage. You can request a wipe certificate to verify that we've\nwiped your data.\nFor more information, refer toSecurity and encryption.\nTo enable you move data quickly and efficiently,\nTransfer Appliance has the following performance features:\nAll SSD drives: Increased reliability over hard disk drives to ensure\nyour transfer is smooth.\nAll SSD drives: Increased reliability over hard disk drives to ensure\nyour transfer is smooth.\nMultiple network connectivity options: Quickly move data from your\ndevices to Transfer Appliance, using either a 10Gbps RJ45 interface\nor a 40Gbps QSFP+ interface.\nMultiple network connectivity options: Quickly move data from your\ndevices to Transfer Appliance, using either a 10Gbps RJ45 interface\nor a 40Gbps QSFP+ interface.\nScalability with multiple appliances: You can scale your transfers by\nordering multiple appliance to increase your transfer speed.\nScalability with multiple appliances: You can scale your transfers by\nordering multiple appliance to increase your transfer speed.\nGlobally distributed processing: Reduced shipping times to and from\nGoogle ensures your data transfer to Cloud Storage is quick.\nGlobally distributed processing: Reduced shipping times to and from\nGoogle ensures your data transfer to Cloud Storage is quick.\nMinimal software: For Linux and Apple macOS systems, copy directly\nto Transfer Appliance by mounting the exposed NFS share on\nthe appliance to your workstation, using common software already installed\non the system. For Microsoft Windows systems, copy directly to\nTransfer Appliance from your workstation using SCP.\nMinimal software: For Linux and Apple macOS systems, copy directly\nto Transfer Appliance by mounting the exposed NFS share on\nthe appliance to your workstation, using common software already installed\non the system. For Microsoft Windows systems, copy directly to\nTransfer Appliance from your workstation using SCP.\nEnabling online mode allows you to perform online transfers by streaming data\ndirectly to your Cloud Storage bucket after copying it to your appliance.\nOnline transfers offer the following benefits:\nQuickly transfer data to Cloud Storage with low latency: Online\ntransfers are an accelerated method of transferring your data to\nCloud Storage, omitting the need to wait for your appliance to be shipped\nback to Google before the data is copied to your destination bucket.\nQuickly transfer data to Cloud Storage with low latency: Online\ntransfers are an accelerated method of transferring your data to\nCloud Storage, omitting the need to wait for your appliance to be shipped\nback to Google before the data is copied to your destination bucket.\nConnect to multiple appliances: Online mode allows parallel connectivity\nto multiple appliances.\nConnect to multiple appliances: Online mode allows parallel connectivity\nto multiple appliances.\nCost-effective: Online capability is offered as a low-cost, fully-managed\nmethod for transferring your data.\nCost-effective: Online capability is offered as a low-cost, fully-managed\nmethod for transferring your data.\nSecure connection: Your data is encrypted during online transfers,\nensuring end-to-end security. After the transfer is complete, your data is\nremoved from the appliance.\nSecure connection: Your data is encrypted during online transfers,\nensuring end-to-end security. After the transfer is complete, your data is\nremoved from the appliance.\nEasy to enable or disable: You can toggle between online and offline mode\nusing simple commands. For more information on how to enable or disable online\nmode, refer to theOnline/offline transferpage.\nEasy to enable or disable: You can toggle between online and offline mode\nusing simple commands. For more information on how to enable or disable online\nmode, refer to theOnline/offline transferpage.\nTransfer Appliance is a good fit for your data transfer needs if:\nYou are an existing Google Cloud customer.\nYour data resides in locations that Transfer Appliance isavailable.\nIt would take more than one week to upload your data over the network.\nOther Google Cloud transfer options include:\nStorage Transfer Serviceto move data to a\nCloud Storage bucket from other cloud storage providers or from youron-premisesstorage.\nStorage Transfer Serviceto move data to a\nCloud Storage bucket from other cloud storage providers or from youron-premisesstorage.\nBigQuery Data Transfer Serviceto move data from software as a\nservice (SaaS) applications to BigQuery.\nBigQuery Data Transfer Serviceto move data from software as a\nservice (SaaS) applications to BigQuery.\nTransfer service for on-premises datato move data from your\non-premises machines to Cloud Storage.\nTransfer service for on-premises datato move data from your\non-premises machines to Cloud Storage.\nTransfer Appliance is available in the following locations:\nFor a complete list of countries where Transfer Appliance is available, refer to the Order Appliance page on the Google Cloud console. If you don't find your country listed, reach out to Support at data-support@google.com.\nWith a typical network bandwidth of 100 Mbps, 300 terabytes of data takes about\n9 months to upload. However, with Transfer Appliance, you can\nreceive the appliance and capture 300 terabytes of data in under 25 days. Your\ndata can be accessed in Cloud Storage within another 25 days, all without\nconsuming any outbound network bandwidth.\nIf you need to transfer data from researchers, vendors, or other sites to\nGoogle Cloud, Transfer Appliance can move that data for you. Once\ntransferred toCloud StorageorBigQuery,\nyour data is accessible via our Dataflow processing service for machine\nlearning projects. Google Cloud Machine Learning Engine is a managed service\nthat enables you to easily build machine learning models, that work on any type\nof data, of any size.\nTransfer Appliance can assist you in taking advantage of hybrid\narchitectures, supporting current operations with existing on-premises\ninfrastructure while experimenting with the cloud. By transferring a copy of\nyour data to Google Cloud, you can decommission duplicate datasets, test cloud\ninfrastructure, and expose your data to machine learning and analysis.\nOffline data transfer is suited for moving large amounts of existing backup\nimages and archives to Cloud Storage, which can be stored in ultra low-cost,\nhighly-durable, and highly available storage classes such asArchive Storage. For structured and\nunstructured data sets, whether they are small and frequently accessed or huge\nand rarely referenced, Google offers solutions like Cloud Storage,\nBigQuery, and Dataproc to store and analyze that data.\nFor customers in the EU, appliances are shipped from Belgium. When\ndata capture is complete, you ship the appliance to Belgium for data upload.\nYour data is then uploaded to a Cloud Storage location in a region\nthat you have specified. If you choose a destination region within the EU, your\ndata never leaves the boundaries of the European Union during any part of the\ndata transfer process.\nRequest Transfer Appliance.\nLearn more about Transfer Appliance pricing.\nReview the procedure for using Transfer Appliance.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-14 UTC."
  },
  {
    "url": "https://cloud.google.com/code/docs/shell",
    "title": "Cloud Code for Cloud Shell documentation",
    "content": "Home\nCloud Code\nDocumentation\nCloud Code for Cloud Shell\nCloud Code for Cloud Shell provides IDE support for the full development cycle of\n  Kubernetes and Cloud Run applications, from creating a cluster to\n  running your finished application.\nBuild and test a proof of concept with the free trial credits and free monthly usage of\n            20+ products.\nAccess 20+ free products for common use cases, including AI APIs, VMs, data warehouses,\n          and more.\nCode with Gemini Code Assist\nCode with Gemini Code Assist\nRun a Kubernetes app with Cloud Code\nRun a Kubernetes app with Cloud Code\nDeploy a Cloud Run service with Cloud Code\nDeploy a Cloud Run service with Cloud Code\nManage Cloud APIs and Libraries\nManage Cloud APIs and Libraries\nMigrate applications to local IDEs\nMigrate applications to local IDEs\nWork with Google Cloud and Kubernetes YAML\nWork with Google Cloud and Kubernetes YAML\nDebug a Kubernetes application\nDebug a Kubernetes application\nCreate and configure a GKE cluster\nCreate and configure a GKE cluster\nDeploy a Cloud Run service\nDeploy a Cloud Run service\nCloud Code features\nCloud Code features\nLimitations and restrictions\nLimitations and restrictions\nGetting support\nGetting support\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-07 UTC."
  },
  {
    "url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "content": "Home\nBigQuery\nDocumentation\nGuides\nThis document describes how BigQuery processes queries, and it provides\nan overview of several features that are useful for understanding and\nanalyzing your data.\nBigQuery is optimized to run analytic queries on large datasets,\nincluding terabytes of data in seconds and petabytes in minutes. Understanding\nits capabilities and how it processes queries can help you maximize your data\nanalysis investments.\nTo take a tour of BigQuery's data analytics features directly\nin the Google Cloud console, clickTake the tour.\nTake the tour\nBigQuery supports several data analysis workflows:\nAd hoc analysis.BigQuery usesGoogleSQL,\nthe SQL dialect in BigQuery, to support ad hoc\nanalysis. You can run queries in the Google Cloud console or throughthird-party toolsthat integrate with BigQuery.\nAd hoc analysis.BigQuery usesGoogleSQL,\nthe SQL dialect in BigQuery, to support ad hoc\nanalysis. You can run queries in the Google Cloud console or throughthird-party toolsthat integrate with BigQuery.\nGeospatial analysis.BigQuery uses geography data types and\nGoogleSQL geography functions to let you analyze and visualize\ngeospatial data. For information about these data types and functions, seeIntroduction to geospatial analytics.\nGeospatial analysis.BigQuery uses geography data types and\nGoogleSQL geography functions to let you analyze and visualize\ngeospatial data. For information about these data types and functions, seeIntroduction to geospatial analytics.\nSearch for data.You canindex your datato\nperform flexible, optimizedsearcheson unstructured\ntext or semi-structured JSON data.\nSearch for data.You canindex your datato\nperform flexible, optimizedsearcheson unstructured\ntext or semi-structured JSON data.\nSearch for Google Cloud resources.Usenatural language search(Preview)\nto discover Google Cloud resources from within BigQuery.\nSearch for Google Cloud resources.Usenatural language search(Preview)\nto discover Google Cloud resources from within BigQuery.\nMachine learning.BigQuery MLuses GoogleSQL queries to let you create and execute machine\nlearning (ML) models in BigQuery.\nMachine learning.BigQuery MLuses GoogleSQL queries to let you create and execute machine\nlearning (ML) models in BigQuery.\nBusiness intelligence.BigQuery BI Engineis a fast, in-memory analysis service that lets you\nbuild rich, interactive dashboards and reports without compromising\nperformance, scalability, security, or data freshness.\nBusiness intelligence.BigQuery BI Engineis a fast, in-memory analysis service that lets you\nbuild rich, interactive dashboards and reports without compromising\nperformance, scalability, security, or data freshness.\nAI assistance.You can useGemini in\nBigQueryto prepare\nand explore your data, generate SQL queries and Python code, and visualize\nyour results.\nAI assistance.You can useGemini in\nBigQueryto prepare\nand explore your data, generate SQL queries and Python code, and visualize\nyour results.\nBigQuery can help you understand\nyour data before you start writing SQL queries. Use the following features\nif you're unfamiliar with your data, don't know which questions to ask,\nor need help writing SQL:\nTable explorer.Visually explore the\nrange and frequency of values in your table and\ninteractively build queries.\nTable explorer.Visually explore the\nrange and frequency of values in your table and\ninteractively build queries.\nData insights.Generate\nnatural language questions about your data, along with the SQL\nqueries to answer those questions.\nData insights.Generate\nnatural language questions about your data, along with the SQL\nqueries to answer those questions.\nData profile scan.See\nstatistical characteristics of your data, including average, unique, maximum,\nand minimum values.\nData profile scan.See\nstatistical characteristics of your data, including average, unique, maximum,\nand minimum values.\nData canvas.Query your data using natural\nlanguage, visualize results with charts, and ask follow-up questions.\nData canvas.Query your data using natural\nlanguage, visualize results with charts, and ask follow-up questions.\nThe primary way to analyze data in BigQuery is torun a SQL query. TheGoogleSQL dialectsupportsSQL:2011and includes extensions that support geospatial analysis and ML.\nBigQuery lets you query the following types of data sources:\nData stored in BigQuery.You canload data into BigQuery,\nmodify existing data by usingdata manipulation language\n(DML)\nstatements,\norwrite query resultsto a table. You canquery historical datafrom a point in\ntime within your time travel window.You can query data stored in\nsingle-region or multi-region locations, but you can't run a query against\nmultiple locations even if one is a single-region location and the other is\nthe multi-region location containing that single-region location. For more\ninformation, seeLocations, reservations, and\njobs.\nData stored in BigQuery.You canload data into BigQuery,\nmodify existing data by usingdata manipulation language\n(DML)\nstatements,\norwrite query resultsto a table. You canquery historical datafrom a point in\ntime within your time travel window.\nYou can query data stored in\nsingle-region or multi-region locations, but you can't run a query against\nmultiple locations even if one is a single-region location and the other is\nthe multi-region location containing that single-region location. For more\ninformation, seeLocations, reservations, and\njobs.\nExternal data.You can query various external data sources such as\nCloud Storage, or database services such as Spanner or\nCloud SQL. For information about how to\nset up connections to external sources, seeIntroduction to external data sources\nExternal data.You can query various external data sources such as\nCloud Storage, or database services such as Spanner or\nCloud SQL. For information about how to\nset up connections to external sources, seeIntroduction to external data sources\nMulti-cloud data.You can query data that's stored in other public clouds\nsuch as AWS or Azure. For information on how to set up connections to\nAmazon Simple Storage Service (Amazon S3) or Azure Blob Storage, seeIntroduction to BigQuery Omni.\nMulti-cloud data.You can query data that's stored in other public clouds\nsuch as AWS or Azure. For information on how to set up connections to\nAmazon Simple Storage Service (Amazon S3) or Azure Blob Storage, seeIntroduction to BigQuery Omni.\nPublic datasets.You can analyze any of\nthe datasets that are available in thepublic dataset marketplace.\nPublic datasets.You can analyze any of\nthe datasets that are available in thepublic dataset marketplace.\nBigQuery sharing (formerly Analytics Hub).You can publish and\nsubscribe to BigQuery datasets and Pub/Sub topics to\nshare data across organizational boundaries. For more information, seeIntroduction to BigQuery sharing.\nBigQuery sharing (formerly Analytics Hub).You can publish and\nsubscribe to BigQuery datasets and Pub/Sub topics to\nshare data across organizational boundaries. For more information, seeIntroduction to BigQuery sharing.\nYou canquery BigQuery databy using one of the following query job types:\nInteractive query jobs. By\ndefault, BigQuery runs queries as interactive query jobs, which\nare intended to start executing as quickly as possible.\nInteractive query jobs. By\ndefault, BigQuery runs queries as interactive query jobs, which\nare intended to start executing as quickly as possible.\nBatch query jobs. Batch queries\nhave lower priority than interactive queries. When a project or reservation\nis using all of its available compute resources, batch queries are more\nlikely to be queued and remain in the queue. After a batch query starts\nrunning, the batch query runs the same as an interactive query. For more\ninformation, seequery queues.\nBatch query jobs. Batch queries\nhave lower priority than interactive queries. When a project or reservation\nis using all of its available compute resources, batch queries are more\nlikely to be queued and remain in the queue. After a batch query starts\nrunning, the batch query runs the same as an interactive query. For more\ninformation, seequery queues.\nContinuous query jobs(Preview).\nWith these jobs, the query runs continuously, letting you analyze\nincoming data in BigQuery in real time and then write the\nresults to a BigQuery table, or export the results to\nBigtable or Pub/Sub. You can use this capability to\nperform time sensitive tasks, such as creating and immediately acting on\ninsights, applying real time machine learning (ML) inference, and\nbuilding event-driven data pipelines.\nContinuous query jobs(Preview).\nWith these jobs, the query runs continuously, letting you analyze\nincoming data in BigQuery in real time and then write the\nresults to a BigQuery table, or export the results to\nBigtable or Pub/Sub. You can use this capability to\nperform time sensitive tasks, such as creating and immediately acting on\ninsights, applying real time machine learning (ML) inference, and\nbuilding event-driven data pipelines.\nYou can run query jobs by using the following methods:\nCompose and run a query in theGoogle Cloud console.\nRun thebq querycommand in thebq command-line tool.\nProgrammatically call thejobs.queryorjobs.insertmethod in the BigQueryREST API.\nUse the BigQueryclient libraries.\nBigQuery lets yousave queriesandshare querieswith others.\nWhen you save a query, it can be private (visible only to you), shared at the\nproject level (visible to specific principals), or public (anyone can view it).\nFor more information, seeWork with saved queries.\nSeveral processes occur when BigQuery runs a query:\nExecution tree.When you run a query, BigQuery\ngenerates anexecution treethat breaks the query into stages. These stages\ncontain steps that can run in parallel.\nExecution tree.When you run a query, BigQuery\ngenerates anexecution treethat breaks the query into stages. These stages\ncontain steps that can run in parallel.\nShuffle tier.Stages communicate with one another by using a fast,\ndistributedshuffle tierthat stores intermediate data produced by the\nworkers of a stage. When possible, the shuffle tier leverages technologies\nsuch as a petabit network and RAM to quickly move data to worker nodes.\nShuffle tier.Stages communicate with one another by using a fast,\ndistributedshuffle tierthat stores intermediate data produced by the\nworkers of a stage. When possible, the shuffle tier leverages technologies\nsuch as a petabit network and RAM to quickly move data to worker nodes.\nQuery plan.When BigQuery has all the information that it\nneeds to run a query, it generates aquery plan. You canview the query planin\nthe Google Cloud console and use it to troubleshoot oroptimize query performance.\nQuery plan.When BigQuery has all the information that it\nneeds to run a query, it generates aquery plan. You canview the query planin\nthe Google Cloud console and use it to troubleshoot oroptimize query performance.\nQuery execution graph.You can review the query plan information in\ngraphical format for any query, whether running or completed, and seeperformance insightsto help you optimize\nyour queries.\nQuery execution graph.You can review the query plan information in\ngraphical format for any query, whether running or completed, and seeperformance insightsto help you optimize\nyour queries.\nQuery monitoring and dynamic planning.Besides the workers that perform\nthe work of the query plan itself, additional workers monitor and direct the\noverall progress of work throughout the system. As the query progresses,\nBigQuery might dynamically adjust the query plan to adapt to\nthe results of the various stages.\nQuery monitoring and dynamic planning.Besides the workers that perform\nthe work of the query plan itself, additional workers monitor and direct the\noverall progress of work throughout the system. As the query progresses,\nBigQuery might dynamically adjust the query plan to adapt to\nthe results of the various stages.\nQuery results.When a query is complete, BigQuery writes\nthe results to persistent storage and returns them to the user. This design\nlets BigQuery servecached resultsthe next time that query is\nrun.\nQuery results.When a query is complete, BigQuery writes\nthe results to persistent storage and returns them to the user. This design\nlets BigQuery servecached resultsthe next time that query is\nrun.\nThe performance of queries that are run repeatedly on the same data can vary\nbecause of the\nshared nature of the BigQuery environment, use of\ncached query results, or because\nBigQuery dynamically adjusts the query plan while the query runs.\nFor a typical busy system where many queries run concurrently,\nBigQuery uses several processes to smooth out variances in query\nperformance:\nBigQuery runs many queries in parallel and canqueue queriesto run when resources are\navailable.\nBigQuery runs many queries in parallel and canqueue queriesto run when resources are\navailable.\nAs queries start and finish, BigQuery redistributes\nresources fairly between new and running queries. This process ensures that\nquery performance doesn't depend on the order in which queries are submitted\nbut rather on the number of queries run at a given time.\nAs queries start and finish, BigQuery redistributes\nresources fairly between new and running queries. This process ensures that\nquery performance doesn't depend on the order in which queries are submitted\nbut rather on the number of queries run at a given time.\nWhen you run a query, you canview the query planin the Google Cloud console. You can also request execution details by using\ntheINFORMATION_SCHEMA.JOBS*viewsor thejobs.getREST API method.\nThe query plan includes details about query stages and steps. These details can\nhelp you identify ways to improve query performance. For example, if you notice\na stage that writes a lot more output than other stages, it might mean that you\nneed to filter earlier in the query.\nFor more information about the query plan and query optimization, see the\nfollowing resources:\nTo learn more about the query plan and see examples of how the plan\ninformation can help you to improve query performance, seeQuery plan and timeline.\nFor more information about query optimization in general, seeIntroduction to optimizing query performance.\nMonitoring and logging are crucial for running reliable applications in the\ncloud. BigQuery workloads are no exception, especially if your\nworkload has high volumes or is mission critical. BigQuery\nprovides various metrics, logs, and metadata views to help you monitor your\nBigQuery usage.\nFor more information, see the following resources:\nTo learn about monitoring options in BigQuery, seeIntroduction to BigQuery monitoring.\nTo learn about audit logs and how to analyze query behavior, seeBigQuery audit logs.\nBigQuery offers two pricing models for analytics:\nOn-demand pricing.You pay for the data scanned by your queries. You have a\nfixed,query-processing capacityfor each project,\nand your cost is based on the number of bytes processed.\nCapacity-based pricing.You purchase dedicated query-processing capacity.\nFor information about the two pricing models and to learn more about making reservations\nfor capacity-based pricing, seeIntroduction to reservations.\nBigQuery enforces project-level quotas on running queries. For\ninformation on query quotas, seeQuotas and limits.\nTo control query costs, BigQuery provides several options,\nincluding custom quotas and billing alerts. For more information, seeCreating custom cost controls.\nBigQuery supports both descriptive and predictive analytics and\nhelps you explore your data with AI powered tools, SQL, machine learning,\nnotebooks, and other third-party integrations.\nBigQuery Studio helps you discover, analyze, and run\ninference on data in BigQuery with the following features:\nA robustSQL editorthat provides code\ncompletion and generation, query validation,\nand estimation of bytes processed.\nEmbeddedPython notebooksbuilt usingColab Enterprise.\nNotebooks provide one-click Python development runtimes, and\nbuilt-in support forBigQuery DataFrames.\nAPySpark editorthat lets you create stored Python procedures for Apache Spark.\nAsset management and version history for code assets such as notebooks andsaved queries, built on top ofDataform.\nAssistive code development in the SQL editor and in notebooks, built on top ofGemini generative AI(Preview).\nDataplexfeatures fordata discovery,\nanddata profilinganddata qualityscans.\nThe ability to viewjob historyon a per-user or per-project basis.\nThe ability to analyze saved query results by connecting to other tools such\nas Looker and Google Sheets, and to export saved query results for\nuse in other applications.\nCompute Engine API\nAnalytics Hub API\nDataform API\nVertex AI API\nBigQuery Connection API\nBigQuery Data Policy API\nBigQuery Reservation API\nDataplex API\nBigQuery ML lets you use SQL in BigQuery to perform\nmachine learning (ML) and predictive analytics. For more information,\nseeIntroduction to BigQuery ML.\nIn addition to running queries in BigQuery, you can analyze your\ndata with various analytics and business intelligence tools that integrate with\nBigQuery, such as the following:\nLooker.Looker is an enterprise platform for\nbusiness intelligence, data applications, and embedded analytics. The\nLooker platform works with many datastores including\nBigQuery. For information on how to connect\nLooker to BigQuery, seeUsing Looker.\nLooker.Looker is an enterprise platform for\nbusiness intelligence, data applications, and embedded analytics. The\nLooker platform works with many datastores including\nBigQuery. For information on how to connect\nLooker to BigQuery, seeUsing Looker.\nLooker Studio.After you run a query, you can launch\nLooker Studio directly from BigQuery in the\nGoogle Cloud console. Then, in Looker Studio you can create\nvisualizations and explore the data that's returned from the query. For\ninformation about Looker Studio, seeLooker Studio overview.\nLooker Studio.After you run a query, you can launch\nLooker Studio directly from BigQuery in the\nGoogle Cloud console. Then, in Looker Studio you can create\nvisualizations and explore the data that's returned from the query. For\ninformation about Looker Studio, seeLooker Studio overview.\nConnected Sheets.You can also launch\nConnected Sheets directly from BigQuery in the\nconsole. Connected Sheets runs\nBigQuery queries on your behalf either upon your request or on\na defined schedule. Results of those queries are saved in your spreadsheet for\nanalysis and sharing. For information about Connected Sheets,\nseeUsing connected sheets.\nConnected Sheets.You can also launch\nConnected Sheets directly from BigQuery in the\nconsole. Connected Sheets runs\nBigQuery queries on your behalf either upon your request or on\na defined schedule. Results of those queries are saved in your spreadsheet for\nanalysis and sharing. For information about Connected Sheets,\nseeUsing connected sheets.\nTableau.You canconnect to a dataset from Tableau. Use\nBigQuery to power your charts, dashboards, and other data\nvisualizations.\nTableau.You canconnect to a dataset from Tableau. Use\nBigQuery to power your charts, dashboards, and other data\nvisualizations.\nSeveral third-party analytics tools work with BigQuery.\nFor example, you can connectTableauto BigQuery data and use its visualization tools to analyze and\nshare your analysis. For more information on considerations when using\nthird-party tools, seeThird-party tool integration.\nODBC and JDBC drivers are available and can be used to integrate your\napplication with BigQuery. The intent of these drivers is to help\nusers leverage the power of BigQuery with existing tooling and\ninfrastructure. For information on latest release and known issues, seeODBC and JDBC drivers for BigQuery.\nThe pandas libraries likepandas-gbqlet you interact with\nBigQuery data in Jupyter notebooks. For information about this\nlibrary and how it compares with using the BigQueryPython client library,\nseeComparison withpandas-gbq.\nYou can also use BigQuery with other notebooks and analysis\ntools. For more information, seeProgrammatic analysis tools.\nFor a full list of BigQuery analytics and broader technology\npartners, see thePartnerslist on the BigQuery product page.\nFor an introduction and overview of supported SQL statements, seeIntroduction to SQL in BigQuery.\nTo learn about the GoogleSQL syntax used for querying data in\nBigQuery, seeQuery syntax in GoogleSQL.\nLearn how torun a queryin\nBigQuery.\nLearn more aboutoptimizing query performance.\nLearn about getting started withnotebooks.\nLearn how toschedule a recurring query.\nExcept as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-05-21 UTC."
  }
]