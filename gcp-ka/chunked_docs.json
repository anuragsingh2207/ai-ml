[
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-0",
    "content": "Home Resource Manager Documentation Guides The Organization Policy Service gives you centralized and programmatic control over your organization's cloud resources. As theorganization policy administrator, you can configure constraints across your entireresource hierarchy. Centralize control to configure restrictions on how your organization's resources can be used. Define and establish guardrails for your development teams to stay within compliance boundaries. Help project owners and their teams move quickly without worry of breaking compliance. Organization policies allow you to do the following: Limit resource sharing based on domain. Limit the usage of Identity and Access Management (IAM) service accounts. Restrict the physical location of newly created resources. There are many more constraints that give you fine-grained control of your organization's resources. For more information, see thelist of all Organization Policy Service constraints. Identity and Access Managementfocuses onwho, and lets the administratorauthorizewho can take action on specific resources based on permissions. Organization Policy focuses onwhat, and lets the administrator set restrictions on specific resources to determine how they can be configured. An organization policy configures a singleconstraintthat restricts one or more Google Cloud services. The organization policy is set on an organization, folder, or project resource to enforce the constraint on that resource and any child resources. An organization policy contains one or morerulesthat specify how, and whether, to enforce the constraint. For example, an organization policy could contain one rule that enforces the constraint only on resources taggedenvironment=development, and another rule that prevents the constraint from being"
  },
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-1",
    "content": "enforced on other resources. Descendants of the resource to which the organization policy is attachedinheritthe organization policy. By applying an organization policy to the organization resource, the organization policy administrator can control enforcement of that organization policy and configuration of restrictions across your organization. A constraint is a particular type of restriction against aGoogle Cloud serviceor a list of Google Cloud services. Think of the constraint as a blueprint that defines what behaviors are controlled. For example, you can restrict project resources from accessing Compute Engine storage resources using thecompute.storageResourceUseRestrictionsconstraint. This blueprint is then set on a resource in yourresource hierarchyas an organization policy, which applies the rules defined in the constraint. The Google Cloud service mapped to that constraint and associated with that resource enforces the restrictions configured within the organization policy. An organization policy is defined in a YAML or JSON file by the constraint it enforces, and optionally by the conditions under which the constraint are enforced. Each organization policy enforces exactly one constraint in active mode, dry-run mode, or both. Predefined constraints have aconstraint typeof list or boolean, which determines the values that can be used for checking enforcement. The enforcing Google Cloud service will evaluate the constraint type and value to determine the restriction that is enforced. Custom constraints are functionally similar to boolean constraints, and are either enforced or not enforced. Managed constraints have list or boolean parameters. The available parameters are determined by the enforcing Google Cloud service. A list constraint is a predefined"
  },
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-2",
    "content": "constraint that allows or disallows a list of values that is defined in an organization policy. This list of values is expressed as a hierarchy subtree string. The subtree string specifies the type of resource it applies to. For example, the list constraintconstraints/compute.trustedImageProjectstakes a list of project IDs in the form ofprojects/PROJECT_ID. Values can be given a prefix in the formprefix:valuefor constraints that support them, which gives the value additional meaning: is:- applies a comparison against the exact value. This is the same behavior as not having a prefix, and is required when the value includes a colon. is:- applies a comparison against the exact value. This is the same behavior as not having a prefix, and is required when the value includes a colon. under:- applies a comparison to the value and all of its child values. If a resource is allowed or denied with this prefix, its child resources are also allowed or denied. The value provided must be the ID of an organization, folder, or project resource. under:- applies a comparison to the value and all of its child values. If a resource is allowed or denied with this prefix, its child resources are also allowed or denied. The value provided must be the ID of an organization, folder, or project resource. in:- applies a comparison to all resources that include this value. For example, you can addin:us-locationsto the denied list of theconstraints/gcp.resourceLocationsconstraint to block all locations that are included in theusregion. in:- applies a comparison to all resources that include this value. For example, you can addin:us-locationsto the denied list of theconstraints/gcp.resourceLocationsconstraint to block all locations that are included in theusregion. If no list of values is provided,"
  },
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-3",
    "content": "or the organization policy is set to the Google-managed default, then the default behavior of the constraint takes effect, which either allows all values or denies all values. The following organization policy enforces a list constraint that allows the Compute Engine VM instancesvm-1andvm-2inorganizations/1234567890123to access external IP addresses: A boolean constraint is a predefined constraint that is either enforced or not enforced. For example, the predefined constraintconstraints/compute.disableSerialPortAccesshas two possible states: Enforced - the constraint is enforced, and serial port access is not allowed. Not enforced - thedisableSerialPortAccessconstraint is not enforced or checked, so serial port access is allowed. If the organization policy is set to the Google-managed default, then the default behavior for the constraint takes effect. The following organization policy enforces a predefined constraint that disables the creation of external service accounts inorganizations/1234567890123: Managed constraints are constraints that have been built on thecustom organization policyplatform. The custom organization policy platform allows for organization policies to be designed with more flexibility, and with greater insight fromPolicy Intelligence tools. Managed constraints are designed to replace equivalent predefined constraints. If the equivalent predefined constraint has a constraint type of boolean, the managed constraint can either be enforced or not in the same way. For example, the following organization policy enforcesiam.managed.disableServiceAccountCreation, which is the equivalent constraint toiam.disableServiceAccountCreation: If the equivalent predefined constraint has a constraint type of list, the managed constraint supports defining parameters"
  },
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-4",
    "content": "that define the resources and behaviors that are restricted by the constraint. For example, the following organization policy enforces a managed constraint that only allows theexample.comandaltostrat.comdomains to be added to Essential Contacts fororganizations/1234567890123: To learn more about using managed constraints, seeUsing constraints. Custom constraints allow or restrict resource creation and updates in the same way that boolean constraints do, but allow administrators to configure conditions based on request parameters and other metadata. You can usePolicy Intelligence toolsto test and analyze your custom organization policies. For a list of service resources that support custom constraints, seeCustom constraint supported services. To learn more about using custom organization policies, seeCreating and managing custom organization policies. An organization policy in dry-run mode is created and enforced similarly to other organization policies, and violations of the policy are audit-logged, but the violating actions aren't denied. You can use organization policies in dry-run mode to monitor how policy changes would impact your workflows before it is enforced. For more information, seeCreate an organization policy in dry-run mode. Tags provide a way to conditionally enforce constraints based on whether a resource has a specific tag. You can use tags and conditional enforcement of constraints to provide centralized control of the resources in your hierarchy. For more information about tags, seeTags overview. To learn how to set a conditional organization policy using tags, seeSetting an organization policy with tags. When an organization policy is set on a resource, all descendants of that resource inherit the organization policy by default. If you set an"
  },
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-5",
    "content": "organization policy on the organization resource, then the configuration of restrictions defined by that policy will be passed down through all descendant folders, projects, and service resources. You can set an organization policy on a descendant resource that either overwrites the inheritance, or inherits the organization policy of the parent resource. In the latter case, the two organization policies are merged based on the rules of hierarchy evaluation. This provides precise control for how your organization policies apply throughout your organization, and where you want exceptions made. To learn more, seeUnderstanding hierarchy evaluation. A violation is when a Google Cloud service acts or is in a state that is counter to the organization policy restriction configuration within the scope of its resource hierarchy. Google Cloud services will enforce constraints to prevent violations, but the application of new organization policies is usually not retroactive. If an organization policy constraint is retroactively enforced, it will be labeled as such on theorganization policy constraintspage. If a new organization policy sets a restriction on an action or state that a service is already in, the policy is considered to be in violation, but the service won't stop its original behavior. You will need to address this violation manually. This prevents the risk of a new organization policy completely shutting down your business continuity. Policy Intelligence is a suite of tools designed to help you manage security policies. These tools can help you understand resource usage, understand and improve existing security policies, and prevent policy misconfigurations. Some Policy Intelligence tools are specifically designed to help test and analyze Organization Policy Service"
  },
  {
    "source_url": "https://cloud.google.com/resource-manager/docs/organization-policy/overview",
    "title": "Introduction to the Organization Policy ServiceStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/resource-manager/docs/organization-policy/overview#chunk-6",
    "content": "policies. We recommend that you test and dry-run all changes to your organization policies. With Policy Intelligence, you can do tasks like the following: Test changesto organization policies and constraints and identify resources that are non-compliant under the proposed policy (Preview)) Create a dry-run organization policyto monitor how a policy change would affect your workflows Analyze existing organization policiesto understand which Google Cloud resources are covered by which organization policy To learn more about these tools and other Policy Intelligence tools, seePolicy Intelligence overview. Read theCreating and managing organization resourcespage to learn how to acquire an organization resource. Learnhow to define organization policies. Explore thesolutions you can accomplishwith organization policy constraints. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-01 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training-overview",
    "title": "Train and use your own modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training-overview#chunk-0",
    "content": "Home Vertex AI Documentation AutoML: Create and train models with minimal technical knowledge and effort. To learn more about AutoML, seeAutoML beginner's guide. Vertex AI custom training: Create and train models at scale using any ML framework. To learn more about custom training on Vertex AI, seeCustom training overview. Ray on Vertex AI: Use open source Ray code to write programs and develop applications on Vertex AI with minimal changes. For help on deciding which of these methods to use, seeChoose a training method. AutoML on Vertex AI lets you build a code-free ML model based on the training data that you provide. AutoML can automate tasks like data preparation, model selection, hyperparameter tuning, and deployment for various data types and prediction tasks, which can make ML more accessible for a wide range of users. The types of models you can build depend on the type of data that you have. Vertex AI offers AutoML solutions for the following data types and model objectives: To learn more about AutoML, seeAutoML training overview. If none of the AutoML solutions address your needs, you can also create your own training application and use it to train custom models on Vertex AI. You can use any ML framework that you want and configure the compute resources to use for training, including the following: Type and number of VMs. Graphics processing units (GPUs). Tensor processing units (TPUs). Type and size of boot disk. To learn more about custom training on Vertex AI, seeCustom training overview. Ray on Vertex AI is a service that lets you use the open-source Ray framework for scaling AI and Python applications directly within the Vertex AI platform. Ray is designed to provide the infrastructure for distributed computing and parallel processing for your ML"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training-overview",
    "title": "Train and use your own modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training-overview#chunk-1",
    "content": "workflow. Ray on Vertex AI provides a managed environment for running distributed applications using the Ray framework, offering scalability and integration with Google Cloud services. To learn more about Ray on Vertex AI seeRay on Vertex AI overview. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-12 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-0",
    "content": "Home Documentation Authentication This document helps you understand some key authentication methods and concepts, and where to get help with implementing or troubleshooting authentication. The primary focus of the authentication documentation is for Google Cloud services, but the list ofauthentication use casesand the introductory material on this page includes use cases for other Google products as well.IntroductionAuthentication is the process by which your identity is confirmed through the use of some kind ofcredential. Authentication is about proving that you are who you say you are.Google provides many APIs and services, which require authentication to access. Google also provides a number of services that host applications written by our customers; these applications also need to determine the identity of their users.How to get help with authenticationI want to...InformationAuthenticate to Vertex AI in express mode (Preview).Use the API key created for you during the sign-on process to authenticate to Vertex AI. For more information, seeVertex AI in express mode overview.Authenticate to a Google Cloud service from my application using a high-level programming language.Set up Application Default Credentials, and then use one of theCloud Client Libraries.Authenticate to an application that requires an ID token.Get an OpenID Connect (OIDC) ID tokenand provide it with your request.Implement user authentication for an application that accesses Google or Google Cloud services and resources.SeeAuthenticate application usersfor a comparison of options.Try out somegcloudcommands in my local development environment.Initialize the gcloud CLI.Try out some Google Cloud REST API requests in my local development environment.Use a command-line tool such ascurltocall the REST"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-1",
    "content": "API.Try out a code snippet included in my product documentation.Set up ADC for a local development environment, and install your product's client library in your local environment. The client libraryfinds your credentials automatically.Get help with another authentication use case.See theAuthentication use casespage.See a list of the products Google provides in the identity and access management space.See theGoogle identity and access management productspage.Choose the right authentication method for your use caseWhen you access Google Cloud services by using the Google Cloud CLI, Cloud Client Libraries, tools that supportApplication Default Credentials (ADC)like Terraform, or REST requests, use the following diagram to help you choose an authentication method:This diagram guides you through the following questions:Are you running code in a single-user development environment, such as your own workstation, Cloud Shell, or a virtual desktop interface?If yes, proceed to question 4.If no, proceed to question 2.Are you running code in Google Cloud?If yes, proceed to question 3.If no, proceed to question 5.Are you running containers in Google Kubernetes Engine?If yes, useWorkload Identity Federation for GKEto attach service accounts to Kubernetes pods.If no,attach a service accountto the resource.Does your use case require a service account?For example, you want to configure authentication and authorization consistently for your application across all environments.If no,authenticate with user credentials.If yes,impersonate a service account with user credentials.Does your workload authenticate with an external identity provider that supportsworkload identity federation?If yes,configure Workload Identity Federationto let applications running on-premises or on other cloud"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-2",
    "content": "providers use a service account.If no,create a service account key.OAuth 2.0Google APIs implement and extend theOAuth 2.0 framework. See the documentation for your environment and use case for details.Authorization methods for Google Cloud servicesGoogle Cloud services useIdentity and Access Management (IAM)for authentication. IAM offers granular control, by principal and by resource. When you authenticate to Google Cloud services, you generally use a scope that includes all Google Cloud services (https://www.googleapis.com/auth/cloud-platform).OAuth 2.0 scopes can provide a second layer of protection, which is useful if your code is running in an environment where token security is a concern, such as a mobile app. In this scenario, you can use finer-grained scopes to reduce risk in the event of a compromised token. You can find the list of scopes accepted by an API method in its API reference pages in the product documentation.Application Default CredentialsApplication Default Credentials (ADC)is a strategy used by the authentication libraries to automatically find credentials based on the application environment. The authentication libraries make those credentials available toCloud Client Libraries and Google API Client Libraries. When you use ADC, your code can run in either a development or production environment without changing how your application authenticates to Google Cloud services and APIs.Using ADC can simplify your development process, because it lets you use the same authentication code in a variety of environments. If you're using a service in express mode, however, you don't need to use ADC.Before you can use ADC,you must provide your credentials to ADC, based on where you want your code to run. ADCautomatically locates credentialsand gets a token in"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-3",
    "content": "the background, enabling your authentication code to run in different environments without modification. For example, the same version of your code could authenticate with Google Cloud APIs when running on a development workstation or on Compute Engine.Your gcloud credentials are not the same as the credentials you provide to ADC using the gcloud CLI. For more information, seegcloud CLI authentication configuration and ADC configuration.TerminologyThe following terms are important to understand when discussing authentication and authorization.AuthenticationAuthentication is the process of determining the identity of the principal attempting to access a resource.AuthorizationAuthorization is the process of determining whether the principal or application attempting to access a resource has been authorized for that level of access.CredentialsWhen this document uses the termuser account, it refers to a Google Account, or a user account managed by your identity provider and federated withWorkforce Identity Federation.For authentication, credentials are a digital object that provide proof of identity. Passwords, PINs, and biometric data can all be used as credentials, depending on the application requirements. For example, when you log into your user account, you provide your password and satisfy any two-factor authentication requirement as proof that the account in fact belongs to you, and you are not being spoofed by a bad actor.Tokensare not credentials. They are a digital object that proves that the caller provided proper credentials.The type of credential you need to provide depends on what you are authenticating to.The following types of credentials can be created in the Google Cloud console:API keysYou can use API keys with APIs that accept them to access the API. API"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-4",
    "content": "keys that are not bound to a service account provide a project, which is used for billing and quota purposes. If the API key is bound to a service account, the API key also provides the identity and authorization of the service account (Preview).For more information about API keys, seeAPI keys. For more information about API keys that are bound to a service account, see theGoogle Cloud express mode FAQ.OAuth Client IDsOAuth Client IDs are used to identify an application to Google Cloud. This is necessary when you want to access resources owned by your end users, also called three-legged OAuth (3LO). For more information about how to get and use an OAuth Client ID, seeSetting up OAuth 2.0.Service account keysService account keys identify a principal (the service account) and the project associated with the service account.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the security of the private key and for other operations described byBest practices for managing service account keys. If you are prevented from creating a service account key, service account key creation might be disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use. For more information, seeSecurity requirements for externally sourced credentials.You can also create credentials by using the gcloud CLI. These credentials include the following types:Local ADC filesCredential configurations used byWorkload Identity FederationCredential configurations used byWorkforce Identity"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-5",
    "content": "FederationNote:If you are accepting credential configurations (JSON, files, or streams) created by an external organization, you must validate the credential configuration before you use it. For more information, seeSecurity requirements when using credential configurations from an external source.PrincipalA principal is an identity that can be granted access to a resource. For authentication, Google APIs support two types of principals:user accountsandservice accounts.Whether you use a user account or a service account to authenticate depends on your use case. You might use both, each at different stages of your project or in different development environments.User accountsUser accounts represent a developer, administrator, or any other person who interacts with Google APIs and services.User accounts are managed asGoogle Accounts, either withGoogle WorkspaceorCloud Identity. They can also be user accounts that are managed by a third-party identity provider and federated withWorkforce Identity Federation.With a user account, you can authenticate to Google APIs and services in the following ways:Use the gcloud CLI toset up Application Default Credentials (ADC).Use your user credentials tosign in to the Google Cloud CLI, and then use the tool to access Google Cloud services.Use your user credentials toimpersonate a service account.Use your user credentials tosign in to the Google Cloud CLI, and then use the tool togenerate access tokens.For an overview of ways to configure identities for users in Google Cloud, seeIdentities for users.Service accountsService accountsare accounts that do not represent a human user. They provide a way to manage authentication and authorization when a human is not directly involved, such as when an application needs to access Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-6",
    "content": "resources. Service accounts are managed by IAM.The following list provides some methods for using a service account to authenticate to Google APIs and services, in order from most secure to least secure. For more information, seeChoose the right authentication method for your use caseon this page.Attach a user-managed service account to the resourceanduse ADC to authenticate.This is the recommended way to authenticate production code running on Google Cloud.Use a service account to impersonate another service account.Service account impersonation lets you temporarily grant more privileges to a service account. Granting extra privileges on a temporary basis enables that service account to perform the required access without having to permanently acquire more privilege.UseWorkload Identity Federationto authenticate workloads that run on-premises or on a different cloud provider.Use thedefault service account.Using the default service account is not recommended, because by default the default service account is highly privileged, which violates theprinciple of least privilege.Use a service account key.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the security of the private key and for other operations described byBest practices for managing service account keys. If you are prevented from creating a service account key, service account key creation might be disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use. For more information,"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-7",
    "content": "seeSecurity requirements for externally sourced credentials.For an overview of ways to configure workload identities, including service accounts, for Google Cloud, seeIdentities for workloads. For best practices, seeBest practices for using service accounts.TokenFor authentication and authorization, a token is a digital object that shows that a caller provided proper credentials that were exchanged for that token. The token contains information about the identity of the principal making the request and what kind of access they are authorized to make.Tokens can be thought of as being like hotel keys. When you check in to a hotel and present the proper documentation to the hotel registration desk, you receive a key that gives you access to specific hotel resources. For example, the key might give you access to your room and the guest elevator, but would not give you access to any other room or the service elevator.With the exception of API keys, Google APIs do not support credentials directly. Your application must acquire or generate a token and provide it to the API. There are several different types of tokens. For more information, seeToken types.Workload and workforceGoogle Cloud identity and access products enable access to Google Cloud services and resources for both programmatic access and human users. Google Cloud uses the termsworkloadfor programmatic access andworkforcefor user access.Workload Identity Federationlets you provide access to on-premises or multi-cloud workloads without having to create and manage service account keys.Workforce Identity Federationlets you use an external identity provider to authenticate and authorize a workforce\u2014a group of users, such as employees, partners, and contractors\u2014using IAM, so that the users can access Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-8",
    "content": "services.What's nextLearn more about how Google Cloud servicesuse IAM to control access to Google Cloud resources.Understandhow Application Default Credentials works, andhow you can set it up for a variety of development environments. Authentication is the process by which your identity is confirmed through the use of some kind ofcredential. Authentication is about proving that you are who you say you are. Google provides many APIs and services, which require authentication to access. Google also provides a number of services that host applications written by our customers; these applications also need to determine the identity of their users. When you access Google Cloud services by using the Google Cloud CLI, Cloud Client Libraries, tools that supportApplication Default Credentials (ADC)like Terraform, or REST requests, use the following diagram to help you choose an authentication method: This diagram guides you through the following questions: Are you running code in a single-user development environment, such as your own workstation, Cloud Shell, or a virtual desktop interface?If yes, proceed to question 4.If no, proceed to question 2. If yes, proceed to question 4. If no, proceed to question 2. Are you running code in Google Cloud?If yes, proceed to question 3.If no, proceed to question 5. If yes, proceed to question 3. If no, proceed to question 5. Are you running containers in Google Kubernetes Engine?If yes, useWorkload Identity Federation for GKEto attach service accounts to Kubernetes pods.If no,attach a service accountto the resource. If yes, useWorkload Identity Federation for GKEto attach service accounts to Kubernetes pods. If no,attach a service accountto the resource. Does your use case require a service account?For example, you want to configure"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-9",
    "content": "authentication and authorization consistently for your application across all environments.If no,authenticate with user credentials.If yes,impersonate a service account with user credentials. Does your use case require a service account? For example, you want to configure authentication and authorization consistently for your application across all environments. If no,authenticate with user credentials. If yes,impersonate a service account with user credentials. Does your workload authenticate with an external identity provider that supportsworkload identity federation?If yes,configure Workload Identity Federationto let applications running on-premises or on other cloud providers use a service account.If no,create a service account key. If yes,configure Workload Identity Federationto let applications running on-premises or on other cloud providers use a service account. If no,create a service account key. Google APIs implement and extend theOAuth 2.0 framework. See the documentation for your environment and use case for details. Google Cloud services useIdentity and Access Management (IAM)for authentication. IAM offers granular control, by principal and by resource. When you authenticate to Google Cloud services, you generally use a scope that includes all Google Cloud services (https://www.googleapis.com/auth/cloud-platform). OAuth 2.0 scopes can provide a second layer of protection, which is useful if your code is running in an environment where token security is a concern, such as a mobile app. In this scenario, you can use finer-grained scopes to reduce risk in the event of a compromised token. You can find the list of scopes accepted by an API method in its API reference pages in the product documentation. Application Default Credentials (ADC)is a strategy used by"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-10",
    "content": "the authentication libraries to automatically find credentials based on the application environment. The authentication libraries make those credentials available toCloud Client Libraries and Google API Client Libraries. When you use ADC, your code can run in either a development or production environment without changing how your application authenticates to Google Cloud services and APIs. Using ADC can simplify your development process, because it lets you use the same authentication code in a variety of environments. If you're using a service in express mode, however, you don't need to use ADC. Before you can use ADC,you must provide your credentials to ADC, based on where you want your code to run. ADCautomatically locates credentialsand gets a token in the background, enabling your authentication code to run in different environments without modification. For example, the same version of your code could authenticate with Google Cloud APIs when running on a development workstation or on Compute Engine. Your gcloud credentials are not the same as the credentials you provide to ADC using the gcloud CLI. For more information, seegcloud CLI authentication configuration and ADC configuration. The following terms are important to understand when discussing authentication and authorization. Authentication is the process of determining the identity of the principal attempting to access a resource. Authorization is the process of determining whether the principal or application attempting to access a resource has been authorized for that level of access. When this document uses the termuser account, it refers to a Google Account, or a user account managed by your identity provider and federated withWorkforce Identity Federation. For authentication, credentials are a digital"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-11",
    "content": "object that provide proof of identity. Passwords, PINs, and biometric data can all be used as credentials, depending on the application requirements. For example, when you log into your user account, you provide your password and satisfy any two-factor authentication requirement as proof that the account in fact belongs to you, and you are not being spoofed by a bad actor. Tokensare not credentials. They are a digital object that proves that the caller provided proper credentials. The type of credential you need to provide depends on what you are authenticating to. The following types of credentials can be created in the Google Cloud console: API keysYou can use API keys with APIs that accept them to access the API. API keys that are not bound to a service account provide a project, which is used for billing and quota purposes. If the API key is bound to a service account, the API key also provides the identity and authorization of the service account (Preview).For more information about API keys, seeAPI keys. For more information about API keys that are bound to a service account, see theGoogle Cloud express mode FAQ. API keys You can use API keys with APIs that accept them to access the API. API keys that are not bound to a service account provide a project, which is used for billing and quota purposes. If the API key is bound to a service account, the API key also provides the identity and authorization of the service account (Preview). For more information about API keys, seeAPI keys. For more information about API keys that are bound to a service account, see theGoogle Cloud express mode FAQ. OAuth Client IDsOAuth Client IDs are used to identify an application to Google Cloud. This is necessary when you want to access resources owned by your end users, also called"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-12",
    "content": "three-legged OAuth (3LO). For more information about how to get and use an OAuth Client ID, seeSetting up OAuth 2.0. OAuth Client IDs OAuth Client IDs are used to identify an application to Google Cloud. This is necessary when you want to access resources owned by your end users, also called three-legged OAuth (3LO). For more information about how to get and use an OAuth Client ID, seeSetting up OAuth 2.0. Service account keysService account keys identify a principal (the service account) and the project associated with the service account.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the security of the private key and for other operations described byBest practices for managing service account keys. If you are prevented from creating a service account key, service account key creation might be disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use. For more information, seeSecurity requirements for externally sourced credentials. Service account keys Service account keys identify a principal (the service account) and the project associated with the service account. If you acquired the service account key from an external source, you must validate it before use. For more information, seeSecurity requirements for externally sourced credentials. You can also create credentials by using the gcloud CLI. These credentials include the following types: Local ADC files Credential configurations used byWorkload Identity Federation Credential"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-13",
    "content": "configurations used byWorkforce Identity Federation A principal is an identity that can be granted access to a resource. For authentication, Google APIs support two types of principals:user accountsandservice accounts. Whether you use a user account or a service account to authenticate depends on your use case. You might use both, each at different stages of your project or in different development environments. User accounts represent a developer, administrator, or any other person who interacts with Google APIs and services. User accounts are managed asGoogle Accounts, either withGoogle WorkspaceorCloud Identity. They can also be user accounts that are managed by a third-party identity provider and federated withWorkforce Identity Federation.With a user account, you can authenticate to Google APIs and services in the following ways:Use the gcloud CLI toset up Application Default Credentials (ADC).Use your user credentials tosign in to the Google Cloud CLI, and then use the tool to access Google Cloud services.Use your user credentials toimpersonate a service account.Use your user credentials tosign in to the Google Cloud CLI, and then use the tool togenerate access tokens.For an overview of ways to configure identities for users in Google Cloud, seeIdentities for users.Service accountsService accountsare accounts that do not represent a human user. They provide a way to manage authentication and authorization when a human is not directly involved, such as when an application needs to access Google Cloud resources. Service accounts are managed by IAM.The following list provides some methods for using a service account to authenticate to Google APIs and services, in order from most secure to least secure. For more information, seeChoose the right authentication method"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-14",
    "content": "for your use caseon this page.Attach a user-managed service account to the resourceanduse ADC to authenticate.This is the recommended way to authenticate production code running on Google Cloud.Use a service account to impersonate another service account.Service account impersonation lets you temporarily grant more privileges to a service account. Granting extra privileges on a temporary basis enables that service account to perform the required access without having to permanently acquire more privilege.UseWorkload Identity Federationto authenticate workloads that run on-premises or on a different cloud provider.Use thedefault service account.Using the default service account is not recommended, because by default the default service account is highly privileged, which violates theprinciple of least privilege.Use a service account key.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the security of the private key and for other operations described byBest practices for managing service account keys. If you are prevented from creating a service account key, service account key creation might be disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use. For more information, seeSecurity requirements for externally sourced credentials.For an overview of ways to configure workload identities, including service accounts, for Google Cloud, seeIdentities for workloads. For best practices, seeBest practices for using service accounts.TokenFor"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-15",
    "content": "authentication and authorization, a token is a digital object that shows that a caller provided proper credentials that were exchanged for that token. The token contains information about the identity of the principal making the request and what kind of access they are authorized to make.Tokens can be thought of as being like hotel keys. When you check in to a hotel and present the proper documentation to the hotel registration desk, you receive a key that gives you access to specific hotel resources. For example, the key might give you access to your room and the guest elevator, but would not give you access to any other room or the service elevator.With the exception of API keys, Google APIs do not support credentials directly. Your application must acquire or generate a token and provide it to the API. There are several different types of tokens. For more information, seeToken types.Workload and workforceGoogle Cloud identity and access products enable access to Google Cloud services and resources for both programmatic access and human users. Google Cloud uses the termsworkloadfor programmatic access andworkforcefor user access.Workload Identity Federationlets you provide access to on-premises or multi-cloud workloads without having to create and manage service account keys.Workforce Identity Federationlets you use an external identity provider to authenticate and authorize a workforce\u2014a group of users, such as employees, partners, and contractors\u2014using IAM, so that the users can access Google Cloud services.What's nextLearn more about how Google Cloud servicesuse IAM to control access to Google Cloud resources.Understandhow Application Default Credentials works, andhow you can set it up for a variety of development environments. With a user account, you can"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-16",
    "content": "authenticate to Google APIs and services in the following ways: Use the gcloud CLI toset up Application Default Credentials (ADC). Use your user credentials tosign in to the Google Cloud CLI, and then use the tool to access Google Cloud services. Use your user credentials toimpersonate a service account. Use your user credentials tosign in to the Google Cloud CLI, and then use the tool togenerate access tokens. For an overview of ways to configure identities for users in Google Cloud, seeIdentities for users. Service accountsare accounts that do not represent a human user. They provide a way to manage authentication and authorization when a human is not directly involved, such as when an application needs to access Google Cloud resources. Service accounts are managed by IAM. The following list provides some methods for using a service account to authenticate to Google APIs and services, in order from most secure to least secure. For more information, seeChoose the right authentication method for your use caseon this page. Attach a user-managed service account to the resourceanduse ADC to authenticate.This is the recommended way to authenticate production code running on Google Cloud. Attach a user-managed service account to the resourceanduse ADC to authenticate. This is the recommended way to authenticate production code running on Google Cloud. Use a service account to impersonate another service account.Service account impersonation lets you temporarily grant more privileges to a service account. Granting extra privileges on a temporary basis enables that service account to perform the required access without having to permanently acquire more privilege. Use a service account to impersonate another service account. Service account impersonation lets you temporarily"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-17",
    "content": "grant more privileges to a service account. Granting extra privileges on a temporary basis enables that service account to perform the required access without having to permanently acquire more privilege. UseWorkload Identity Federationto authenticate workloads that run on-premises or on a different cloud provider. UseWorkload Identity Federationto authenticate workloads that run on-premises or on a different cloud provider. Use thedefault service account.Using the default service account is not recommended, because by default the default service account is highly privileged, which violates theprinciple of least privilege. Use thedefault service account. Using the default service account is not recommended, because by default the default service account is highly privileged, which violates theprinciple of least privilege. Use a service account key.Note:Service account keys are a security risk if not managed correctly. You shouldchoose a more secure alternative to service account keyswhenever possible. If you must authenticate with a service account key, you are responsible for the security of the private key and for other operations described byBest practices for managing service account keys. If you are prevented from creating a service account key, service account key creation might be disabled for your organization. For more information, seeManaging secure-by-default organization resources.If you acquired the service account key from an external source, you must validate it before use. For more information, seeSecurity requirements for externally sourced credentials. Use a service account key. If you acquired the service account key from an external source, you must validate it before use. For more information, seeSecurity requirements for externally sourced"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-18",
    "content": "credentials. For an overview of ways to configure workload identities, including service accounts, for Google Cloud, seeIdentities for workloads. For best practices, seeBest practices for using service accounts. For authentication and authorization, a token is a digital object that shows that a caller provided proper credentials that were exchanged for that token. The token contains information about the identity of the principal making the request and what kind of access they are authorized to make. Tokens can be thought of as being like hotel keys. When you check in to a hotel and present the proper documentation to the hotel registration desk, you receive a key that gives you access to specific hotel resources. For example, the key might give you access to your room and the guest elevator, but would not give you access to any other room or the service elevator. With the exception of API keys, Google APIs do not support credentials directly. Your application must acquire or generate a token and provide it to the API. There are several different types of tokens. For more information, seeToken types. Google Cloud identity and access products enable access to Google Cloud services and resources for both programmatic access and human users. Google Cloud uses the termsworkloadfor programmatic access andworkforcefor user access. Workload Identity Federationlets you provide access to on-premises or multi-cloud workloads without having to create and manage service account keys. Workforce Identity Federationlets you use an external identity provider to authenticate and authorize a workforce\u2014a group of users, such as employees, partners, and contractors\u2014using IAM, so that the users can access Google Cloud services. Learn more about how Google Cloud servicesuse IAM to control"
  },
  {
    "source_url": "https://cloud.google.com/docs/authentication",
    "title": "Authentication methods at GoogleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/authentication#chunk-19",
    "content": "access to Google Cloud resources. Understandhow Application Default Credentials works, andhow you can set it up for a variety of development environments. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-13 UTC."
  },
  {
    "source_url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch",
    "title": "Google Distributed Cloud air-gapped documentationStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch#chunk-0",
    "content": "Home Google Distributed Cloud air-gapped The Google Distributed Cloud (GDC) air-gapped option doesn't require connectivity to Google Cloud at any time to manage infrastructure, services, APIs or tools, and uses a local control plane for operations. Distributed Cloud runs sensitive workloads and supports public-sector customers and commercial entities to address data residency and strict security and privacy requirements. Distributed Cloud provides you with a safe and secure way to modernize an on-premises deployment, whether you do it yourself or choose to host through a designated, trusted partner. View Distributed Cloudgeneral availability announcement,product page,overview,release notes,FAQ, andapplication programming interfaces."
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-0",
    "content": "Home Google Cloud Free Program Documentation The Google Cloud Free Program comprises the following: 90-day, $300 Free Trial: New Google Cloud and Google Maps Platform users can take advantage of a 90-day trial period that includes $300 in free Cloud Billing credits to explore and evaluate Google Cloud and Google Maps Platform products and services. You can use these credits toward one or a combination of products. Free Tier: All Google Cloud customers can use select Google Cloud products\u2014like Compute Engine, Cloud Storage, and BigQuery\u2014free of charge, within specified monthly usage limits. When you stay withinthe Free Tier limits, these resources are not charged against your Free Trial credits or to your Cloud Billing account's payment method after your trial ends. Google Maps Platform free usage caps: Google Maps Platform offers key pricing incentives, including free usage caps and volume discounts. For more information,see the billing and pricing overview. For more information about Google Maps Platform pay-as-you-go pricing, seethe Google Maps Platform pricing page, which includes a monthly cost calculator for Maps, Routes, Places, and Environment. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. The Free Trial provides you with free Cloud Billing credits to pay for resources used while you learn about Google Cloud. You're eligible for the Free Trial if you meet the following conditions: You've never been a paying customer of Google Cloud, Google Maps Platform, or Firebase. You haven't previously signed up for the Free Trial. If you're in India, you must have an INR-based"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-1",
    "content": "Cloud Billing account before creating Firebase billing accounts to sign up for the Free Trial. To complete your Free Trial signup, you must provide acredit card or other payment methodwhich is used to set up a Free Trial Cloud Billing account and to verify your identity. Be assured, when a Cloud Billing account has thestatus of aFree trial account, Google won't charge you. When you complete theStart freesignup, the 90-day, $300 Free Trial period starts automatically. TheFree Trial periodcan't be paused or extended. After you initiate the Free Trial, Google uses the information you provided at signup to create a limited Google Cloud account called aFree trial account. This account has the following features: A newGoogle Cloud projectnamed \"My First Project\":Use this project to create and manageGoogle Cloud resources and services.Thisproject is linked to the new Cloud Billing account, described in the following information.To control project access, the username provided during the free trial signup is granted thelegacy basic Owner roleon the project, which provides admin-level permissions. Learn more aboutIAM permissions. Use this project to create and manageGoogle Cloud resources and services. Thisproject is linked to the new Cloud Billing account, described in the following information. To control project access, the username provided during the free trial signup is granted thelegacy basic Owner roleon the project, which provides admin-level permissions. Learn more aboutIAM permissions. A newCloud Billing accountnamed \"My Billing Account\":The billing account accrues Google Cloud usage costs that are generated in the Google Cloud project, and the available Free Trial credits pay for those usage costs.ThisCloud Billing account is linked to the Google payments profile,"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-2",
    "content": "described in the following information.To control billing account access, the username provided during the free trial signup is granted theBilling Account Administratorrole on the Cloud Billing account.Until youactivate a full, paid account, thebillable status of the accountisFree trial account, subject to theprogram coverage limitationsand theFree Trial Terms and Conditions. The billing account accrues Google Cloud usage costs that are generated in the Google Cloud project, and the available Free Trial credits pay for those usage costs. ThisCloud Billing account is linked to the Google payments profile, described in the following information. To control billing account access, the username provided during the free trial signup is granted theBilling Account Administratorrole on the Cloud Billing account. Until youactivate a full, paid account, thebillable status of the accountisFree trial account, subject to theprogram coverage limitationsand theFree Trial Terms and Conditions. A newGoogle payments profileconfigured with the information provided in the signup process:The payments profile stores information like name, address, and tax ID (when required legally) of who is responsible for the profile.Theform of paymentprovided during Free Trial signup is stored and managed in the Google payments profile.To control access to the payments profile, the username provided during the free trial signup is designated theAdmin and Primary Contactfor this payments profile. The payments profile stores information like name, address, and tax ID (when required legally) of who is responsible for the profile. Theform of paymentprovided during Free Trial signup is stored and managed in the Google payments profile. To control access to the payments profile, the username provided during the"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-3",
    "content": "free trial signup is designated theAdmin and Primary Contactfor this payments profile. Your Free Trial credits apply to all Google Cloud resources, includingGoogle Maps Platformusage, but with the following exceptions: You can't add GPUs to your VM instances. You can't request a quota increase. For an overview of Compute Engine quotas, seeResource quotas. You can't access or use Free Trial credits for generative AI partner models offered as managed APIs (also known asmodel as a service). You can't create VM instances that are based on Windows Serverimages. You can't create Google Cloud VMware Engine resources. To perform any of the actions in the preceding list, you mustactivate a full Cloud Billing account. In addition to resource constraints, theFree Trial Terms and Conditionsdescribe use cases that are prohibited during the Free Trial. For example, you may not use Google Cloud services to mine cryptocurrency during your Free Trial. YourFree Trial endswhen one of the following occurs: You've spent the $300 in credits. 90 days have elapsed since you signed up. Throughout your Free Trial period, yourremaining credits and daysare displayed on the Billing Account Overview page in the Google Cloud console. You are not billed during your Free Trial. When the Free Trial ends, all resources you created during the trial are stopped, and you won't be charged, unless youactivate a full, paid Cloud Billing account. You must accept theFree Trial Terms and Conditionsand theGoogle Cloud Terms of Service. Service level agreements don't apply during the Free Trial. The Free Trial is intended to help you explore and evaluate Google Cloud. We don't recommend running production applications on Google Cloud during the Free Trial. During your Free Trial period, when you use resources"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-4",
    "content": "covered by theFree Tier, the Free Tier usage is not charged against your Free Trial credits. When you sign up for the Free Trial, Google requires a credit card or other payment method. Google uses this payment information for the following purposes: To verify your identity. To distinguish actual people from robots. To create your Google payments profile. To create your Free Trial Cloud Billing account. For more information about payment methods, including which types of credit cards are accepted, seeavailable payment methods. If your payment method expires or otherwise becomes invalid during the Free Trial, your Cloud Billing account may be suspended. After you submit your payment information, Google submits a one-time transaction for verification purposes only.No charges are made after this verification process, unless youactivate a full, paid Cloud Billing account. The transaction has the following attributes: The transaction is an authorization request to validate your Cloud Billing account. It is not a permanent charge. The transaction is an authorization request to validate your Cloud Billing account. It is not a permanent charge. The transaction appears on your statement as being from Google. The transaction appears on your statement as being from Google. The transaction is between $0.00 and $1.00 USD. Your bank might convert this amount to a local currency. The transaction is between $0.00 and $1.00 USD. Your bank might convert this amount to a local currency. If you provide bank account information, the transaction might take up to 3 days to appear on your statement. If you provide bank account information, the transaction might take up to 3 days to appear on your statement. If you provide credit card information, this transaction might appear on your statement"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-5",
    "content": "for up to one month before being automatically reversed. If you provide credit card information, this transaction might appear on your statement for up to one month before being automatically reversed. Depending on your country, you might need to verify your bank account to complete the signup process. For information about verifying bank accounts,see verify your bank account. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. You can access Cloud Billing reports in the Google Cloud console to monitor and analyze your usage costs and credits. During the Free Trial offer, you'll typically see bills with a net zero balance, but you can view the details of the charges and credits so you can better understand the costs of using Google Cloud after yourFree Trial ends. Use thebilling reportto view and analyze your Google Cloud usage costs using many selectablesettings and filters. Open Cloud Billing Reports To view the individual services that contribute to your usage costs and credits,group your costs by SKU. In the report, your usage costs calculated at the on-demand rate appear in theCostcolumn, Free Trial credits appear in thePromotions and otherscolumn, and any credits for usage covered byFree Tier limitsappear in theDiscountscolumn. Example when viewing your Cloud Billing report grouped by SKU:SKUCostDiscountsPromotions and othersSubtotalN1 Predefined Instance Core running in Americas$33.75$0.00-$33.75$0.00Regional Kubernetes Clusters$55.80$0.00-$55.80$0.00Zonal Kubernetes Clusters$120.40-$74.40-$26.00$0.00 Learn about other reports available to view and analyze your Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-6",
    "content": "usage costs. The Free Trial ends when you use all of your credit, or after 90 days, whichever happens first. At that time, the following conditions apply: To continue using Google Cloud, you mustactivate a full, paid Cloud Billing account.If you choose not to activate a full, paid account, when the Free Trial ends, billing becomes disabled on your projects. To continue using Google Cloud, you mustactivate a full, paid Cloud Billing account. If you choose not to activate a full, paid account, when the Free Trial ends, billing becomes disabled on your projects. All resources you created during the trial are stopped. All resources you created during the trial are stopped. Any data you stored in Compute Engine is marked for deletion and might be lost.Learn more about data deletion on Google Cloud. Any data you stored in Compute Engine is marked for deletion and might be lost.Learn more about data deletion on Google Cloud. Your Cloud Billing account enters a 30-day grace period, during which you canactivate a full, paid Cloud Billing accountto recover resources and data you stored in any Google Cloud services during the trial period. Your Cloud Billing account enters a 30-day grace period, during which you canactivate a full, paid Cloud Billing accountto recover resources and data you stored in any Google Cloud services during the trial period. You might receive a message stating that your Cloud Billing account has been canceled, which indicates that your account has been suspended to prevent charges. You might receive a message stating that your Cloud Billing account has been canceled, which indicates that your account has been suspended to prevent charges. There is no action you need to take to cancel your Free Trial, unless you have alreadyactivated a full, paid Cloud"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-7",
    "content": "Billing account. If your Cloud Billing account is limited to aFree Trial status, you aren't billed during the Free Trial period. When theFree Trial ends, all resources you created during the trial are automatically stopped, your Free Trial billing account is suspended, and you aren't charged. If youactivateda full, paid account, you can cancel your paid Cloud Billing account byclosing the Cloud Billing account. Closing a billing account stops all billable services and prevents your Cloud Billing account fromincurring charges. Learn about additional methods tominimize or stop charges to your paid Cloud Billing account. You can activate a full, paid Cloud Billing account at any time after starting the Free Trial, to ensure that your resources keep running uninterrupted after the trial ends. You might also want to activate a full, paid account if you want to usefeatures that aren't included in the Free Trial, such as GPUs and Windows servers. When you activate a full, paid account, the following conditions apply: If you activate a full, paid account before the trial is over: Any remaining, unexpired Free Trial credits remain in your Cloud Billing account. You can continue to use the resources you created during the Free Trial without interruption.For resources you use in excess of what's covered by any remaining credit, your form of payment on file is charged (credit card or bank account). If you activate a full, paid account before the trial is over: Any remaining, unexpired Free Trial credits remain in your Cloud Billing account. You can continue to use the resources you created during the Free Trial without interruption. For resources you use in excess of what's covered by any remaining credit, your form of payment on file is charged (credit card or bank account). If"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-8",
    "content": "you activate a full, paid account within 30 days after the end of the trial: Your resources are marked for deletion, but you might be able to recover them.Learn more about data deletion on Google Cloud. If you activate a full, paid account within 30 days after the end of the trial: Your resources are marked for deletion, but you might be able to recover them.Learn more about data deletion on Google Cloud. If you activate a full, paid account more than 30 days after the end of the trial, your Free Trial resources are lost. If you activate a full, paid account more than 30 days after the end of the trial, your Free Trial resources are lost. In the Google Cloud console, you can convert your Free Trial billing account to a full, paid Cloud Billing account using theActivateoption. You must be aBilling Account Administratoron the Cloud Billing account to make this change. To activate your full, paid Cloud Billing account, complete the following steps: Sign in to the Google Cloud console.Sign in to the Google Cloud console Sign in to the Google Cloud console. Sign in to the Google Cloud console Look for theFree trial statusbanner at the top of the page. Look for theFree trial statusbanner at the top of the page. ClickActivate.If theFree trial bannerwith theActivatebutton is not visible, on the Google Cloud console menu bar, click theFree trial statusicon to open the banner and access theActivatebutton. ClickActivate. If theFree trial bannerwith theActivatebutton is not visible, on the Google Cloud console menu bar, click theFree trial statusicon to open the banner and access theActivatebutton. If you don't see anActivatebutton, the reasons could include the following: You don't have the permissions needed to convert this Free Trial Cloud Billing account to a full, paid"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-9",
    "content": "account. You must be aBilling Account Administratoron the Cloud Billing account to activate to a full, paid account. This Cloud Billing account is already activated as a full, paid account. You canconfirm the billing statusof your Cloud Billing account by looking at the BillingOverviewpage. In the Cloud Billing console, you can confirm the free or paid (billable) status of your Cloud Billing account and the status of your free trial credits. In the Google Cloud console, go to your Cloud Billing account.Go to your Cloud Billing account In the Google Cloud console, go to your Cloud Billing account. Go to your Cloud Billing account If you have more than one billing account, at the prompt, choose the Cloud Billing account you want to view. The BillingOverviewpage opens for the selected billing account. If you have more than one billing account, at the prompt, choose the Cloud Billing account you want to view. The BillingOverviewpage opens for the selected billing account. On the BillingOverviewpage, your billing account status is displayed at the top of the page:Free trial account: The Cloud Billing account isn't billable and is subject to theprogram coverage limitationsand theFree Trial Terms and ConditionsThe account might be suspended if it's more than 30 days after yourFree Trial ends.Paid account: The Cloud Billing account is billable. If you upgraded to a paid Cloud Billing account before theFree Trial ends, you can use any remaining, unexpired Free Trial credits. Your Cloud Billing account is charged for resources you use in excess of what's covered by any remaining credit. On the BillingOverviewpage, your billing account status is displayed at the top of the page: Free trial account: The Cloud Billing account isn't billable and is subject to theprogram coverage"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-10",
    "content": "limitationsand theFree Trial Terms and ConditionsThe account might be suspended if it's more than 30 days after yourFree Trial ends. Paid account: The Cloud Billing account is billable. If you upgraded to a paid Cloud Billing account before theFree Trial ends, you can use any remaining, unexpired Free Trial credits. Your Cloud Billing account is charged for resources you use in excess of what's covered by any remaining credit. To learn the status of any remaining free trial credits, on theBilling Account Overviewtab, look at theCreditinfo card.If the Cloud Billing account is still limited to aFree TrialCloud Billing account, you will see aFree trial creditinfo card.This card displays the status of any remaining free trial credits, and provides anActivatebutton. If you want to convert your limited Free Trial billing account to a full, paid Cloud Billing account, clickActivate.If the Cloud Billing account is a paid account, you will see aCreditsinfo card.This card displays the status of any remaining free trial credits. To view the details of the free trial, clickCredit details. To learn the status of any remaining free trial credits, on theBilling Account Overviewtab, look at theCreditinfo card. If the Cloud Billing account is still limited to aFree TrialCloud Billing account, you will see aFree trial creditinfo card.This card displays the status of any remaining free trial credits, and provides anActivatebutton. If you want to convert your limited Free Trial billing account to a full, paid Cloud Billing account, clickActivate. If the Cloud Billing account is still limited to aFree TrialCloud Billing account, you will see aFree trial creditinfo card. This card displays the status of any remaining free trial credits, and provides anActivatebutton. If you want to convert"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-11",
    "content": "your limited Free Trial billing account to a full, paid Cloud Billing account, clickActivate. If the Cloud Billing account is a paid account, you will see aCreditsinfo card.This card displays the status of any remaining free trial credits. To view the details of the free trial, clickCredit details. If the Cloud Billing account is a paid account, you will see aCreditsinfo card. This card displays the status of any remaining free trial credits. To view the details of the free trial, clickCredit details. When youactivate a full, paid Cloud Billing account, you'll incur charges after theend of the Free Trial. You'll also incur chargesduring the free trial periodif you useservices that aren't covered by the Free Trial. Google Cloud and Google Maps Platform services charge you only for resources you use. Each service has its own pricing model, which you can find in thepricing documentation for each individual service. Cloud Billing offers several tools to help you monitor and optimize your Google Cloud and Google Maps Platform usage costs. Reports- AccessCloud Billing reportsto monitor and analyze your usage costs and credits. Budgets- CreateCloud Billing budgetsto help you stay informed about how your actual Google Cloud spend is tracking against your planned spend. Committed use discounts- When you have workloads with predictable resource needs, optimize your costs by purchasingcommitted use discounts (CUDs). CUDs provide discounted prices in exchange for your commitment to use a minimum level of resources for a specified term. Several options are available to help you estimate your Google Cloud and Google Maps Platform usage costs. ForGoogle Cloud, you can estimate the cost of using Google Cloud services by using thepricing calculatoror by consulting thepricing page.Also,"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-12",
    "content": "for some Google Cloud services, you can use theCost estimation toolto estimate your monthly costs for simulated workloads. ForGoogle Cloud, you can estimate the cost of using Google Cloud services by using thepricing calculatoror by consulting thepricing page. Also, for some Google Cloud services, you can use theCost estimation toolto estimate your monthly costs for simulated workloads. ForGoogle Maps Platform, you can estimate your monthly bill by consulting thepricing page.To understand your Google Maps Platform monthly bill, consultGoogle Maps Platform Billing. ForGoogle Maps Platform, you can estimate your monthly bill by consulting thepricing page. To understand your Google Maps Platform monthly bill, consultGoogle Maps Platform Billing. After you have activated your full, paid Cloud Billing account,sign up for Customer Care. Choose the support service that best fits your organization's technical support needs. To minimize costs or prevent your Cloud Billing account from incurring charges, you can take the following actions: Limit your usage of each service to theFree Tier limits. Shut down services in your projects. Follow theguidance in the Google Cloud docsfor each of your services. Disable billingon the associatedprojects linked to your Cloud Billing account. Close your Cloud Billing accountto stop all billable services and prevent your Cloud Billing account from incurring charges. The Free Tier provides limited access to many common Google Cloud products and services at no cost. Unlike the Free Trial, the Free Tier is available to all Google Cloud users. Free Tier resources are provided at intervals, usually monthly. Free Tier resources are not credits; they don't accumulate or roll over from one interval to the next. Free Tier limits are calculated per"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-13",
    "content": "billing account. You're eligible for the Free Tier program if you meet the following criteria: You don't have a negotiated pricing contract or a custom rate card with Google, except as described for certain services listed in theFree Tier usage limitstable. You are actively in theFree Trialperiod or you have activated your full, paid Cloud Billing account. Your Cloud Billing account is active and in good standing. If you are deemed ineligible, you are charged at the normal rates for the resources you use. If you think you are being incorrectly charged for Free Tier usage, contactCloud Billing Supportfor assistance. No special action is required by you before you can begin using resources that offer Free Tier usage (withinFree Tier limits). Free Tier coveragevaries by service. Not all Google Cloud services offer resources as part of the Free Tier. The Free Tier has no end date, but Google reserves the right to change the offering, including changing or eliminating usage limits, subject to 30 days advance notice. You must accept theGoogle Cloud Terms of Service. When you use resources covered by Free Tier during your Free Trial period, those resources are not charged against your Free Trial credits. Free Tier resources are available for the Google Cloud services listed in the following table, subject to the listed limitations. For information about Google Maps Platform, see thepricing page. 28 hours per day ofF1 instances 9 hours per day ofB1 instances 1 GB of outbound data transfer per day The Google Cloud Free Tier is available only for the Standard Environment. Learn more 0.5 GB storage per month Learn more 5000 units of prediction per month Learn more 6 node hours for training and prediction Learn more 500,000 translated characters per month Learn more 1 TB of"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-14",
    "content": "querying per month 10 GB of storage Learn more 2,500 build-minutes per month formachine typee2-standard-2 Learn more First active delivery pipeline (per billing account) Learn more 100 free active key versions per month 10,000 free cryptographic operations per month The monthly free usage only applies to key versions created using Cloud KMS Autokey. Learn more Free monthly logging allotment Free monthly metrics allotment Learn more 5,000 units per month Learn more 2 million requests per month 360,000 GB-seconds of memory, 180,000 vCPU-seconds of compute time 1 GB of outbound data transfer from North America per month Learn more 2 million invocations per month (includes both background and HTTP invocations) 400,000 GB-seconds, 200,000 GHz-seconds of compute time 5 GB of outbound data transfer per month Learn more Free access to Cloud Shell, including 5 GB of persistent disk storage Learn more Up to 5 users 50 GB of storage 50 GB of outbound data transfer Learn more 5 GB-months of regional storage (US regions only) per month 5,000 Class A Operations per month 50,000 Class B Operations per month 100 GB of outbound data transfer from North America to all region destinations (excluding China and Australia) per month Free Tier is only available inus-east1,us-west1, andus-central1regions. Usage calculations are combined across those regions. Learn more 1,000 units per month Learn more 1 non-preemptiblee2-microVM instance per month in one of the following US regions:Oregon:us-west1Iowa:us-central1South Carolina:us-east1 Oregon:us-west1 Iowa:us-central1 South Carolina:us-east1 30 GB-months standard persistent disk 1 GB of outbound data transfer from North America to all region destinations (excluding China and Australia) per month Your Free Tiere2-microinstance limit is by time,"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-15",
    "content": "not by instance. Each month, eligible use of all of youre2-microinstances is free until you have used a number of hours equal to the total hours in the current month. Usage calculations are combined across the supportedregions. Compute Engine free tier does not charge for an external IP address. GPUs and TPUs are not included in the Free Tier offer. You are always charged for GPUs and TPUs that you add to VM instances.Learn more 1 GB storage per project 50,000 reads, 20,000 writes, 20,000 deletes per day, per project Learn more No cluster management fee for one Autopilot or Zonal cluster per Cloud Billing account. For clusters created in Autopilot mode, pods are billed per second for vCPU, memory and disk resource requests. For clusters created in Standard mode, each user node is charged at standard Compute Engine pricing. No fee for GKE Enterprise pay-as-you-go SKUs for 90 days, for a one-time trial period on any project linked to your Cloud Billing account.Note: During the 90-day period, GKE Enterprise is not meant for production workloads, and we don't offer an SLA. Only the fee for the GKE Enterprise pay-as-you-go SKU is waived. You are still charged for all usage related to existing services and services deployed using GKE Enterprise. No fee for GKE Enterprise pay-as-you-go SKUs for 90 days, for a one-time trial period on any project linked to your Cloud Billing account. Learn more For more information, see thePricing page. 10 GB of messages per month Learn more 10,000assessments.createorsiteverifycalls per month for SKU ID F553-86ED-15F4. 1 millionassessments.createorsiteverifycalls per month for SKU IDs A1ED-4B1C-A692 and 922B-4FA3-9210. Learn more 6 active secret versions per month 10,000 access operations per month 3 secret rotation notifications per month"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-16",
    "content": "Learn more 60 minutes per month Learn more 1,000 units per month Learn more 100,000uris.searchcalls per month 100uris.submitcalls per month This Free Tier usage is also available if you have a negotiated pricing contract for Web Risk. Learn more 5,000 internal steps per month 2,000 external HTTP calls per month Learn more Any usage that exceeds Free Tier usage limits is automatically billed at standard rates. You can help monitor and control costs by setting upbudgets and alertsthrough the Google Cloud console. You incur normal expenses for anyGoogle Cloud Marketplaceproducts andpremium OS licensesyou use, even if your Compute Engine use is covered by the Free Tier. Billing supportis included with all Cloud Billing accounts. You must be aBilling Account Administratorto interact with billing support. Upgrade your support planto access technical support for Google Cloud services. Google Cloud documentationdescribes how to use Google Cloud services. Google Maps Platform documentationdescribes how to use Google Maps Platform APIs and industry solutions. Cloud Billing documentationdescribes how to use Cloud Billing, a collection of tools that help you track and understand your Google Cloud spending, pay your bill, and optimize your costs. Google Cloud console Helpanswers common support questions. Google Cloud Free Trial Troubleshooterhelps you find answers to questions about the Free Trial. Find answers, ask questions, and connect with experts in theGoogle Cloud Community forums. Join theInnovators program, which gives you the latest updates, access to technologies and expertise, and exclusive benefits to build your skills on Google Cloud. Join acommunity meetupof knowledgeable Google Cloud enthusiasts. Except as otherwise noted, the content of this page is licensed under"
  },
  {
    "source_url": "https://cloud.google.com/free/docs/free-cloud-features",
    "title": "Free cloud features and trial offerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/free/docs/free-cloud-features#chunk-17",
    "content": "theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-20 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-0",
    "content": "Home Vertex AI Documentation The predictive AI evaluation service lets you evaluate model performance across specific use cases. You might also refer to evaluation as observability into a model's performance. The model evaluation provided by Vertex AI can fit in the typical machine learning workflow in several ways: After you train your model, review model evaluation metrics before you deploy your model. You can compare evaluation metrics across multiple models to help you decide which model you should deploy. After you train your model, review model evaluation metrics before you deploy your model. You can compare evaluation metrics across multiple models to help you decide which model you should deploy. After your model is deployed to production, periodically evaluate your model with new incoming data. If the evaluation metrics show that your model performance is degrading, consider re-training your model. This process is calledcontinuous evaluation. After your model is deployed to production, periodically evaluate your model with new incoming data. If the evaluation metrics show that your model performance is degrading, consider re-training your model. This process is calledcontinuous evaluation. How you interpret and use those metrics depends on your business need and the problem your model is trained to solve. For example, you might have a lower tolerance for false positives than for false negatives, or the other way around. These kinds of questions affect which metrics you would focus on as youiterate on your model. Some key metrics provided by the predictive AI model evaluation service include the following: Precision Recall AuPRC Confusion matrix To evaluate a model with Vertex AI, you should have a trained model, a batch prediction output, and a ground truth"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-1",
    "content": "dataset. The following is a typical model evaluation workflow using Vertex AI: Train a model. You can do this in Vertex AI using AutoML or custom training. Train a model. You can do this in Vertex AI using AutoML or custom training. Run a batch prediction job on the model to generate prediction results. Run a batch prediction job on the model to generate prediction results. Prepare theground truthdata, which is the \"correctly labeled\" data as determined by humans. The ground truth is usually in the form of the test dataset you used during the model training process. Prepare theground truthdata, which is the \"correctly labeled\" data as determined by humans. The ground truth is usually in the form of the test dataset you used during the model training process. Run an evaluation job on the model, which evaluates the accuracy of the batch prediction results compared to the ground truth data. Run an evaluation job on the model, which evaluates the accuracy of the batch prediction results compared to the ground truth data. Analyze the metrics that result from the evaluation job. Analyze the metrics that result from the evaluation job. Iterate on your model to see you if you can improve your model's accuracy. You can run multiple evaluation jobs, and compare the results of multiple jobs across models or model versions. Iterate on your model to see you if you can improve your model's accuracy. You can run multiple evaluation jobs, and compare the results of multiple jobs across models or model versions. You can run model evaluation in Vertex AI in several ways: Create evaluations through the Vertex AI Model Registry in the Google Cloud console. Create evaluations through the Vertex AI Model Registry in the Google Cloud console. Use model evaluations from Vertex AI as apipeline"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-2",
    "content": "componentwith Vertex AI Pipelines. You can create pipeline runs and templates that include model evaluations as a part of your automated MLOps workflow.You can run themodel evaluation component by itself, or with other pipeline components such as thebatch prediction component. Use model evaluations from Vertex AI as apipeline componentwith Vertex AI Pipelines. You can create pipeline runs and templates that include model evaluations as a part of your automated MLOps workflow. You can run themodel evaluation component by itself, or with other pipeline components such as thebatch prediction component. Vertex AI supports evaluation of the following model types: You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ AuPRC: Thearea under the precision-recall (PR) curve, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model. Log loss: The cross-entropy between the model predictions and the target values. This ranges from zero to infinity, where a lower value indicates a higher-quality model. Confidence threshold: A confidence score that determines which predictions to return. A model returns predictions that are at this value or higher. A higher confidence threshold increases precision but lowers recall. Vertex AI returns confidence metrics at different threshold values to show how the threshold affectsprecisionandrecall. Recall: The fraction of predictions with this class that the model correctly predicted. Also calledtrue positive rate. Precision: The fraction of classification predictions"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-3",
    "content": "produced by the model that were correct. Confusion matrix: Aconfusion matrixshows how often a model correctly predicted a result. For incorrectly predicted results, the matrix shows what the model predicted instead. The confusion matrix helps you understand where your model is \"confusing\" two results. You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ AuPRC: Thearea under the precision-recall (PR) curve, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model. AuROC: Thearea under receiver operating characteristic curve. This ranges from zero to one, where a higher value indicates a higher-quality model. Log loss: The cross-entropy between the model predictions and the target values. This ranges from zero to infinity, where a lower value indicates a higher-quality model. Confidence threshold: A confidence score that determines which predictions to return. A model returns predictions that are at this value or higher. A higher confidence threshold increases precision but lowers recall. Vertex AI returns confidence metrics at different threshold values to show how the threshold affectsprecisionandrecall. Recall: The fraction of predictions with this class that the model correctly predicted. Also calledtrue positive rate. Recall at 1: The recall (true positive rate) when only considering the label that has the highest prediction score and not below the confidence threshold for each example. Precision: The fraction of classification predictions produced by the model that were correct."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-4",
    "content": "Precision at 1: The precision when only considering the label that has the highest prediction score and not below the confidence threshold for each example. F1 score: The harmonic mean of precision and recall. F1 is a useful metric if you're looking for a balance between precision and recall and there's an uneven class distribution. F1 score at 1: The harmonic mean of recall at 1 and precision at 1. Confusion matrix: Aconfusion matrixshows how often a model correctly predicted a result. For incorrectly predicted results, the matrix shows what the model predicted instead. The confusion matrix helps you understand where your model is \"confusing\" two results. True negative count: The number of times a model correctly predicted a negative class. True positive count: The number of times a model correctly predicted a positive class. False negative count: The number of times a model mistakenly predicted a negative class. False positive count: The number of times a model mistakenly predicted a positive class. False positive rate: The fraction of incorrectly predicted results out of all predicted results. False positive rate at 1: The false positive rate when only considering the label that has the highest prediction score and not below the confidence threshold for each example. Model feature attributions:Vertex AI shows you how much each feature impacts a model. The values are provided as a percentage for each feature: the higher the percentage, the more impact the feature had on model training. Review this information to ensure that all of the most important features make sense for your data and business problem. You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ You can view and download schema"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-5",
    "content": "files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ MAE: The mean absolute error (MAE) is the average absolute difference between the target values and the predicted values. This metric ranges from zero to infinity; a lower value indicates a higher quality model. RMSE: The root-mean-squared error is the square root of the average squared difference between the target and predicted values. RMSE is more sensitive to outliers than MAE,so if you're concerned about large errors, then RMSE can be a more useful metric to evaluate. Similar to MAE, a smaller value indicates a higher quality model (0 represents a perfect predictor). RMSLE: The root-mean-squared logarithmic error metric is similar to RMSE, except that it uses the natural logarithm of the predicted and actual values plus 1. RMSLE penalizes under-prediction more heavily than over-prediction. It can also be a good metric when you don't want to penalize differences for large prediction values more heavily than for small prediction values. This metric ranges from zero to infinity; a lower value indicates a higher quality model. The RMSLE evaluation metric is returned only if all label and predicted values are non-negative. r^2: r squared (r^2) is the square of the Pearson correlation coefficient between the labels and predicted values. This metric ranges between zero and one. A higher value indicates a closer fit to the regression line. MAPE: Mean absolute percentage error (MAPE) is the average absolute percentage difference between the labels and the predicted values. This metric ranges between zero and infinity; a lower value indicates a higher quality model.MAPE is not shown if the target column contains any 0 values. In this case, MAPE is undefined. Model feature"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-6",
    "content": "attributions:Vertex AI shows you how much each feature impacts a model. The values are provided as a percentage for each feature: the higher the percentage, the more impact the feature had on model training. Review this information to ensure that all of the most important features make sense for your data and business problem. You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ MAE: The mean absolute error (MAE) is the average absolute difference between the target values and the predicted values. This metric ranges from zero to infinity; a lower value indicates a higher quality model. RMSE: The root-mean-squared error is the square root of the average squared difference between the target and predicted values. RMSE is more sensitive to outliers than MAE,so if you're concerned about large errors, then RMSE can be a more useful metric to evaluate. Similar to MAE, a smaller value indicates a higher quality model (0 represents a perfect predictor). RMSLE: The root-mean-squared logarithmic error metric is similar to RMSE, except that it uses the natural logarithm of the predicted and actual values plus 1. RMSLE penalizes under-prediction more heavily than over-prediction. It can also be a good metric when you don't want to penalize differences for large prediction values more heavily than for small prediction values. This metric ranges from zero to infinity; a lower value indicates a higher quality model. The RMSLE evaluation metric is returned only if all label and predicted values are non-negative. r^2: r squared (r^2) is the square of the Pearson correlation"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-7",
    "content": "coefficient between the labels and predicted values. This metric ranges between zero and one. A higher value indicates a closer fit to the regression line. MAPE: Mean absolute percentage error (MAPE) is the average absolute percentage difference between the labels and the predicted values. This metric ranges between zero and infinity; a lower value indicates a higher quality model.MAPE is not shown if the target column contains any 0 values. In this case, MAPE is undefined. WAPE: Weighted absolute percentage error (WAPE) is the overall difference between the value predicted by a model and the values observed over the values observed. Compared to RMSE, WAPE is weighted towards the overall differences rather than individual differences, which can be highly influenced by low or intermittent values. A lower value indicates a higher quality model. RMSPE: Root mean squared percentage error (RMPSE) shows RMSE as a percentage of the actual values instead of an absolute number. A lower value indicates a higher quality model. Quantile: The percent quantile, which indicates the probability that an observed value will be below the predicted value. For example, at the 0.5 quantile, the observed values are expected to be lower than the predicted values 50% of the time. Observed quantile: Shows the percentage of true values that were less than the predicted value for a given quantile. Scaled pinball loss: The scaled pinball loss at a particular quantile. A lower value indicates a higher quality model at the given quantile. You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ You can view and download schema files from the following Cloud Storage location:gs://google-cloud-"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-8",
    "content": "aiplatform/schema/modelevaluation/ AuPRC: Thearea under the precision-recall (PR) curve, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model. Log loss: The cross-entropy between the model predictions and the target values. This ranges from zero to infinity, where a lower value indicates a higher-quality model. Confidence threshold: A confidence score that determines which predictions to return. A model returns predictions that are at this value or higher. A higher confidence threshold increases precision but lowers recall. Vertex AI returns confidence metrics at different threshold values to show how the threshold affectsprecisionandrecall. Recall: The fraction of predictions with this class that the model correctly predicted. Also calledtrue positive rate. Recall at 1: The recall (true positive rate) when only considering the label that has the highest prediction score and not below the confidence threshold for each example. Precision: The fraction of classification predictions produced by the model that were correct. Precision at 1: The precision when only considering the label that has the highest prediction score and not below the confidence threshold for each example. Confusion matrix: Aconfusion matrixshows how often a model correctly predicted a result. For incorrectly predicted results, the matrix shows what the model predicted instead. The confusion matrix helps you understand where your model is \"confusing\" two results. F1 score: The harmonic mean of precision and recall. F1 is a useful metric if you're looking for a balance between precision and recall and there's an uneven class distribution. F1 score at 1: The harmonic mean of recall at 1 and precision at 1. You can view and"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-9",
    "content": "download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ You can view and download schema files from the following Cloud Storage location:gs://google-cloud-aiplatform/schema/modelevaluation/ AuPRC: Thearea under the precision-recall (PR) curve, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model. Confidence threshold: A confidence score that determines which predictions to return. A model returns predictions that are at this value or higher. A higher confidence threshold increases precision but lowers recall. Vertex AI returns confidence metrics at different threshold values to show how the threshold affectsprecisionandrecall. Recall: The fraction of predictions with this class that the model correctly predicted. Also calledtrue positive rate. Precision: The fraction of classification predictions produced by the model that were correct. Confusion matrix: Aconfusion matrixshows how often a model correctly predicted a result. For incorrectly predicted results, the matrix shows what the model predicted instead. The confusion matrix helps you understand where your model is \"confusing\" two results. F1 score: The harmonic mean of precision and recall. F1 is a useful metric if you're looking for a balance between precision and recall and there's an uneven class distribution. To learn more, run the following Jupyter notebooks in the environment of your choice: \"Vertex AI: Evaluating batch prediction results from an AutoML Tabular classification model\":Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub \"Vertex AI: Evaluating batch prediction results from an AutoML Tabular classification"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-10",
    "content": "model\": Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub \"Vertex AI Pipelines: Evaluating batch prediction results from AutoML Tabular regression model\":Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub \"Vertex AI Pipelines: Evaluating batch prediction results from AutoML Tabular regression model\": Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To learn more, run the \"Vertex AI Pipelines: AutoML text classification pipelines using google-cloud-pipeline-components\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To learn more, run the \"Vertex AI Pipelines: Evaluating batch prediction results from AutoML Video classification model\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To learn more, run the following Jupyter notebooks in the environment of your choice: \"Vertex AI Pipelines: Evaluating BatchPrediction results from a Custom Tabular classification model\":Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub \"Vertex AI Pipelines: Evaluating BatchPrediction results from a Custom Tabular classification model\": Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub \"Vertex AI Pipelines: Evaluating batch prediction results from custom tabular regression model\":Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub \"Vertex AI Pipelines:"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction",
    "title": "Model evaluation in Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/evaluation/introduction#chunk-11",
    "content": "Evaluating batch prediction results from custom tabular regression model\": Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To learn more, run the \"Get started with importing a custom model evaluation to the Vertex AI Model Registry\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub Learn how toperform model evaluation using Vertex AI. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/router",
    "title": "Cloud Router documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/router#chunk-0",
    "content": "Documentation Cloud Router enables you to dynamically exchange routes between your Virtual Private Cloud (VPC) and peer network by usingBorder Gateway Protocol (BGP). For example, if you use a Cloud VPN tunnel to connect your networks, you can use Cloud Router to establish a BGP session with a router in your peer network over a Cloud VPN tunnel. The peer network can be an on-premises network, multicloud network, or another VPC network. Cloud Router automatically learns new subnet IP address ranges in your VPC network and can announce them to your peer network. Setting the dynamic routing mode Setting the dynamic routing mode Quickstart: Create a Cloud Router to connect a VPC network to a peer network Quickstart: Create a Cloud Router to connect a VPC network to a peer network Establishing BGP sessions Establishing BGP sessions Advertised routes Advertised routes Viewing Cloud Router details Viewing Cloud Router details Viewing logs and metrics Viewing logs and metrics Cloud Router overview Cloud Router overview Key terms Key terms Best practices Best practices Troubleshoot BGP sessions Troubleshoot BGP sessions Troubleshoot BGP peering Troubleshoot BGP peering BGP routes and route selection BGP routes and route selection Troubleshoot Cloud Router log messages Troubleshoot Cloud Router log messages Release notes Release notes Getting support Getting support Billing questions Billing questions Pricing Pricing Quotas Quotas APIs APIs Creating an HA VPN gateway to a Peer VPN gateway Create a highly available VPN gateway that connects to a peer VPN gateway.VPNNetwork Connectivity Migration to Google Cloud: Transferring your large datasets Explore the process of getting your data into Google Cloud, from planning a data transfer to using best practices in implementing a"
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/router",
    "title": "Cloud Router documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/router#chunk-1",
    "content": "plan.MigrationCloud InterconnectNetwork Connectivity TCP optimization for network performance in Google Cloud and hybrid scenarios Learn about ways to improve connection latency between processes within Google Cloud, including how to compute correct settings for decreasing the latency of TCP connections.Network Connectivity Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/application-hosting",
    "title": "Application hosting",
    "chunk_id": "https://cloud.google.com/docs/application-hosting#chunk-0",
    "content": "Home Documentation Run and manage applications on a secure platform. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Develop and deploy highly scalable applications and functions on a fully managed serverless platform. Provision, deploy, scale, and manage containerized applications. Simplify managing multi-cluster deployments. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI Feature Store is a managed, cloud-native feature store service that's integral to Vertex AI. It streamlines your ML feature management and online serving processes by letting you manage your feature data in a BigQuery table or view. You can then serve features online directly from the BigQuery data source. Vertex AI Feature Store provisions resources that let you set up online serving by specifying your feature data sources. It then acts as a metadata layer interfacing with the BigQuery data sources and serves the latest feature values directly from BigQuery for online predictions at low latencies. In Vertex AI Feature Store, the BigQuery tables or views containing the feature data collectively form theoffline store. You can maintain feature values, including historical feature data, in the offline store. Because all the feature data is maintained in BigQuery, Vertex AI Feature Store doesn't need to provision a separate offline store within Vertex AI. Moreover, if you want to use the data in the offline store to train ML models, you can use the APIs and capabilities in BigQuery to export or fetch the data. The workflow to set up and start online serving using Vertex AI Feature Store can be summarized as follows: Prepare your data source in BigQuery. Prepare your data source in BigQuery. Optional: Register your data sources by creating feature groups and features. Optional: Register your data sources by creating feature groups and features. Set up online store and feature view resources to connect the feature data sources with online serving clusters. Set up online store and feature view resources to connect the feature data sources with online serving clusters. Serve the latest feature values online from a feature view. Serve the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-1",
    "content": "latest feature values online from a feature view. This section explains the data models and resources associated with the following aspects of Vertex AI Feature Store: Data source preparation in BigQuery Data source preparation in BigQuery Feature Registry setup Feature Registry setup Online serving setup Online serving setup Online serving Online serving During online serving, Vertex AI Feature Store uses feature data from BigQuery data sources. Before you set up Feature Registry or online serving resources, you must store your feature data in one or more BigQuery tables or views. Within a BigQuery table or view, each column represents a feature. Each row contains feature values corresponding to a unique ID. For more information about how to prepare the feature data in BigQuery, seePrepare data source. For example, in figure 1, the BigQuery table includes the following columns: f1andf2: Feature columns. f1andf2: Feature columns. entity_id: An ID column containing the unique IDs to identify each feature record. entity_id: An ID column containing the unique IDs to identify each feature record. feature_timestamp:A timestamp column. feature_timestamp:A timestamp column. Because you prepare the data source in BigQuery and not in Vertex AI, you don't need to create any Vertex AI resources at this stage. After you've prepared your data sources in BigQuery, you can register those data sources, including specific feature columns, in the Feature Registry. Registering your features is optional. You can serve features online even if you don't add your BigQuery data sources to the Feature Registry. However, registering your features is advantageous in the following scenarios: Your data contains multiple instances of the same entity ID and you need to prepare your data in a time-"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-2",
    "content": "series format with a timestamp column. When you register your features, Vertex AI Feature Store looks up the timestamp and serves only the latest feature values. Your data contains multiple instances of the same entity ID and you need to prepare your data in a time-series format with a timestamp column. When you register your features, Vertex AI Feature Store looks up the timestamp and serves only the latest feature values. You want to register specific feature columns from a data source. You want to register specific feature columns from a data source. You want to aggregate specific columns from multiple data sources to define a feature view instance. You want to aggregate specific columns from multiple data sources to define a feature view instance. You want to monitor the feature statistics and detect feature drift. You want to monitor the feature statistics and detect feature drift. There are two types of Vertex AI Feature Store resources in the Feature Registry: Feature Registry resources for feature data Feature Registry resources for feature data Feature Registry resources for feature monitoring Feature Registry resources for feature monitoring To register your feature data in the Feature Registry, you need to create the following Vertex AI Feature Store resources: Feature group(FeatureGroup): AFeatureGroupresource is associated with a specific BigQuery source table or view. It represents a logical grouping of feature columns, which are represented byFeatureresources. A feature group also contains one or multiple entity ID columns to identify the feature records. If the feature data is in a time-series format, the feature group must also contain a timestamp column. For information about how to create a feature group, seeCreate a feature group. Feature"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-3",
    "content": "group(FeatureGroup): AFeatureGroupresource is associated with a specific BigQuery source table or view. It represents a logical grouping of feature columns, which are represented byFeatureresources. A feature group also contains one or multiple entity ID columns to identify the feature records. If the feature data is in a time-series format, the feature group must also contain a timestamp column. For information about how to create a feature group, seeCreate a feature group. Feature(Feature): AFeatureresource represents a specific column containing feature values from the feature data source associated with its parentFeatureGroupresource. For information about how to create features within a feature group, seeCreate a feature. Feature(Feature): AFeatureresource represents a specific column containing feature values from the feature data source associated with its parentFeatureGroupresource. For information about how to create features within a feature group, seeCreate a feature. For example, figure 2 illustrates a feature group including feature columnsf1andf2, sourced from a BigQuery table associated with the feature group. The BigQuery data source contains four feature columns\u2014two columns are aggregated to form the feature group. The feature group also contains an entity ID column and a feature timestamp column. Feature monitoring resources let you monitor the feature data registered usingFeatureGroupandFeatureresources. You can create the following resources related to feature monitoring: Feature monitor(FeatureMonitor): AFeatureMonitorresource is associated with aFeatureGroupresource and one or more features within that feature group. It specifies the monitoring schedule. You can create multiple feature monitor resources to set up different monitoring schedules for"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-4",
    "content": "the same a set of features within a feature group. For example, if the featuresf1andf2are updated every hour, but the featuresf3andf4are updated every day, you can create two feature monitor resources to efficiently monitor these features:Feature monitorfm1that runs a monitoring job every hour on the featuresf1andf2.Feature monitorfm2that runs a monitoring job every day on the featuresf3andf4. Feature monitor(FeatureMonitor): AFeatureMonitorresource is associated with aFeatureGroupresource and one or more features within that feature group. It specifies the monitoring schedule. You can create multiple feature monitor resources to set up different monitoring schedules for the same a set of features within a feature group. For example, if the featuresf1andf2are updated every hour, but the featuresf3andf4are updated every day, you can create two feature monitor resources to efficiently monitor these features: Feature monitorfm1that runs a monitoring job every hour on the featuresf1andf2. Feature monitorfm1that runs a monitoring job every hour on the featuresf1andf2. Feature monitorfm2that runs a monitoring job every day on the featuresf3andf4. Feature monitorfm2that runs a monitoring job every day on the featuresf3andf4. Feature monitor job(FeatureMonitorJob): AFeatureMonitorJobresource contains the feature statistics and information retrieved when a feature monitoring job is run. It can also contain information about anomalies, such as feature drift, detected in the feature data. Feature monitor job(FeatureMonitorJob): AFeatureMonitorJobresource contains the feature statistics and information retrieved when a feature monitoring job is run. It can also contain information about anomalies, such as feature drift, detected in the feature data. For more information about how"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-5",
    "content": "to create feature monitoring resources, seeMonitor features for anomalies. To serve features for online predictions, you must define and configure at least one online serving cluster, and associate it with your feature data source or Feature Registry resources. In Vertex AI Feature Store, the online serving cluster is called anonline storeinstance. An online store instance can contain multiplefeature viewinstances, where each feature view is associated with a feature data source. To set up online serving, you must create the following Vertex AI Feature Store resources: Online store(FeatureOnlineStore): AFeatureOnlineStoreresource represents an online serving cluster instance and contains the online serving configuration, such as the number of online serving nodes. An online store instance doesn't specify the source of the feature data, but containsFeatureViewresources that specify the feature data sources in either BigQuery or the Feature Registry. For information about how to create an online store instance, seeCreate an online store instance. Online store(FeatureOnlineStore): AFeatureOnlineStoreresource represents an online serving cluster instance and contains the online serving configuration, such as the number of online serving nodes. An online store instance doesn't specify the source of the feature data, but containsFeatureViewresources that specify the feature data sources in either BigQuery or the Feature Registry. For information about how to create an online store instance, seeCreate an online store instance. Feature view(FeatureView): AFeatureViewresource is a logical collection of features in an online store instance. When you create a feature view, you can specify the location of the feature data source in either of the following ways:Associate one or more"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-6",
    "content": "feature groups and features from the Feature Registry. A feature group specifies the location of the BigQuery data source. A feature within the feature group points to a specific feature column within that data source.Alternatively, associate a BigQuery source table or view.For information about how to create feature view instances within an online store, seeCreate a feature view. Feature view(FeatureView): AFeatureViewresource is a logical collection of features in an online store instance. When you create a feature view, you can specify the location of the feature data source in either of the following ways: Associate one or more feature groups and features from the Feature Registry. A feature group specifies the location of the BigQuery data source. A feature within the feature group points to a specific feature column within that data source. Associate one or more feature groups and features from the Feature Registry. A feature group specifies the location of the BigQuery data source. A feature within the feature group points to a specific feature column within that data source. Alternatively, associate a BigQuery source table or view. Alternatively, associate a BigQuery source table or view. For information about how to create feature view instances within an online store, seeCreate a feature view. For example, figure 3 illustrates a feature view comprising feature columnsf2andf4, which are sourced from two separate feature groups associated with a BigQuery table. Vertex AI Feature Store provides the following types of online serving for real-time online predictions: Bigtable online servingis useful for serving large data volumes (terabytes of data). It's similar to online serving in Vertex AI Feature Store (Legacy) and provides improved caching to mitigate"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-7",
    "content": "hotspotting. Bigtable online serving doesn't supportembeddings. If you need to serve large volumes of data that are frequently updated and don't need to serve embeddings, use Bigtable online serving. Bigtable online servingis useful for serving large data volumes (terabytes of data). It's similar to online serving in Vertex AI Feature Store (Legacy) and provides improved caching to mitigate hotspotting. Bigtable online serving doesn't supportembeddings. If you need to serve large volumes of data that are frequently updated and don't need to serve embeddings, use Bigtable online serving. Optimized online servinglets you online serve features at ultra-low latencies. Although online serving latencies depend on the workload, Optimized online serving can provide lower latencies than Bigtable online serving and is recommended for most scenarios. Optimized online serving also supports embeddings management.To use Optimized online serving, you need to configure either a public endpoint or a dedicated Private Service Connect endpoint. Optimized online servinglets you online serve features at ultra-low latencies. Although online serving latencies depend on the workload, Optimized online serving can provide lower latencies than Bigtable online serving and is recommended for most scenarios. Optimized online serving also supports embeddings management. To use Optimized online serving, you need to configure either a public endpoint or a dedicated Private Service Connect endpoint. To learn how to set up online serving in Vertex AI Feature Store after you set up features, seeOnline serving types. Because you don't need to copy or import your feature data from BigQuery to a separate offline store in Vertex AI, you can use the data management and export capabilities of BigQuery to do the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-8",
    "content": "following: Query feature data, includinghistorical data at a point in time. Query feature data, includinghistorical data at a point in time. Preprocessandexportfeature data for model training and batch predictions. Preprocessandexportfeature data for model training and batch predictions. For more information about machine learning using BigQuery, seeBigQuery ML introduction. feature engineeringFeature engineering is the process of transforming raw machine learning (ML) data into features that can be used to train ML models or to make predictions. Feature engineering is the process of transforming raw machine learning (ML) data into features that can be used to train ML models or to make predictions. featureIn machine learning (ML), a feature is a characteristic or attribute of an instance or entity that's used as an input to train an ML model or to make predictions. In machine learning (ML), a feature is a characteristic or attribute of an instance or entity that's used as an input to train an ML model or to make predictions. feature valueA feature value corresponds to the actual and measurable value of a feature (attribute) of an instance or entity. A collection of feature values for the unique entity represent the feature record corresponding to the entity. A feature value corresponds to the actual and measurable value of a feature (attribute) of an instance or entity. A collection of feature values for the unique entity represent the feature record corresponding to the entity. feature timestampA feature timestamp indicates when the set of feature values in a specific feature record for an entity were generated. A feature timestamp indicates when the set of feature values in a specific feature record for an entity were generated. feature recordA feature record is an"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-9",
    "content": "aggregation of all feature values that describe the attributes of a unique entity at a specific point in time. A feature record is an aggregation of all feature values that describe the attributes of a unique entity at a specific point in time. feature registryA feature registry is a central interface for recording feature data sources that you want to serve for online predictions. For more information, seeFeature Registry setup. A feature registry is a central interface for recording feature data sources that you want to serve for online predictions. For more information, seeFeature Registry setup. feature groupA feature group is a feature registry resource that corresponds to a BigQuery source table or view containing feature data. A feature view might contain features and can be thought of as a logical grouping of feature columns in the data source. A feature group is a feature registry resource that corresponds to a BigQuery source table or view containing feature data. A feature view might contain features and can be thought of as a logical grouping of feature columns in the data source. feature servingFeature serving is the process of exporting or fetching feature values for training or inference. In Vertex AI, there are two types of feature serving\u2014online serving and offline serving. Online serving retrieves the latest feature values of a subset of the feature data source for online predictions. Offline or batch serving exports high volumes of feature data\u2014including historical data\u2014for offline processing, such as ML model training. Feature serving is the process of exporting or fetching feature values for training or inference. In Vertex AI, there are two types of feature serving\u2014online serving and offline serving. Online serving retrieves the latest feature"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-10",
    "content": "values of a subset of the feature data source for online predictions. Offline or batch serving exports high volumes of feature data\u2014including historical data\u2014for offline processing, such as ML model training. offline storeThe offline store is a storage facility storing recent and historical feature data, which is typically used for training ML models. An offline store also contains the latest feature values, which you can serve for online predictions. The offline store is a storage facility storing recent and historical feature data, which is typically used for training ML models. An offline store also contains the latest feature values, which you can serve for online predictions. online storeIn feature management, an online store is a storage facility for the latest feature values to be served for online predictions. In feature management, an online store is a storage facility for the latest feature values to be served for online predictions. feature viewA feature view is a logical collection of features materialized from a BigQuery data source to an online store instance. A feature view stores and periodically refreshes the customer's feature data, which is refreshed periodically from the BigQuery source. A feature view is associated with the feature data storage either directly or through associations to feature registry resources. A feature view is a logical collection of features materialized from a BigQuery data source to an online store instance. A feature view stores and periodically refreshes the customer's feature data, which is refreshed periodically from the BigQuery source. A feature view is associated with the feature data storage either directly or through associations to feature registry resources. All Vertex AI Feature Store resources must be located in"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-11",
    "content": "the same region or the same multi-regional location as your BigQuery data source. For example, if the feature data source is located inus-central1, you must create yourFeatureOnlineStoreinstance only inus-central1or in theUSmulti-region location. Vertex AI Feature Store is integrated with Dataplex to provide feature governance capabilities, including feature metadata. Online store instances, feature views, and feature groups are automatically registered as data assets in Data Catalog, a Dataplex feature that catalogs metadata from these resources. You can then use the metadata search capability of Dataplex to search for, view, and manage the metadata for these resources. For more information about searching for Vertex AI Feature Store resources in Dataplex, seeSearch for resource metadata in Data Catalog. You can add labels to resources during or after the resource creation. For more information about adding labels to existing Vertex AI Feature Store resources, seeUpdate labels. Vertex AI Feature Store only supports the version0for features. Vertex AI Feature Store lets you set up feature monitoring to retrieve feature statistics and detect anomalies in feature data. You can either set up monitoring schedules to periodically run monitoring jobs, or manually run a monitoring job. For more information about setting up feature monitoring and running feature monitoring jobs, seeMonitor features for anomalies. Optimized online serving in Vertex AI Feature Store supports embedding management. You can store embeddings in BigQuery as regulardoublearrays. Using the embedding management capabilities of Vertex AI Feature Store, you can perform vector similarity searches to retrieve entities that are approximate nearest neighbors for a specified entity or embedding value. To use"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-12",
    "content": "embedding management in Vertex AI Feature Store, you need to do the following: Set up the BigQuery data source to support embeddings by including theembeddingcolumn. Optionally, include filtering and crowding columns. For more information, seeData source preparation guidelines. Set up the BigQuery data source to support embeddings by including theembeddingcolumn. Optionally, include filtering and crowding columns. For more information, seeData source preparation guidelines. Create an online store instance for Optimized online serving. Create an online store instance for Optimized online serving. Specify theembeddingcolumn while creating the feature view. For more information about how to create a feature view that supports embeddings, seeConfigure vector retrieval for a feature view. Specify theembeddingcolumn while creating the feature view. For more information about how to create a feature view that supports embeddings, seeConfigure vector retrieval for a feature view. For information about how to perform a vector similarity search in Vertex AI Feature Store, seePerform a vector search for entities. Vertex AI Feature Store retains the latest feature values for a unique ID, based on the timestamp associated with the feature values in the data source. There's no data retention limit in the online store. Because the offline store is provisioned by BigQuery, data retention limits or quotas from BigQuery might apply to the feature data source, including historical feature values.Learn more about quotas and limits in BigQuery. Vertex AI Feature Store enforces quotas and limits to help you manage resources by setting usage limits, and to protect the community of Google Cloud users by preventing unforeseen spikes in usage. To efficiently use Vertex AI Feature Store resources"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-13",
    "content": "without hitting these constraints, review theVertex AI Feature Store quotas and limits. For information about resource usage pricing for Vertex AI Feature Store, seeVertex AI Feature Store pricing. Use the following samples and tutorials to learn more about Vertex AI Feature Store.Online feature serving and fetching of BigQuery data with Vertex AI Feature Store Bigtable online servingIn this tutorial, you learn how to use Bigtable online serving in Vertex AI Feature Store for online serving and fetching of feature values in BigQuery.Open in Colab|Open in Colab Enterprise|View on GitHub In this tutorial, you learn how to use Bigtable online serving in Vertex AI Feature Store for online serving and fetching of feature values in BigQuery. Open in Colab|Open in Colab Enterprise|View on GitHub In this tutorial, you learn how to use Optimized online serving in Vertex AI Feature Store for serving and fetching of feature values from BigQuery. Open in Colab|Open in Colab Enterprise|View on GitHub In this tutorial, you learn how to use Vertex AI Feature Store for online serving and vector retrieval of feature values in BigQuery. Open in Colab|Open in Colab Enterprise|View on GitHub In this tutorial, you learn how to enable feature view Service Agents and grant each feature view access to the specific source data that is used. Open in Colab|Open in Colab Enterprise|View on GitHub In this tutorial, you learn how to chunk user-provided data, and then generate embedding vectors for each chunk using a Large Language Model (LLM) that has embedding generation capabilities. The resulting embedding vector dataset can then be loaded into Vertex AI Feature Store, enabling fast feature retrieval and efficient online serving. Open in Colab|Open in Colab Enterprise|View on GitHub In this"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview",
    "title": "About Vertex AI Feature StoreStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview#chunk-14",
    "content": "tutorial, you learn how to build a low-latency vector search system for your Gen AI application using BigQuery vector search and Vertex AI Feature Store. Open in Colab|Open in Colab Enterprise|View on GitHub In this tutorial, you learn how to configure an IAM policy to control access to resources and data stored within Vertex AI Feature Store. Open in Colab|Open in Colab Enterprise|View on GitHub Learn how toset up your data in BigQuery. Learn how toset up your data in BigQuery. Learn how to createfeature groupsandfeatures. Learn how to createfeature groupsandfeatures. Learn how tocreate an online store instance. Learn how tocreate an online store instance. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/mysql",
    "title": "Cloud SQL for MySQL documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/mysql#chunk-0",
    "content": "Home Cloud SQL Documentation MySQL Cloud SQL for MySQL is a fully-managed database service that helps you set up, maintain, manage, and administer your MySQL relational databases on Google Cloud Platform. For information specific to MySQL, see theMySQL documentationorlearn more about Cloud SQL for MySQL. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create instances Create instances Connection overview Connection overview Enable and disable high availability on an instance Enable and disable high availability on an instance Create and manage MySQL databases Create and manage MySQL databases Create and manage MySQL users Create and manage MySQL users Export and import using SQL dump file Export and import using SQL dump file Export and import using CSV files Export and import using CSV files Create backups Create backups Create read replicas Create read replicas gcloud sql command-line gcloud sql command-line Use the Cloud SQL Admin API Use the Cloud SQL Admin API REST API REST API Best practices Best practices Performance tips Performance tips Authorize requests Authorize requests Configure VPC Service Controls Configure VPC Service Controls Cloud SQL Admin API error messages Cloud SQL Admin API error messages Pricing Pricing Quotas and limits Quotas and limits Troubleshoot Troubleshoot Cloud SQL feature support by database engine Cloud SQL feature support by database engine Release notes Release notes Billing questions Billing questions Get support Get support Security bulletins Security bulletins Google Cloud Fundamentals: Core Infrastructure These lectures, demos, and hands-on labs give you an overview of Google"
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/mysql",
    "title": "Cloud SQL for MySQL documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/mysql#chunk-1",
    "content": "Cloud products and services so that you can learn the value of Google Cloud and how to incorporate cloud-based solutions into your business strategies. Architecting with Google Cloud: Design and Process This course features a combination of lectures, design activities, and hands-on labs to show you how to use proven design patterns on Google Cloud to build highly reliable and efficient solutions and operate deployments that are highly available and cost-effective. Converting and optimizing queries from Oracle Database to Cloud SQL for MySQL Discusses the basic query differences between Oracle\u00ae and Cloud SQL for MySQL, and how features in Oracle map to features in Cloud SQL for MySQL.Migration Migrating Oracle users to Cloud SQL for MySQL: Terminology and functionality Part of a series that provides key information and guidance related to planning and performing Oracle 11g/12c database migrations to Cloud SQL for MySQL version 5.7, second-generation instances.Migration Data residency overview Learn how to use Cloud SQL to enforce data residency requirements for data.Data residencydata Use Secret Manager to handle secrets in Cloud SQL Learn how to use Secret Manager to store sensitive information about Cloud SQL instances and users as secrets.Secret Managersecret Python SQLAlchemy Use SQLAlchemy with your Cloud SQL for MySQL database Node.js sample Connecting to your Cloud SQL for MySQL database in Node.js PHP PDO Connecting your Cloud SQL for MySQL database using PHP PDO Go web app sample Simple examples of connecting to Cloud SQL for MySQL using Go .NET sample This sample application demonstrates how to store data in Google Cloud SQL with a MySQL database when running in Google App Engine Flexible Environment. Java servlet Connecting to Cloud SQL for MySQL from a Java"
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/mysql",
    "title": "Cloud SQL for MySQL documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/mysql#chunk-2",
    "content": "application Terraform for Cloud SQL networking Use Terraform to create Cloud SQL for MySQL instances with private networking options. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data",
    "title": "View Carbon Footprint data",
    "chunk_id": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data#chunk-0",
    "content": "Home Carbon Footprint Documentation The Carbon Footprint dashboard displays estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account. To access the Carbon Footprint dashboard, you need the following IAM permissions: billing.accounts.getCarbonInformation billing.accounts.list billing.accounts.get These IAM permissions enable you to do all of the following: List and view the names of billing accounts. Access Carbon Footprint data associated with these billing accounts. To obtain these IAM permissions, a billing account administrator must grant you one or more IAM roles listed below for the billing account that contain such permissions: roles/billing.carbonViewer roles/billing.admin roles/billing.viewer Read more aboutCloud Carbon IAM permission and rolesandCloud Billing access control. The Carbon Footprint dashboard is located in theToolssection within Google Cloud console. Go to Carbon Footprint Carbon Footprint data is computed automatically for your billing account, there is no API to enable or setup required. It can take up to 21 days for data of the previous month to become available. If you have the properIAM permissionand data is available, data is displayed on the dashboard. Carbon footprint data is given in metric tons of CO2equivalent (tCO2e) in the UI dashboard and in kilograms of CO2equivalent (kgCO2e) in the data export. The dashboard is divided into two tabs:Market-based emissionsandLocation-based emissions. These two tabs display emissions data estimated using different Scope 2 carbon accounting definitions according to the Greenhouse Gas Protocol (GHGP). Scope 1 and Scope 3 emissions are identical across the two tabs. In summary: Market-based emissions: This tab displays emissions data"
  },
  {
    "source_url": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data",
    "title": "View Carbon Footprint data",
    "chunk_id": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data#chunk-1",
    "content": "broken down by scope 1, scope 2 market-based, and scope 3 GHG emissions. Learn more aboutscope 2 market-based emissions methodology. Location-based emissions: This tab displays emissions data broken down by scope 1, scope 2 location-based, and scope 3 GHG emissions. Learn more aboutscope 2 location-based emissions methodology. Each tab displays an overview of the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account. Annual carbon footprint: The total estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account over the past 12 complete months. Carbon footprint for the past month: The total estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account for the most recent completed month, compared to the previous month. Carbon Footprint has integrated with Google Cloud Active Assist Unattended Project Recommender, which analyzes usage activity across all projects, provides you with the recommendations to reclaim or shut down unattended projects, and helps you reduce both cost and carbon emissions. If your billing account has associated projects that are deemed \"unattended\", you will see the \"Recommendations to reduce emissions\" card next to your annual and monthly summary cards. To dig deeper into the specifics of your carbon emissions, the dashboard includes four charts: Monthly carbon emissions (in both market-based emissions tab and location-based emissions tab): Displays the total estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account over all available months, broken down by month. Carbon emissions by"
  },
  {
    "source_url": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data",
    "title": "View Carbon Footprint data",
    "chunk_id": "https://cloud.google.com/carbon-footprint/docs/view-carbon-data#chunk-2",
    "content": "region (in both market-based emissions tab and location-based emissions tab): Displays the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account for the past complete month broken down by Google Cloud region. Carbon emissions by project (only in the location-based emissions tab): Displays the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account for the past complete month broken down by Google Cloud project. Carbon emissions by product (only in the location-based emissions tab): Displays the estimated greenhouse gas emissions associated with the usage ofcovered Google Cloud servicesfor the selected billing account for the past complete month broken down by Google Cloud service. Export your carbon footprint Create custom dashboards with the exported data. Understand the methodology behind Carbon Footprint Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-29 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI provides a managed training service that lets you operationalize large scale model training. You can use Vertex AI to run training applications based on any machine learning (ML) framework on Google Cloud infrastructure. For the following popular ML frameworks, Vertex AI also has integrated support that simplifies the preparation process for model training and serving: PyTorch TensorFlow scikit-learn XGBoost This page explains the benefits of custom training on Vertex AI, the workflow involved, and the various training options that are available. There are several challenges to operationalizing model training. These challenges include the time and cost needed to train models, the depth of skills required to manage the compute infrastructure, and the need to provide enterprise-level security. Vertex AI addresses these challenges while providing a host of other benefits. Model training on Vertex AI is a fully managed service that requires no administration of physical infrastructure. You can train ML models without the need to provision or manage servers. You only pay for the compute resources that you consume. Vertex AI also handles job logging, queuing, and monitoring. Model training on Vertex AI is a fully managed service that requires no administration of physical infrastructure. You can train ML models without the need to provision or manage servers. You only pay for the compute resources that you consume. Vertex AI also handles job logging, queuing, and monitoring. Vertex AI training jobs are optimized for ML model training, which can provide faster performance than directly running your training application on a GKE cluster. You can also identify and debug performance bottlenecks in your training job by using Cloud Profiler."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-1",
    "content": "Vertex AI training jobs are optimized for ML model training, which can provide faster performance than directly running your training application on a GKE cluster. You can also identify and debug performance bottlenecks in your training job by using Cloud Profiler. Reduction Serveris an all-reduce algorithm in Vertex AI that can increase throughput and reduce latency of multi-node distributed training on NVIDIA graphics processing units (GPUs). This optimization helps reduce the time and cost of completing large training jobs. Reduction Serveris an all-reduce algorithm in Vertex AI that can increase throughput and reduce latency of multi-node distributed training on NVIDIA graphics processing units (GPUs). This optimization helps reduce the time and cost of completing large training jobs. Hyperparameter tuning jobsrun multiple trials of your training application using different hyperparameter values. You specify a range of values to test and Vertex AI discovers the optimal values for your model within that range. Hyperparameter tuning jobsrun multiple trials of your training application using different hyperparameter values. You specify a range of values to test and Vertex AI discovers the optimal values for your model within that range. Vertex AI provides the following enterprise security features:VPC peeringto limit network access.VPC Service Controlsto mitigate the risks of data exfiltration.Customer-managed encryption keysto help you meet specific compliance or regulatory requirements related to data protection.Identity and Access Managementfor fine-grained control over service account access.Data isolation with single-tenant project boundaries. Vertex AI provides the following enterprise security features: VPC peeringto limit network access. VPC Service Controlsto"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-2",
    "content": "mitigate the risks of data exfiltration. Customer-managed encryption keysto help you meet specific compliance or regulatory requirements related to data protection. Identity and Access Managementfor fine-grained control over service account access. Data isolation with single-tenant project boundaries. Vertex AI provides a suite ofintegrated MLOps toolsand features that you can use for the following purposes:Orchestrate end-to-end ML workflows.Perform feature engineering.Run experiments.Manage and iterate your models.Track ML metadata.Monitor and evaluate model quality. Vertex AI provides a suite ofintegrated MLOps toolsand features that you can use for the following purposes: Orchestrate end-to-end ML workflows. Perform feature engineering. Run experiments. Manage and iterate your models. Track ML metadata. Monitor and evaluate model quality. The following diagram shows a high-level overview of the custom training workflow on Vertex AI. The sections that follow describe each step in detail. For the best performance and support, use one of the following Google Cloud services as your data source: Cloud Storage BigQuery NFS shares on Google Cloud For a comparison of these services, seeData preparation overview. You can also specify aVertex AI managed datasetas the data source when using a training pipeline to train your model. Training a custom model and an AutoML model using the same dataset lets you compare the performance of the two models. To prepare your training application for use on Vertex AI, do the following: Implement training code best practices for Vertex AI. Determine a type of container image to use. Package your training application into a supported format based on the selected container image type. Your training application should implement thetraining"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-3",
    "content": "code best practices for Vertex AI. These best practices relate to the ability of your training application to do the following: Access Google Cloud services. Load input data. Enable autologging for experiment tracking. Export model artifacts. Use the environment variables of Vertex AI. Ensure resilience to VM restarts. Vertex AI runs your training application in aDocker container image. A Docker container image is a self-contained software package that includes code and all dependencies, which can run in almost any computing environment. You can either specify the URI of aprebuilt container imageto use, or create and upload acustom container imagethat has your training application and dependencies pre-installed. The following table shows the differences between prebuilt and custom container images: Python source distribution. Single Python file. Greater customization and control. Non-Python training applications. Private or custom dependencies. Training applications that use an ML framework or framework version that has no prebuilt container image available. After you've determined the type of container image to use, package your training application into one of the following formats based on the container image type: Single Python file for use in a prebuilt containerWrite your training application as a single Python file and use theVertex AI SDK for Pythonto create aCustomJoborCustomTrainingJobclass. The Python file is packaged into a Python source distribution and installed to a prebuilt container image. Delivering your training application as a single Python file is suitable for prototyping. For production training applications, you'll likely have your training application arranged into more than one file. Single Python file for use in a prebuilt container Write your"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-4",
    "content": "training application as a single Python file and use theVertex AI SDK for Pythonto create aCustomJoborCustomTrainingJobclass. The Python file is packaged into a Python source distribution and installed to a prebuilt container image. Delivering your training application as a single Python file is suitable for prototyping. For production training applications, you'll likely have your training application arranged into more than one file. Python source distribution for use in a prebuilt containerPackage your training applicationinto one or more Python source distributions and upload them to a Cloud Storage bucket. Vertex AI installs the source distributions to a prebuilt container image when you create a training job. Python source distribution for use in a prebuilt container Package your training applicationinto one or more Python source distributions and upload them to a Cloud Storage bucket. Vertex AI installs the source distributions to a prebuilt container image when you create a training job. Custom container imageCreate your own Docker container imagethat has your training application and dependencies pre-installed, and upload it to Artifact Registry. If your training application is written in Python, you canperform these steps by using one Google Cloud CLI command. Custom container image Create your own Docker container imagethat has your training application and dependencies pre-installed, and upload it to Artifact Registry. If your training application is written in Python, you canperform these steps by using one Google Cloud CLI command. A Vertex AI training job performs the following tasks: Provisions one (single node training) or more (distributed training) virtual machines (VMs). Runs your containerized training application on the provisioned VMs. Deletes the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-5",
    "content": "VMs after the training job completes. Vertex AI offersthree types of training jobsfor running your training application: Custom jobA custom job (CustomJob) runs your training application. If you're using a prebuilt container image, model artifacts are output to the specified Cloud Storage bucket. For custom container images, your training application can also output model artifacts to other locations. Custom job A custom job (CustomJob) runs your training application. If you're using a prebuilt container image, model artifacts are output to the specified Cloud Storage bucket. For custom container images, your training application can also output model artifacts to other locations. Hyperparameter tuning jobA hyperparameter tuning job (HyperparameterTuningJob) runs multiple trials of your training application using different hyperparameter values until it produces model artifacts with the optimal performing hyperparameter values. You specify the range of hyperparameter values to test and the metrics to optimize for. Hyperparameter tuning job A hyperparameter tuning job (HyperparameterTuningJob) runs multiple trials of your training application using different hyperparameter values until it produces model artifacts with the optimal performing hyperparameter values. You specify the range of hyperparameter values to test and the metrics to optimize for. Training pipelineA training pipeline (CustomTrainingJob) runs a custom job or hyperparameter tuning job and optionally exports the model artifacts to Vertex AI to create a model resource. You can specify a Vertex AI managed dataset as your data source. Training pipeline A training pipeline (CustomTrainingJob) runs a custom job or hyperparameter tuning job and optionally exports the model artifacts to Vertex AI to create a"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-6",
    "content": "model resource. You can specify a Vertex AI managed dataset as your data source. When creating a training job, specify the compute resources to use for running your training application and configure your container settings. Specify the compute resourcesto use for a training job. Vertex AI supports single-node training, where the training job runs on one VM, anddistributed training, where the training job runs on multiple VMs. The compute resources that you can specify for your training job are as follows: VM machine typeDifferent machine types offer different CPUs, memory size, and bandwidth. VM machine type Different machine types offer different CPUs, memory size, and bandwidth. Graphics processing units (GPUs)You can add one or more GPUs to A2 or N1 type VMs. If your training application is designed to use GPUs, adding GPUs can significantly improve performance. Graphics processing units (GPUs) You can add one or more GPUs to A2 or N1 type VMs. If your training application is designed to use GPUs, adding GPUs can significantly improve performance. Tensor Processing Units (TPUs)TPUs are designed specifically for accelerating machine learning workloads. When using a TPU VM for training, you can specify only one worker pool. That worker pool can have only one replica. Tensor Processing Units (TPUs) TPUs are designed specifically for accelerating machine learning workloads. When using a TPU VM for training, you can specify only one worker pool. That worker pool can have only one replica. Boot disksYou can use SSDs (default) or HDDs for your boot disk. If your training application reads and writes to disk, using SSDs can improve performance. You can also specify the size of your boot disk based on the amount of temporary data that your training application writes to"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-7",
    "content": "disk. Boot disks can have between 100 GiB (default) and 64,000 GiB. All VMs in a worker pool must use the same type and size of boot disk. Boot disks You can use SSDs (default) or HDDs for your boot disk. If your training application reads and writes to disk, using SSDs can improve performance. You can also specify the size of your boot disk based on the amount of temporary data that your training application writes to disk. Boot disks can have between 100 GiB (default) and 64,000 GiB. All VMs in a worker pool must use the same type and size of boot disk. Thecontainer configurationsthat you need to make depend on whether you're using a prebuilt or custom container image. Prebuilt container configurations:Specify the URI of the prebuilt container image that you want to use.If your training application is packaged as a Python source distribution, specify the Cloud Storage URI where the package is located.Specify the entry point module of your training application.Optional: Specify a list of command-line arguments to pass to the entry point module of your training application. Prebuilt container configurations: Specify the URI of the prebuilt container image that you want to use. If your training application is packaged as a Python source distribution, specify the Cloud Storage URI where the package is located. Specify the entry point module of your training application. Optional: Specify a list of command-line arguments to pass to the entry point module of your training application. Custom container configurations:Specify the URI of your custom container image, which can be a URI from Artifact Registry or Docker Hub.Optional: Override theENTRYPOINTorCMDinstructions in your container image. Custom container configurations: Specify the URI of your custom container image,"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/overview",
    "title": "Vertex AI custom training overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/overview#chunk-8",
    "content": "which can be a URI from Artifact Registry or Docker Hub. Optional: Override theENTRYPOINTorCMDinstructions in your container image. After your data and training application are prepared, run your training application by creating one of the following training jobs: Create a custom job. Create a hyperparameter tuning job. Create a training pipeline. To create the training job, you can use the Google Cloud console, Google Cloud CLI, Vertex AI SDK for Python, or the Vertex AI API. Your training application likely outputs one or more model artifacts to a specified location, usually a Cloud Storage bucket. Before you can get predictions in Vertex AI from your model artifacts, firstimport the model artifacts into Vertex AI Model Registry. Like container images for training, Vertex AI gives you the choice of usingprebuiltorcustomcontainer images for predictions. If a prebuilt container image for predictions is available for your ML framework and framework version, we recommend using a prebuilt container image. Get predictionsfrom your model. Evaluate your model. Try theHello custom trainingtutorial for step-by-step instructions on training a TensorFlow Keras image classification model on Vertex AI. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-12 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",
    "title": "Ray on Vertex AI overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview#chunk-0",
    "content": "Home Vertex AI Documentation Rayis an open-source framework for scaling AI and Python applications. Ray provides the infrastructure to perform distributed computing and parallel processing for your machine learning (ML) workflow. If you already use Ray, you can use the same open source Ray code to write programs and develop applications on Vertex AI with minimal changes. You can then use Vertex AI's integrations with other Google Cloud services such asVertex AI PredictionandBigQueryas part of your machine learning workflow. If you already use Vertex AI and need a simpler way to manage compute resources, you can use Ray code to scale training. Use Colab Enterprise and Vertex AI SDK for Python to connect to the Ray Cluster. Ray clusters are built in to ensure capacity availability for critical ML workloads or during peak seasons. Unlike custom jobs, where the training service releases the resource after job completion, Ray clusters remain available until deleted. Note: Use long running Ray clusters in these scenarios: If you are submitting the same Ray job multiple times and can benefit from data and image caching by running the jobs on the same long running Ray clusters. If you run many short-lived Ray jobs where the actual processing time is shorter than the job startup time, it may be beneficial to have a long-running cluster. Ray clusters on Vertex AI can be set up either with public or private connectivity. The following diagrams show the architecture and workflow for Ray on Vertex AI. SeePublic or private connectivityfor more information. Create the Ray cluster on Vertex AI using the following options:a. Use the Google Cloud console to create the Ray cluster on Vertex AI.b. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python. Create the Ray"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",
    "title": "Ray on Vertex AI overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview#chunk-1",
    "content": "cluster on Vertex AI using the following options: a. Use the Google Cloud console to create the Ray cluster on Vertex AI. b. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python. Connect to the Ray cluster on Vertex AI for interactive development using the following options:a. UseColab Enterprisein the Google Cloud console for seamless connection.b. Use any Python environment accessible to the public internet. Connect to the Ray cluster on Vertex AI for interactive development using the following options: a. UseColab Enterprisein the Google Cloud console for seamless connection. b. Use any Python environment accessible to the public internet. Develop your application and train your model on the Ray cluster on Vertex AI:Use the Vertex AI SDK for Python in your preferred environment (Colab Enterprise or any Python notebook).Write a Python script using your preferred environment.Submit a Ray Job to the Ray cluster on Vertex AI using the Vertex AI SDK for Python, Ray Job CLI, or Ray Job Submission API. Develop your application and train your model on the Ray cluster on Vertex AI: Use the Vertex AI SDK for Python in your preferred environment (Colab Enterprise or any Python notebook). Use the Vertex AI SDK for Python in your preferred environment (Colab Enterprise or any Python notebook). Write a Python script using your preferred environment. Write a Python script using your preferred environment. Submit a Ray Job to the Ray cluster on Vertex AI using the Vertex AI SDK for Python, Ray Job CLI, or Ray Job Submission API. Submit a Ray Job to the Ray cluster on Vertex AI using the Vertex AI SDK for Python, Ray Job CLI, or Ray Job Submission API. Deploy the trained model to an online Vertex AI endpoint for live prediction. Deploy the trained model to an online"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",
    "title": "Ray on Vertex AI overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview#chunk-2",
    "content": "Vertex AI endpoint for live prediction. Use BigQueryto manage your data. Use BigQueryto manage your data. The following diagram shows the architecture and workflow for Ray on Vertex AI after you set up your Google Cloud project and VPC network, which is optional: Set up your (a) Google project and (b) VPC network. Set up your (a) Google project and (b) VPC network. Create the Ray cluster on Vertex AI using the following options:a. Use the Google Cloud console to create the Ray cluster on Vertex AI.b. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python. Create the Ray cluster on Vertex AI using the following options: a. Use the Google Cloud console to create the Ray cluster on Vertex AI. b. Create the Ray cluster on Vertex AI using the Vertex AI SDK for Python. Connect to the Ray cluster on Vertex AI through a VPC peered network using the following options:UseColab Enterprisein the Google Cloud console.Use aVertex AI Workbenchnotebook. Connect to the Ray cluster on Vertex AI through a VPC peered network using the following options: UseColab Enterprisein the Google Cloud console. UseColab Enterprisein the Google Cloud console. Use aVertex AI Workbenchnotebook. Use aVertex AI Workbenchnotebook. Develop your application and train your model on the Ray cluster on Vertex AI using the following options:Use the Vertex AI SDK for Python in your preferred environment (Colab Enterprise or a Vertex AI Workbench notebook).Write a Python script using your preferred environment. Submit a Ray Job to the Ray cluster on Vertex AI using the Vertex AI SDK for Python, Ray Job CLI, or Ray dashboard. Develop your application and train your model on the Ray cluster on Vertex AI using the following options: Use the Vertex AI SDK for Python in your preferred environment (Colab"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",
    "title": "Ray on Vertex AI overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview#chunk-3",
    "content": "Enterprise or a Vertex AI Workbench notebook). Use the Vertex AI SDK for Python in your preferred environment (Colab Enterprise or a Vertex AI Workbench notebook). Write a Python script using your preferred environment. Submit a Ray Job to the Ray cluster on Vertex AI using the Vertex AI SDK for Python, Ray Job CLI, or Ray dashboard. Write a Python script using your preferred environment. Submit a Ray Job to the Ray cluster on Vertex AI using the Vertex AI SDK for Python, Ray Job CLI, or Ray dashboard. Deploy the trained model to an online Vertex AI endpoint for predictions. Deploy the trained model to an online Vertex AI endpoint for predictions. Use BigQuery to manage your data. Use BigQuery to manage your data. Pricing for Ray on Vertex AI is calculated as follows: The compute resources you use are charged based on the machine configuration you select when creating your Ray cluster on Vertex AI. For Ray on Vertex AI pricing, see thepricing page. The compute resources you use are charged based on the machine configuration you select when creating your Ray cluster on Vertex AI. For Ray on Vertex AI pricing, see thepricing page. Regarding Ray clusters, you are only charged during RUNNING and UPDATINGstates. No other states are charged. The amount charged is based on the actual cluster size at the moment. Regarding Ray clusters, you are only charged during RUNNING and UPDATINGstates. No other states are charged. The amount charged is based on the actual cluster size at the moment. When you perform tasks using the Ray cluster on Vertex AI, logs are automatically generated and charged based onCloud Logging pricing. When you perform tasks using the Ray cluster on Vertex AI, logs are automatically generated and charged based onCloud Logging pricing. If you deploy your model"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview",
    "title": "Ray on Vertex AI overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview#chunk-4",
    "content": "to an endpoint for online predictions, see the\"Prediction and explanation\"section of the Vertex AI pricing page. If you deploy your model to an endpoint for online predictions, see the\"Prediction and explanation\"section of the Vertex AI pricing page. If you use BigQuery with Ray on Vertex AI, seeBigQuery pricing. If you use BigQuery with Ray on Vertex AI, seeBigQuery pricing. Set up for Ray on Vertex AI Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-0",
    "content": "Home Google Cloud NetApp Volumes Documentation Guides This page provides an overview of Google Cloud NetApp Volumes and how it works. NetApp Volumes is a fully managed, cloud-based data storage service that provides advanced data management capabilities and highly scalable performance. NetApp Volumes helps to accelerate deployment times, manage your workloads and applications, and migrate workloads to the cloud while keeping the performance and features of on-premises storage. NetApp Volumes lets you move file-based applications to Google Cloud. It has support for Network File System (NFSv3 and NFSv4.1) and Server Message Block (SMB) protocols built-in, so you don't need to re-architect your applications and can continue to get persistent storage for your applications. NetApp Volumes offers four service levels: Flex, Standard, Premium, and Extreme. Performance, features, and capabilities vary by service level. NetApp Volumes offers the following features. For a comparison of features across service levels, seeservice levels. Fully-managed service: provides fully-managed service with no operations, integrated with theGoogle Cloud console Fully-managed service: provides fully-managed service with no operations, integrated with theGoogle Cloud console Volume provisioning: provisions volumes from 1 GiB to 1 PiB in seconds Volume provisioning: provisions volumes from 1 GiB to 1 PiB in seconds Multiprotocol support: supports NFSv3, NFSv4.1, and SMB 2.1, 3.0, and 3.1.1 protocols. Multiprotocol support: supports NFSv3, NFSv4.1, and SMB 2.1, 3.0, and 3.1.1 protocols. Automated snapshots: protects data with automated, efficient snapshots. Automated snapshots: protects data with automated, efficient snapshots. Auto-tiering: moves infrequently used data transparently to affordable"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-1",
    "content": "cold storage. Auto-tiering: moves infrequently used data transparently to affordable cold storage. Backup: provides manual and automated backups for long-term retention. Backup: provides manual and automated backups for long-term retention. Volume replication: enables business continuity with asynchronous volume replication across Google Cloud. Volume replication: enables business continuity with asynchronous volume replication across Google Cloud. High availability: provides high availability with options for multi-region redundancy, backed by theNetApp Volumes service level agreement High availability: provides high availability with options for multi-region redundancy, backed by theNetApp Volumes service level agreement Rapid cloning: accelerates application development with rapid cloning Rapid cloning: accelerates application development with rapid cloning Multiple service level offerings: offers multiple service levels based on location, allowing you to pick a service level that best fits your needs:Flex: highly available, general purpose storage with advanced data management capabilities.Performance:Default: up to 16 KiBps per GiB of storage pool capacity shared by all volumes in the pool inselected regions. Maximum of 1.6 GiBps per pool.Custom: independent provisioning of capacity and performance with zonal pools in selected regions. You can provision throughput from 64 MiBps to a maximum of 5 GiBps in increments of 1 MiBps. Each MiBps provisioned throughput includes 16 IOPS. Additional IOPS beyond the included throughput of 16 IOPS can be provisioned as required up to a maximum of 160,000 IOPS. For more information about available regions, seeSupported regions for Flex custom performance.Sample use cases: common enterprise workloads such as Network File System"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-2",
    "content": "(NFS) and Server Message Block (SMB) file shares, SAP shared files, containerized workloads, and Google Cloud VMware Engine.Standard: highly available, general purpose storage with advanced data management capabilities.Performance: up to 16 KiBps per GiB of volume capacity. Maximum of 1.6 GiBps per volume.Sample use cases: common enterprise workloads such as Network File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, and Google Cloud VMware Engine.Premium: highly available, high-performance storage with advanced data management capabilities.Performance: up to 64 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes.Sample use cases: performance-critical workloads requiring low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, virtual desktop infrastructure (VDI), and VMware Engine.Extreme: highly available, high-throughput storage with advanced data management capabilities.Performance: up to 128 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes.Sample use cases: performance-critical workloads requiring high throughput and low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, VDI, and VMware Engine. Multiple service level offerings: offers multiple service levels based on location, allowing you to pick a service level that best fits your needs: Flex: highly available, general purpose storage with advanced data management capabilities.Performance:Default: up to 16 KiBps per GiB of storage pool capacity shared by all volumes in the pool inselected regions. Maximum of 1.6 GiBps per pool.Custom: independent provisioning of capacity and performance with zonal pools"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-3",
    "content": "in selected regions. You can provision throughput from 64 MiBps to a maximum of 5 GiBps in increments of 1 MiBps. Each MiBps provisioned throughput includes 16 IOPS. Additional IOPS beyond the included throughput of 16 IOPS can be provisioned as required up to a maximum of 160,000 IOPS. For more information about available regions, seeSupported regions for Flex custom performance.Sample use cases: common enterprise workloads such as Network File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, containerized workloads, and Google Cloud VMware Engine. Flex: highly available, general purpose storage with advanced data management capabilities. Performance:Default: up to 16 KiBps per GiB of storage pool capacity shared by all volumes in the pool inselected regions. Maximum of 1.6 GiBps per pool.Custom: independent provisioning of capacity and performance with zonal pools in selected regions. You can provision throughput from 64 MiBps to a maximum of 5 GiBps in increments of 1 MiBps. Each MiBps provisioned throughput includes 16 IOPS. Additional IOPS beyond the included throughput of 16 IOPS can be provisioned as required up to a maximum of 160,000 IOPS. For more information about available regions, seeSupported regions for Flex custom performance. Performance: Default: up to 16 KiBps per GiB of storage pool capacity shared by all volumes in the pool inselected regions. Maximum of 1.6 GiBps per pool. Default: up to 16 KiBps per GiB of storage pool capacity shared by all volumes in the pool inselected regions. Maximum of 1.6 GiBps per pool. Custom: independent provisioning of capacity and performance with zonal pools in selected regions. You can provision throughput from 64 MiBps to a maximum of 5 GiBps in increments of 1 MiBps. Each MiBps provisioned"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-4",
    "content": "throughput includes 16 IOPS. Additional IOPS beyond the included throughput of 16 IOPS can be provisioned as required up to a maximum of 160,000 IOPS. For more information about available regions, seeSupported regions for Flex custom performance. Custom: independent provisioning of capacity and performance with zonal pools in selected regions. You can provision throughput from 64 MiBps to a maximum of 5 GiBps in increments of 1 MiBps. Each MiBps provisioned throughput includes 16 IOPS. Additional IOPS beyond the included throughput of 16 IOPS can be provisioned as required up to a maximum of 160,000 IOPS. For more information about available regions, seeSupported regions for Flex custom performance. Sample use cases: common enterprise workloads such as Network File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, containerized workloads, and Google Cloud VMware Engine. Sample use cases: common enterprise workloads such as Network File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, containerized workloads, and Google Cloud VMware Engine. Standard: highly available, general purpose storage with advanced data management capabilities.Performance: up to 16 KiBps per GiB of volume capacity. Maximum of 1.6 GiBps per volume.Sample use cases: common enterprise workloads such as Network File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, and Google Cloud VMware Engine. Standard: highly available, general purpose storage with advanced data management capabilities. Performance: up to 16 KiBps per GiB of volume capacity. Maximum of 1.6 GiBps per volume. Performance: up to 16 KiBps per GiB of volume capacity. Maximum of 1.6 GiBps per volume. Sample use cases: common enterprise workloads such as Network"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-5",
    "content": "File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, and Google Cloud VMware Engine. Sample use cases: common enterprise workloads such as Network File System (NFS) and Server Message Block (SMB) file shares, SAP shared files, and Google Cloud VMware Engine. Premium: highly available, high-performance storage with advanced data management capabilities.Performance: up to 64 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes.Sample use cases: performance-critical workloads requiring low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, virtual desktop infrastructure (VDI), and VMware Engine. Premium: highly available, high-performance storage with advanced data management capabilities. Performance: up to 64 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes. Performance: up to 64 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes. Sample use cases: performance-critical workloads requiring low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, virtual desktop infrastructure (VDI), and VMware Engine. Sample use cases: performance-critical workloads requiring low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, virtual desktop infrastructure (VDI), and VMware Engine. Extreme: highly available, high-throughput storage with advanced data management capabilities.Performance: up to 128 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes.Sample use cases: performance-critical workloads requiring high throughput and low"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-6",
    "content": "latency, for example, Windows and enterprise NFS, self-managed databases and file shares, VDI, and VMware Engine. Extreme: highly available, high-throughput storage with advanced data management capabilities. Performance: up to 128 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes. Performance: up to 128 KiBps per GiB exclusive to volume. Maximum of 4.5 GiBps per volume. 30 GiBps with large capacity volumes. Sample use cases: performance-critical workloads requiring high throughput and low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, VDI, and VMware Engine. Sample use cases: performance-critical workloads requiring high throughput and low latency, for example, Windows and enterprise NFS, self-managed databases and file shares, VDI, and VMware Engine. NetApp Volumes provides fully managed NFS and SMB remote file systems as a service. Service administrators create and manage remote file systems as volumes and share them with NFS and SMB clients over a network. Clients such as Compute Engine VMs mount file system volumes, their users, and the applications within the client store files in the file system volumes. You can control access using Windows or UNIX-based permission models. You can use Google Cloud NetApp Volumes using the following tools: Google Cloud SDK: theGoogle Cloud command line interfacelets you interact with NetApp Volumes through a terminal Google Cloud SDK: theGoogle Cloud command line interfacelets you interact with NetApp Volumes through a terminal Google Cloud console: theGoogle Cloud consoleprovides a visual interface that gives you a holistic view of your applications and projects Google Cloud console: theGoogle Cloud consoleprovides a visual interface"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-7",
    "content": "that gives you a holistic view of your applications and projects Terraform Google Cloud Platform Provider: NetApp Volumes resources are part of the GoogleTerraform provider. For more information about how to provision NetApp Volumes resources using Terraform, see introduction toTerraform integration. Terraform Google Cloud Platform Provider: NetApp Volumes resources are part of the GoogleTerraform provider. For more information about how to provision NetApp Volumes resources using Terraform, see introduction toTerraform integration. NetApp Volumes uses theGoogle Cloud Private Service Access framework, which creates a private connection linking your Virtual Private Cloud (VPC) to the NetApp Volumes VPC. The Google Cloud private service access framework assigns private addresses (RFC 1918) or non-private addresses (non-RFC 1918) to it using the Service Networking API and VPC peering constructs. Network peering is integrated in the storage pool creation workflow. All volumes in a pool are accessible from Network-attached storage (NAS) clients on the same VPC, but are subject to NAS access control. For Shared VPC, this enables data access across different projects. You can't attach a single volume or pool to multiple VPCs. Independent of data access at the VPC level, all resources belong only to the project they're created in and can only be managed within that projectIdentity and Access Management (IAM)protects management access. NetApp Volumes is available in several regions. For details about region availability, seeNetApp Volumes locations. Read aboutservice levelsof Google Cloud NetApp Volumes. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For"
  },
  {
    "source_url": "https://cloud.google.com/netapp/volumes/docs/discover/overview",
    "title": "What is Google Cloud NetApp Volumes?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/netapp/volumes/docs/discover/overview#chunk-8",
    "content": "details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-20 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/industry",
    "title": "Industry solutions",
    "chunk_id": "https://cloud.google.com/docs/industry#chunk-0",
    "content": "Home Documentation Transform your business with Google Cloud solutions for specific industries like retail, healthcare, and financial services. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Transform AML detection by replacing rules-based transaction monitoring with AI. Digitally transform your healthcare and life sciences business though data-powered innovation. Transform audience experiences with innovation and insights. Provide Google Search-quality product search, browsing, and recommendations to your retail customers. Use AI to solve problems at scale, increase quality hires, and improve subscriber acquisition and retention. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/interconnect",
    "title": "Cloud Interconnect documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/interconnect#chunk-0",
    "content": "Home Network Connectivity Documentation Cloud Interconnect Cloud Interconnect extends your external network to the Google network through a high-availability, low-latency connection. You can use Dedicated Interconnect to connect directly to Google. Alternatively, you can use Partner Interconnect to connect to Google through a supported service provider. You can use Cross-Cloud Interconnect to connect to your network that's hosted by another cloud service provider.Learn more Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Cloud Interconnect overview Cloud Interconnect overview Dedicated Interconnect Dedicated Interconnect Partner Interconnect Partner Interconnect Cross-Cloud Interconnect Cross-Cloud Interconnect Partner Cross-Cloud Interconnect for OCI Partner Cross-Cloud Interconnect for OCI Cross-Site Interconnect Cross-Site Interconnect HA-VPN over Cloud Interconnect HA-VPN over Cloud Interconnect MACsec for Cloud Interconnect MACsec for Cloud Interconnect Create Dedicated Interconnect connections Create Dedicated Interconnect connections Create Partner Interconnect connections Create Partner Interconnect connections Create Cross-Cloud Interconnect connections for AWS Create Cross-Cloud Interconnect connections for AWS Create Cross-Cloud Interconnect connections for Azure Create Cross-Cloud Interconnect connections for Azure Create Cross-Cloud Interconnect connections for OCI Create Cross-Cloud Interconnect connections for OCI Create Cross-Cloud Interconnect connections for Alibaba Create Cross-Cloud Interconnect connections for Alibaba Create Partner Cross-Cloud Interconnect connections for OCI Create Partner Cross-"
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/interconnect",
    "title": "Cloud Interconnect documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/interconnect#chunk-1",
    "content": "Cloud Interconnect connections for OCI Create Cross-Site Interconnect connections Create Cross-Site Interconnect connections Cloud Interconnect APIs Cloud Interconnect APIs Pricing Pricing Quotas and limits Quotas and limits Troubleshooting Troubleshooting Release notes Release notes Service Level Agreement Service Level Agreement TCP optimization for network performance in Google Cloud and hybrid scenarios Learn about ways to improve connection latency between processes within Google Cloud, including how to compute correct settings for decreasing the latency of TCP connections.Network Connectivity Migration to Google Cloud: Transferring your large datasets This tutorial explores the process of adding your data to Google Cloud, from planning a data transfer to using best practices in implementing a plan.Network ConnectivityMigration Patterns for connecting other cloud service providers with Google Cloud Decide how to connect Google Cloud with other cloud service providers (CSP) such as Amazon Web Services (AWS), Microsoft Azure, Oracle Cloud Infrastructure, and Alibaba Cloud.Cloud InterconnectPartner InterconnectDedicated Interconnect Calculate network throughput over Interconnect This tutorial shows you how to calculate network throughput, within Google Cloud and for your on-premises or third-party cloud locations that are connected using Cloud Interconnect.Network Connectivity Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-0",
    "content": "Home Documentation Security This content was last updated in July 2024, and represents the status quo as of the time it was written. Google's security policies and systems may change going forward, as we continually improve protection for our customers. Download pdf version Traditionally, businesses have looked to the public cloud to save costs, experiment with new technology, and provide growth capacity. Increasingly, businesses are also looking to the public cloud for their security, realizing that cloud providers can invest more than the businesses can in technology, people, and processes to deliver a more secure infrastructure. As a cloud innovator, Google understands security in the cloud. Our cloud services are designed to deliver better security than many on-premises approaches. We make security a priority in our operations\u2014operations that serve users across the world. Security drives our organizational structure, culture, training priorities, and hiring processes. It shapes the design of our data centers and the technology that they house. It's central to our everyday operations and disaster planning, including how we address threats. It's prioritized in the way we handle customer data, our account controls, our compliance audits, and our certifications. This document describes our approach to security, privacy, and compliance for Google Cloud, which is our suite of public cloud products and services. The document focuses on the physical, administrative, and technical controls that we have deployed to help protect your data. Google's culture stresses the importance of protecting the large volume of information that belongs to our customers. This culture influences our hiring processes and employee onboarding. We advance and reinforce data protection guidelines"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-1",
    "content": "and technologies through ongoing training and events that are focused on security and privacy. Our dedicated security team includes some of the world's foremost experts in information security, application security, cryptography, and network security. This team maintains our defense systems, develops security review processes, builds security infrastructure, and implements our security policies. The team actively scans for security vulnerabilities using commercial and custom tools. The team also conducts penetration tests and performs quality assurance and security reviews. Members of the security team review security plans for our networks and services, and they provide project-specific consulting services to our product and engineering teams. For example, our cryptography engineers review product launches that include cryptography implementations. The security team monitors for suspicious activity on our networks and addresses information security threats as needed. The team also performs routine security evaluations and audits, which can involve engaging outside experts to conduct regular security assessments. We have long enjoyed a close relationship with the security research community, and we greatly value their help with identifying potential vulnerabilities in Google Cloud and other Google products. Our security teams take part in research and outreach activities to benefit the online community. For example, we runProject Zero, which is a team of security researchers who are dedicated to researching zero-day vulnerabilities. Some examples of this research are the discovery of theSpectreexploit, theMeltdownexploit, thePOODLE SSL 3.0 exploit, andcipher suite weaknesses. Google's security engineers and researchers actively participate andpublishin the academic"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-2",
    "content": "security community and the privacy research community. They also organize and participate inopen-source projectsand academic conferences. Google's security teams have published an in-depth account of our practices and experience in theBuilding Secure and Reliable Systemsbook. OurVulnerability Reward Programoffers rewards in the tens of thousands of dollars for each confirmed vulnerability. The program encourages researchers to report design and implementation issues that might put customer data at risk. In 2023, we awarded researchers over 10 million dollars in prize money. To help improve the security of open-source code, the Vulnerability Reward Program also provides avariety of initiativesto researchers. For more information about this program, including the rewards that we've given, seeBug Hunters Key Stats. Our world-class cryptographers participate in industry-leading cryptography projects. For example, we designed theSecure AI Framework (SAIF)to help secure AI systems. In addition, to protect TLS connections against quantum computer attacks, we developed thecombined elliptic-curve and post-quantum (CECPQ2) algorithm. Our cryptographers developedTink, which is an open-source library of cryptographic APIs. We also use Tink in our internal products and services. For more information about how you can report security issues, seeHow Google handles security vulnerabilities. All Google employees undergo security and privacy training as part of the orientation process, and they receive ongoing security and privacy training throughout their Google careers. During orientation, new employees agree to ourCode of Conduct, which highlights our commitment to keeping customer data safe and secure. Depending on their job role, employees might be required to take additional"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-3",
    "content": "training on specific aspects of security. For example, the information security team instructs new engineers on secure coding practices, product design, and automated vulnerability testing tools. Engineers attend regular security briefings and receive security newsletters that cover new threats, attack patterns, mitigation techniques, and more. Security and privacy are an ever-changing area, and we recognize that dedicated employee engagement is a key means of raising awareness. We host regular internal conferences that are open to all employees to raise awareness and drive innovation in security and data privacy. We host events across global offices to raise awareness of security and privacy in software development, data handling, and policy enforcement. Our dedicated privacy team supports internal privacy initiatives that help improve critical processes, internal tools, products, and privacy infrastructure. The privacy team operates separately from product development and security organizations. They participate in Google product launches by reviewing design documentation and performing code reviews to ensure that privacy requirements are followed. The team helps release products that incorporate strong privacy standards around the collection of user data. Our products are designed to provide users and administrators with meaningful privacy configuration options. After products are launched, the privacy team oversees ongoing automated processes to verify that data collected by the products is handled appropriately. In addition, the privacy team conducts research on privacy best practices for our emerging technologies. To understand how we stay committed to user data privacy and to compliance with applicable privacy regulations and laws, seeour commitment to complying"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-4",
    "content": "with data protection laws. For more information, see thePrivacy Resource Center. We have a dedicated internal audit team that reviews our products' compliance with security laws and regulations around the world. As new auditing standards are created and existing standards are updated, the internal audit team determines what controls, processes, and systems are needed in order to help meet them. This team supports independent audits and assessments by third parties. For more information, seeSupport for compliance requirementslater in this document. Security is an integral part of our cloud operations, not an afterthought. This section describes our vulnerability management programs, malware prevention program, security monitoring, and incident management programs. Our internal vulnerability management process actively scans for security threats across technology stacks. This process uses a combination of commercial, open-source, and purpose-built in-house tools, and includes the following: Quality assurance processes Software security reviews Intensive automated and manual penetration efforts, including extensive Red Team exercises External audits The vulnerability management organization and its partners are responsible for tracking and following up on vulnerabilities. Because security improves only after issues are fully addressed, automation pipelines continually reassess the state of patch deployment to mitigate vulnerabilities and flag incorrect or partial deployment. To help improve detection capabilities, the vulnerability management organization focuses on high-quality indicators that separate noise from signals that indicate real threats. The organization also fosters interaction with the industry and with the open-source community. For example, they run aPatch"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-5",
    "content": "Reward Programfor theTsunaminetwork security scanner, which rewards developers who create open-source detectors for vulnerabilities. For more about vulnerabilities that we have mitigated, seeGoogle Cloud security bulletins. Google maintains malware protections for our core products (like Gmail, Google Drive, Google Chrome, YouTube, Google Ads, and Google Search) that use a variety of malware detection techniques. To discover malware files proactively, we use web crawling, file detonation, custom static detection, dynamic detection, and machine-learning detection. We also use multiple antivirus engines. To help protect our employees, we use the built-in advanced security capabilities of Chrome Enterprise Premium and the Enhanced Safe Browsing feature in Google Chrome. These capabilities enable proactive detection of phishing and malware sites as our employees browse the web. We also enable the most rigorous security settings that are available in Google Workspace, such as Gmail Security Sandbox, to proactively scan suspicious attachments. Logs from these capabilities feed into our security monitoring systems, as described in the following section. Our security monitoring program is focused on information that's gathered from internal network traffic, from employee actions on systems, and from outside knowledge of vulnerabilities. A core Google principle is to aggregate and store security telemetry data in one location for unified security analysis. At many points across our global network, internal traffic is inspected for suspicious behavior, such as the presence of traffic that might indicate botnet connections. We use a combination of open-source and commercial tools to capture and parse traffic so that we can perform this analysis. A proprietary correlation system"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-6",
    "content": "built on top of our technology also supports this analysis. We supplement network analysis by examining system logs to identify unusual behavior, such as attempts to access customer data. Our security engineers review inbound security reports and monitor public mailing lists, blog posts, and wikis. Automated network analysis and automated analysis of system logs helps determine when an unknown threat might exist; if the automated processes detect an issue, they escalate it to our security staff. We have a rigorous incident-management process for security events that might affect the confidentiality, integrity, or availability of systems or data. Our security incident-management program aligns with the NIST guidance on handling incidents (NIST SP 800\u201361). Key members of our staff are trained in forensics and in handling evidence in preparation for an event, including the use of third-party and proprietary tools. We test incident response plans for key areas, such as systems that store customer data. These tests consider various scenarios, including insider threats and software vulnerabilities. To help ensure the swift resolution of security incidents, the Google security team is available 24/7 to all employees. If an incident impacts your data, Google or its partners inform you and our team investigates the incident. For more information about our data incident response process, seeData incident response process. Google Cloud runs on a technology platform that is designed and built to operate securely. We are an innovator in hardware, software, network, and system management technologies. We design our servers, our proprietary operating system, and our geographically distributed data centers. Using the principles of defense in depth, we've created an IT infrastructure"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-7",
    "content": "that is more secure and easier to manage than more conventional technologies. Our focus on security and protection of data is amongour primary design criteria. The physical security in Google data centers is a layered security model. Physical security includes safeguards like custom-designed electronic access cards, alarms, vehicle access barriers, perimeter fencing, metal detectors, and biometrics. In addition, to detect and track intruders, we use security measures such as laser beam intrusion detection and 24/7 monitoring by high-resolution interior and exterior cameras. Access logs, activity records, and camera footage are available in case an incident occurs. Experienced security guards, who have undergone rigorous background checks and training, routinely patrol our data centers. As you get closer to the data center floor, security measures also increase. Access to the data center floor is only possible through a security corridor that implements multi-factor access control using security badges and biometrics. Only approved employees with specific roles may enter. Very few Google employees will ever gain access to one of our data centers. Inside our data centers, we employ security controls in thephysical-to-logical space, defined as \"arm's length from a machine in a rack to the machine's runtime environment.\" These controls include hardware hardening, task-based access control, anomalous event detection, and system self-defense. For more information, seeHow Google protects the physical-to-logical space in a data center. To keep things running 24/7 and provide uninterrupted services, our data centers have redundant power systems and environmental controls. Every critical component has a primary and alternate power source, each with equal power. Backup generators"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-8",
    "content": "can provide enough emergency electrical power to run each data center at full capacity. Cooling systems maintain a constant operating temperature for servers and other hardware, which reduces the risk of service outages whileminimizing environmental impact. Fire detection and suppression equipment help prevent damage to hardware. Heat detectors, fire detectors, and smoke detectors trigger audible and visible alarms at security operations consoles and at remote monitoring desks. We are the first major internet services company to get external certification of our high environmental, workplace safety, and energy management standards throughout our data centers. For example, to demonstrate our commitment to energy management practices, we obtained voluntaryISO 50001certifications for our data centers in Europe. For more information about how we reduce our environmental impact in Google Cloud, seeEfficiency. Our data centers have purpose-built servers and network equipment, some of which we design ourselves. While our servers are customized to maximize performance, cooling, and power efficiency, they are also designed to help protect against physical intrusion attacks. Unlike most commercially available hardware, our servers don't include unnecessary components such as video cards, chipsets, or peripheral connectors, all of which can introduce vulnerabilities. We vet component vendors and choose components with care, working with vendors to audit and validate the security properties that are provided by the components. We design custom chips, such asTitan, that help us securely identify and authenticate legitimate Google devices at the hardware level, including the code that these devices use to boot up. Server resources are dynamically allocated. Dynamic allocation gives"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-9",
    "content": "us flexibility for growth and lets us adapt quickly and efficiently to customer demand by adding or reallocating resources. This environment is maintained by proprietary software that continually monitors systems for binary-level modifications. Our automated, self-healing mechanisms are designed to enable us to monitor and remediate destabilizing events, receive notifications about incidents, and slow down potential compromises on the network. We meticulously track the location and status of equipment within our data centers using barcodes and asset tags. We deploy metal detectors and video surveillance to help make sure that no equipment leaves the data center floor without authorization. If a component fails to pass a performance test at any point during its lifecycle, it's removed from inventory and retired. Our storage devices, including hard drives, solid-state drives, and non-volatile dual inline memory modules (DIMMs), use technologies like full disk encryption (FDE) and drive locking to protect data at rest. When a storage device is retired, authorized individuals verify that the device is sanitized. They also perform a multiple-step verification process to ensure the device contains no data. If a device cannot be erased for any reason, it's physically destroyed. Physical destruction is performed using a shredder that breaks the device into small pieces, which are then recycled at a secure facility. Each data center adheres to a strict disposal policy and any variances are immediately addressed. For more information, seeData deletion on Google Cloud. We proactively seek to limit the opportunities for vulnerabilities to be introduced by using source control protections and two-party reviews. We also provide libraries that prevent developers from introducing"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-10",
    "content": "certain classes of security bugs. For example, we have libraries and frameworks that are designed to eliminate XSS vulnerabilities in web apps. We also have automated tools for automatically detecting security bugs; these tools include fuzzers, static analysis tools, and web security scanners. For more information, seeSafe software development. Google Cloud services are designed to deliver better security than many on-premises solutions. This section describes the main security controls that we use to help protect your data. Encryption adds a layer of defense for protecting data. Encryption ensures that if an attacker gets access to your data, the attacker cannot read the data without also having access to the encryption keys. Even if an attacker gets access to your data (for example, by accessing the wire connection between data centers or by stealing a storage device), they won't be able to understand or decrypt it. Encryption provides an important mechanism in how we help protect the privacy of your data. It allows systems to manipulate data\u2014for example, for backup\u2014and engineers to support our infrastructure, without providing access to content for those systems or employees. By default, Google Cloud uses several layers of encryption to protect user data that's stored in Google production data centers. Encryption is applied at the application layer, the storage device layer, or both layers. For more information about encryption at rest, including encryption key management and Keystore, seeEncryption at rest in Google Cloud. Data can be vulnerable to unauthorized access as it travels across the internet or within networks. Traffic between your devices and theGoogle Front End (GFE)is encrypted using strong encryption protocols such as TLS. For more information,"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-11",
    "content": "seeEncryption in transit in Google Cloud. Software supply chain integrity ensures that the underlying code and binaries for the services that process your data are verified and that they pass attestation tests. In Google Cloud, we developedBinary Authorization for Borg (BAB)to review and authorize production software that we deploy. BAB helps ensure that only authorized code can process your data. In addition to BAB, we use hardware security chips (called Titan) that we deploy on servers, devices, and peripherals. These chips offer core security features such as secure key storage, root of trust, and signing authority. To help secure your software supply chain, you can implementBinary Authorizationto enforce your policies before deploying your code. For information about securing your supply chain, seeSLSA. Google Cloud supports data encryption for data in use withConfidential Computing. Confidential Computing provides hardware isolation and attestation using a Trusted Execution Environment (TEE). Confidential Computing protects workloads by performing computation in cryptographic isolation, which helps to ensure confidentiality in a multi-tenant cloud environment. This type of cryptographically isolated environment helps prevent unauthorized access or modifications to applications and data while the applications and data are in use. TEE provides independently verifiable attestations that attest to the system state and code run. Confidential Computing might be a good option for organizations that manage sensitive and regulated data and that need verifiable security and privacy assurances. In other cloud services and on-premises solutions, customer data travels between devices across the public internet in paths known ashops. The number of hops depends on the optimal"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-12",
    "content": "route between the customer's ISP and the data center. Each additional hop introduces a new opportunity for data to be attacked or intercepted. Because our global network is linked to most ISPs in the world, our network limits hops across the public internet, and therefore helps limit access to that data by bad actors. Our network uses multiple layers of defense\u2014defense in depth\u2014to help protect the network against external attacks. Only authorized services and protocols that meet our security requirements are allowed to traverse it; anything else is automatically dropped. To enforce network segregation, we use firewalls and access control lists. Traffic is routed through GFE servers to help detect and stop malicious requests and distributed denial-of-service (DDoS) attacks. Logs are routinely examined to reveal any exploitation of programming errors. Access to networked devices is restricted to only authorized employees. Our global infrastructure allows us to runProject Shield. Project Shield provides free, unlimited protection to websites that are vulnerable to DDoS attacks that are used to censor information. Project Shield is available for news websites, human rights websites, and election-monitoring websites. Our IP data network consists of our own fiber, of publicly available fiber, and of undersea cables. This network allows us to deliver highly available and low-latency services across the globe. We design the components of our platform to be highly redundant. This redundancy applies to our server design, to how we store data, to network and internet connectivity, and to the software services themselves. This \"redundancy of everything\" includes exception handling and creates a solution that is not dependent on a single server, data center, or network connection."
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-13",
    "content": "Our data centers are geographically distributed to minimize the effects of regional disruptions on global products, such as when natural disasters or local outages occur. If hardware, software, or a network fails, platform services and control planes are automatically and swiftly shifted from one facility to another so that platform services can continue without interruption. Our highly redundant infrastructure also helps you protect your business from data loss. You can create and deploy Google Cloud resources across multiple regions and zones to build resilient and highly available systems. Our systems are designed to minimize downtime or maintenance windows for when we need to service or upgrade our platform. For more information about how Google Cloud builds resilience and availability into its core infrastructure and services, from design through operations, see theGoogle Cloud infrastructure reliability guide. Some Google Cloud services are not available in all geographies. Some service disruptions are temporary (due to an unanticipated event, such as a network outage), but other service limitations are permanent due to government-imposed restrictions. Our comprehensiveTransparency Reportandstatus dashboardshowrecent and ongoing disruptions of trafficand availability of Google Cloud services. We provide this data to help you analyze and understand the availability of services. This section describes how we restrict access to data and how we respond to data requests from law enforcement agencies. Data that you store on our systems is yours. We don't scan your data for advertising purposes, we don't sell it to third parties, and we don't use it to train our AI models without your permission. TheData Processing Addendumfor Google Cloud describes our commitment to"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-14",
    "content": "protecting your data. That document states that we won't process data for any purpose other than to meet our contractual obligations. If you choose to stop using our services, we provide tools that let you take your data with you, without penalty or additional cost. For more information about our commitments for Google Cloud, see ourtrust principles. Our infrastructure is designed to logically isolate each customer's data from the data of other customers and users, even when it's stored on the same physical server. Only a small group of employees have access to customer data. Access rights and levels are based on an employee's job function and role, using the principles of least privilege and need-to-know that match access privileges to defined responsibilities. Our employees are granted only a limited set of default permissions to access company resources, such as employee email and Google's internal employee portal. Requests for additional access must follow a formal process that involves a request and an approval from the data or system owner, manager, or other executives, as dictated by our security policies. Approvals are managed by workflow tools that maintain audit records of all changes. These tools control both the modification of authorization settings and the approval process to help ensure that approval policies are consistently applied. An employee's authorization settings are used to control access to resources, including data and systems for Google Cloud products. Support services are provided only to authorized customer administrators. Our dedicated security teams, privacy teams, and internal audit teams monitor and audit employee access, and we provide audit logs to you throughAccess Transparencyfor Google Cloud. Also, when you enableAccess Approval,"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-15",
    "content": "our support personnel and our engineers require your explicit approval to access your data. As the data owner, you are primarily responsible for responding to law enforcement data requests. However, like many technology companies, we receive direct requests from governments and courts to disclose customer information. Google has operational policies and procedures and other organizational measures in place to help protect against unlawful or excessive requests for user data by public authorities. When we receive such a request, our team reviews the request to make sure that it satisfies legal requirements and Google's policies. Generally speaking, for us to comply, the request must be made in writing, issued under an appropriate law, and signed by an authorized official of the requesting agency. We believe that the public deserves to know the full extent to which governments request information from us. We became the first company to start regularly publishing reports about government data requests. Detailed information about data requests and our response to them is available in ourTransparency Report. It's our policy to notify you about requests for your data unless we are specifically prohibited by law or court order from doing so. For more information, seeGovernment Requests for Cloud Customer Data. For most data-processing activities, we provide our services in our own infrastructure. However, we might engage some third-party suppliers to provide services related to Google Cloud, including customer support and technical support. Before onboarding a supplier, we assess their security and privacy practices. This assessment checks whether the supplier provides a level of security and privacy that is appropriate for their access to data and for the scope of the"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-16",
    "content": "services that they are engaged to provide. After we have assessed the risks that are presented by the third-party supplier, the supplier is required to enter into appropriate security, confidentiality, and privacy contract terms. For more information, see theSupplier Code of Conduct. Google Cloud regularly undergoes independent verification of its security, privacy, and compliance controls, and receives certifications, attestations, and audit reports to demonstrate compliance. Our information security includes specific customer data privacy-related controls that help keep customer data secure. Some key international standards that we are audited against are the following: ISO/IEC 27001 (Information Security Management) ISO/IEC 27017 (Cloud Security) ISO/IEC 27018 (Cloud Privacy) ISO/IEC 27701 (Privacy) In addition, ourSOC 2andSOC 3reports are available to our customers. We also participate in sector and country-specific frameworks, such asFedRAMP(US government),BSI C5(Germany), andMTCS(Singapore). We provide resource documents and mappings for certain frameworks where formal certifications or attestations might not be required or applied. If you operate in regulated industries, such as finance, government, healthcare, or education, Google Cloud provides products and services that help you be compliant with numerous industry-specific requirements. SeeAssured Workloads overviewfor information about how you can implement regulatory requirements in Google Cloud. For a complete listing of our compliance offerings, see theCompliance resource center. We maintain a robust insurance program for many risk types, including cyber and privacy liability insurance coverage. These policies include coverage for Google Cloud in events such as unauthorized use or access of our network;"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-17",
    "content": "regulatory action where insurable; failure to adequately protect confidential information; notification costs; and crisis management costs, including forensic investigation. Security is a shared responsibility. Generally, you are responsible for securing what you bring to the cloud, whereas we are responsible for protecting the cloud itself. Therefore, while you're always responsible for securing your data, we are responsible for securing the underlying infrastructure. The following image visualizes this relationship as the shared responsibility model, which describes the responsibilities that we and you have in Google Cloud. In the infrastructure as a service (IaaS) model, only the hardware, storage, and network are our responsibility. In the software as a service (SaaS) model, the security of everything except the data and its access and usage are our responsibility. Google Cloud offers a range of security services that you can take advantage of to secure your cloud environment at scale. For more information, seeSecurity and identity products in Google Cloud. You can also find more information in oursecurity best practices center. The protection of your data is a primary design consideration for our infrastructure, products, and operations. Our scale of operations and our collaboration with the security research community enable us to address vulnerabilities quickly, and often to prevent them entirely. We run our own services, such as Search, YouTube, and Gmail, on the same infrastructure that we make available to our customers, who benefit directly from our security controls and practices. We offer a level of protection that few public cloud providers or private enterprise IT teams can match. Protecting data is core to our business, so we make extensive investments"
  },
  {
    "source_url": "https://cloud.google.com/docs/security/overview/whitepaper",
    "title": "Google security overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/security/overview/whitepaper#chunk-18",
    "content": "in security, resources, and expertise at a scale that others cannot. Our investment frees you to focus on your business and innovation. Our strong contractual commitments help you maintain control over your data and how it's processed. We don't use your data for advertising or any purpose other than to deliver Google Cloud services. Many innovative organizations trust us with their most valuable asset: their data. We will continue to invest in the security of Google Cloud services to let you benefit from our services in a secure and transparent manner. To learn more about our security culture and philosophy, readBuilding Secure and Reliable Systems (O'Reilly book). For information about our novel approach to cloud security, readBeyondProd, which describes how to protect code change and access to user data in microservices. To adopt similar security principles for your own workloads, deploy theenterprise foundations blueprint. To learn more about Google Workspace security, seeGoogle Workspace security. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama",
    "title": "Self-deployed Llama modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Llama is a collection of open models developed by Meta that you can fine-tune and deploy on Vertex AI. Llama offers pre-trained and instruction-tuned generative text and multimodal models. The Llama 4 family of models is a collection of multimodal models that use the Mixture-of-Experts (MoE) architecture. By using the MoE architecture, models with very large parameter counts can activate a subset of those parameters for any given input, which leads to more efficient inferences. Additionally, Llama 4 uses early fusion, which integrates text and vision information from the initial processing stages. This method enables Llama 4 models to more effectively grasp complex, nuanced relationships between text and images. Model Garden on Vertex AI offers two Llama 4 models: Llama 4 Scout and Llama 4 Maverick. For more information, see theLlama 4model card in Model Garden or view theIntroducing Llama 4 on Vertex AI blog post. Llama 4 Maverick is the largest and most capable Llama 4 model, offering industry-leading capabilities on coding, reasoning, and image benchmarks. It features 17 billion active parameters out of 400 billion total parameters with 128 experts. Llama 4 Maverick uses alternating dense and MoE layers, where each token activates a shared expert plus one of the 128 routed experts. You can use the model as a pretrained (PT) model or instruction-tuned (IT) model with FP8 support. The model is pretrained on 200 languages and optimized for high-quality chat interactions through a refined post-training pipeline. Llama 4 Maverick is multimodal and has a 1M context length. It is suited for advanced image captioning, analysis, precise image understanding, visual Q&A, creative text generation, general-purpose AI assistants, and"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama",
    "title": "Self-deployed Llama modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama#chunk-1",
    "content": "sophisticated chatbots requiring top-tier intelligence and image understanding. Llama 4 Scout delivers state-of-the-art results for its size class with a large 10 million token context window, outperforming previous Llama generations and other open and proprietary models on several benchmarks. It features 17 billion active parameters out of the 109 billion total parameters with 16 experts and is available as a pretrained (PT) or instruction-tuned (IT) model. Llama 4 Scout is suited for retrieval tasks within long contexts and tasks that demand reasoning over large amounts of information, such as summarizing multiple large documents, analyzing extensive user interaction logs for personalization and reasoning across large codebases. Llama 3.3 is a text-only 70B instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B and to Llama 3.2 90B when used for text-only applications. Moreover, for some applications, Llama 3.3 70B approaches the performance of Llama 3.1 405B. For more information, see theLlama 3.3model card in Model Garden. Llama 3.2 enables developers to build and deploy the latest generative AI models and applications that use Llama's capabilities to ignite new innovations, such as image reasoning. Llama 3.2 is also designed to be more accessible for on-device applications. The following list highlights Llama 3.2 features: Offers a more private and personalized AI experience, with on-device processing for smaller models. Offers models that are designed to be more efficient, with reduced latency and improved performance, making them suitable for a wide range of applications. Built on top of the Llama Stack, which makes building and deploying applications easier. Llama Stack is a standardized interface for building canonical toolchain"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama",
    "title": "Self-deployed Llama modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama#chunk-2",
    "content": "components and agentic applications. Supports vision tasks, with a new model architecture that integrates image encoder representations into the language model. The 1B and 3B models are lightweight text-only models that support on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting. Llama 11B and 90B models are small and medium-sized multimodal models with image reasoning. For example, they can analyze visual data from charts to provide more accurate responses and extract details from images to generate text descriptions. For more information, see theLlama 3.2model card in Model Garden. When using the 11B and 90B, there are no restriction when you send text-only prompts. However, if you include an image in your prompt, the image must be at beginning of your prompt, and you can include only one image. You cannot, for example, include some text and then an image. Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction-tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text-only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. For more information, see theLlama 3.1model card in Model Garden. The Llama 3 instruction-tuned models are a collection of LLMs optimized for dialogue use cases. Llama 3 models outperform many of the available open source chat models on common industry benchmarks. For more information, see theLlama 3model card in Model Garden. The Llama 2 LLMs is a collection of pre-trained and fine-tuned generative text models, ranging in size from 7B to 70B parameters. For more information,"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama",
    "title": "Self-deployed Llama modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama#chunk-3",
    "content": "see theLlama 2model card in Model Garden. Meta's Code Llama models are designed for code synthesis, understanding, and instruction. For more information, see theCode Llamamodel card in Model Garden. Llama Guard 3 builds on the capabilities of Llama Guard 2, adding three new categories: Defamation, Elections, and Code Interpreter Abuse. Additionally, this model is multilingual and has a prompt format that is consistent with Llama 3 or later instruct models. For more information, see theLlama Guardmodel card in Model Garden. For more information about Model Garden, seeExplore AI models in Model Garden. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-13 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models",
    "title": "Use Hugging Face ModelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Hugging Faceprovides pre-trained models, fine-tuning scripts, and development APIs that make the process of creating and discovering LLMs easier.Model Gardencan serveText Embeddings,Text To Image,Text Generation, andImage Text To Textmodels in HuggingFace. You can deploy supported Hugging Face models in Vertex AI or Google Kubernetes Engine (GKE). The deployment option you choose can depend on the model you're using and how much control you want over your workloads. Vertex AI offers a managed platform for building and scaling machine learning projects without in-house MLOps expertise. You can use Vertex AI as the downstream application that serves the Hugging Face models. We recommend using Vertex AI if you want end-to-end MLOps capabilities, value-added ML features, and a serverless experience for streamlined development. To deploy a supported Hugging Face model in Vertex AI, go to Model Garden.Go to Model Garden To deploy a supported Hugging Face model in Vertex AI, go to Model Garden. Go to Model Garden Go to theOpen models on Hugging Facesection and clickShow more. Go to theOpen models on Hugging Facesection and clickShow more. Find and select a model to deploy. Find and select a model to deploy. Optional: For theDeployment environment, selectVertex AI. Optional: For theDeployment environment, selectVertex AI. Optional: Specify the deployment details. Optional: Specify the deployment details. ClickDeploy. ClickDeploy. To get started, see the following examples: Some models have detailed model cards and the deployment settings are verified by Google, such asgoogle/gemma-3-27b-it,meta-llama/Llama-4-Scout-17B-16E-Instruct,Qwen/QwQ-32B,BAAI/bge-m3,intfloat/multilingual-e5-large-instruct,black-forest-labs/FLUX.1-dev,"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models",
    "title": "Use Hugging Face ModelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models#chunk-1",
    "content": "andHuggingFaceFW/fineweb-edu-classifier. Some models have the deployment settings verified by Google but no detailed model cards, such asNousResearch/Genstruct-7B. Some models have deployment settings generated automatically. Some models have automatically generated deployment settings that are based on model metadata, such as some latest trending models intext generation,text embeddings,text to image generation, andimage text to text. Google Kubernetes Engine (GKE) is the Google Cloud solution for managed Kubernetes that provides scalability, security, resilience, and cost effectiveness. We recommend this option if you have existing Kubernetes investments, your organization has in-house MLOps expertise, or if you need granular control over complex AI/ML workloads with unique security, data pipeline, and resource management requirements. To deploy a supported Hugging Face model in GKE, go to Model Garden.Go to Model Garden To deploy a supported Hugging Face model in GKE, go to Model Garden. Go to Model Garden Go to theOpen models on Hugging Facesection and clickShow more. Go to theOpen models on Hugging Facesection and clickShow more. Find and select a model to deploy. Find and select a model to deploy. For theDeployment environment, selectGKE. For theDeployment environment, selectGKE. Follow the deployment instructions. Follow the deployment instructions. To get started, see the following examples: Some models have detailed model cards and verified deployment settings, such asgoogle/gemma-3-27b-it,meta-llama/Llama-4-Scout-17B-16E-Instruct, andQwen/QwQ-32B. Some models have verified deployment settings, but no detailed model cards, such asNousResearch/Genstruct-7B. We automatically add the latest, most popular Hugging Face models to Model Garden. This process includes"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models",
    "title": "Use Hugging Face ModelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models#chunk-2",
    "content": "the automatic generation of a deployment configuration for each model. To address concerns regarding vulnerabilities and malicious code, we use theHugging Face Malware Scannerto assess the safety of files within each Hugging Face model repository on a daily basis. If a model repository is flagged as containing malware, we immediately remove the model from the Hugging Face gallery page. While a model being designated assupported by Vertex AIsignifies that it has undergone testing and is deployable on Vertex AI, we don't guarantee the absence of vulnerabilities or malicious code. We recommend that you conduct your own security verifications before deploying any model in your production environment. The default deployment configuration that is provided with the one-click deployment option can't satisfy every requirement given the diverse range of use cases and varying priorities with latency, throughput, cost, and accuracy. Therefore, you can initially experiment with the one-click deployment to establish a baseline, and then fine-tune the deployment configurations by using the Colab notebook (vLLM,TGI,TEI,HF pytorch inference) or the Python SDK. This iterative approach lets you to tailor the deployment to your precise needs to get the best possible performance for your specific application. If you're looking for a specific model that's not listed in Model Garden, the model is not supported by Vertex AI. The following sections describe the reasoning and what you can do. The following reasons explain why a model might not be in Model Garden: It's not a top trending model: We often prioritize models that are widely popular and have strong community interest. It's not yet compatible: The model might not work with a supported serving container. For example, thevLLM"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models",
    "title": "Use Hugging Face ModelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models#chunk-3",
    "content": "containerfortext-generationandimage-text-to-textmodels. Unsupported pipeline tasks: The model has ataskwhich we don't yet fully support at the moment. We support the following tasks:text-generation,text2text-generation,text-to-image,feature-extraction,sentence-similarity, andimage-text-to-text. You can still work with models that available in Model Garden: Deploy it yourself using the Colab Notebook: We have the following Colab Notebooks: (vLLM,TGI,TEI,HF pytorch inference), which provide the flexibility to deploy models with custom configurations. This gives you complete control over the process. Submit a Feature Request: work with your support engineer and submit a feature request through the Model Garden, or refer toVertex Generative AI supportfor additional help. Keep an eye on updates: We regularly add new models to Model Garden. The model you're looking for might become available in the future, so check back periodically! Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/access-resources",
    "title": "Access and resource management",
    "chunk_id": "https://cloud.google.com/docs/access-resources#chunk-0",
    "content": "Home Documentation Organize, analyze, and manage access to your Google Cloud resources and services. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Define who can access resources in your organization. Manage internal enterprise solutions and Google Cloud APIs. Optimize your service usage, monitor application and resource health, and identify disruptive events. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/java/docs/spring",
    "title": "Spring Framework supportStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/java/docs/spring#chunk-0",
    "content": "Home Java Documentation Guides The Spring Cloud GCP project brings thePivotal-developedSpring Framework to the Google Cloud APIs. Spring simplifies application development by providing the infrastructure for enterprise applications to accomplish common tasks, such as exposing services and interacting with databases and messaging systems. TheSpring Cloud GCPpage provides the full list of features and the documentation on how to get started. You can find the source code and additional resources in theGitHub repository. The repository also containscode samplesto help you develop your application. Thereference documentationprovides detailed information on how to integrate Google Cloud APIs with your Spring and Spring Boot applications. Learn how to get started using Spring on Google Cloud. Thesecodelabsare designed to get you up and running quickly using GCP products and resources. See our latest talks and presentations. Contribute to the project byfiling an issueor submitting apull request. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-17 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/compute-area",
    "title": "Compute technology",
    "chunk_id": "https://cloud.google.com/docs/compute-area#chunk-0",
    "content": "Home Documentation Run your workloads on virtual machines with specialized offerings for ML, high-performance computing, and other workloads to match your needs. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create a VM. OS images for virtual machines. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/workbench/instances",
    "title": "Vertex AI Workbench instances\n documentation",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/workbench/instances#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI Workbench Vertex AI Workbench instances are Jupyter notebook-based development environments for the entire data science workflow. Vertex AI Workbench instances are prepackaged withJupyterLaband have a preinstalled suite of deep learning packages, including support for the TensorFlow and PyTorch frameworks.Learn more. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create a Vertex AI Workbench instance Create a Vertex AI Workbench instance Introduction to Vertex AI Workbench instances Introduction to Vertex AI Workbench instances Query data in BigQuery from within JupyterLab Query data in BigQuery from within JupyterLab Add a conda environment Add a conda environment Manage your conda environment Manage your conda environment Change machine type and configure GPUs of a Vertex AI Workbench instance Change machine type and configure GPUs of a Vertex AI Workbench instance Pricing Pricing Release notes Release notes Get support Get support Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings",
    "title": "Embeddings APIs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Embeddings are numerical representations of text, images, or videos that capture relationships between inputs. Machine learning models, especially generative AI models, are suited for creating embeddings by identifying patterns within large datasets. Applications can use embeddings to process and produce language, recognizing complex meanings and semantic relationships specific to your content. You interact with embeddings every time you complete a Google Search or see music streaming recommendations. Embeddings work by converting text, image, and video into arrays of floating point numbers, called vectors. These vectors are designed to capture the meaning of the text, images, and videos. The length of the embedding array is called the vector's dimensionality. For example, one passage of text might be represented by a vector containing hundreds of dimensions. Then, by calculating the numerical distance between the vector representations of two pieces of text, an application can determine the similarity between the objects. Vertex AI supports two types of embeddings models, text and multimodal. Some common use cases for text embeddings include: Semantic search: Search text ranked by semantic similarity. Classification: Return the class of items whose text attributes are similar to the given text. Clustering: Cluster items whose text attributes are similar to the given text. Outlier Detection: Return items where text attributes are least related to the given text. Conversational interface: Clusters groups of sentences which can lead to similar responses, like in a conversation-level embedding space. If you want to develop a book recommendation chatbot, the first thing to do is to use a deep neural network (DNN) to convert each"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings",
    "title": "Embeddings APIs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings#chunk-1",
    "content": "book into an embedding vector, where one embedding vector represents one book. You can feed, as input to the DNN, just the book title or just the text content. Or you can use both of these together, along with any other metadata describing the book, such as the genre. The embeddings in this example could be comprised of thousands of book titles with summaries and their genre, and it might have representations for books likeWuthering Heightsby Emily Bront\u00eb andPersuasionby Jane Austen that are similar to each other (small distance between numerical representation). Whereas the numerical representation for the bookThe Great Gatsbyby F. Scott Fitzgerald would be further, as the time period, genre, and summary is less similar. The inputs are the main influence to the orientation of the embedding space. For example, if we only had book title inputs, then two books with similar titles, but very different summaries, could be close together. However, if we include the title and summary, then these same books are less similar (further away) in the embedding space. Working with generative AI, this book-suggestion chatbot could summarize, suggest, and show you books which you might like (or dislike), based on your query. Some common use cases for multimodal embeddings include: Image and text use cases:Image classification: Takes an image as input and predicts one or more classes (labels).Image search: Search relevant or similar images.Recommendations: Generate product or ad recommendations based on images. Image and text use cases: Image classification: Takes an image as input and predicts one or more classes (labels). Image search: Search relevant or similar images. Recommendations: Generate product or ad recommendations based on images. Image, text, and video use"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings",
    "title": "Embeddings APIs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings#chunk-2",
    "content": "cases:Recommendations: Generate product or advertisement recommendations based on videos (similarity search).Video content searchUsing semantic search: Take a text as an input, and return a set of ranked frames matching the query.Using similarity search:Take a video as an input, and return a set of videos matching the query.Take an image as an input, and return a set of videos matching the query.Video classification: Takes a video as input and predicts one or more classes. Image, text, and video use cases: Recommendations: Generate product or advertisement recommendations based on videos (similarity search). Video content search Using semantic search: Take a text as an input, and return a set of ranked frames matching the query. Using similarity search:Take a video as an input, and return a set of videos matching the query.Take an image as an input, and return a set of videos matching the query. Take a video as an input, and return a set of videos matching the query. Take an image as an input, and return a set of videos matching the query. Video classification: Takes a video as input and predicts one or more classes. Online retailers are increasingly leveraging multimodal embeddings to enhance customer experience. Every time you see personalized product recommendations while shopping, and get visual results from a text search, you are interacting with an embedding. If you want to create a multimodal embedding for an online retail use case, start by processing each product image to generate a unique image embedding, which is a mathematical representation of its visual style, color palette, key details, and more. Simultaneously, convert product descriptions, customer reviews, and other relevant textual data into text embeddings that capture their semantic meaning and"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings",
    "title": "Embeddings APIs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings#chunk-3",
    "content": "context. By merging these image and text embeddings into a unified search and recommendation engine, the store can offer personalized recommendations of visually similar items based on a customer's browsing history and preferences. Additionally, it enables customers to search for products using natural language descriptions, with the engine retrieving and displaying the most visually similar items that match their search query. For example, if a customer searches \"Black summer dress\", the search engine can display dresses which are black, and also are in summer dress cuts, made out of lighter material, and might be sleeveless. This powerful combination of visual and textual understanding creates a streamlined shopping experience that enhances customer engagement, satisfaction, and ultimately can drive sales. To learn more about embeddings, seeMeet AI's multitool: Vector embeddings. To take a foundational ML crash course on embeddings, seeEmbeddings. To learn more about how to store vector embeddings in a database, see theDiscoverpage and theOverview of Vector Search. To learn about responsible AI best practices and Vertex AI's safety filters, seeResponsible AI. To learn how to get embeddings, see the following documents:Get text embeddingsGet multimodal embeddings Get text embeddings Get multimodal embeddings Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/costs-usage",
    "title": "Costs and usage management",
    "chunk_id": "https://cloud.google.com/docs/costs-usage#chunk-0",
    "content": "Home Documentation Manage costs and usage across Google Cloud products and services. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Track your Google Cloud costs, analyze your billing data, control and optimize your costs, and take advantage of committed use discounts. Optimize your Google Cloud resource usage and increase efficiency. Measure, report, and reduce your Cloud carbon emissions. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation You can evaluate the performance of foundation models and your tuned generative AI models on Vertex AI. The models are evaluated using a set of metrics against an evaluation dataset that you provide. This page explains how computation-based model evaluation through the evaluation pipeline service works, how to create and format the evaluation dataset, and how to perform the evaluation using the Google Cloud console, Vertex AI API, or the Vertex AI SDK for Python. To evaluate the performance of a model, you first create an evaluation dataset that contains prompt and ground truth pairs. For each pair, the prompt is the input that you want to evaluate, and the ground truth is the ideal response for that prompt. During evaluation, the prompt in each pair of the evaluation dataset is passed to the model to produce an output. The output generated by the model and the ground truth from the evaluation dataset are used to compute the evaluation metrics. The type of metrics used for evaluation depends on the task that you are evaluating. The following table shows the supported tasks and the metrics used to evaluate each task: Model evaluation is supported for the following models: text-bison: Base and tuned versions. text-bison: Base and tuned versions. Gemini: All tasks except classification. Gemini: All tasks except classification. The evaluation dataset that's used for model evaluation includes prompt and ground truth pairs that align with the task that you want to evaluate. Your dataset must include a minimum of 1 prompt and ground truth pair and at least 10 pairs for meaningful metrics. The more examples you give, the more meaningful the results. Your evaluation dataset must be inJSON Lines(JSONL) format where each line contains"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#chunk-1",
    "content": "a single prompt and ground truth pair specified in theinput_textandoutput_textfields, respectively. Theinput_textfield contains the prompt that you want to evaluate, and theoutput_textfield contains the ideal response for the prompt. The maximum token length forinput_textis 8,192, and the maximum token length foroutput_textis 1,024. You can eithercreate a new Cloud Storage bucketor use an existing one to store your dataset file. The bucket must be in the same region as the model. After your bucket is ready,uploadyour dataset file to the bucket. You can evaluate models by using the REST API or the Google Cloud console. To perform this task, you must grantIdentity and Access Management (IAM)roles to each of the following service accounts: Vertex AI User (roles/aiplatform.user) Storage Object User (roles/storage.objectUser) Depending on your input and output data sources, you may also need to grant the Vertex AI Pipelines Service Account additional roles: To create a model evaluation job, send aPOSTrequest by using thepipelineJobsmethod. Before using any of the request data, make the following replacements: PROJECT_ID: The Google Cloud project that runs the pipeline components. PIPELINEJOB_DISPLAYNAME: A display name for the pipelineJob. LOCATION: The region to run the pipeline components. Currently, onlyus-central1is supported. DATASET_URI: The Cloud Storage URI of your reference dataset. You can specify one or multiple URIs. This parameter supportswildcards. To learn more about this parameter, seeInputConfig. OUTPUT_DIR: The Cloud Storage URI to store evaluation output. MODEL_NAME: Specify a publisher model or a tuned model resource as follows:Publisher model:publishers/google/models/MODEL@MODEL_VERSIONExample:publishers/google/models/text-bison@002Tuned"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#chunk-2",
    "content": "model:projects/PROJECT_NUMBER/locations/LOCATION/models/ENDPOINT_IDExample:projects/123456789012/locations/us-central1/models/1234567890123456789The evaluation job doesn't impact any existing deployments of the model or their resources. Publisher model:publishers/google/models/MODEL@MODEL_VERSIONExample:publishers/google/models/text-bison@002 Example:publishers/google/models/text-bison@002 Tuned model:projects/PROJECT_NUMBER/locations/LOCATION/models/ENDPOINT_IDExample:projects/123456789012/locations/us-central1/models/1234567890123456789 Example:projects/123456789012/locations/us-central1/models/1234567890123456789 The evaluation job doesn't impact any existing deployments of the model or their resources. EVALUATION_TASK: The task that you want to evaluate the model on. The evaluation job computes a set of metrics relevant to that specific task. Acceptable values include the following:summarizationquestion-answeringtext-generationclassification summarization question-answering text-generation classification INSTANCES_FORMAT: The format of your dataset. Currently, onlyjsonlis supported. To learn more about this parameter, seeInputConfig. PREDICTIONS_FORMAT: The format of the evaluation output. Currently, onlyjsonlis supported. To learn more about this parameter, seeInputConfig. MACHINE_TYPE: (Optional) The machine type for running the evaluation job. The default value ise2-highmem-16. For a list of supported machine types, seeMachine types. SERVICE_ACCOUNT: (Optional) The service account to use for running the evaluation job. To learn how to create a custom service account, seeConfigure a service account with granular permissions. If unspecified, theVertex AI Custom Code Service Agentis used. NETWORK: (Optional) The fully qualified name of the Compute Engine network to"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#chunk-3",
    "content": "peer the evaluatiuon job to. The format of the network name isprojects/PROJECT_NUMBER/global/networks/NETWORK_NAME. If you specify this field, you need to have aVPC Network Peering for Vertex AI. If left unspecified, the evaluation job is not peered with any network. KEY_NAME: (Optional) The name of the customer-managed encryption key (CMEK). If configured, resources created by the evaluation job is encrypted using the provided encryption key. The format of the key name isprojects/PROJECT_ID/locations/REGION/keyRings/KEY_RING/cryptoKeys/KEY. The key needs to be in the same region as the evaluation job. HTTP method and URL: Request JSON body: To send your request, choose one of these options: Save the request body in a file namedrequest.json, and execute the following command: Save the request body in a file namedrequest.json, and execute the following command: You should receive a JSON response similar to the following. Note thatpipelineSpechas been truncated to save space. To learn how to install or update the Vertex AI SDK for Python, seeInstall the Vertex AI SDK for Python. For more information, see theVertex AI SDK for Python API reference documentation. To create a model evaluation job by using the Google Cloud console, perform the following steps: In the Google Cloud console, go to theVertex AI Model Registrypage.Go to Vertex AI Model RegistryClick the name of the model that you want to evaluate.In theEvaluatetab, clickCreate evaluationand configure as follows:Objective: Select the task that you want to evaluate.Target column or field: (Classification only) Enter the target column for prediction. Example:ground_truth.Source path: Enter or select the URI of your evaluation dataset.Output format: Enter the format of the evaluation output. Currently, onlyjsonlis"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#chunk-4",
    "content": "supported.Cloud Storage path: Enter or select the URI to store evaluation output.Class names: (Classification only) Enter the list of possible class names.Number of compute nodes: Enter the number of compute nodes to run the evaluation job.Machine type: Select a machine type to use for running the evaluation job.ClickStart evaluation Go to Vertex AI Model Registry Click the name of the model that you want to evaluate. In theEvaluatetab, clickCreate evaluationand configure as follows: Objective: Select the task that you want to evaluate. Target column or field: (Classification only) Enter the target column for prediction. Example:ground_truth. Source path: Enter or select the URI of your evaluation dataset. Output format: Enter the format of the evaluation output. Currently, onlyjsonlis supported. Cloud Storage path: Enter or select the URI to store evaluation output. Class names: (Classification only) Enter the list of possible class names. Number of compute nodes: Enter the number of compute nodes to run the evaluation job. Machine type: Select a machine type to use for running the evaluation job. ClickStart evaluation You can find the evaluation results in the Cloud Storage output directory that you specified when creating the evaluation job. The file is namedevaluation_metrics.json. For tuned models, you can also view evaluation results in the Google Cloud console: In the Vertex AI section of the Google Cloud console, go to theVertex AI Model Registrypage.Go to Vertex AI Model Registry In the Vertex AI section of the Google Cloud console, go to theVertex AI Model Registrypage. Go to Vertex AI Model Registry Click the name of the model to view its evaluation metrics. Click the name of the model to view its evaluation metrics. In theEvaluatetab, click the name of the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models",
    "title": "Run a computation-based evaluation pipelineStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#chunk-5",
    "content": "evaluation run that you want to view. In theEvaluatetab, click the name of the evaluation run that you want to view. Learn aboutgenerative AI evaluation. Learn about online evaluation withGen AI Evaluation Service. Learn how totune a foundation model. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-15 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions",
    "title": "Model versions and lifecycleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Stable model: A publicly released version of the model that is available and supported for production use starting on the release date. A stable model version is typically released with aretirement date, which indicates the last day that the model is available. After this date, the model is no longer accessible or supported by Google. Latest stable model: The latest version within the model family recommended for new and active projects and should be the target for migrations from earlier versions. SeeLatest stable models. Legacy stable model: A model version that's been superseded by the Latest Stable Model. Although legacy stable models are still supported, you should strongly consider migrating to the latest model to receive the latest features and improvements. Access to legacy stable models might be restricted for new projects. SeeLegacy stable models. Retired model: The model version is past its retirement date and has been permanently deactivated. Retired models are no longer accessible or supported by Google. API requests referencing a retired model ID typically returns a 404 error. SeeRetired models. Recommended upgrade: The latest stable model that we recommend switching to. Latest stable models tend to offer better performance and more capabilities as compared to legacy stable models. See the recommended upgrades in theLegacy stable modelsandRetired modelssections. The following table lists the latest stable models: The following table lists legacy stable models: *: Restricted for new projects. To learn how to migrate to a latest stable model, seeMigrate your application to Gemini 2 with the Vertex AI Gemini API. This guide gives you a set of migration steps that aims to minimize some potential risks involved in"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions",
    "title": "Model versions and lifecycleStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions#chunk-1",
    "content": "model migration and helps you use new models in an optimal way. However, if you don't have time to follow the guide and just need to quickly resolve the errors caused by models reaching their retirement dates, do the following: Update your application to point to the recommended upgrades. Test all mission critical features to make sure everything works as expected. Deploy the updates like you normally would. The auto-updated alias of a Gemini model always points to the latest stable model. When a new latest stable model is available, the auto-updated alias automatically points to the new version. The following table shows the auto-updated aliases for Gemini models and the latest stable models that they point to. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-0",
    "content": "Home Virtual Private Cloud Documentation Guides This document provides an overview of Private Service Connect. Private Service Connect is a capability of Google Cloud networking that allowsconsumersto accessmanaged servicesprivately from inside their VPC network. Similarly, it allows managed serviceproducersto host these services in their own separate VPC networks and offer a private connection to their consumers. For example, when you use Private Service Connect to access Cloud SQL, you are the service consumer, and Google is the service producer. With Private Service Connect, consumers can use their own internal IP addresses to access services without leaving their VPC networks. Traffic remains entirely within Google Cloud. Private Service Connect provides service-oriented access between consumers and producers with granular control over how services are accessed. Private Service Connect supports access to the following types of managed services: Published VPC-hosted services, which include the following:Google published services, such as Apigee or the GKE control planeThird-party published servicesprovided by Private Service Connect partnersIntra-organizationpublished services, where the consumer and producer might be two different VPC networks within the same company Google published services, such as Apigee or the GKE control plane Third-party published servicesprovided by Private Service Connect partners Intra-organizationpublished services, where the consumer and producer might be two different VPC networks within the same company Google APIs, such as Cloud Storage or BigQuery Figure 1.Private Service Connect lets you send traffic to endpoints and backends that forward the traffic to managed services, including Google APIs and published services. Private Service"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-1",
    "content": "Connect interfaces let managed services initiate connections to consumer VPC networks. Private Service Connect provides private connectivity that has the following characteristics: Service-oriented design:Producer services are published through load balancers that expose a single IP address to the consumer VPC network. Consumer traffic that accesses producer services is unidirectional and can only access the service IP address, rather than having access to an entire peered VPC network. Explicit authorization:Private Service Connect provides an authorization model that gives consumers and producers granular control, ensuring that only the intended service endpoints and no other resources can connect to a service. No shared dependencies:Traffic between consumer and producers uses NAT so that no IP address coordination or other shared resource dependencies exist between the consumer and producer VPC networks. This independence simplifies deployment and lets you more easily scale managed services. Line-rate performance:Private Service Connect traffic goes directly from consumer clients to producer backends without intermediate hops or proxies. NAT is performed directly on the physical host machines that host the consumer and producer VMs, which reduces latency and increases bandwidth capacity. The bandwidth capacity of Private Service Connect is limited only by the bandwidth capacity of the client and server machines that are directly communicating. Private Service Connect is available in different types that provide different capabilities and modes of communication. Service producerspublish their applications to consumers by creating Private Service Connect services.Service consumersaccess those Private Service Connect services directly through one of these Private Service"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-2",
    "content": "Connect types: Private Service Connect endpoints: Endpoints are deployed by using forwarding rules that provide the consumer an IP address that is mapped to the Private Service Connect service. Private Service Connect backends: Backends are deployed by using network endpoint groups (NEGs) that let consumers direct traffic to their load balancer before reaching a Private Service Connect service. Service producers can initiate connections to service consumers by usingPrivate Service Connect interfaces. Private Service Connect interfaces provide bidirectional communication and can be used in the same VPC network as endpoints and backends. Private Service Connect endpoints are internal IP addresses in a consumer VPC network that can be directly accessed by clients in that network. Endpoints are created by deploying aforwarding rulethat references aservice attachmentor abundle of Google APIs. The following diagram shows a Private Service Connect endpoint that targets a published service that is running in a separate VPC network and organization. Private Service Connect endpoints and published services let two independent companies communicate with each other by using internal IP addresses. For more information, seeAbout accessing published services through endpoints. Figure 2.Private Service Connect lets you send traffic to endpoints that forward the traffic to published services in another VPC network. Similarly, a Private Service Connect endpoint can be used to access Google APIs such as Cloud Storage or BigQuery. This functionality is similar to Private Google Access, except that you can use your own internal IP addresses for endpoints. Private Service Connect lets you more directly control routing and create as many endpoints as necessary for your network. For more"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-3",
    "content": "information, seeAbout accessing Google APIs through endpoints. Figure 3.Private Service Connect lets you send traffic to endpoints that forward the traffic to Google APIs. Private Service Connect backends let Google Cloud load balancers send traffic through Private Service Connect to reach published services or Google APIs. The backends are deployed through Private Service Connectnetwork endpoint groups (NEGs)that reference a producer service attachment or a supported Google API. Placing a load balancer in front of a managed service provides the consumer with more visibility and control than is possible through a Private Service Connect endpoint. Backends let you create configurations such as the following: Customer-owned domains and certificates in front of managed services Consumer-controlled failover between managed services in different regions Centralized security configuration and access control for managed services The following diagram shows an internal Application Load Balancer deployed with Private Service Connect backends that reference a published service. There are two load balancers in the configuration: The consumer load balancer that provides control, visibility, and security of traffic to the service. The producer load balancer that load balances traffic across the service backends. Figure 4.Private Service Connect lets you send traffic to backends that forward the traffic to published services. Similarly to Private Service Connect endpoints, backends also support targeting Google APIs. The following diagram shows an internal Application Load Balancer that targets a Cloud Storage bucket and terminates traffic by using a customer-owned domain. Figure 5.Private Service Connect lets you send traffic to backends that forward the traffic to a regional Google"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-4",
    "content": "API. APrivate Service Connect interfaceis a special type ofnetwork interfacethat refers to anetwork attachment. A service producer can create a Private Service Connect interface and request a connection to a network attachment. If the service consumer accepts the connection, Google Cloud allocates the interface an IP address from a subnet in the consumer VPC network that's specified by the network attachment. The VM of the Private Service Connect interface has a second standard network interface that connects to the producer's VPC network. A connection between a Private Service Connect interface and a network attachment is similar to the connection between a Private Service Connectendpointand aservice attachment, but it has two key differences: A Private Service Connect interface lets a producer VPC network initiate connections to a consumer VPC network (managed service egress). An endpoint works in the reverse direction, letting a consumer VPC network initiate connections to a producer VPC network (managed service ingress). A Private Service Connect interface connection is transitive. This means that workloads in a producer network can initiate connections to other workloads that areconnected to the consumer VPC network. Private Service Connect endpoints can only initiate connections to the producer VPC network. Figure 6.Private Service Connect interfaces let service producers initiate connections to service consumers. Managed services are services that are owned and managed by someone other than the service consumer. Private Service Connect can be used to access managed services that are owned by Google, third-party software as a service (SaaS) companies, or other teams within the consumer's own company. Both published services and Google APIs can be targets of"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-5",
    "content": "Private Service Connect. Published servicesare VPC-hosted services that are deployed in the producer's VPC network and are accessed from the consumer's VPC network. Publishing a service lets the service producer own and control the deployment of the service in their own VPC network. Published services can include the following: Google services, such as GKE, Apigee, or Cloud Composer. These services run in tenant projects and VPC networks that are managed by Google. Third-party services, where third parties offer private access to a published service in Google Cloud. Intra-organization services, where a single company has clients accessing internal applications across different VPC networks. Some organizations use separate VPC networks for internal segmentation. Given that configuration, one team can offer a managed service to a different team that operates in a separate VPC network. Service attachmentsare resources that are used to create Private Service Connect published services. Service attachments can be accessed by usingendpointsorbackends. Multiple backends or endpoints can connect to the same service attachment, which lets multiple VPC networks or multiple consumers access the same service instance. A service attachment targets a producer load balancer and lets clients in a consumer VPC network access the load balancer. The service attachment configuration defines the following: A consumer accept list that defines which consumers are allowed to connect to the service. TheNAT subnetwhere translated traffic is sourced from in the producer VPC network. An optionalDNS domain, if provided, that is used in theDNS entries for endpointsthat are automatically created in the consumer's Cloud DNS zone. Using Private Service Connect to access Google APIs is an alternative to"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/private-service-connect",
    "title": "Private Service Connect",
    "chunk_id": "https://cloud.google.com/vpc/docs/private-service-connect#chunk-6",
    "content": "using Private Google Access or the public domain names for Google APIs. In this case, the producer is Google. Google APIs can be accessed by using endpoints or backends. Endpoints let you target abundle of global Google APIs, or asingle regional Google API. Backends let you target asingle global Google APIorsingle regional Google API. Using Private Service Connect lets you do the following: Create one or more internal IP addresses to access Google APIs for different use cases. Direct on-premises traffic to specific IP addresses and regions when accessing Google APIs. Centralize Google API traffic through asupported load balancerto apply your own certificates, security policies, or observability. Learn aboutaccessing published services through endpoints. Learn aboutaccessing Google APIs through endpoints. Learn aboutbackends. Learn aboutpublishing services. Complete a codelab touse Private Service Connect to publish and consume services with GKE. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design",
    "title": "Introduction to promptingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation To see an example of prompt design, run the \"Intro to prompt design\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub This page introduces some basic concepts to get you started in designing prompts. A prompt is a natural language request submitted to a language model to receive a response back. Prompts can contain questions, instructions, contextual information, few-shot examples, and partial input for the model to complete or continue. After the model receives a prompt, depending on the type of model being used, it can generate text, embeddings, code, images, videos, music, and more. Prompt designis the process of creating prompts that elicit the desired response from language models. Writing well structured prompts can be an essential part of ensuring accurate, high quality responses from a language model. The iterative process of repeatedly updating prompts and assessing the model's responses is sometimes calledprompt engineering. Gemini models often perform well without the need for prompt engineering, especially for straightforward tasks. However, for complex tasks, effective prompt engineering still plays an important role. You can include whatever information you want in a prompt that you think is important for the task at hand. Generally, prompt content fall within one of the following components: Task (required) System instructions (optional) Few-shot examples (optional) Contextual information (optional) A task is the text in the prompt that you want the model to provide a response for. Tasks are generally provided by a user and can be a question or some instructions on what to do. Example question task:"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design",
    "title": "Introduction to promptingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#chunk-1",
    "content": "Example instruction task: System instructions are instructions that get passed to the model before any user input in the prompt. You can add system instructions in the dedicatedsystemInstructionparameter. In the following example, system instructions are used to dictate the style and tone of the model, while adding constraints to what it can and can't talk about: Few-shot examples are examples that you include in a prompt to show the model what getting it right looks like. Few-shot examples are especially effective at dictating the style and tone of the response and for customizing the model's behavior. Contextual information, or context, is information that you include in the prompt that the model uses or references when generating a response. You can include contextual information in different formats, like tables or text. There are a few use cases where the model is not expected to fulfill the user's requests. Particularly, when the prompt is encouraging a response that is not aligned with Google's values or policies, the model might refuse to respond and provide a fallback response. Here are a few cases where the model is likely to refuse to respond: Hate Speech:Prompts with negative or harmful content targeting identity and/or protected attributes. Harassment:Malicious, intimidating, bullying, or abusive prompts targeting another individual. Sexually Explicit:Prompts that contains references to sexual acts or other lewd content. Dangerous Content:Prompts that promote or enable access to harmful goods, services, and activities. To learn about task-specific guidance for common use cases check out the following pages: Multimodal prompts Overview of prompting strategies Chat prompts Image generation and editing prompts Learn aboutprompting strategies. Explore more"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design",
    "title": "Introduction to promptingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#chunk-2",
    "content": "examples of prompts in thePrompt gallery. Learn how to optimize prompts for use withGoogle modelsby using theVertex AI prompt optimizer (Preview). Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart",
    "title": "Quickstart: Send text prompts to Gemini using Vertex AI StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation You can use Vertex AI Studio to design, test, and manage prompts for Google'sGeminilarge language models (LLMs) and third-party models. Vertex AI Studio supports certain third-party models that are offered on Vertex AI asmodels as a service (MaaS), such as Anthropic's Claude models and Meta's Llama models. In this quickstart, you: Send these prompts to the Gemini API using samples from the generative AI prompt gallery, including the following:A summarization text promptA code generation prompt A summarization text prompt A code generation prompt View the code used to generate the responses This quickstart requires you to complete the following steps to set up a Google Cloud project and enable the Vertex AI API. Sign in to your Google Cloud account. If you're new to Google Cloud,create an accountto evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads. In the Google Cloud console, on the project selector page, select or create a Google Cloud project.Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.Go to project selector In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API.Enable the API Enable the Vertex AI API. Enable the API In the Google Cloud console, on the project selector page, select or create a Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart",
    "title": "Quickstart: Send text prompts to Gemini using Vertex AI StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart#chunk-1",
    "content": "project.Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.Go to project selector In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API.Enable the API Enable the Vertex AI API. Enable the API A prompt is a natural language request submitted to a language model that generates a response. Prompts can contain questions, instructions, contextual information,few-shot examples, and partial input for the model to complete. After the model receives a prompt, depending on the type of model used, it can generate text, embeddings, code, images, videos, music, and more. The sample prompts in Vertex AI Studioprompt galleryare predesigned to help demonstrate model capabilities. Each prompt is preconfigured with specified model and parameter values so you can open the sample prompt and clickSubmitto generate a response. Send a summarization text prompt to the Vertex AI Gemini API. A summarization task extracts the most important information from text. You can provide information in the prompt to help the model create a summary, or ask the model to create a summary on its own. Go to thePrompt gallerypage from the Vertex AI section in the Google Cloud console.Go to prompt gallery Go to thePrompt gallerypage from the Vertex AI section in the Google Cloud console.Go to prompt gallery In theTasksdrop-down menu, selectSummarize. In theTasksdrop-down menu, selectSummarize. Open"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart",
    "title": "Quickstart: Send text prompts to Gemini using Vertex AI StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart#chunk-2",
    "content": "theAudio summarizationcard.This sample prompt includes an audio file and requests a summary of the file contents in a bulleted list. Open theAudio summarizationcard. This sample prompt includes an audio file and requests a summary of the file contents in a bulleted list. Notice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model by clickingSwitch model. Notice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model by clickingSwitch model. ClickSubmitto generate the summary.The output is displayed in the response. ClickSubmitto generate the summary. The output is displayed in the response. To view the Vertex AI API code used to generate the transcript summary, clickBuild with code>Get code.In theGet codepanel, you can choose your preferred language to get the sample code for the prompt, or you can open the Python code in a Colab Enterprise notebook. To view the Vertex AI API code used to generate the transcript summary, clickBuild with code>Get code. In theGet codepanel, you can choose your preferred language to get the sample code for the prompt, or you can open the Python code in a Colab Enterprise notebook. Send a code generation prompt to the Vertex AI Gemini API. A code generation task generates code using a natural language description. Go to thePrompt gallerypage from the Vertex AI section in the Google Cloud console.Go to prompt gallery Go to thePrompt gallerypage from the Vertex AI section in the Google Cloud console.Go to prompt gallery In theTasksdrop-down menu, selectCode. In theTasksdrop-down menu, selectCode. Open theGenerate code from commentscard.This sample prompt includes asystem instructionthat tells the model"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart",
    "title": "Quickstart: Send text prompts to Gemini using Vertex AI StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart#chunk-3",
    "content": "how to respond and some incomplete Java methods. Open theGenerate code from commentscard. This sample prompt includes asystem instructionthat tells the model how to respond and some incomplete Java methods. Notice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model by clickingSwitch model. Notice that in the settings panel, the model's default value is set toGemini-2.0-flash-001. You can choose a different Gemini model by clickingSwitch model. To complete each method by generating code in the areas marked<WRITE CODE HERE>, clickSubmit.The output is displayed in the response. To complete each method by generating code in the areas marked<WRITE CODE HERE>, clickSubmit. The output is displayed in the response. To view the Vertex AI API code used to generate the transcript summary, clickBuild with code>Get code.In theGet codepanel, you can choose your preferred language to get the sample code for the prompt, or you can open the Python code in a Colab Enterprise notebook. To view the Vertex AI API code used to generate the transcript summary, clickBuild with code>Get code. In theGet codepanel, you can choose your preferred language to get the sample code for the prompt, or you can open the Python code in a Colab Enterprise notebook. See anintroduction to prompt design. Learn aboutdesigning multimodal promptsandchat prompts. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-15 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/#chunk-0",
    "content": "Home Documentation Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/azure",
    "title": "GKE on Azure documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/azure#chunk-0",
    "content": "Home Google Kubernetes Engine (GKE) GKE Enterprise Clusters Documentation GKE on Azure GKE on Azure lets you manage GKE clusters running on Azure infrastructure through theGKE Multi-Cloud API. Combined withConnect, GKE on Azure lets you manage GKE clusters on both Google Cloud and Azure from the Google Cloud console. When you create a cluster with GKE on Azure, Google creates the Azure resources you need and brings up a cluster on your behalf. You can then deploy your workloads with thegcloudandkubectlcommand-line tools. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Quickstart: Deploy an application on GKE on Azure Quickstart: Deploy an application on GKE on Azure Prerequisites Prerequisites Create a cluster Create a cluster Architecture Architecture Supported VM sizes Supported VM sizes Supported regions Supported regions gcloud container azure commands gcloud container azure commands Get support Get support Release notes Release notes Quotas and limits Quotas and limits Pricing Pricing Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-0",
    "content": "Home Vertex AI Documentation This page provides an overview of Vertex AI Model Monitoring. Vertex AI Model Monitoring lets you run monitoring jobs as needed or on a regular schedule to track the quality of your tabular models. If you've set alerts, Vertex AI Model Monitoring informs you when metrics surpass a specified threshold. For example, assume that you have a model that predicts customer lifetime value. As customer habits change, the factors that predict customer spending also change. Consequently, the features and feature values that you used to train your model before might not be relevant for making predictions today. This deviation in the data is known as drift. Vertex AI Model Monitoring can track and alert you when deviations exceed a specified threshold. You can then re-evaluate or retrain your model to ensure the model is behaving as intended. For example, Vertex AI Model Monitoring can provide visualizations like in the following figure, which overlays two graphs from two datasets. This visualization lets you quickly compare and see deviations between the two sets of data. Vertex AI Model Monitoring provides two offerings: v2 and v1. Model Monitoring v2 is inPreviewand is the latest offering that associates all monitoring tasks with a model version. In contrast, Model Monitoring v1 is Generally Available and is configured on Vertex AI endpoints. If you need production-level support and want to monitor a model that's deployed on a Vertex AI endpoint, use Model Monitoring v1. For all other use cases, use Model Monitoring v2, which provides all the capabilities of Model Monitoring v1 and more. For more information, see the overview for each version: Model Monitoring v2 overview Model Monitoring v1 overview For existing Model Monitoring v1 users, Model"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-1",
    "content": "Monitoring v1 is maintained as is. You aren't required to migrate to Model Monitoring v2. If you want to migrate, you can use both versions concurrently until you have fully migrated to Model Monitoring v2 to help you avoid monitoring gaps during your transition. Preview This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. Model Monitoring v2 lets you track metrics over time after you configure a model monitor and run monitoring jobs. You can run on-demand monitoring jobs or set up scheduled runs. By using scheduled runs, Model Monitoring automatically runs monitoring jobs based on a schedule that you define. The metrics and thresholds you monitor are mapped tomonitoring objectives. For each model version, you can specify one or more monitoring objectives. The following table details each objective: Measures the distribution of input feature values compared to a baseline data distribution. L-Infinity Jensen Shannon Divergence Measures the model's predictions data distribution compared to a baseline data distribution. L-Infinity Jensen Shannon Divergence Measures the change in contribution of features to a model's prediction compared to a baseline. For example, you can track if a highly important feature suddenly drops in importance. After a model is deployed in production, the input data can deviate from the data that was used to train the model or the distribution of feature data in production could shift significantly over time. Model Monitoring v2 can monitor changes in the distribution of production data compared to the training data or to track the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-2",
    "content": "evolution of production data distribution over time. Similarly, for prediction data, Model Monitoring v2 can monitor changes in the distribution of predicted outcomes compared to the training data or production data distribution over time. Feature attributions indicate how much each feature in your model contributed to the predictions for each given instance. Attribution scores are proportional to the contribution of the feature to a model's prediction. They are typically signed, indicating whether a feature helps push the prediction up or down. Attributions across all features must add up to the model's prediction score. By monitoring feature attributions, Model Monitoring v2 tracks changes in a feature's contributions to a model's predictions over time. A change in a key feature's attribution score often signals that the feature has changed in a way that can impact the accuracy of the model's predictions. For more information about feature attributions and metrics, seeFeature-based explanationsandSampled Shapley method. You must first register your models in Vertex AI Model Registry. If you are serving models outside of Vertex AI, you don't need to upload the model artifact. You then create a model monitor, which you associate with a model version, and define your model schema. For some models, such as AutoML models, the schema is provided for you. In the model monitor, you can optionally specify default configurations such as monitoring objectives, a training dataset, monitoring output location, and notification settings. For more information, seeSet up model monitoring. After you create a model monitor, you can run a monitoring job on demand or schedule regular jobs for continuous monitoring. When you run a job, Model Monitoring uses the default configuration set in"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-3",
    "content": "the model monitor unless you provide a different monitoring configuration. For example, if you provide different monitoring objectives or a different comparison dataset, Model Monitoring uses the job's configurations instead of the default configuration from the model monitor. For more information, seeRun a monitoring job. You are not charged for Model Monitoring v2 during thePreview. You are still charged for the usage of other services, such as Cloud Storage, BigQuery, Vertex AI batch predictions, Vertex Explainable AI, and Cloud Logging. The following tutorials demonstrate how to use the Vertex AI SDK for Python to set up Model Monitoring v2 for your model. To learn more, run the \"Model Monitoring for Vertex AI Custom Model Batch Prediction Job\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To learn more, run the \"Model Monitoring for Vertex AI custom model online prediction\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To learn more, run the \"Model Monitoring for Models Outside Vertex AI\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub To help you maintain a model's performance, Model Monitoring v1 monitors the model's prediction input data for featureskewanddrift: Training-serving skewoccurs when the feature data distribution in production deviates from the feature data distribution used to train the model. If the original training data is available, you can enable skew detection to monitor your models for training-serving skew."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-4",
    "content": "Training-serving skewoccurs when the feature data distribution in production deviates from the feature data distribution used to train the model. If the original training data is available, you can enable skew detection to monitor your models for training-serving skew. Prediction driftoccurs when feature data distribution in production changes significantly over time. If the original training data isn't available, you can enable drift detection to monitor the input data for changes over time. Prediction driftoccurs when feature data distribution in production changes significantly over time. If the original training data isn't available, you can enable drift detection to monitor the input data for changes over time. You can enable both skew and drift detection. Model Monitoring v1 supports feature skew and drift detection forcategoricalandnumericalfeatures: Categoricalfeatures are data limited by number of possible values, typically grouped by qualitative properties. For example, categories such as product type, country, or customer type. Categoricalfeatures are data limited by number of possible values, typically grouped by qualitative properties. For example, categories such as product type, country, or customer type. Numericalfeatures are data that can be any numeric value. For example, weight and height. Numericalfeatures are data that can be any numeric value. For example, weight and height. Once the skew or drift for a model's feature exceeds an alerting threshold that you set, Model Monitoring v1 sends you an email alert. You can also view the distributions for each feature over time to evaluate whether you need to retrain your model. To detect drift for v1, Vertex AI Model Monitoring usesTensorFlow Data Validation (TFDV)to calculate the distributions anddistance"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-5",
    "content": "scores. Calculate thebaselinestatistical distribution:For skew detection, the baseline is the statistical distribution of the feature's values in the training data.For drift detection, the baseline is the statistical distribution of the feature's values seen in production in the past.The distributions for categorical and numerical features are calculated as follows:For categorical features, the computed distribution is the number or percentage of instances of each possible value of the feature.For numerical features, Vertex AI Model Monitoring divides the range of possible feature values into equal intervals and computes the number or percentage of feature values that falls in each interval.The baseline is calculated when youcreate a Vertex AI Model Monitoring job, and is only recalculated if you update the training dataset for the job. Calculate thebaselinestatistical distribution: For skew detection, the baseline is the statistical distribution of the feature's values in the training data. For skew detection, the baseline is the statistical distribution of the feature's values in the training data. For drift detection, the baseline is the statistical distribution of the feature's values seen in production in the past. For drift detection, the baseline is the statistical distribution of the feature's values seen in production in the past. The distributions for categorical and numerical features are calculated as follows: For categorical features, the computed distribution is the number or percentage of instances of each possible value of the feature. For categorical features, the computed distribution is the number or percentage of instances of each possible value of the feature. For numerical features, Vertex AI Model Monitoring divides the range of possible feature"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-6",
    "content": "values into equal intervals and computes the number or percentage of feature values that falls in each interval. For numerical features, Vertex AI Model Monitoring divides the range of possible feature values into equal intervals and computes the number or percentage of feature values that falls in each interval. The baseline is calculated when youcreate a Vertex AI Model Monitoring job, and is only recalculated if you update the training dataset for the job. Calculate the statistical distribution of the latest feature values seen in production. Calculate the statistical distribution of the latest feature values seen in production. Compare the distribution of the latest feature values in production against the baseline distribution by calculating adistance score:For categorical features, the distance score is calculated using theL-infinity distance.For numerical features, the distance score is calculated using theJensen-Shannon divergence. Compare the distribution of the latest feature values in production against the baseline distribution by calculating adistance score: For categorical features, the distance score is calculated using theL-infinity distance. For categorical features, the distance score is calculated using theL-infinity distance. For numerical features, the distance score is calculated using theJensen-Shannon divergence. For numerical features, the distance score is calculated using theJensen-Shannon divergence. When the distance score between two statistical distributions exceeds the threshold you specify, Vertex AI Model Monitoringidentifies the anomaly as skew or drift. When the distance score between two statistical distributions exceeds the threshold you specify, Vertex AI Model Monitoringidentifies the anomaly as skew or drift. The following"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-7",
    "content": "example shows skew or drift between the baseline and latest distributions of a categorical feature: The following example shows skew or drift between the baseline and latest distributions of a numerical feature: For cost efficiency, you can set aprediction request sampling rateto monitor a subset of the production inputs to a model. For cost efficiency, you can set aprediction request sampling rateto monitor a subset of the production inputs to a model. You can set a frequency at which a deployed model's recently logged inputs are monitored for skew or drift. Monitoring frequency determines the timespan, or monitoring window size, of logged data that is analyzed in each monitoring run. You can set a frequency at which a deployed model's recently logged inputs are monitored for skew or drift. Monitoring frequency determines the timespan, or monitoring window size, of logged data that is analyzed in each monitoring run. You can specify alerting thresholds for each feature you want to monitor. An alert is logged when the statistical distance between the input feature distribution and its corresponding baseline exceeds the specified threshold. By default, every categorical and numerical feature is monitored, with threshold values of 0.3. You can specify alerting thresholds for each feature you want to monitor. An alert is logged when the statistical distance between the input feature distribution and its corresponding baseline exceeds the specified threshold. By default, every categorical and numerical feature is monitored, with threshold values of 0.3. An online prediction endpoint can host multiple models. When you enable skew or drift detection on an endpoint, the following configuration parameters are shared across all models hosted in that endpoint:Type of"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview",
    "title": "Introduction to Vertex AI Model MonitoringStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#chunk-8",
    "content": "detectionMonitoring frequencyFraction of input requests monitoredFor the otherconfiguration parameters, you can set different values for each model. An online prediction endpoint can host multiple models. When you enable skew or drift detection on an endpoint, the following configuration parameters are shared across all models hosted in that endpoint: Type of detection Monitoring frequency Fraction of input requests monitored For the otherconfiguration parameters, you can set different values for each model. Get started with Model Monitoring v2 Provide schema to Model Monitoring v1 Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-registry/introduction",
    "title": "Introduction to Vertex AI Model RegistryStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-registry/introduction#chunk-0",
    "content": "Home Vertex AI Documentation To see an example of getting started with Vertex AI Model Registry, run the \"Get started with Vertex AI Model Registry\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub The Vertex AI Model Registry is a central repository where you can manage the lifecycle of your ML models. From the Model Registry, you have an overview of your models so you can better organize, track, and train new versions. When you have a model version you would like to deploy, you can assign it to an endpoint directly from the registry, or using aliases, deploy models to an endpoint. The Vertex AI Model Registry supports custom models and all AutoML data types - text, tabular, image, and video. The Model Registry can also support BigQuery ML models. If you have models trained in BigQuery ML, you can register them with the Model Registry without needing to export them from BigQuery ML or import them into the Model Registry. From the model version details page you can evaluate, deploy to an endpoint, set up batch prediction, and view specific model details. The Vertex AI Model Registry provides a straightforward and streamlined interface to manage and deploy your best models to production. There are many valid workflows for working in the Model Registry. To get started, you might want to follow these guidelines to understand what you can do in the Model Registry and at what stage in your model-training journey. Import models to the Model Registry. Create new models, assign a model version the default alias, ready for production. Add other aliases, or labels to help you manage and organize your models and model versions. Deploy your models to an endpoint for online"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/model-registry/introduction",
    "title": "Introduction to Vertex AI Model RegistryStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/model-registry/introduction#chunk-1",
    "content": "prediction. Run batch prediction, and start your model evaluation pipeline. View your model details and view performance metrics from the model details page. To learn more about how to integrate your BigQuery ML models with Vertex AI, see theBigQuery ML documentation. Dataplex's Data Catalog service is a fully managed, scalable metadata management service that provides a centralized location to search for models across projects and regions. For details, seeUse Data Catalog to search for model and dataset resources. To get started using Vertex AI Model Registry, seeImport models to Vertex AI Model RegistryVersioning with Vertex AI Model RegistryHow to use aliases with Vertex AI Model RegistryIntegrate a BigQuery ML model with Model RegistryHow to copy a model to a different region from the Vertex AI Model Registry Import models to Vertex AI Model Registry Versioning with Vertex AI Model Registry How to use aliases with Vertex AI Model Registry Integrate a BigQuery ML model with Model Registry How to copy a model to a different region from the Vertex AI Model Registry Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/start/explore-models",
    "title": "Overview of Model GardenStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/start/explore-models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Model Garden is an AI/ML model library that helps you discover, test, customize, and deploy models and assets from Google and Google partners. When you're working with AI models, Model Garden provides the following advantages: Available models are all grouped in a single location Model Garden provides a consistent deployment pattern for different types of models Model Garden provides built-in integration with other parts of Vertex AI such as model tuning, evaluation, and serving Serving generative AI models can be difficult\u2014Vertex AI handles model deployment and serving for you To view the list of available Vertex AI and open source foundation, tunable, and task-specific models, go to the Model Garden page in the Google Cloud console. Go to Model Garden The model categories available in Model Garden are: To filter models in the filter pane, specify the following: Modalities: Click the modalities (data types) that you want in the model. Tasks: Click the task that you want the model to perform. Features: Click the features that you want in the model. Provider: Click the provider of the model. To learn more about each model, click its model card. For a list of models available in Model Garden, seeModels available in Model Garden. Google does thorough testing and benchmarking on the serving and tuning containers that we provide. Active vulnerability scanning is also applied to container artifacts. Third-party models from featured partners undergo model checkpoint scans to ensure authenticity. Third-party models from HuggingFace Hub are scanned directly by HuggingFace and theirthird-party scannerfor malware, pickle files, Keras Lambda layers, and secrets. Models deemed unsafe from these scans are flagged by HuggingFace and"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/start/explore-models",
    "title": "Overview of Model GardenStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/start/explore-models#chunk-1",
    "content": "blocked from deployment in Model Garden. Models deemed suspicious or those that have the ability to potentially execute remote code are indicated in Model Garden but can still be deployed. We recommend you perform a thorough review of any suspicious model before deploying it within Model Garden. For the open source models in Model Garden, you are charged for use of following on Vertex AI: Model tuning: You are charged for the compute resources used at the same rate as custom training. Seecustom training pricing. Model deployment: You are charged for the compute resources used to deploy the model to an endpoint. Seepredictions pricing. Colab Enterprise: SeeColab Enterprise pricing. You can set aModel Garden organization policyat the organization, folder, or project level to control access to specific models in Model Garden. For example, you can allow access to specific models that you've vetted and deny access to all others. For more information about the deployment options and customizations that you can do with models in Model Garden, view the resources in the following sections, which include links to tutorials, references, notebooks, and YouTube videos. Learn more about customizing deployments and advance serving features. Deploy and serve open source model using Python SDK, CLI, REST API, or consoleDeveloper blog: Introducing the new Vertex AI Model Garden CLI and SDKDeploy open models by using the SDK tutorial notebookGet started with Vertex AI Model Garden SDK notebook Developer blog: Introducing the new Vertex AI Model Garden CLI and SDK Deploy open models by using the SDK tutorial notebook Get started with Vertex AI Model Garden SDK notebook Deploying and fine-tuning Gemma 3 in Model Garden YouTube video Deploying Gemma and making predictions Serve open models"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/start/explore-models",
    "title": "Overview of Model GardenStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/start/explore-models#chunk-2",
    "content": "with a Hex-LLM container on Cloud TPUs Use prefix caching and speculative decoding with Hex-LLM or vLLM tutorial notebook Use vLLM to serve text-only and multimodel language models on Cloud GPUsText-only models tutorial notebookMultimodal models tutorial notebook Text-only models tutorial notebook Multimodal models tutorial notebook Use xDiT GPU serving container for image and video generation Serving Gemma 2 with multiple LoRA adapters with HuggingFace DLC for PyTorch inference tutorial on Medium Use custom handles to serve PaliGemma for image captioning with HuggingFace DLC for PyTorch inference tutorial on LinkedIn Deploy and serve a model that uses Spot VMs or a Compute Engine reservation tutorial notebook Deploy and serve a HuggingFace model Learn more about tuning models to tailor responses for specific use cases. Workbench fine-tuning tutorial notebook Fine-tuning and evaluation tutorial notebook Deploying and fine-tuning Gemma 3 in Model Garden YouTube video Learn more about assessing model responses with Vertex AI Evaluate Gemma 2 with the generative AI evaluation service YouTube video Model and user journey-specific Model Garden notebooks Vertex AI open model serving, fine-tuning and evaluation notebooks Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/firewall/docs/firewall-rules-logging#chunk-0",
    "content": "Home Cloud NGFW Documentation Guides Firewall Rules Logging lets you audit, verify, and analyze the effects of your firewall rules. For example, you can determine if a firewall rule designed to deny traffic is functioning as intended. Firewall Rules Logging is also useful if you need to determine how many connections are affected by a given firewall rule. You enable Firewall Rules Logging individually for each firewall rule whose connections you need to log. Firewall Rules Logging is an option for any firewall rule, regardless of the action (allowordeny) or direction (ingress or egress) of the rule. Firewall Rules Logging logs traffic to and fromCompute Engine virtual machine (VM) instances. This includes Google Cloud products built on Compute Engine VMs, such asGoogle Kubernetes Engine (GKE) clustersandApp Engine flexible environmentinstances. When you enable logging for a firewall rule, Google Cloud creates an entry called aconnection recordeach time the rule allows or denies traffic. You can view these records inCloud Logging, and you can export logs to any destination that Cloud Logging export supports. Each connection record contains the source and destination IP addresses, the protocol and ports, date and time, and a reference to the firewall rule that applied to the traffic. Firewall Rules Logging is available for both VPC firewall rules and hierarchical firewall policies. For information about viewing logs, seeUse Firewall Rules Logging. Firewall Rules Logging has the following specifications: You can only enable Firewall Rules Logging for rules in aVirtual Private Cloud (VPC) network.Legacy networksarenotsupported. Firewall Rules Logging only records TCP and UDP connections. Although you cancreate a firewall rule applicable to other protocols, you cannot log"
  },
  {
    "source_url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/firewall/docs/firewall-rules-logging#chunk-1",
    "content": "their connections. If you want to also log other protocols, consider usingPacket Mirroring. Youcannotenable Firewall Rules Logging for theimplied deny ingress and implied allow egress rules. Log entries are written from the perspective of VMs. Log entries are only created if a firewall rule has logging enabled and if the rule applies to traffic sent to or from the VM. Entries are created according to the connection logging limits on abest effortbasis. The number of connections that can belogged in a given intervalis based on the machine type. Changes to firewall rules can be viewed inVPC audit logs. A log entry is generated each time that a firewall rule with logging enabled applies to traffic. A given packet flow can generate more than one log entry in total. However, from the perspective of a given VM, at most only one log entry can be generated if the firewall rule that applies to it has logging enabled. The following examples demonstrate how firewall logs work. In this example: Traffic between VM instances in theexample-netVPC network in theexample-projproject is considered. The two VM instances are:VM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region).VM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region). VM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region). VM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region). Rule A: An egress deny firewall rule has a target of all instances in the network, a destination of10.20.0.99(VM2), and applies to TCP port 80.Logging is enabled for this rule. Logging is enabled for this rule. Rule B: An ingress allow firewall rule has a target of all instances in the network, a source of10.10.0.99(VM1), and applies to TCP port"
  },
  {
    "source_url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/firewall/docs/firewall-rules-logging#chunk-2",
    "content": "80.Logging is also enabled for this rule. Logging is also enabled for this rule. The followinggcloudcommands can be used to create the firewall rules: Rule A: egress deny rule for TCP 80, applicable to all instances, destination10.20.0.99:gcloud compute firewall-rules create rule-a \\ --network example-net \\ --action deny \\ --direction EGRESS \\ --rules tcp:80 \\ --destination-ranges 10.20.0.99/32 \\ --priority 10 \\ --enable-logging Rule A: egress deny rule for TCP 80, applicable to all instances, destination10.20.0.99: Rule B: ingress allow rule for TCP 80, applicable to all instances, source10.10.0.99:gcloud compute firewall-rules create rule-b \\ --network example-net \\ --action allow \\ --direction INGRESS \\ --rules tcp:80 \\ --source-ranges 10.10.0.99/32 \\ --priority 10 \\ --enable-logging Rule B: ingress allow rule for TCP 80, applicable to all instances, source10.10.0.99: Suppose VM1 attempts to connect to VM2 on TCP port 80. The following firewall rules are logged: A log entry for rule A from the perspective of VM1 is generated as VM1 attempts to connect to10.20.0.99(VM2). Because rule A actually blocks the traffic, rule B is never considered, so there is no log entry for rule B from the perspective of VM2. The firewall log record is generated in the following example. In this example: Traffic between VM instances in theexample-netVPC network in theexample-projproject is considered. The two VM instances are:VM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region).VM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region). VM1 in zoneus-west1-awith IP address10.10.0.99in thewest-subnet(us-west1region). VM2 in zoneus-east1-bwith IP address10.20.0.99in theeast-subnet(us-east1region). Rule A: An egress allow firewall rule has a"
  },
  {
    "source_url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/firewall/docs/firewall-rules-logging#chunk-3",
    "content": "target of all instances in the network, a destination of10.20.0.99(VM2), and applies to TCP port 80.Logging is enabled for this rule. Logging is enabled for this rule. Rule B: An ingress allow firewall rule has a target of all instances in the network, a source of10.10.0.99(VM1), and applies to TCP port 80.Logging is also enabled for this rule. Logging is also enabled for this rule. The followinggcloudcommands can be used to create the two firewall rules: Rule A: egress allow rule for TCP 80, applicable to all instances, destination10.20.0.99(VM2):gcloud compute firewall-rules create rule-a \\ --network example-net \\ --action allow \\ --direction EGRESS \\ --rules tcp:80 \\ --destination-ranges 10.20.0.99/32 \\ --priority 10 \\ --enable-logging Rule A: egress allow rule for TCP 80, applicable to all instances, destination10.20.0.99(VM2): Rule B: ingress allow rule for TCP 80, applicable to all instances, source10.10.0.99(VM1):gcloud compute firewall-rules create rule-b \\ --network example-net \\ --action allow \\ --direction INGRESS \\ --rules tcp:80 \\ --source-ranges 10.10.0.99/32 \\ --priority 10 \\ --enable-logging Rule B: ingress allow rule for TCP 80, applicable to all instances, source10.10.0.99(VM1): Suppose VM1 attempts to connect to VM2 on TCP port 80. The following firewall rules are logged: A log entry for rule A from the perspective of VM1 is generated as VM1 connects to10.20.0.99(VM2). A log entry for rule B from the perspective of VM2 is generated as VM2 allows incoming connections from10.10.0.99(VM1). The firewall log record reported by VM1 is generated in the following example. The firewall log record reported by VM2 is generated in the following example. In this example: Traffic from a system outside theexample-netVPC network to a VM instance in that network is"
  },
  {
    "source_url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/firewall/docs/firewall-rules-logging#chunk-4",
    "content": "considered. The network is in theexample-projproject. The system on the internet has IP address203.0.113.114. VM1 in zoneus-west1-ahas IP address10.10.0.99in thewest-subnet(us-west1region). Rule C: An ingress allow firewall rule has a target of all instances in the network, a source of any IP address (0.0.0.0/0), and applies to TCP port 80.Logging is enabled for this rule. Logging is enabled for this rule. Rule D: An egress deny firewall rule has a target of all instances in the network, a destination of any IP address (0.0.0.0/0), and applies to all protocols.Logging is also enabled for this rule. Logging is also enabled for this rule. The followinggcloudcommands can be used to create the firewall rules: Rule C: ingress allow rule for TCP 80, applicable to all instances, any source:gcloud compute firewall-rules create rule-c \\ --network example-net \\ --action allow \\ --direction INGRESS \\ --rules tcp:80 \\ --source-ranges 0.0.0.0/0 \\ --priority 10 \\ --enable-logging Rule C: ingress allow rule for TCP 80, applicable to all instances, any source: Rule D: egress deny rule for all protocols, applicable to all instances, any destination:gcloud compute firewall-rules create rule-d \\ --network example-net \\ --action deny \\ --direction EGRESS \\ --rules all \\ --destination-ranges 0.0.0.0/0 \\ --priority 10 \\ --enable-logging Rule D: egress deny rule for all protocols, applicable to all instances, any destination: Suppose the system with IP address203.0.113.114attempts to connect to VM1 on TCP port 80. The following happens: A log entry for rule C from the perspective of VM1 is generated as VM1 accepts traffic from203.0.113.114. Despite rule D, VM1 is allowed to reply to the incoming request because Google Cloud firewall rules are stateful. If the incoming request is allowed,"
  },
  {
    "source_url": "https://cloud.google.com/firewall/docs/firewall-rules-logging",
    "title": "Firewall Rules LoggingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/firewall/docs/firewall-rules-logging#chunk-5",
    "content": "established responses cannot be blocked by any kind of egress rule. Because rule D does not apply, it is never considered, so there is no log entry for rule D. The firewall log record is generated in the following example. Subject to thespecifications, a log entry is created in Cloud Logging for each firewall rule that has logging enabled if that rule applies to traffic to or from a VM instance. Log records are included in the JSON payload field of a LoggingLogEntry. Log records contain base fields, which are the core fields of every log record, and metadata fields that add additional information. You can control whether metadata fields are included. If you omit them, you can save on storage costs. Some log fields support values that are also fields. These fields can have more than one piece of data in a given field. For example, theconnectionfield is of theIpConnectionformat, which contains the source and destination IP address and port, plus the protocol, in a single field. These fields are described in the following tables. To set up logging and view logs, seeUse Firewall Rules Logging. To get insights about how your firewall rules are being used, seeFirewall Insights. To store, search, analyze, monitor, and alert on log data and events, seeCloud Logging. To route log entries, seeConfigure and manage sinks. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-24 UTC."
  },
  {
    "source_url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/healthcare-api/docs/concepts/nlp#chunk-0",
    "content": "Home Cloud Healthcare API Documentation Guides The Healthcare Natural Language API is a part of the Cloud Healthcare API that uses natural language models to extract healthcare information from medical text. This conceptual guide explains the basics of using the Healthcare Natural Language API, including: The types of requests you can make to the Healthcare Natural Language API How to construct requests to the Healthcare Natural Language API How to handle responses from the Healthcare Natural Language API The Healthcare Natural Language API extracts healthcare information from medical text. This healthcare information can include: Medical concepts, such as medications, procedures, and medical conditions Functional features, such as temporal relationships, subjects, and certainty assessments Relations, such as side effects and medication dosage The Healthcare Natural Language API offers pre-trained natural language models to extract medical concepts and relationships from medical text. The Healthcare Natural Language API maps text into a predefined set ofmedical knowledge categories. AutoML Entity Extraction for Healthcare lets you create a custom entity extraction model trained using your own annotated medical text and using your own categories. For more information, see theAutoML Entity Extraction for Healthcare documentation. The Healthcare Natural Language API is available in the following locations: The Healthcare Natural Language API inspects medical text for medical concepts and relations. You perform entity analysis using theanalyzeEntitiesmethod. The Healthcare Natural Language API is a REST API and consists of JSON requests and responses. The following sections show how to extract different medical insights from a given medical text: Extract entities,"
  },
  {
    "source_url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/healthcare-api/docs/concepts/nlp#chunk-1",
    "content": "relations, and contextual attributes Include licensed vocabularies Extract output as a FHIR R4 bundle The entity analysis request contains the following fields: documentContent: The data for the request, which consists of medical text. The maximum size of the medical text is 20,000 Unicode characters. licensedVocabularies[]: Optional. TheSNOMED CTvocabulary. Available only for US users. alternativeOutputFormat: Optional. The FHIR bundle format. The entity analysis returns a set of detected medical knowledge mentions, medical concepts, and relations between medical knowledge mentions, including the following: entityMentions: occurrences of medical knowledge entities in the source medical text. Each entity mention has the following fields:mentionId: a unique identifier for an entity mention in the response.type: themedical knowledge categoryof the entity mention.text: consists of thetextContentfield, and describes the excerpt of the medical text containing the entity mention, andoffset, the location of the entity mention in the source medical text.temporalAssessment: specifies how the linked entity relates to the entity mention, one ofCURRENT,CLINICAL_HISTORY,FAMILY_HISTORY,UPCOMING, orOTHER.certaintyAssessment: the negation or qualification of the medical concept, one ofLIKELY,SOMEWHAT_LIKELY,UNCERTAIN,SOMEWHAT_UNLIKELY,UNLIKELY, orCONDITIONAL.subject: specifies the subject that the medical concept relates to, one ofPATIENT,FAMILY_MEMBER, orOTHER.linkedEntities: a list of medical concepts that might be related to this entity mention. Linked entities specify theentityId, which links a medical concept to an an entity inentities. entityMentions: occurrences of medical knowledge entities in the source medical text. Each entity mention has the following fields: mentionId: a"
  },
  {
    "source_url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/healthcare-api/docs/concepts/nlp#chunk-2",
    "content": "unique identifier for an entity mention in the response. type: themedical knowledge categoryof the entity mention. text: consists of thetextContentfield, and describes the excerpt of the medical text containing the entity mention, andoffset, the location of the entity mention in the source medical text. temporalAssessment: specifies how the linked entity relates to the entity mention, one ofCURRENT,CLINICAL_HISTORY,FAMILY_HISTORY,UPCOMING, orOTHER. certaintyAssessment: the negation or qualification of the medical concept, one ofLIKELY,SOMEWHAT_LIKELY,UNCERTAIN,SOMEWHAT_UNLIKELY,UNLIKELY, orCONDITIONAL. subject: specifies the subject that the medical concept relates to, one ofPATIENT,FAMILY_MEMBER, orOTHER. linkedEntities: a list of medical concepts that might be related to this entity mention. Linked entities specify theentityId, which links a medical concept to an an entity inentities. entities: describes the medical concepts from the linked entities fields. Each entity is described using the following fields:entityId: a unique identifier from thelinkedEntitiesfield.preferredTerm: a preferred term for the medical concept.vocabularyCodes: the representation of the medical concept in supportedmedical vocabularies. entities: describes the medical concepts from the linked entities fields. Each entity is described using the following fields: entityId: a unique identifier from thelinkedEntitiesfield. preferredTerm: a preferred term for the medical concept. vocabularyCodes: the representation of the medical concept in supportedmedical vocabularies. relationships: define directed relationships between entity mentions. In the sample, the subject of the relationship is \"Insulin regimen human\" and the object of the relationship is \"5 units\". relationships: define directed"
  },
  {
    "source_url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/healthcare-api/docs/concepts/nlp#chunk-3",
    "content": "relationships between entity mentions. In the sample, the subject of the relationship is \"Insulin regimen human\" and the object of the relationship is \"5 units\". confidence: an indication of the model's confidence in the relationship as a number between 0 and 1. confidence: an indication of the model's confidence in the relationship as a number between 0 and 1. Apart from the listed fields, the response might also contain theadditionalInfofield, which states any additional description about the entity mention type. SeeAdditional information. The Healthcare Natural Language API only supports extracting healthcare information from English text. The Healthcare Natural Language API supports the following medical vocabularies: Foundational Model of Anatomy Gene Ontology HUGO Gene Nomenclature Committee Human Phenotype Ontology ICD-10 Procedure Coding System ICD-10-CM ICD-9-CM LOINC MeSH MedlinePlus Health Topics Metathesaurus Names NCBI Taxonomy NCI Thesaurus National Drug File Online Mendelian Inheritance in Man RXNORM SNOMED CT (available for US users only) The Healthcare Natural Language API assigns a medical knowledge category to theentityMentions.typefield. A list of supported medical knowledge categories is as follows. The entity mention types that belong to the oncology, social determinants of health (SDOH), and protected health information (PHI) groups are only available inPreview: Note: HIPAA classifies the age of a person as PHI only when it's above 90. For more information, seeSummary of the HIPAA Privacy Rule. The Healthcare Natural Language API can infer functional features, or attributes, of an entity mention from context. For example, in the statement \"Kusuma's mother has diabetes\", the condition \"diabetes\" has the functional feature ofsubjectFAMILY_MEMBER."
  },
  {
    "source_url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/healthcare-api/docs/concepts/nlp#chunk-4",
    "content": "Temporal relationships, returned in thetemporalAssessmentfield, describe how this entity mention relates to the subject temporally. The Healthcare Natural Language API supports the following temporal relationships: CURRENT CLINICAL_HISTORY FAMILY_HISTORY UPCOMING OTHER Subjects, returned in thesubjectfield, describe the individual the entity mention relates to. The Healthcare Natural Language API supports the following subjects: PATIENT FAMILY_MEMBER OTHER Certainty assessments, returned in thecertaintyAssessmentfield, describe the original note taker's confidence. For example, if the original note contains \"The patient has a sore throat\", the certainty assessment returns aLIKELYvalue to indicate the note taker's confidence that it was likely that the patient had a sore throat. If the original note contains \"The patient does not have a sore throat\", the certainty assessment returns anUNLIKELYvalue to indicate the note taker's confidence that it was unlikely that the patient had a sore throat. Certainty assessments can be one of the following values: LIKELY SOMEWHAT_LIKELY UNCERTAIN SOMEWHAT_UNLIKELY UNLIKELY CONDITIONAL TheadditionalInfofield provides additional details about an entity mention. For example, theadditionalInfofield for aDATEentity mention might consist of details about the type of the date, categorized as one of the following: ADMISSION_DATE CONSULTATION_DATE DISCHARGE_DATE SERVICE_DATE VISIT_DATE DIAGNOSIS_DATE MED_STARTED_DATE MED_ENDED_DATE NOTE_DATE PROCEDURE_DATE RADIATION_STARTED_DATE RADIATION_ENDED_DATE STAGE_DATE The Healthcare Natural Language API can infer relationships between entity mentions based on the surrounding medical text. In the response, the subject of the relationship is identified bysubjectIdand the object of the relationship is"
  },
  {
    "source_url": "https://cloud.google.com/healthcare-api/docs/concepts/nlp",
    "title": "Healthcare Natural Language APIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/healthcare-api/docs/concepts/nlp#chunk-5",
    "content": "identified byobjectId. The Healthcare Natural Language API supports the following relationships between entity mentions: When you request theanalyzeEntitiesmethod with thealternativeOutputFormatfield set toFHIR_BUNDLE, the response includes the following JSON objects: The entity mentions, the entities, and the relationships A FHIR R4 bundle represented as a string, that includes all the entities, the entity mentions, and the relationships in JSON format To create the FHIR R4 bundle, the Healthcare Natural Language API maps the entity mentions, entities, and relationships to FHIR resources and their elements. The following table lists some of these mappings. To extract entities from text as a FHIR R4 bundle, seeExtract output as a FHIR R4 bundle. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-28 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/quotas",
    "title": "Cloud Quotas documentation",
    "chunk_id": "https://cloud.google.com/docs/quotas#chunk-0",
    "content": "Home Documentation Cloud Quotas Cloud Quotas enables customers to manage quotas for all of their Google Cloud services. With Cloud Quotas, users are able to easily monitor quota usage, create and modify quota alerts, and request limit adjustments for quotas. Quotas are managed through the Cloud Quotas dashboard or the Cloud Quotas API.Learn more. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Overview Overview Understand quotas Understand quotas Predefined roles and permissions Predefined roles and permissions REST API REST API Client libraries Client libraries Release notes Release notes Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-0",
    "content": "Home Documentation AI/ML orchestration on GKE Run optimized AI/ML workloads withGoogle Kubernetes Engine (GKE)platform orchestration capabilities. With Google Kubernetes Engine (GKE), you can implement a robust, production-ready AI/ML platform with all the benefits of managed Kubernetes and these capabilities: Infrastructure orchestration that supports GPUs and TPUs for training and serving workloads at scale. Flexible integration with distributed computing and data processing frameworks. Support for multiple teams on the same infrastructure to maximize utilization of resources Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. NEW!About model inference on GKE About model inference on GKE NEW!Run best practice inference with GKE Inference Quickstart recipes Run best practice inference with GKE Inference Quickstart recipes NEW!Serve LLMs like Deepseek-R1 671B or Llama 3.1 405B on GKE Serve LLMs like Deepseek-R1 671B or Llama 3.1 405B on GKE TutorialServe Gemma using GPUs on GKE with vLLM Serve Gemma using GPUs on GKE with vLLM TutorialServe an LLM using TPU Trillium on GKE with vLLM Serve an LLM using TPU Trillium on GKE with vLLM TutorialDiscover more tutorials for model inference on GKE Discover more tutorials for model inference on GKE NEW!Quickstart: Deploy GPU-accelerated Ray for AI workloads on GKE Quickstart: Deploy GPU-accelerated Ray for AI workloads on GKE NEW!Optimize GKE resource utilization for mixed AI/ML training and inference workloads Optimize GKE resource utilization for mixed AI/ML training and inference workloads VideoIntroduction to Cloud TPUs for machine"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-1",
    "content": "learning. Introduction to Cloud TPUs for machine learning. VideoBuild large-scale machine learning on Cloud TPUs with GKE Build large-scale machine learning on Cloud TPUs with GKE VideoServing Large Language Models with KubeRay on TPUs Serving Large Language Models with KubeRay on TPUs BlogMachine learning with JAX on Kubernetes with NVIDIA GPUs Machine learning with JAX on Kubernetes with NVIDIA GPUs NEW!Reference architecture for a batch processing platform on GKE Reference architecture for a batch processing platform on GKE Best practiceOptimize GPU obtainability with flex-start provisioning mode Optimize GPU obtainability with flex-start provisioning mode BlogHigh performance AI/ML storage through Local SSD support on GKE High performance AI/ML storage through Local SSD support on GKE BlogSimplifying MLOps using Weights & Biases with Google Kubernetes Engine Simplifying MLOps using Weights & Biases with Google Kubernetes Engine Best practiceBest practices for running batch workloads on GKE Best practices for running batch workloads on GKE Best practiceRun cost-optimized Kubernetes applications on GKE Run cost-optimized Kubernetes applications on GKE Best practiceImproving launch time of Stable Diffusion on GKE by 4x Improving launch time of Stable Diffusion on GKE by 4x Serve open source models using TPUs on GKE with Optimum TPU Learn how to deploy LLMs using Tensor Processing Units (TPUs) on GKE with the Optimum TPU serving framework from Hugging Face.TutorialAI/ML InferenceTPU Create and use a volume backed by a Parallelstore instance in GKE Learn how to create storage backed by fully managed Parallelstore instances, and access them as volumes. The CSI driver is optimized for AI/ML training workloads involving smaller file sizes and random reads.TutorialAI/ML Data"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-2",
    "content": "Loading Accelerate AI/ML data loading with Hyperdisk ML Learn how to how to simplify and accelerate the loading of AI/ML model weights on GKE using Hyperdisk ML.TutorialAI/ML Data Loading Serve an LLM using TPUs on GKE with JetStream and PyTorch Learn how to serve a LLM using Tensor Processing Units (TPUs) on GKE with JetStream through PyTorch.TutorialAI/ML InferenceTPUs Best practices for optimizing LLM inference with GPUs on GKE Learn best practices for optimizing LLM inference performance with GPUs on GKE using the vLLM and Text Generation Inference (TGI) serving frameworks.TutorialAI/ML InferenceGPUs Manage the GPU Stack with the NVIDIA GPU Operator on GKE Learn when to use the NVIDIA GPU operator and how to enable the NVIDIA GPU Operator on GKE.TutorialGPUs Configure autoscaling for LLM workloads on TPUs Learn how to set up your autoscaling infrastructure by using the GKE Horizontal Pod Autoscaler (HPA) to deploy the Gemma LLM using single-host JetStream.TutorialTPUs Fine-tune Gemma open models using multiple GPUs on GKE Learn how to fine-tune Gemma LLM using GPUs on GKE with the Hugging Face Transformers library.TutorialAI/ML InferenceGPUs Deploy a Ray Serve application with a Stable Diffusion model on GKE with TPUs Learn how to deploy and serve a Stable Diffusion model on GKE using TPUs, Ray Serve, and the Ray Operator add-on.TutorialAI/ML InferenceRayTPUs Configure autoscaling for LLM workloads on GPUs with GKE Learn how to set up your autoscaling infrastructure by using the GKE Horizontal Pod Autoscaler (HPA) to deploy the Gemma LLM with the Hugging Face Text Generation Interface (TGI) serving framework.TutorialGPUs Train Llama2 with Megatron-LM on A3 Mega virtual machines Learn how to run a container-based, Megatron-LM PyTorch workload on A3 Mega.TutorialAI/ML"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-3",
    "content": "TrainingGPUs Deploy GPU workloads in Autopilot Learn how to request hardware accelerators (GPUs) in your GKE Autopilot workloads.TutorialGPUs Serve a LLM with multiple GPUs in GKE Learn how to serve Llama 2 70B or Falcon 40B using multiple NVIDIA L4 GPUs with GKE.TutorialAI/ML InferenceGPUs Getting started with Ray on GKE Learn how to easily start using Ray on GKE by running a workload on a Ray cluster.TutorialRay Serve an LLM on L4 GPUs with Ray Learn how to serve Falcon 7b, Llama2 7b, Falcon 40b, or Llama2 70b using the Ray framework in GKE.TutorialAI/ML InferenceRayGPUs Orchestrate TPU Multislice workloads using JobSet and Kueue Learn how to orchestrate a Jax workload on multiple TPU slices on GKE by using JobSet and Kueue.TutorialTPUs Monitoring GPU workloads on GKE with NVIDIA Data Center GPU Manager (DCGM) Learn how to observe GPU workloads on GKE with NVIDIA Data Center GPU Manager (DCGM).TutorialAI/ML ObservabilityGPUs Quickstart: Train a model with GPUs on GKE Standard clusters This quickstart shows you how to deploy a training model with GPUs in GKE and store the predictions in Cloud Storage.TutorialAI/ML TrainingGPUs Running large-scale machine learning on GKE This video shows how GKE helps solve common challenges of training large AI models at scale, and the best practices for training and serving large-scale machine learning models on GKE.VideoAI/ML TrainingAI/ML Inference TensorFlow on GKE Autopilot with GPU acceleration This blog post is a step-by-step guide to the creation, execution, and teardown of a Tensorflow-enabled Jupiter notebook.BlogAI/ML TrainingAI ML InferenceGPUs Implement a Job queuing system with quota sharing between namespaces on GKE This tutorial uses Kueue to show you how to implement a Job queueing system, and configure workload"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-4",
    "content": "resource and quota sharing between different namespaces on GKE.TutorialAI/ML Batch Build a RAG chatbot with GKE and Cloud Storage This tutorial shows you how to integrate a Large Language Model application based on retrieval-augmented generation with PDF files that you upload to a Cloud Storage bucket.TutorialAI/ML Data Loading Analyze data on GKE using BigQuery, Cloud Run, and Gemma This tutorial shows you how to analyze big datasets on GKE by leveraging BigQuery for data storage and processing, Cloud Run for request handling, and a Gemma LLM for data analysis and predictions.TutorialAI/ML Data Loading Distributed data preprocessing with GKE and Ray: Scaling for the enterprise Learn how to leverage GKE and Ray to efficiently preprocess large datasets for machine learning.MLOpsTrainingRay Data loading best practices for AI/ML inference on GKE Learn how to speed up data loading times for your machine learning applications on Google Kubernetes Engine.InferenceHyperdisk MLCloud Storage FUSE Save on GPUs: Smarter autoscaling for your GKE inferencing workloads Learn how to optimize your GPU inference costs by fine-tuning GKE's Horizontal Pod Autoscaler for maximum efficiency.InferenceGPUHPA Efficiently serve optimized AI models with NVIDIA NIM microservices on GKE Learn how to deploy cutting-edge NVIDIA NIM microservices on GKE with ease and accelerate your AI workloads.AINVIDIANIM Accelerate Ray in production with new Ray Operator on GKE Learn how Ray Operator on GKE simplifies your AI/ML production deployments, boosting performance and scalability.AITPURay Maximize your LLM serving throughput for GPUs on GKE \u2014 a practical guide Learn how to maximize large language model (LLM) serving throughput for GPUs on GKE, including infrastructure decisions and model server"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-5",
    "content": "optimizations.LLMGPUNVIDIA Search engines made simple: A low-code approach with GKE and Vertex AI Agent Builder How to build a search engine with Google Cloud, using Vertex AI Agent Builder, Vertex AI Search, and GKE.SearchAgentVertex AI LiveX AI reduces customer support costs with AI agents trained and served on GKE and NVIDIA AI How LiveX AI uses GKE to build AI agents that enhance customer satisfaction and reduce costs.GenAINVIDIAGPU Infrastructure for a RAG-capable generative AI application using GKE Reference architecture for running a generative AI application with retrieval-augmented generation (RAG) using GKE, Cloud SQL, Ray, Hugging Face, and LangChain.GenAIRAGRay Innovating in patent search: How IPRally leverages AI with GKE and Ray How IPRally uses GKE and Ray to build a scalable, efficient ML platform for faster patent searches with better accuracy.AIRayGPU Performance deep dive of Gemma on Google Cloud Leverage Gemma on Cloud GPUs and Cloud TPUs for inference and training efficiency on GKE.AIGemmaPerformance Gemma on GKE deep dive: New innovations to serve open generative AI models Use best-in-class Gemma open models to build portable, customizable AI applications and deploy them on GKE.AIGemmaPerformance Advanced scheduling for AI/ML with Ray and Kueue Orchestrate Ray applications in GKE with KubeRay and Kueue.KueueRayKubeRay How to secure Ray on Google Kubernetes Engine Apply security insights and hardening techniques for training AI/ML workloads using Ray on GKE.AIRaySecurity Design storage for AI and ML workloads in Google Cloud Select the best combination of storage options for AI and ML workloads on Google Cloud.AIMLStorage Automatic driver installation simplifies using NVIDIA GPUs in GKE Automatically install Nvidia GPU drivers in"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-6",
    "content": "GKE.GPUNVIDIAInstallation Accelerate your generative AI journey with NVIDIA NeMo framework on GKEE Train generative AI models using GKE and NVIDIA NeMo framework.GenAINVIDIANeMo Why GKE for your Ray AI workloads? Improve scalability, cost-efficiency, fault tolerance, isolation, and portability by using GKE for Ray workloads.AIRayScale Running AI on fully managed GKE, now with new compute options, pricing and resource reservations Gain improved GPU support, performance, and lower pricing for AI/ML workloads with GKE Autopilot.GPUAutopilotPerformance How SEEN scaled output 89x and reduced GPU costs by 66% using GKE Startup scales personalized video output with GKE.GPUScaleContainers How Spotify is unleashing ML Innovation with Ray and GKE How Ray is transforming ML development at Spotify.MLRayContainers How Orda\u014ds Bio takes advantage of generative AI on GKE Orda\u014ds Bio, one of the leading AI accelerators for biomedical research and discovery, is finding solutions to novel immunotherapies in oncology and chronic inflammatory disease.PerformanceTPUCost optimization GKE from a growing startup powered by ML How Moloco, a Silicon Valley startup, harnessed the power of GKE and Tensor Flow Enterprise to supercharge its machine learning (ML) infrastructure.MLScaleCost optimization Google Kubernetes Engine (GKE) Samples View sample applications used in official GKE product tutorials. GKE AI Labs Samples View experimental samples for leveraging GKE to accelerate your AI/ML initiatives. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra",
    "title": "AI/ML orchestration on GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra#chunk-7",
    "content": "updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/data-governance#chunk-0",
    "content": "Home BigQuery Documentation Guides BigQuery has built-in governance capabilities that simplify how you discover, manage, monitor, govern, and use your data and AI assets. Administrators, data stewards, data governance managers, and data custodians can use the governance capabilities in BigQuery to do the following: Discover data. Curate data. Gather and enrich metadata. Manage data quality. Ensure that data is used consistently and in compliance with organizational policies. Share data at scale and in a secure fashion. At the heart of BigQuery governance capabilities isuniversal catalog, a centralized inventory of all data assets in your organization. Universal catalog holds business, technical, and runtime metadata for all of your data. It helps you discover relationships and semantics in the metadata by applying artificial intelligence and machine learning. Universal catalog brings together a data catalog and a fully managed runtime metastore. The metastore in BigQuery lets you use multiple data processing engines to query a single copy of data with a single schema, without data duplication. The data processing engines that you can use include BigQuery, Apache Spark, Apache Flink, and Apache Hive. Your data can be stored in locations like BigQuery storage tables, BigQuery tables for Apache Iceberg, or BigLake external tables. BigQuery supports an end-to-end data lifecycle, from discovery to use of data. Universal catalog powers BigQuery governance features and capabilities. Governance features are also available in Dataplex. BigQuery discovers data across the organization in Google Cloud, whether the data is in BigQuery, Spanner, Cloud SQL, Pub/Sub, or Cloud Storage. BigQuery automatically extracts the metadata and stores it in universal catalog. For example, you can"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/data-governance#chunk-1",
    "content": "use BigQuery to extract metadata for structured and unstructured data from Cloud Storage, and you can automatically create query-ready BigLake tables at scale. This lets you perform analytics with an open source engine without data duplication. You can also extract and catalog metadata from third-party data sources using custom connectors. BigQuery offers the following data discovery capabilities: Search.Search for data and AI resources across projects and the organization. Within BigQuery in the Google Cloud console, usesemantic search(Preview) to search for resources by using everyday language. Or, find resources by usingkeyword searchin Dataplex. Automatic discovery of Cloud Storage data.Scan for data in Cloud Storage buckets to extract and then catalog metadata. Automatic discovery creates tables for both structured and unstructured data. Metadata import.Import metadata at scale from third-party systems into universal catalog. You can build custom connectors to extract data from your data sources, and then run managed connectivity pipelines that orchestrate the metadata import workflow. Metadata export.Export metadata at scale out of universal catalog. You can analyze the exported metadata with BigQuery, or integrate the metadata into custom applications or programmatic processing workflows. To improve the discoverability and usability of data, data stewards and administrators can use BigQuery to review, update, and analyze metadata. BigQuery data curation and stewardship capabilities help you ensure that your data is accurate, consistent, and aligned with your organization's policies. BigQuery offers the following data curation and stewardship capabilities: Business glossary(Preview).Improve context, collaboration, and search by defining your organization's"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/data-governance#chunk-2",
    "content": "terminology in a glossary. Identify data stewards for the terms, and attach terms to data asset fields. Data insights.Gemini uses metadata to generate natural language questions about your table and the SQL queries to answer them. These data insights help you uncover patterns, assess data quality, and perform statistical analysis. Data profiling.Identify common statistical characteristics of the columns in BigQuery tables to understand and analyze your data more effectively. Data quality.Define and run data quality checks across tables in BigQuery and Cloud Storage, and apply regular and ongoing data controls in BigQuery environments. Data lineage.Track how data moves through your systems: where it comes from, where it's passed to, and what transformations are applied to it. BigQuery supports data lineage at the table- and column-levels. The following table outlines next steps that you can take to learn more about curation and data stewardship features: Run adata profile scanto gain insights about your data, including the limits or averages of your data. Enabledata lineagein your BigQuery project to automatically record lineage information for BigQuery operations like load, copy, and data modifications. Set up a recurringdata quality scanto alert you to possible data issues by usingpredefined scan rules. Set upcustom data quality rulesfor your data quality scans so that your scans are tailored to your specific needs. Data access management is the process of defining, enforcing, and monitoring the rules and policies governing who has access to data. Access management ensures that data is only accessible to those who are authorized to access it. BigQuery offers the following security and access control capabilities: Identity and Access Management (IAM).IAM lets you"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/data-governance#chunk-3",
    "content": "control who has access to your BigQuery resources, such as projects, datasets, tables, and views. You can grant IAM roles to users, groups, and service accounts. These roles define what they can do with your resources. Column-level access controlsandrow-level access controls.Column-level and row-level access controls let you restrict access to specific columns and rows in a table, based on user attributes or data values. This control lets you implement fine-grained access to help protect sensitive data from unauthorized access. Data transfer management.VPC Service Controls lets you create perimeters around Google Cloud resources and control access to those resources based on your organization's policies. Audit logs.Audit logs provide you with a detailed record of user activity and system events in your organization. These logs help you enforce data governance policies and identify potential security risks. Data masking.Data masking lets you obscure sensitive data in a table while still permitting authorized users to access the surrounding data. Data masking can also obscure data that matches sensitive data patterns, safeguarding against accidental data disclosure. Encryption.BigQuery automatically encrypts all data at rest and in transit, while letting you customize your encryption settings to meet your specific requirements. The following table outlines next steps that you can take to learn more about access control features: Take a look atpredefined rolesin BigQuery and consider how to assign them based on theprinciple of least privilege. Learn how Google encrypts your dataat restandin transitby default. For greater flexibility and granularity in managing your permissions, considercreating custom rolesthat match your needs. Addrowandcolumn controlsto help control"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/data-governance#chunk-4",
    "content": "access to specific rows and columns in your tables. Establish an access perimeter around your Google Cloud resources bysetting up VPC Service Controls. Addcolumn-level data maskingto your table to share information through your organization without revealing sensitive data. UseSensitive Data Protectionto scan your data for sensitive and high-risk information, such as personally identifiable information (PII), financial data, and health information. BigQuery lets you share data and insights at scale within and across organizational boundaries. It has a robust security and privacy framework through a built-in data exchange platform. UsingBigQuery sharing, you can discover, access, and consume a data library that's curated by a wide selection of data providers. BigQuery offers the following sharing capabilities: Share more than data.You can share a wide range of data and AI assets such as BigQuery datasets, tables, views, real-time streams with Pub/Sub topics, SQL stored procedures, and BigQuery ML models. Access Google datasets.Augment your analytics and ML initiatives with Google datasets from Search Trends, DeepMind WeatherNext models, Google Maps Platform, Google Earth Engine, and more. Integrate with data governance principles.Data owners retain control over their data and have the ability to define and configure rules or policies to restrict access and usage. Live, zero-copy data sharing.Data is shared in place with no integration, data movement, or replication needed, ensuring analysis is based on the latest information. Linked datasets created are a live pointer to the shared asset. Enhance security posture.You can use access controls to reduce overprovisioning access, including built-in VPC Service Controls support. Increase visibility with provider usage"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/data-governance",
    "title": "Introduction to data governance in\nBigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/data-governance#chunk-5",
    "content": "metrics.Data publishers can view and monitor usage for shared assets such as the number of jobs executed, total bytes scanned, and subscribers for each organization. Collaborate on sensitive data with data clean rooms.Data clean rooms provide a security-enhanced environment in which multiple parties can share, join, and analyze their data assets without moving or revealing the underlying data. Built on BigQuery.You can build on the scalability and massive processing capabilities in BigQuery, allowing for large scale collaborations. The following table outlines next steps that you can take to learn more about sharing features: Learn how to create and manageexchangesandlistingsto start sharing within or outside of your organization. Share real-time streaming data withPub/Sub topics. Share and collaborate on sensitive data withdata clean rooms. Further protect data exfiltration by configuringVPC Service Controlsaround your shared assets. Commercializeand sell your assets on Google Cloud Marketplace Learn aboutauthentication at Google. Learn aboutdata deletion on Google Cloud. Learn more aboutIAM best practices. Learn theresource hierarchy on Google Cloud. Learn aboutIAM on Google Cloud. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-14 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/product-list",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/product-list#chunk-0",
    "content": "Home Documentation Cloud BillingBilling & cost management tools Cloud IdentityManage users, devices & apps Cloud QuotasManage service quotas & usage Config ConnectorKubernetes add-on to manage resources Deployment ManagerTemplated infrastructure deployment Identity PlatformCustomer identity access management Infra ManagerAutomate infrastructure resource deployment Managed Microsoft ADManaged Microsoft Active Directory RecommenderCloud usage recommendations & insights Service CatalogManage internal enterprise solutions Service UsageManage APIs & services in projects Cloud TPUHardware acceleration for ML Colab EnterpriseColab notebooks for enterprises Deep Learning ContainersPreconfigured containers for deep learning Deep Learning VMPreconfigured VMs for deep learning DocAIAnalyze, classify, search documents Enterprise Knowledge GraphConsolidate & reconcile organizational knowledge Immersive Stream for XRHost & serve interactive 3D & AR Speech-to-TextConvert audio to text TensorFlow EnterpriseScalable TensorFlow development experience Text-to-SpeechConvert text to audio TranslationLanguage detection & translation Translation HubManaged document translation service Vertex AIManaged platform for ML Vertex AI VisionIngest, analyze & store video data Vertex AI WorkbenchJupyter-based environment for data science Vision APIImage recognition & classification API GatewayFully-managed API gateway ApigeeNative API management platform App HubView & understand app resources Application IntegrationEnterprise application integrations Artifact AnalysisAutomated security scanning Cloud BuildDevOps automation platform Cloud CodeIntelliJ Google Cloud tools Cloud DeployContinuous delivery for GKE Cloud SchedulerManaged cron job service Cloud ShellBrowser-based terminal/CLI Cloud"
  },
  {
    "source_url": "https://cloud.google.com/docs/product-list",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/product-list#chunk-1",
    "content": "TasksAsynchronous task execution Cloud WorkstationsCloud-based developer workstations Developer ConnectConnect source control systems EventarcAsynchronous task delivery gcloud CLIGoogle Cloud command-line tool Integration ConnectorsEnterprise application connectivity Pub/SubGlobal real-time messaging Secure Source ManagerManaged single-tenant source code repository Service InfrastructureCross-organization foundational service platform Software supply chain securitySecure software supply chain Tools for EclipseEclipse Google Cloud tools Tools for PowerShellDeveloper powershell tools Tools for Visual StudioVisual Studio Google Cloud tools WorkflowsHTTP services orchestration App EngineManaged app platform Blockchain Node EngineFully-managed blockchain nodes Blockchain RPCRead/write to multiple blockchains Cloud RunFully-managed serverless application platform Google Kubernetes Engine (GKE)Managed Kubernetes/containers AI HypercomputerSupercomputer architecture for AI BatchManaged batch processing service Capacity PlannerView & forecast compute usage Cluster toolkitDeploy HPC, AI & ML workloads Compute EngineVMs, GPUs, TPUs, disks Migrate to ContainersMigrate VMs to Kubernetes Engine Migrate to VMsMigrate VMs to Compute Engine Migration CenterAccelerate end-to-end migration Shielded VMsVM boot-time protection VM ManagerManage OS VM Fleets VMware EngineVMware as a service Workload ManagerRule-based workload evaluation BigQueryData warehouse & analytics Blockchain AnalyticsIndexed blockchain data for analysis Cloud ComposerManaged workflow orchestration service Cloud Data FusionGraphically manage data pipelines DataflowStream/batch data processing DataformELT & SQL workflow tool DataplexCentrally manage & govern data DataprocManaged Spark & Hadoop Dataproc ServerlessRun"
  },
  {
    "source_url": "https://cloud.google.com/docs/product-list",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/product-list#chunk-2",
    "content": "serverless Spark batch workloads DatastreamChange data capture/replication service LookerEnterprise BI & analytics AlloyDB for PostgreSQLScalable & performant PostgreSQL-compatible DB BigtablePetabyte-scale, low-latency, non-relational Cloud SQLManaged MySQL, PostgreSQL, SQL Server Database Migration ServiceMigrate to Google Cloud databases FirestoreServerless NoSQL document DB Memorystore for MemcachedManaged Memcached service SpannerHorizontally scalable relational DB GKE Multi-CloudMulti-cloud cluster creation Anti Money Laundering AI (AML AI)Detect potential money laundering activity Healthcare Data EngineHealthcare system Google Cloud interoperability Talent SolutionsJob search with ML Telecom Network AutomationManaged cloud implementation of Nephio Telecom Subscriber Insights APIInsights for communication service providers Vertex AI Search for retailPersonalized search for retail Cloud CDNCache content near users Cloud DNSProgrammable DNS serving Cloud Intrusion Detection System (Cloud IDS)Detect network-based threats Cloud InterconnectConnect networks to Google Cloud Load BalancingMulti-region load distribution/balancing Cloud NATNetwork address translation service Cloud Next Generation Firewall (Cloud NGFW)Fully-distributed firewall service Cloud RouterVPC/on-prem network route exchange (BGP) Cloud Service MeshService mesh traffic management Cloud VPNVirtual private network connection Google Cloud ArmorDDoS protection & WAF Media CDNCDN for streaming & videos Network Connectivity CenterConnect VPC & on-prem Network Intelligence CenterNetwork monitoring & topology Network Service TiersPrice versus performance tiering Secure Web ProxySecure egress web traffic Service ExtensionsAdd custom logic for edge applications Virtual Private Cloud (VPC)Software-defined"
  },
  {
    "source_url": "https://cloud.google.com/docs/product-list",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/product-list#chunk-3",
    "content": "networking VPC Service ControlsSecurity perimeters for service segregation Error ReportingApp error reporting LoggingCentralized data & event logging MonitoringInfrastructure & application monitoring ProfilerCPU & heap reporting TraceApp latency insights Access Context ManagerFine-grained, attribute-based access control Access TransparencyAudit cloud provider support access Advisory NotificationsPrivacy & security event notifications Assured OSSGet verified open-source packages Binary AuthorizationKubernetes deploy-time security Certificate AuthorityCreate & manage private CAs Certificate ManagerAcquire & manage TLS certificates Chrome Enterprise PremiumZero-trust secure access Cloud Asset InventoryAll assets, one place Cloud KMSHosted key management service Confidential VMEncrypt data in-use Endpoint VerificationAccess control for business devices Google SecOpsTool for SecOps, SIEM & SOAR Identity and Access Management (IAM)Resource access control Identity-Aware ProxyIdentity-based app access Policy IntelligenceUnderstand policies & usage reCAPTCHAAdvanced bot & fraud detection Resource ManagerCloud project metadata management Secret ManagerStore & manage secrets Security Command CenterSecurity management & data risk platform Sensitive Data ProtectionClassify & redact sensitive data Service HealthIdentify cloud service disruptions Web RiskCheck URLs for safety Backup and DR ServiceBackup & DR SaaS Cloud StorageMulti-class multi-region object storage FilestoreManaged network-attached storage Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its"
  },
  {
    "source_url": "https://cloud.google.com/docs/product-list",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/product-list#chunk-4",
    "content": "affiliates. Last updated 2025-05-13 UTC."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-0",
    "content": "Home Compute Engine Documentation Guides You can use GPUs on Compute Engine to accelerate specific workloads on your VMs such as machine learning (ML) and data processing. To use GPUs, you can either deploy an accelerator-optimized VM that has attached GPUs, or attach GPUs to an N1 general-purpose VM. Compute Engine provides GPUs for your VMs in passthrough mode so that your VMs have direct control over the GPUs and their associated memory. For more information about GPUs on Compute Engine, seeAbout GPUs. If you have graphics-intensive workloads, such as 3D visualization, 3D rendering, or virtual applications, you can use NVIDIA RTX virtual workstations (formerly known as NVIDIA GRID). This document provides an overview of the different GPU VMs that are available on Compute Engine. To view available regions and zones for GPUs on Compute Engine, seeGPUs regions and zone availability. For compute workloads, GPUs are supported for the following machine types: A4 VMs: these VMs have NVIDIA B200 GPUs automatically attached. A3 VMs: these VMs have NVIDIA H100 80GB or NVIDIA H200 141GB GPUs automatically attached. A2 VMs: these VMs have either NVIDIA A100 80GB or NVIDIA A100 40GB GPUs automatically attached. G2 VMs: these VMs have NVIDIA L4 GPUs automatically attached. N1 VMs: for these VMs, you can attach the following GPU models: NVIDIA T4, NVIDIA V100, NVIDIA P100, or NVIDIA P4. To use NVIDIA B200 GPUs (nvidia-b200), you must use anA4 accelerator-optimizedmachine type. Each A4 machine type has a fixed GPU count, vCPU count, and memory size. *GPU memory is the memory on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020A"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-1",
    "content": "vCPU is implemented as a single hardware hyper-thread on one of the availableCPU platforms.\u2021Maximum egress bandwidth cannot exceed the number given. Actual egress bandwidth depends on the destination IP address and other factors. SeeNetwork bandwidth. *GPU memory is the memory on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020A vCPU is implemented as a single hardware hyper-thread on one of the availableCPU platforms.\u2021Maximum egress bandwidth cannot exceed the number given. Actual egress bandwidth depends on the destination IP address and other factors. SeeNetwork bandwidth. To use NVIDIA H100 80GB or NVIDIA H200 141GB GPUs, you must use anA3 accelerator-optimizedmachine type. Each A3 machine type has a fixed GPU count, vCPU count, and memory size. To use NVIDIA H200 141GB GPUs, you must use the A3 Ultra machine type. This machine type has H200 141GB GPUs (nvidia-h200-141gb) and provide the highest network performance. They are ideal for foundation model training and serving. *GPU memory is the memory on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020A vCPU is implemented as a single hardware hyper-thread on one of the availableCPU platforms.\u2021Maximum egress bandwidth cannot exceed the number given. Actual egress bandwidth depends on the destination IP address and other factors. SeeNetwork bandwidth. *GPU memory is the memory on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-2",
    "content": "the higher bandwidth demands of your graphics-intensive workloads.\u2020A vCPU is implemented as a single hardware hyper-thread on one of the availableCPU platforms.\u2021Maximum egress bandwidth cannot exceed the number given. Actual egress bandwidth depends on the destination IP address and other factors. SeeNetwork bandwidth. To use NVIDIA H100 80GB you have the following options: A3 Mega: these machine types have H100 80GB GPUs (nvidia-h100-mega-80gb) and are ideal for large-scale training and serving workloads. A3 High: these machine types have H100 80GB GPUs (nvidia-h100-80gb) and are well-suited for both training and serving tasks. A3 Edge: these machine types have H100 80GB GPUs (nvidia-h100-80gb), are designed specifically for serving, and are available in alimited set of regions. To create Google Kubernetes Engine cluster, seeDeploy an A3 Mega cluster with GKE. To create a Slurm cluster, seeDeploy an A3 Mega Slurm cluster. To create Spot VMs, set the provisioning model toSPOTwhen youCreate an accelerator-optimized VM. To create a resize request in a MIG, which uses DWS, seeCreate a MIG with GPU VMs. To get started with A3 Edge machines, seeCreate an A3 VM with GPUDirect-TCPX enabled. 800:for asia-south1 and northamerica-northeast2 400:for all otherA3 Edge regions *GPU memory is the memory on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020A vCPU is implemented as a single hardware hyper-thread on one of the availableCPU platforms.\u2021Maximum egress bandwidth cannot exceed the number given. Actual egress bandwidth depends on the destination IP address and other factors. SeeNetwork bandwidth. *GPU memory is the memory on a"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-3",
    "content": "GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020A vCPU is implemented as a single hardware hyper-thread on one of the availableCPU platforms.\u2021Maximum egress bandwidth cannot exceed the number given. Actual egress bandwidth depends on the destination IP address and other factors. SeeNetwork bandwidth. To useNVIDIA A100GPUs on Google Cloud, you must use anA2 accelerator-optimizedmachine type. Each A2 machine type has a fixed GPU count, vCPU count, and memory size. A2 machine series are available in two types: A2 Ultra: these machine types have A100 80GB GPUs (nvidia-a100-80gb) and Local SSD disks attached. A2 Standard: these machine types have A100 40GB GPUs (nvidia-tesla-a100) attached. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. To useNVIDIA L4GPUs (nvidia-l4ornvidia-l4-vws), you must use aG2 accelerator-optimizedmachine type. Each G2 machine type has a fixed number ofNVIDIA L4 GPUsand vCPUs attached. Each G2 machine type also has a default memory and a custom memory range. The custom memory range defines the amount of memory that you can allocate to your VM for each machine type. You can specify your custom memory during VM creation. *GPU memory is the memory available on a GPU device"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-4",
    "content": "that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. You can attach the following GPU models to anN1 machine typewith the exception of the N1 shared-core machine type. N1 VMs with lower numbers of GPUs are limited to a maximum number of vCPUs. In general, a higher number of GPUs lets you create VM instances with a higher number of vCPUs and memory. You can attachNVIDIA T4GPUs to N1 general-purpose VMs with the following VM configurations. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. You can attachNVIDIA P4GPUs to N1 general-purpose VMs with the following VM configurations. *GPU memory is the memory that is available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020For VMs with attached NVIDIA P4 GPUs, Local SSD disks are only supported in zonesus-central1-candnorthamerica-northeast1-b. *GPU memory is the memory"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-5",
    "content": "that is available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. \u2020For VMs with attached NVIDIA P4 GPUs, Local SSD disks are only supported in zonesus-central1-candnorthamerica-northeast1-b. You can attachNVIDIA V100GPUs to N1 general-purpose VMs with the following VM configurations. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020For VMs with attached NVIDIA V100 GPUs, Local SSD disks aren't supported inus-east1-c. *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads.\u2020For VMs with attached NVIDIA V100 GPUs, Local SSD disks aren't supported inus-east1-c. You can attachNVIDIA P100GPUs to N1 general-purpose VMs with the following VM configurations. For some NVIDIA P100 GPUs, the maximum CPU and memory that is available for some configurations is dependent on the zone in which the GPU resource is running. 1 to 64(us-east1-c, europe-west1-d, europe-west1-b) 1 to 96(all P100 zones) 1 to 208(us-east1-c, europe-west1-d, europe-west1-b) 1 to 624(all P100 zones) *GPU memory is the memory available on a GPU device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. *GPU memory is the memory available on a GPU"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-6",
    "content": "device that can be used for temporary storage of data. It is separate from the VM's memory and is specifically designed to handle the higher bandwidth demands of your graphics-intensive workloads. If you have graphics-intensive workloads, such as 3D visualization, you can create virtual workstations that useNVIDIA RTX Virtual Workstations (vWS)(formerly known as NVIDIA GRID). When you create a virtual workstation, an NVIDIA RTX Virtual Workstation (vWS) license is automatically added to your VM. For information about pricing for virtual workstations, seeGPU pricing page. For graphics workloads, NVIDIA RTX virtual workstation (vWS) models are available: G2 machine series: forG2 machine typesyou can enable NVIDIA L4 Virtual Workstations (vWS):nvidia-l4-vws G2 machine series: forG2 machine typesyou can enable NVIDIA L4 Virtual Workstations (vWS):nvidia-l4-vws N1 machine series: forN1 machine types, you can enable the following virtual workstations:NVIDIA T4 Virtual Workstations:nvidia-tesla-t4-vwsNVIDIA P100 Virtual Workstations:nvidia-tesla-p100-vwsNVIDIA P4 Virtual Workstations:nvidia-tesla-p4-vws N1 machine series: forN1 machine types, you can enable the following virtual workstations: NVIDIA T4 Virtual Workstations:nvidia-tesla-t4-vws NVIDIA P100 Virtual Workstations:nvidia-tesla-p100-vws NVIDIA P4 Virtual Workstations:nvidia-tesla-p4-vws The following table describes the GPU memory size, feature availability, and ideal workload types of different GPU models that are available on Compute Engine. To compare GPU pricing for the different GPU models and regions that are available on Compute Engine, seeGPU pricing. The following table describes the performance specifications of different GPU models that are available on Compute Engine. *To allow FP64 code to work"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/gpus",
    "title": "GPU machine typesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/gpus#chunk-7",
    "content": "correctly, a small number of FP64 hardware units are included in the T4, L4, and P4 GPU architecture.\u2020TeraOperations per Second. *To allow FP64 code to work correctly, a small number of FP64 hardware units are included in the T4, L4, and P4 GPU architecture.\u2020TeraOperations per Second. *For mixed precision training, NVIDIA B200, H200, H100, A100, and L4 GPUs also support thebfloat16data type.\u2020For NVIDIA B200, H200, H100 and L4 GPUs, structural sparsity is supported which you can use to double the performance value. The values shown are with sparsity. Specifications are one-half lower without sparsity. *For mixed precision training, NVIDIA B200, H200, H100, A100, and L4 GPUs also support thebfloat16data type.\u2020For NVIDIA B200, H200, H100 and L4 GPUs, structural sparsity is supported which you can use to double the performance value. The values shown are with sparsity. Specifications are one-half lower without sparsity. For more information about GPUs on Compute Engine, seeAbout GPUs. Review theGPU regions and zones availability. ReviewNetwork bandwidths and GPUs. Learn aboutGPU pricing. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-14 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/get-started",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/get-started#chunk-0",
    "content": "Home Documentation Click to show or hide setup steps by job function: Establish administrators, billing accounts, and other settings in your Google Cloud environment. Cloud Quotas overview Google Cloud deployment archetypes (Architecture Center) Set up billing, spending notifications, and resource structure to facilitate cost monitoring and optimization. Monitor costs using billing reports Optimize costs with FinOps hub Resource hierarchy options for cost tracking Implement cost optimization strategies (Architecture Center) Start automating infrastructure and secure collaboration with teammates using Google Cloud tools and best practices. Observability in Google Cloud Terraform and Infrastructure Manager CI/CD pipeline for containerized apps (Architecture Center) Get basic API access and set up a development environment that can interact with Google Cloud services. Build a generative AI application Analyze sample data using Google Cloud products with minimal setup. Set up the bq command-line tool Gemini in BigQuery overview Data analytics design patterns (Architecture Center) Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/memorystore/docs/cluster",
    "title": "Memorystore for Redis Cluster documentation",
    "chunk_id": "https://cloud.google.com/memorystore/docs/cluster#chunk-0",
    "content": "Home Documentation Memorystore Memorystore for Redis Cluster Memorystore for Redis Cluster is a fully managed Redis service for Google Cloud. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments. Memorystore for Redis Cluster distributes (or \"shards\") your data across primary nodes and replicates your data across optional replica nodes to ensure high availability. The horizontally scalable cluster architecture provides better performance over vertically scalable architecture because Redis performance is better on many smaller nodes instead of fewer larger nodes. Memorystore for Redis Cluster is based on and is compatible with open-source Redis versions 7.2 and earlier and supports a subset of the total Redis command library. Not sure what database option is right for you? Learn more about ourdatabase services. Learn moreabout Memorystore for Redis Cluster. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Memorystore for Redis Cluster overview Memorystore for Redis Cluster overview High availability and replicas High availability and replicas Cluster and node specification Cluster and node specification General best practices General best practices Operational guidelines Operational guidelines Client library code samples Client library code samples Supported commands Supported commands Terraform reference Terraform reference Create instances Create instances Connect to an instance Connect to an instance Networking Networking About in-transit"
  },
  {
    "source_url": "https://cloud.google.com/memorystore/docs/cluster",
    "title": "Memorystore for Redis Cluster documentation",
    "chunk_id": "https://cloud.google.com/memorystore/docs/cluster#chunk-1",
    "content": "encryption About in-transit encryption REST API reference REST API reference Locations Locations Pricing Pricing Quotas and limits Quotas and limits Release notes Release notes Service Level Agreement Service Level Agreement Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Redis is a trademark of Redis Ltd. All rights therein are reserved to Redis Ltd. Any use by Google is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and Google. Memorystore is based on and is compatible with open-source Redis versions 7.2 and earlier and supports a subset of the total Redis command library. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-0",
    "content": "Home Vertex AI Documentation Vector Search is a powerful vector search engine built on groundbreaking technology developed by Google Research. Leveraging theScaNNalgorithm, Vector Search lets you build next-generation search and recommendation systems as well as generative AI applications. You can benefit from the very same research and technology that power core Google products, including Google Search, YouTube, and Google Play. This means you get the scalability, availability, and performance that's trusted to handle massive datasets and deliver lightning-fast results at a global scale. With Vector Search, you have an enterprise-grade solution for implementing cutting-edge semantic search capabilities in your own applications. Blog:Multimodal search with Vector Search Next 24 Infinite Nature Demo Infinite Fleurs: Discover AI-assisted creativity in full bloom Experience multimodal AI with manga ONE PIECE Vector Search interactive demo: Check out the live demo for a realistic example of what vector search technology can do and get a headstart with Vector Search. Vector Search quickstart: Try Vector Search in 30 minutes by building, deploying, and querying a Vector Search index using a sample dataset. This tutorial covers setup, data preparation, index creation, deployment, querying, and cleanup. Before you begin: Prepare your embeddings by choosing and training a model, and preparing your data. Then, choose a public or private endpoint to deploy your query index to. Vector Search pricing and pricing calculator: Vector Search pricing includes the cost of virtual machines used to host deployed indexes, as well as expenses for building and updating indexes. Even a minimal setup (under $100 per month) can accommodate high throughput for moderate-sized use cases. To estimate"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-1",
    "content": "your monthly costs: Go toGoogle Cloud's pricing calculator. ClickAdd to estimate. Search for Vertex AI. Click theVertex AIbutton. ChooseVertex AI Vector Searchfrom theService typedrop-down. Keep the default settings or configure your own. The estimated cost per month is shown in theCost detailspanel. Manage Indexes and EndpointsIndex OverviewConfigure an indexCreate an indexCreate an index endpointQuery dataAdvanced topicsHybrid search Manage Indexes and Endpoints Index Overview Configure an index Create an index Create an index endpoint Query data Advanced topics Hybrid search APIsVertex SDK versus Client LibrariesInstall the Vertex AI client librariesVertex AI SDK for PythonClass MatchingEngineIndexClass MatchingEngineIndexEndpointPython ClientPackage index_endpoint_servicePackage index_servicePackage match_serviceJava ClientClass IndexServiceClientClass IndexEndpointServiceClientClass MatchServiceClient APIs Vertex SDK versus Client Libraries Install the Vertex AI client libraries Vertex AI SDK for Python Class MatchingEngineIndex Class MatchingEngineIndexEndpoint Python Client Package index_endpoint_service Package index_service Package match_service Java Client Class IndexServiceClient Class IndexEndpointServiceClient Class MatchServiceClient More resourcesVector Search quotas and limitGet support More resources Vector Search quotas and limit Get support Vector search technology is becoming a central hub for businesses using AI. Similar to how relational databases function in IT systems, it connects various business elements like documents, content, products, users, events, and other entities based on their relevance. Beyond searching conventional media like documents and images, Vector Search can also power intelligent recommendations, match business problems with"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-2",
    "content": "solutions, and even link IoT signals to monitoring alerts. It's a versatile tool that's essential for navigating the growing landscape of AI-enabled enterprise data. Search / Information Retrieval RecommendationSystems How Vertex AI vector search helps unlock high-performance gen AI apps:Vector Search powers diverse applications, including ecommerce, RAG systems, and recommendation engines, alongside chatbots, multimodal search, and more. Hybrid search further enhances results for niche terms. Customers like Bloomreach, eBay, and Mercado Libre use Vertex AI for its performance, scalability, and cost-effectiveness, achieving benefits like faster search and increased conversions. eBay uses Vector Search for recommendations:Highlights how eBay uses Vector Search for its recommendation system. This technology allows eBay to find similar products within its extensive catalog, improving the user experience. Mercari leverages Google's vector search technology to create a new marketplace:Explains how Mercari uses Vector Search to improve its new marketplace platform. Vector Search powers the platform's recommendations, helping users find relevant products more effectively. Vertex AI Embeddings for Text: Grounding LLMs made easy:Focuses on grounding LLMs using Vertex AI Embeddings for text data. Vector Search plays an important role in finding relevant text passages that ensure the model's responses are grounded in factual information. What is Multimodal Search: \"LLMs with vision\" change businesses:Discusses Multimodal Search, which combines LLMs with visual understanding. It explains how Vector Search processes and compares both text and image data, allowing for more comprehensive search experiences. Unlock multimodal search at scale: Combine text & image power with Vertex"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-3",
    "content": "AI:Describes building a multimodal search engine with Vertex AI that combines text and image search using a weighted Rank-Biased Reciprocal Rank ensemble method. This improves user experience and provides more relevant results. Scaling deep retrieval with TensorFlow Recommenders and Vector Search:Explains how to build a playlist recommendation system using TensorFlow Recommenders and Vector Search, covering deep retrieval models, training, deployment, and scaling. Gen AI: retrieval for RAG and Agents Vertex AI and Denodo unlock enterprise data with Gen AI:Showcases how Vertex AI's integration with Denodo enables businesses to use generative AI for gaining insights from their data. Vector Search is key for efficiently accessing and analyzing relevant data within an enterprise environment. Infinite Nature and the nature of industries: This 'wild' demo shows the diverse possibilities of AI:Showcases a demo that illustrates AI's potential across different industries. It utilizes Vector Search to power generative recommendations and multimodal semantic search. Infinite Fleurs: Discover AI-assisted creativity in full bloom:Google's Infinite Fleurs, an AI experiment using Vector Search, Gemini and Imagen models, generates unique flower bouquets based on user prompts. This technology showcases AI's potential to inspire creativity across various industries. LlamaIndex for RAG on Google Cloud:Describes how to use LlamaIndex to facilitate Retrieval Augmented Generation (RAG) with large language models. LlamaIndex utilizes Vector Search to retrieve relevant information from a knowledge base, resulting in more accurate and contextually appropriate responses. RAG and grounding on Vertex AI:Examines RAG and grounding techniques on Vertex AI. Vector Search helps identify relevant"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-4",
    "content": "grounding information during retrieval, which makes generated content more accurate and reliable. Vector Search on LangChain:provides a guide to using Vector Search with LangChain for building and deploying a vector database index for text data, including question-answering and PDF processing. BI, data analytics, monitoring, and more Enabling real-time AI with Streaming Ingestion in Vertex AI:Explores Streaming Update in Vector Search and how it provides real-time AI capabilities. This technology allows for real-time processing and analysis of incoming data streams. You can use the following resources to get started with Vector Search: Notebooks and solutions Tutorials and training Related products In-depth deep dive material Vertex AI Vector Search Quickstart:Provides an overview of Vector Search. It is designed for users who are new to the platform and want to get started quickly. Getting Started with Text Embeddings and Vector Search:Introduces text embeddings and vector search. It explains how these technologies work and how they can be used to improve search results. Combining Semantic & Keyword Search: A Hybrid Search Tutorial with Vertex AI Vector Search:Provides instructions on how to use Vector Search for hybrid search. It covers the steps involved in setting up and configuring a hybrid search system. Vertex AI RAG Engine with Vector Search:Explores the use of Vertex AI RAG Engine with Vector Search. It discusses the benefits of using these two technologies together and provides examples of how they can be used in real-world applications. Infrastructure for a RAG-capable generative AI application using Vertex AI and Vector Search:Details the architecture for building a generative AI application and RAG using Vector Search, Cloud Run and Cloud Storage, covering"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-5",
    "content": "use cases, design choices, and key considerations. Getting Started with Vector Search and EmbeddingsVector Search is used to find similar or related items. It can be used for recommendations, search, chatbots, and text classification. The process involves creating embeddings, uploading them to Google Cloud, and indexing them for querying. This lab focuses on text embeddings using Vertex AI, but embeddings can be generated for other data types. Vector Search and EmbeddingsThis course introduces Vector Search and describes how it can be used to build a search application with large language model (LLM) APIs for embeddings. The course consists of conceptual lessons on Vector Search and text embeddings, practical demos on how to build Vector Search on Vertex AI, and a practice lab. Understanding and Applying Text EmbeddingsThe Vertex AI Embeddings API generates text embeddings, which arenumerical representations of text used for tasks like identifying similar items. In this course, you'll use text embeddings for tasks like classification and semantic search, and combine semantic search with LLMs to build question-answering systems using Vertex AI. Machine Learning Crash Course: EmbeddingsThis course introduces word embeddings, contrasting them with sparse representations. It explores methods for obtaining embeddings and differentiates between static and contextual embeddings. Vertex AI EmbeddingsProvides an overview of Embeddings API. Text and multimodal embedding use cases, along with links to additional resources and related Google Cloud services. AI Applications ranking APIThe ranking API reranks documents based on relevance to a query using a pre-trained language model, providing precise scores. It's ideal for improving search results from various sources including"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-6",
    "content": "Vector Search. Vertex AI Feature StoreLets you manage and serve feature data using BigQuery as the data source. It provisions resources for online serving, acting as a metadata layer to serve the latest feature values directly from BigQuery. Feature Store allows for the instant retrieval of feature values for the items Vector Store returned for queries. Vertex AI PipelinesVertex AI Pipelines enables the automation, monitoring, and governance of your ML systems in a serverless manner by orchestrating ML workflows with ML pipelines. You can run ML pipelines defined using Kubeflow Pipelines or the TensorFlow Extended (TFX) framework in batches. Pipelines allows for building automated pipelines to generate embeddings, create and update Vector Search indexes, and form an MLOps setup for production search and recommendation systems. Enhancing your gen AI use case with Vertex AI embeddings and task typesFocuses on improving Generative AI applications using Vertex AI Embeddings and task types. Vector Search can be used with task type embeddings to enhance the context and accuracy of generated content by finding more relevant information. TensorFlow RecommendersAn open-source library for building recommendation systems. It simplifies the process from data preparation to deployment and supports flexible model building. TFRS offers tutorials and resources and enables the creation of sophisticated recommendation models. TensorFlow RankingTensorFlow Ranking is an open-source library for building scalable neural learning-to-rank (LTR) models. It supports various loss functions and ranking metrics, with applications in search, recommendation, and other fields. The library is actively developed by Google AI. Announcing ScaNN: Efficient Vector Similarity SearchGoogle's ScaNN, an"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-7",
    "content": "algorithm for efficient vector similarity search, utilizes a novel technique to improve accuracy and speed in finding nearest neighbors. It outperforms existing methods and has broad applications in machine learning tasks requiring semantic search. Google's research efforts span various areas, including foundational ML and societal impacts of AI. SOAR: New algorithms for even faster Vector Search with ScaNNGoogle's SOAR algorithm improves Vector Search efficiency by introducing controlled redundancy, allowing faster searches with smaller indexes. SOAR assigns vectors to multiple clusters, creating \"backup\" search paths for improved performance. Get Started with Vector Search using Vertex AI Vector Search is a powerful tool for building AI-powered applications. This video introduces the technology and provides a step-by-step guide to getting started. Learn Hybrid Search with Vector Search Vector Search can be used for hybrid search, allowing you to combine the power of vector search with the flexibility and speed of a conventional search engine. This video introduces hybrid search and shows you how to use Vector Search for hybrid search. You're Already Using Vector Search! Here's How to Be an Expert Did you know you're probably using vector search every day without realizing it? From finding that elusive product on social media to tracking down a song stuck in your head, vector search is the AI magic behind these everyday experiences. New \"task type\" embedding from the DeepMind team improves RAG search quality Improve the accuracy and relevance of your RAG systems with newtask typeembeddings developed by the Google DeepMind team. Watch along and learn about the common challenges in RAG search quality and how task type embeddings can effectively bridge the semantic gap"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-8",
    "content": "between questions and answers, leading to more effective retrieval and enhanced RAG performance. This list contains some important terminology that you'll need to understand to use Vector Search: Vector: A vector is a list of float values that has magnitude and direction. It can be used to represent any kind of data, such as numbers, points in space, and directions. Vector: A vector is a list of float values that has magnitude and direction. It can be used to represent any kind of data, such as numbers, points in space, and directions. Embedding: An embedding is a type of vector that's used to represent data in a way that captures its semantic meaning. Embeddings are typically created using machine learning techniques, and they are often used in natural language processing (NLP) and other machine learning applications.Dense embeddings: Dense embeddings represent the semantic meaning of text, using arrays that mostly contain non-zero values. With dense embeddings, similar search results can be returned based on semantic similarity.Sparse embeddings: Sparse embeddings represent text syntax, using high-dimensional arrays that contain very few non-zero values compared to dense embeddings. Sparse embeddings are often used for keyword searches. Embedding: An embedding is a type of vector that's used to represent data in a way that captures its semantic meaning. Embeddings are typically created using machine learning techniques, and they are often used in natural language processing (NLP) and other machine learning applications. Dense embeddings: Dense embeddings represent the semantic meaning of text, using arrays that mostly contain non-zero values. With dense embeddings, similar search results can be returned based on semantic similarity. Dense embeddings: Dense embeddings"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-9",
    "content": "represent the semantic meaning of text, using arrays that mostly contain non-zero values. With dense embeddings, similar search results can be returned based on semantic similarity. Sparse embeddings: Sparse embeddings represent text syntax, using high-dimensional arrays that contain very few non-zero values compared to dense embeddings. Sparse embeddings are often used for keyword searches. Sparse embeddings: Sparse embeddings represent text syntax, using high-dimensional arrays that contain very few non-zero values compared to dense embeddings. Sparse embeddings are often used for keyword searches. Hybrid search: Hybrid search uses both dense and sparse embeddings, which lets you search based on a combination of keyword search and semantic search. Vector Search supports search based on dense embeddings, sparse embeddings, and hybrid search. Hybrid search: Hybrid search uses both dense and sparse embeddings, which lets you search based on a combination of keyword search and semantic search. Vector Search supports search based on dense embeddings, sparse embeddings, and hybrid search. Index: A collection of vectors deployed together for similarity search. Vectors can be added to or removed from an index. Similarity search queries are issued to a specific index and search the vectors in that index. Index: A collection of vectors deployed together for similarity search. Vectors can be added to or removed from an index. Similarity search queries are issued to a specific index and search the vectors in that index. Ground truth: A term that refers to verifying machine learning for accuracy against the real world, like a ground truth dataset. Ground truth: A term that refers to verifying machine learning for accuracy against the real world, like a ground truth dataset."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/matching-engine",
    "title": "Vector SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/matching-engine#chunk-10",
    "content": "Recall: The percentage of nearest neighbors returned by the index that are actually true nearest neighbors. For example, if a nearest neighbor query for 20 nearest neighbors returned 19 of the ground truth nearest neighbors, the recall is 19/20x100 = 95%. Recall: The percentage of nearest neighbors returned by the index that are actually true nearest neighbors. For example, if a nearest neighbor query for 20 nearest neighbors returned 19 of the ground truth nearest neighbors, the recall is 19/20x100 = 95%. Restrict: Feature that limits searches to a subset of the index by using Boolean rules. Restrict is also referred to as \"filtering\". With Vector Search, you can use numeric filtering and text attribute filtering. Restrict: Feature that limits searches to a subset of the index by using Boolean rules. Restrict is also referred to as \"filtering\". With Vector Search, you can use numeric filtering and text attribute filtering. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/migrate/virtual-machines/docs/5.0",
    "title": "Migrate to Virtual Machines documentation",
    "chunk_id": "https://cloud.google.com/migrate/virtual-machines/docs/5.0#chunk-0",
    "content": "Home Migrate to Virtual Machines Documentation Migrate to Virtual Machines Migrate to Virtual Machines lets you migrate virtual machine (VM) instances and disks of VMs from different migration sources such as vSphere on-premises data center, AWS cloud computing services, Azure cloud computing services, and Google Cloud VMware Engine to VM instances or Persistent Disk volumes on Google Cloud.Learn more. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. VM Migration lifecycle VM Migration lifecycle Migrate to Virtual Machines Architecture Migrate to Virtual Machines Architecture Enabling Migrate to Virtual Machines services Enabling Migrate to Virtual Machines services Migrate from an on-premises VMware source Migrate from an on-premises VMware source Migrate from VMware Engine Migrate from VMware Engine Migrate from an AWS source Migrate from an AWS source Migrate from an Azure source Migrate from an Azure source Migrate individual VMs Migrate individual VMs Migrate VM disks Migrate VM disks Import virtual disk images Import virtual disk images Import machine images Import machine images Release notes Release notes Supported operating systems Supported operating systems Locations Locations Troubleshooting Troubleshooting Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/storage-transfer/docs/overview",
    "title": "What is Storage Transfer Service?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/storage-transfer/docs/overview#chunk-0",
    "content": "Home Cloud Storage Transfer Service Documentation Guides Storage Transfer Service enables seamless data movement across object and file storage systems, including: Amazon S3, Azure Blob Storage, or Cloud Storage to Cloud Storage On-premises storage to Cloud Storage, or Cloud Storage to on-premises Between on-premises storage systems From publicly-accessible URLs to Cloud Storage From HDFS to Cloud Storage Storage Transfer Service is optimized for transfers involving more than 1TiB of data. For smaller transfers, see ourrecommendations. With Storage Transfer Service, you can: Automate data transfers: Eliminate the need for manual processes and custom scripts. Transfer data at scale: Move petabytes of data quickly and reliably. Optimize network performance: Choose between Google-managed transfers for simplicity or self-hosted agents for granular control over network routing and bandwidth consumption. Support diverse storage systems: Transfer data seamlessly between cloud providers and on-premises environments. Migrating data to Cloud Storage: Storage Transfer Service can be used to migrate data from other cloud storage providers, on-premises data centers, or HTTP/HTTPS URLs to Cloud Storage. Backup: Replicate your data to Google Cloud, or create a copy of a Cloud Storage bucket in another region Data processing pipelines: Move data generated on other clouds, your data center, and the edge to Google Cloud for analytics usingBigQueryorDataproc. Archival: Move cold data from costly on-premises storage systems to Cloud Storage to reduce storage cost. Secure End-to-end encryption protects your data in transit. Storage Transfer Service supports TLS 1.3 for all HTTPS communication. Data integrity validation ensures that your data is not corrupted during transfer. Performant"
  },
  {
    "source_url": "https://cloud.google.com/storage-transfer/docs/overview",
    "title": "What is Storage Transfer Service?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/storage-transfer/docs/overview#chunk-1",
    "content": "Highly-parallelized architecture accelerates transfer speeds. Automatic retries and load balancing ensure reliable transfers. Fully managed No need to manage infrastructure or write code. Focus on your applications, not data transfer. Check to see if yoursource and sink combination is supported. There are a number of ways that you can work with Storage Transfer Service: The Google Cloud console REST APIs Thegcloudcommand-line tool Java and Python client libraries SeeCreate transfersto get started. Storage Transfer Service does not provide an SLA, including for transfer performance or latency, and some performance fluctuations may occur. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-17 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/get-started/",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/get-started/#chunk-0",
    "content": "Home Documentation Click to show or hide setup steps by job function: Establish administrators, billing accounts, and other settings in your Google Cloud environment. Cloud Quotas overview Google Cloud deployment archetypes (Architecture Center) Set up billing, spending notifications, and resource structure to facilitate cost monitoring and optimization. Monitor costs using billing reports Optimize costs with FinOps hub Resource hierarchy options for cost tracking Implement cost optimization strategies (Architecture Center) Start automating infrastructure and secure collaboration with teammates using Google Cloud tools and best practices. Observability in Google Cloud Terraform and Infrastructure Manager CI/CD pipeline for containerized apps (Architecture Center) Get basic API access and set up a development environment that can interact with Google Cloud services. Build a generative AI application Analyze sample data using Google Cloud products with minimal setup. Set up the bq command-line tool Gemini in BigQuery overview Data analytics design patterns (Architecture Center) Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction",
    "title": "Introduction to Vertex AI SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction#chunk-0",
    "content": "Home AI Applications Documentation Guides This page introduces the key search and recommendations features of Vertex AI Search. For information about search and recommendations for media, seeIntroduction to AI Applications for media. Vertex AI Search brings together the power of deep information retrieval, state-of-the-art natural language processing, and the latest in large language model (LLM) processing to understand user intent and return the most relevant results for the user. With Vertex AI Search, you can build a Google-quality search app on data you control. You also have the option to use the search results that you retrieve to ground generative AI LLM responses. For more information, see the blog postYour RAG powered by Google Search. With recommendations, you can build a recommendations app across your data that suggests content similar to the content that the user is viewing. Vertex AI Search makes it easy to get started with high-quality search or recommendations based on data that you provide. As part of the setup experience, you can: Use your existing Google Account or sign up for one. Use your existing Google Cloud project or create one. Create an app and attach a data store to it. Provide data to search or recommend by entering the URLs for your website content, importing your data from BigQuery or Cloud Storage, or importing FHIR R4 data from Cloud Healthcare API, or uploading through RESTful CRUD APIs. Syncing data from third-party data sources is available in Preview with allowlist. Embed JavaScript widgets and API samples to integrate search or recommendations into your website or applications. With Vertex AI Search, you create a search or recommendations app and attach it to a data store. You import your data into a data store and index your data."
  },
  {
    "source_url": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction",
    "title": "Introduction to Vertex AI SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction#chunk-1",
    "content": "Apps and data stores have a one-to-one relationship. There are various kinds of data stores that you can create, based on the type of data you use. Each data store can contain one type of data: Website data: You can provide domains such asyourexamplewebsite.com/faqandyourexamplewebsite.com/eventsand enable search over the content at those domains. Website data: You can provide domains such asyourexamplewebsite.com/faqandyourexamplewebsite.com/eventsand enable search over the content at those domains. Structured data: A data store with structured data enables hybrid search (keyword and semantic) or recommendations over structured data such as a BigQuery table or NDJSON files. For example, you can enable search or recommendations over a product catalog for your ecommerce experience, a movie catalog for movie search or recommendations, or a directory of doctors for provider search or recommendations. Structured data: A data store with structured data enables hybrid search (keyword and semantic) or recommendations over structured data such as a BigQuery table or NDJSON files. For example, you can enable search or recommendations over a product catalog for your ecommerce experience, a movie catalog for movie search or recommendations, or a directory of doctors for provider search or recommendations. Structured data for media: A data store with a structured data schema that is specific for the media industry. For example, a data store for media might contain information about videos, news articles, music files, or podcasts. Structured data for media: A data store with a structured data schema that is specific for the media industry. For example, a data store for media might contain information about videos, news articles, music files, or podcasts. Unstructured data: An"
  },
  {
    "source_url": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction",
    "title": "Introduction to Vertex AI SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction#chunk-2",
    "content": "unstructured data store enables hybrid search (keyword and semantic) over data such as documents and images. For example, a financial institution can enable search over their private corpus (index) of financial research publications, or a biotech company can enable search over their private repository of medical research. Unstructured data: An unstructured data store enables hybrid search (keyword and semantic) over data such as documents and images. For example, a financial institution can enable search over their private corpus (index) of financial research publications, or a biotech company can enable search over their private repository of medical research. Healthcare data: A healthcare data store enables hybrid search (keyword and semantic) over healthcare FHIR R4 data imported from Cloud Healthcare API. For example, a healthcare provider can search over a patient's clinical history using exploratory queries. Healthcare data: A healthcare data store enables hybrid search (keyword and semantic) over healthcare FHIR R4 data imported from Cloud Healthcare API. For example, a healthcare provider can search over a patient's clinical history using exploratory queries. For more information, seeAbout apps and data stores. You can implement Vertex AI Search in any of the following ways: Use the Google Cloud console.Use theAI Applicationspage of the console for a quick-start experience using a web interface. From the console, you can create your search app, import your data, test the user experience, and view analytics. Use the AI Applications API.Use the AI Applications API when you're ready to integrate search or recommendations into your website or applications. Use both the Google Cloud console and the API.You can set up your app and import your data using the console,"
  },
  {
    "source_url": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction",
    "title": "Introduction to Vertex AI SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction#chunk-3",
    "content": "for example, and then use the API to test the user experience and integrate it into your website or application. About apps and data stores Get started with generic search Get started with generic recommendations Get started with media search Get started with media recommendations Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-21 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/cross-product-overviews",
    "title": "Access and resource management",
    "chunk_id": "https://cloud.google.com/docs/cross-product-overviews#chunk-0",
    "content": "Home Documentation Organize, analyze, and manage access to your Google Cloud resources and services. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Define who can access resources in your organization. Manage internal enterprise solutions and Google Cloud APIs. Optimize your service usage, monitor application and resource health, and identify disruptive events. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/security",
    "title": "Security",
    "chunk_id": "https://cloud.google.com/docs/security#chunk-0",
    "content": "Home Documentation Google Cloud security products help organizations secure their cloud environment, protect their data, and comply with industry regulations. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Handle key management for secrets, disks, images, and log retention. Centrally manage network resources, establish scalable segmentation for different security zones, and detect network threats. Protect your workloads against denial-of-service attacks, web application attacks, and other security threats. Detect vulnerabilities, threats, and misconfigurations. Provide unified, federated identity with least privilege policies to reduce the risk of data breaches and other security incidents. Collect, store, analyze, and monitor your organization's aggregated platform and system logs with a comprehensive solution. Manage your resources in a secure and compliant way with visibility and control over your cloud environment. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-0",
    "content": "Home Compute Engine Documentation Guides This document describes the features of Google Cloud Hyperdisk. Hyperdisk is the fastest and most efficient durable disk for Compute Engine. If you need boot or data disks for your compute instances\u2014virtual machine (VM) instances, containers, and bare metal instances\u2014Google recommends using Hyperdisk. For information about the other block storage options in Compute Engine, seeChoose a disk type. To create a new Hyperdisk volume, seeCreate a Hyperdisk volume. With Hyperdisk you can provision, manage, and scale your Compute Engine workloads without the cost and complexity of a typical on-premises storage area network (SAN). Hyperdisk volumes have the following features: Function as physical disks: you can use a Hyperdisk volume with a compute instance as if it were a physical disk attached to the instance. When you read to or write from a Hyperdisk volume, data is transmitted over the network. Function as physical disks: you can use a Hyperdisk volume with a compute instance as if it were a physical disk attached to the instance. When you read to or write from a Hyperdisk volume, data is transmitted over the network. Higher performance: Hyperdisk offers higher IOPS and throughput than Persistent Disk by leveraging Google's Titanium storage offload technology. Higher performance: Hyperdisk offers higher IOPS and throughput than Persistent Disk by leveraging Google's Titanium storage offload technology. Customizable performance: you can choose the performance\u2014IOPS and/or throughput\u2014of each Hyperdisk volume. You can also increase or decrease a Hyperdisk volume's performance while it's in use. Customizable performance: you can choose the performance\u2014IOPS and/or throughput\u2014of each Hyperdisk volume. You can also increase or decrease a"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-1",
    "content": "Hyperdisk volume's performance while it's in use. Support for high availability: in the unlikely event of a zonal or regional outage, you can ensure high availability for your data by enabling one or both of the following features:To protect data your data in case of a zonal outage, useHyperdisk Balanced High Availability. Data on Hyperdisk Balanced High Availability volumes is synchronously replicated across two zones within the same region to protect against up to one zonal outage.To protect your data from a regional outage, maintain a replica of your data in another region by usingAsynchronous Replication. When you enable Asynchronous Replication for a disk, data in one region is continuously copied to a replica in a secondary region. If a regional outage occurs, you canfailoveryour data to a secondary region. Asynchronous Replication is available for Hyperdisk Balanced, Hyperdisk Balanced High Availability, and Hyperdisk Extreme volumes. Support for high availability: in the unlikely event of a zonal or regional outage, you can ensure high availability for your data by enabling one or both of the following features: To protect data your data in case of a zonal outage, useHyperdisk Balanced High Availability. Data on Hyperdisk Balanced High Availability volumes is synchronously replicated across two zones within the same region to protect against up to one zonal outage. To protect data your data in case of a zonal outage, useHyperdisk Balanced High Availability. Data on Hyperdisk Balanced High Availability volumes is synchronously replicated across two zones within the same region to protect against up to one zonal outage. To protect your data from a regional outage, maintain a replica of your data in another region by usingAsynchronous Replication. When you enable"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-2",
    "content": "Asynchronous Replication for a disk, data in one region is continuously copied to a replica in a secondary region. If a regional outage occurs, you canfailoveryour data to a secondary region. Asynchronous Replication is available for Hyperdisk Balanced, Hyperdisk Balanced High Availability, and Hyperdisk Extreme volumes. To protect your data from a regional outage, maintain a replica of your data in another region by usingAsynchronous Replication. When you enable Asynchronous Replication for a disk, data in one region is continuously copied to a replica in a secondary region. If a regional outage occurs, you canfailoveryour data to a secondary region. Asynchronous Replication is available for Hyperdisk Balanced, Hyperdisk Balanced High Availability, and Hyperdisk Extreme volumes. Portability: you can change the compute instance that a Hyperdisk volume is attached to. Portability: you can change the compute instance that a Hyperdisk volume is attached to. Shareable between VMs: for high availability workloads, certain Hyperdisk types can be shared by multiple VMs. Each VM has simultaneous read-write or read-only access to the volume. Shareable between VMs: for high availability workloads, certain Hyperdisk types can be shared by multiple VMs. Each VM has simultaneous read-write or read-only access to the volume. Support for pooled capacity and performance: to simplify planning, avoid overprovisioning storage, and reduce costs, you can purchase Hyperdisk storage and performance in bulk by using Hyperdisk Storage Pools. Support for pooled capacity and performance: to simplify planning, avoid overprovisioning storage, and reduce costs, you can purchase Hyperdisk storage and performance in bulk by using Hyperdisk Storage Pools. To add Hyperdisk volumes to your workloads, you"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-3",
    "content": "must choose a Hyperdisk type. Each Hyperdisk type is designed and optimized for a specific type of workload. The following is a list of the available Hyperdisk types. Hyperdisk Balanced Hyperdisk Balanced High Availability Hyperdisk Extreme Hyperdisk Throughput Hyperdisk ML For most workloads, we recommend Hyperdisk Balanced. To select a Hyperdisk type, compare your workload's type and its performance requirements with the information in the following table. For detailed information about a specific Hyperdisk type, see the linked page in theRecommended Hyperdisk typecolumn. Most enterprise applications Boot disks Virtual desktops Postgres, MySQL Designed to be the best fit for the majority of workloads Best combination of price and performance Supports simultaneous read-write access to the same volume from up to 8 instances Highly-available, mission-critical applications that require arecovery point objectiveof 0 Offers data replication in two zones within the same region for quick failover Supports simultaneous read-write access to the same volume from up to 8 instances SAP HANA High-end SQL Server, Oracle, and in-memory RDBMS Offers the highest IOPS Designed for workloads that need more than 5,000 MiB/s of throughput or 350,000 IOPS, such as:High-performance computing (HPC)Machine learning, AI inference or trainingAccelerator-optimized workloads High-performance computing (HPC) Machine learning, AI inference or training Accelerator-optimized workloads Supports attaching a single volume in read-only mode to up to 2500 instances. Offers the highest read-only throughput Scale out analytics workloads like Hadoop, Spark, and Kafka Cold disks High throughput for bandwidth and capacity-intensive applications that don't need high IOPS Cost-effective data disks for cost-"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-4",
    "content": "sensitive applications 1You can't specify a throughput level for Hyperdisk Extreme volumes. The provisioned throughput is based on the IOPS level you specify.2You can't specify an IOPS level for Hyperdisk Throughput and Hyperdisk ML volumes. The provisioned IOPS is based on the throughput level you specify. The following is a summary of key Hyperdisk performance concepts: You can configure the performance (IOPS and/or throughput) limit and size of each Hyperdisk volume. You can also increase or decrease a Hyperdisk volume's performance without changing its size. The performance limit you specify is referred to as theprovisioned performance. The provisioned performance isn't the expected performance, rather, it's the maximum performance the disk can achieve. The actual performance for a Hyperdisk volume is the observed performance while the volume is in use. For a Hyperdisk volume to reach its provisioned performance, you must attach it to a compute instance that supports the same level of performance or higher. For a discussion of how Hyperdisk performance works, seeAbout Hyperdisk performance.For performance limits for each Hyperdisk type, seeHyperdisk performance limits. Each Hyperdisk type has different latency profiles. Google recommends comparing Hyperdisk Throughput to the latency of a hard disk drive. You can compare the latency for Hyperdisk Balanced, Hyperdisk Balanced High Availability, Hyperdisk Extreme, and Hyperdisk ML to the latency of enterprise SSDs. Hyperdisk Balanced and Hyperdisk Extreme offer sub-millisecond latency. This section lists themachine seriesthat each Hyperdisk type supports. If a machine series doesn't support Hyperdisk, use Persistent Disk. Select one or more machine series to see the supported Hyperdisk types."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-5",
    "content": "C4AC4C4D(Preview)C3C3DN4N2N2DN1T2DT2AE2Z3H3C2C2DX4M4M3M2M1N1+GPUA4A3 (H200)A3 (H100)A2G2 This section lists the restrictions that apply to the machine series that each Hyperdisk type supports. To use Hyperdisk Balanced with A3 VMs, the VM must have at least 8 GPUs. To use Hyperdisk Balanced with A3 VMs, the VM must have at least 8 GPUs. For Hyperdisk Extreme, the following restrictions apply:A3 machine types require at least 4 GPUs.C3 machine type require at least 88 vCPUs.C3D machine types require at least 60 vCPUs.C4 machine types require at least 96 vCPUs.M1 machine types require at least 80 vCPUs.C4A, C4D (Preview), and M3 machine types require at least 64 vCPUs.M4 machine types require at least 112 vCPUs.N2 requires 80 or more vCPUs; Custom N2 machine types aren't supported. For Hyperdisk Extreme, the following restrictions apply: A3 machine types require at least 4 GPUs. C3 machine type require at least 88 vCPUs. C3D machine types require at least 60 vCPUs. C4 machine types require at least 96 vCPUs. M1 machine types require at least 80 vCPUs. C4A, C4D (Preview), and M3 machine types require at least 64 vCPUs. M4 machine types require at least 112 vCPUs. N2 requires 80 or more vCPUs; Custom N2 machine types aren't supported. You can't use Hyperdisk Throughput withc3-*-metalmachine types. You can't use Hyperdisk Throughput withc3-*-metalmachine types. You can share a Hyperdisk volume between multiple VMs by simultaneously attaching the same volume to multiple VMs. The following scenarios are supported: Concurrent read-write access to a single volume from multiple VMs. Recommended for clustered file systems and highly available workloads like SQL Server Failover Cluster Infrastructure. Supported for Hyperdisk Balanced and Hyperdisk Balanced High Availability"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-6",
    "content": "volumes. Concurrent read-write access to a single volume from multiple VMs. Recommended for clustered file systems and highly available workloads like SQL Server Failover Cluster Infrastructure. Supported for Hyperdisk Balanced and Hyperdisk Balanced High Availability volumes. Concurrent read-only access to a single volume from multiple VMs. This is more cost effective than having multiple disks with the same data. Recommended for accelerator-optimized machine learning workloads. Supported for Hyperdisk ML volumes. Concurrent read-only access to a single volume from multiple VMs. This is more cost effective than having multiple disks with the same data. Recommended for accelerator-optimized machine learning workloads. Supported for Hyperdisk ML volumes. You can't attach a Hyperdisk Throughput or Hyperdisk Extreme volume to more than one VM. To learn about disk sharing, seeShare a disk between VMs. You can protect your data in the rare event of a zonal or regional outage by enabling replication, that is, maintaining a copy of the data in another zone or region. To replicate data to another zone within the same region, you must use Hyperdisk Balanced High Availability volumes. Hyperdisk Balanced High Availability is the only supported Hyperdisk type for zonal replication. For more information, seeAbout synchronous disk replication. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-7",
    "content": "east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-8",
    "content": "design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-9",
    "content": "Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-10",
    "content": "VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-11",
    "content": "information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-12",
    "content": "interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-13",
    "content": "charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode,"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-14",
    "content": "seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-15",
    "content": "across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-16",
    "content": "data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-17",
    "content": "Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-18",
    "content": "outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-19",
    "content": "about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-20",
    "content": "creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-21",
    "content": "writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-22",
    "content": "ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-23",
    "content": "losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-24",
    "content": "the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-25",
    "content": "ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-26",
    "content": "Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-27",
    "content": "instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-28",
    "content": "MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-29",
    "content": "Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-30",
    "content": "loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-31",
    "content": "make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-32",
    "content": "for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-33",
    "content": "example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-34",
    "content": "the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-35",
    "content": "Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-36",
    "content": "Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-37",
    "content": "Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-38",
    "content": "agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally,"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-39",
    "content": "you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-40",
    "content": "or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4.You can use Asynchronous Replication with the following Hyperdisk types:Hyperdisk BalancedHyperdisk ExtremeHyperdisk Balanced High AvailabilityTo learn more about cross-regional replication, seeAsynchronous Replication.Encryption for Hyperdisk volumesBy default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK).For more information, seeAbout disk encryption.Confidential Computing with Hyperdisk volumesYou can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-41",
    "content": "Hyperdisk Balanced disks that are attached to Confidential VMs.For more information, seeConfidential mode for Hyperdisk Balanced volumes.Durability of HyperdiskCompute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance.Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes many steps to mitigate the industry-wide risk of silent data corruption.Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type.Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability.The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one.Note:Durability is an aggregate for each disk type, and doesn't represent a financially backed service level agreement (SLA).Hyperdisk BalancedHyperdisk ExtremeHyperdisk MLHyperdisk ThroughputHyperdisk Balanced High AvailabilityBetter than 99.999%Better than 99.9999%Better than 99.999%Better than 99.999%Better than 99.9999%Supported disk interfacesHyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance.Hyperdisk Storage PoolsHyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-42",
    "content": "simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency.Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need.You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools:Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk Balanced disksHyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disksFor information about using Hyperdisk Storage Pools, seeAbout storage pools.PricingYou are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following:Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput.Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS.Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume.Hyperdisk Throughput charges a"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-43",
    "content": "monthly rate based on the provisioned throughput (in MiB/s).Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage.For more pricing information, seeDisk pricing.Hyperdisk and committed use discountsHyperdisk volumes are not eligible for:Resource-based committed use discounts (CUDs)Sustained use discounts (SUDs)Hyperdisk and preemptible VM instancesHyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk.Limitations for HyperdiskYou can't create amachine imagefrom a Hyperdisk volume.You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst.You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume.You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume.You can't attach Hyperdisk Throughput or Hyperdisk Extreme volumes to more than one VM.Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks.You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds.You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode.If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode.If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations.Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput.What's next?Learn how tocreate a Hyperdisk volume.Learn how toclone a Hyperdisk volume.Learn aboutsynchronous disk replicationwith"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-44",
    "content": "Hyperdisk Balanced High Availability.Learn aboutHyperdisk Storage Pools.ReviewDisk pricinginformation.Learn how tooptimize performance of Hyperdisk. You can protect your data in the unlikely event of a regional outage by enabling Asynchronous Replication. Asynchronous Replication maintains a copy of the data on your volume in another region. For example, to protect a Hyperdisk volume inus-west1, you can use Asynchronous Replication to replicate the volume to a secondary volume in theus-east4region. If the volume inus-west1became unavailable, then you could use the secondary volume inus-east4. You can use Asynchronous Replication with the following Hyperdisk types: Hyperdisk Balanced Hyperdisk Extreme Hyperdisk Balanced High Availability To learn more about cross-regional replication, seeAsynchronous Replication. By default, Compute Engine protects your Hyperdisk volumes with Google-owned and Google-managed encryption keys. You can also encrypt your Hyperdisk volumes with customer-managed encryption keys (CMEK). For more information, seeAbout disk encryption. You can add hardware-based encryption to a Hyperdisk Balanced disk by enabling Confidential mode for the disk when you create it. You can use Confidential mode only with Hyperdisk Balanced disks that are attached to Confidential VMs. For more information, seeConfidential mode for Hyperdisk Balanced volumes. Compute Engine distributes the data on Hyperdisk volumes across several physical disks to ensure durability and optimize performance. Disk durability represents the probability of data loss, by design, for a typical disk in a typical year. Hyperdisk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google takes"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-45",
    "content": "many steps to mitigate the industry-wide risk of silent data corruption. Durability is calculated with a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type. Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Hyperdisk durability. The table below shows durability for each disk type's design. 99.999% durability means that with 1,000 Hyperdisk volumes, you would likely go a hundred years without losing a single one. Hyperdisk volumes are mounted as a disk on a VM using the NVMe or SCSI interface, depending on the machine type of the instance. Hyperdisk Storage Pools make it easier to lower your block storage total cost of ownership and simplify block storage management. With Hyperdisk Storage Pools, you can share a pool of capacity and performance across a maximum of 1,000 disks in a single project. Because storage pools offer thin-provisioning and data reduction, you can achieve higher efficiency. Storage pools simplify migrating your on-premises SAN to the cloud, and also make it easier to provide your workloads with the capacity and performance that they need. You create a storage pool with the estimated capacity and performance for all workloads in a project in a specific zone. You then create disks in this storage pool and attach the disks to existing VMs. You can also create a disk in the storage pool as part of creating a new VM. Each storage pool contains one type of disk, such as Hyperdisk Throughput. There are two types of Hyperdisk Storage Pools: Hyperdisk Balanced Storage Pool: for general purpose workloads that are best served by Hyperdisk"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-46",
    "content": "Balanced disks Hyperdisk Throughput Storage Pool: for streaming, cold data, and analytics workloads that are best served by Hyperdisk Throughput disks For information about using Hyperdisk Storage Pools, seeAbout storage pools. You are billed for the total provisioned capacity of your Hyperdisk volumes until you delete them. You are charged per GiB per month. Additionally, you are billed for the following: Hyperdisk Balanced charges a monthly rate for the provisioned IOPS and provisioned throughput (in MiB/s) in excess of the baseline values of 3,000 IOPS and 140 MiB/s throughput. Hyperdisk Extreme charges a monthly rate based on the provisioned IOPS. Hyperdisk ML charges a monthly rate based on the provisioned throughput (in MiB/s). There is no additional charge for attaching multiple VMs to a single Hyperdisk ML volume. Hyperdisk Throughput charges a monthly rate based on the provisioned throughput (in MiB/s). Because the data for regional disks is written to two locations, the cost of Hyperdisk Balanced High Availability storage is twice the cost of Hyperdisk Balanced storage. For more pricing information, seeDisk pricing. Hyperdisk volumes are not eligible for: Resource-based committed use discounts (CUDs) Sustained use discounts (SUDs) Hyperdisk can be used with Spot VMs (or preemptible VMs). However, there are no discounted spot prices for Hyperdisk. You can't create amachine imagefrom a Hyperdisk volume. You can't back up a disk in multi-writer mode with snapshots or images. You mustdisable multi-writer modefirst. You can'tcreate an imagefrom a Hyperdisk Extreme, Hyperdisk Throughput, or Hyperdisk Balanced High Availability volume. You can't create an instant snapshot from a Hyperdisk ML or Hyperdisk Throughput volume. You can't attach Hyperdisk Throughput or"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/hyperdisks",
    "title": "Google Cloud Hyperdisk overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/hyperdisks#chunk-47",
    "content": "Hyperdisk Extreme volumes to more than one VM. Hyperdisk Extreme, Hyperdisk ML and Hyperdisk Throughput volumes can't be used as boot disks. You can attach a Hyperdisk ML volume to up to 100 VMs at most once every 30 seconds. You can't create a Hyperdisk ML disk in read-write mode from a snapshot or a disk image. You must create the disk in read-only mode. If you enable read-only mode for a Hyperdisk ML volume, you can't re-enable read-write mode. If you create a Hyperdisk Balanced volume in Confidential mode, seeadditional limitations. Confidential VMs with AMD SEV on C3D machine types don't support Hyperdisk Balanced and Hyperdisk Throughput. Learn how tocreate a Hyperdisk volume. Learn how toclone a Hyperdisk volume. Learn aboutsynchronous disk replicationwith Hyperdisk Balanced High Availability. Learn aboutHyperdisk Storage Pools. ReviewDisk pricinginformation. Learn how tooptimize performance of Hyperdisk. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-0",
    "content": "Home BigQuery Documentation Guides BigQuery sharing (formerly Analytics Hub) is a data exchange platform that lets you share data and insights at scale across organizational boundaries with a robust security and privacy framework. Sharing lets you discover and access a data library curated by various data providers. This data library also includes Google-provided datasets. For example, you can use sharing to augment your analytics and ML initiatives with third-party and Google datasets. Analytics Hub Identity and Access Management (IAM) roles let you perform the following sharing tasks: The Analytics Hub Publisher role lets you monetize data by sharing it with your partner network or within your own organization in real time.Listingslet you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences. You can also manage subscriptions and view the usage metrics for your listings. The Analytics Hub Publisher role lets you monetize data by sharing it with your partner network or within your own organization in real time.Listingslet you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences. You can also manage subscriptions and view the usage metrics for your listings. The Analytics Hub Subscriber role lets you discover the data that you are looking for, combine shared data with your existing data, and use thebuilt-in features of BigQuery. When you subscribe to a listing, alinked datasetor linked Pub/Sub subscription is created in your project. You can manage your subscriptions by using theSubscription resource, which stores relevant"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-1",
    "content": "information about the subscriber and represents the connection between publisher and subscriber. The Analytics Hub Subscriber role lets you discover the data that you are looking for, combine shared data with your existing data, and use thebuilt-in features of BigQuery. When you subscribe to a listing, alinked datasetor linked Pub/Sub subscription is created in your project. You can manage your subscriptions by using theSubscription resource, which stores relevant information about the subscriber and represents the connection between publisher and subscriber. The Analytics Hub Viewer role lets you browse through the resources that you have access to in Sharing and request the publisher to access the shared data. The Analytics Hub Viewer role lets you browse through the resources that you have access to in Sharing and request the publisher to access the shared data. The Analytics Hub Admin role lets you createdata exchangesthat enable data sharing, and then give permissions to data publishers and subscribers to access these data exchanges. The Analytics Hub Admin role lets you createdata exchangesthat enable data sharing, and then give permissions to data publishers and subscribers to access these data exchanges. For more information, seeConfigure Analytics Hub roles. Sharing is built on a publish and subscribe model of Google Cloud data resources, allowing for zero-copy sharing in place. Sharing supports the following Google Cloud resources: BigQuery datasets Pub/Sub topics The following diagram describes how a publisher shares assets: The following sections describe the features in this workflow. Shared resources are the unit of sharing by a publisher in BigQuery sharing. A shared dataset is a BigQuery dataset that is the unit of data sharing in BigQuery sharing. The"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-2",
    "content": "separation of compute and storage in the BigQuery architecture enables data publishers to share datasets with as many subscribers as they want, without having to make multiple copies of the data. As a publisher, you create or use an existing BigQuery dataset in your project with the following supported objects that you want to deliver to your subscribers: Authorized views Authorized datasets BigQuery ML models External tables Materialized views RoutinesNot all routines are supported in shared datasets. For more information, seeLimitations. Routines Not all routines are supported in shared datasets. For more information, seeLimitations. Tables Tables Table snapshots Table snapshots Views Views Shared datasets supportcolumn-level securityandrow-level security. A shared topic is aPub/Sub topicthat is the unit ofstreaming data sharing in BigQuery. As a publisher, you create or use an existing Pub/Sub topic in your project and distribute that with your subscribers. A data exchange is a container that enables self-service data sharing. It contains listings that reference shared resources. Publishers and administrators can grant access to subscribers at the exchange and the listing level. This method helps to avoid granting access on the underlying shared resources explicitly. A subscriber can browse through data exchanges, discover data that they can access, and subscribe to shared resources. When youcreate a data exchange, you can assign a primary contact email to it. The primary contact email provides a way for users to contact the owner of a data exchange with questions or concerns about the data exchange. A data exchange can be of the following types: Private data exchange.By default, a data exchange is private and only users or groups that have access to that exchange"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-3",
    "content": "can view or subscribe to its listings. Public data exchange.By default, a data exchange is private and only users or groups that have access to that exchange can view or subscribe to its listings. However, you can choose to make a data exchange public. Listings in public data exchanges can bediscoveredandsubscribed tobyGoogle Cloud users (allAuthenticatedUsers). For more information about public data exchanges, seeMake a data exchange public. The Analytics Hub Admin role lets you create multiple data exchanges and manage other users performing sharing tasks. A listing is a reference to a shared resource that a publisher lists in a data exchange. As a publisher, you can create a listing and specify the resource description, sample queries to run or sample message data, links to any relevant documentation, and any additional information that can help subscribers to use your shared resource. When you create a listing, you can assign a primary contact email, a provider name and contact, and a publisher name and contact. The primary contact email provides a way for users to contact the owner of a listing with questions or concerns about the data exchange. The provider name and contact is the information of the agency that originally provided the data for the listing. This information is optional. The publisher name and contact is the agency that published the data for use in BigQuery sharing. This information is optional. For more information, seeManage listings. A listing can be of the following two types based on the Identity and Access Management (IAM) policy that is set for the listing and the type of data exchange that contains the listing: Public listing.It is shared with allGoogle Cloud users (allAuthenticatedUsers). Listings in a public data exchange are public"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-4",
    "content": "listings. These listings can be references of afree public resourceor acommercial resource. If the listing is of a commercial resource, subscribers can either request access to the listing directly from the data provider, or they can browse and purchaseGoogle Cloud Marketplace-integrated commercial listings. Private listing.It is shared directly with individuals or groups. For example, a private listing can reference marketing metrics dataset that you share with other internal teams within your organization. The following diagram describes how subscribers interact with shared resources: The following sections describe the features in the subscriber workflow. Linked resources are created when subscribing to a Sharing listing, connecting a subscriber to the underlying shared resource. A linked dataset is aread-onlyBigQuery dataset that serves as a pointer or reference to a shared dataset. Subscribing to a listing creates a linked dataset in your project and not a copy of the dataset, so subscribers can read the data but cannot add or update objects within it. When you query objects such as tables and views through a linked dataset, the data from the shared dataset is returned. For more information about linked datasets, seeView and subscribe to listings. Linked datasets are authorized to access tables and views of a shared dataset. Subscribers with linked datasets access tables and views of a shared dataset without any additional Identity and Access Management authorization. Linked datasets supports the following objects: Authorized views Authorized datasets Authorized routines Subscribing to a listing with a shared topic creates a linked Pub/Sub subscription in the subscriber project. No copies of the shared topic or message data are created. Subscribers of the"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-5",
    "content": "linkedPub/Sub subscriptioncan access the messages published to the shared topic. Subscribers access message data of a shared topic without any additional Identity and Access Management authorization. Publishers can manage subscriptions both in Pub/Sub directly or through Sharing subscription management. For more information about linked Pub/Sub subscriptions, seeStream sharing with Pub/Sub. Data egress options let publishers restrict the export by subscribers of data out of BigQuery linked datasets. Publishers can enable data egress restriction on a listing, the results of a query, or both. When data egress is restricted, the following restrictions are applied: Copy, clone, export, and snapshot APIs are disabled. Copy, clone, export, and snapshot APIs are disabled. Copy, clone, export, and snapshot options in the Google Cloud console are disabled. Copy, clone, export, and snapshot options in the Google Cloud console are disabled. Connecting the restricted dataset to the table explorer is disabled. Connecting the restricted dataset to the table explorer is disabled. BigQuery Data Transfer Service is disabled on the restricted dataset. BigQuery Data Transfer Service is disabled on the restricted dataset. CREATE TABLE AS SELECTstatementsandwriting to a destination tableare disabled. CREATE TABLE AS SELECTstatementsandwriting to a destination tableare disabled. CREATE VIEW AS SELECTstatementsand writing to a destination view are disabled. CREATE VIEW AS SELECTstatementsand writing to a destination view are disabled. When youcreate a listing, you can set the appropriate data egress options. Sharing has the following limitations: A shared dataset can have a maximum of 1,000 linked datasets. A shared dataset can have a maximum of 1,000 linked datasets. A shared topic can have"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-6",
    "content": "amaximumof 10,000 Pub/Sub subscriptions. This limit includes linked Pub/Sub subscriptions and Pub/Sub subscriptions created outside of Sharing (for example, directly from Pub/Sub). A shared topic can have amaximumof 10,000 Pub/Sub subscriptions. This limit includes linked Pub/Sub subscriptions and Pub/Sub subscriptions created outside of Sharing (for example, directly from Pub/Sub). A dataset with unsupported resources cannot be selected as a shared dataset when youcreate a listing. For more information about the BigQuery objects that Sharing supports, seeShared datasetsin this document. A dataset with unsupported resources cannot be selected as a shared dataset when youcreate a listing. For more information about the BigQuery objects that Sharing supports, seeShared datasetsin this document. You can't setIAM rolesorIAM policieson individual tables within a linked dataset. Apply them at the linked dataset level instead. You can't setIAM rolesorIAM policieson individual tables within a linked dataset. Apply them at the linked dataset level instead. You can't attachIAM tagson tables within a linked dataset. Apply them at the linked dataset level instead. You can't attachIAM tagson tables within a linked dataset. Apply them at the linked dataset level instead. Linked datasets created before July 25, 2023, aren't backfilled by thesubscription resource. Only subscriptions created after July 25, 2023 work with the API methods. Linked datasets created before July 25, 2023, aren't backfilled by thesubscription resource. Only subscriptions created after July 25, 2023 work with the API methods. If you are a publisher, the following BigQuery interoperability limitations apply:Subscribers must be given explicit permissions to read the source dataset to be able to query views within"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-7",
    "content": "linked datasets. To grant access to views, as a best practice publishers shouldcreate authorized views. Authorized views can grant subscribers access to the view data without giving them access to the underlying source data.Thequery planreveals the shared view query and the routine query, including project IDs, and other datasets involved in authorized views. Never include anything such as encryption keys that you consider sensitive in the shared view or routine query.Shared datasets are indexed inData Catalog(deprecated) andBigQuery universal catalog. Updates on a shared dataset, such as adding tables or views, are made available to subscribers without any delay. However, in certain scenarios, for example, when there are more than one hundred subscribers or tables in a shared dataset, the updates might take up to 18 hours to get indexed in these services. Due to the delay in indexing, subscribers cannot search for these updated resources in the Google Cloud console immediately.Shared topics are indexed in Data Catalog (deprecated) anduniversal catalog, but you cannot filter specifically for its resource type.If you have set uprow-level securityordata maskingpolicies on the tables that are listed, then subscribers must be an Enterprise or Enterprise Plus customer to run the query job on the linked dataset. For information about editions, seeIntroduction to BigQuery editions. If you are a publisher, the following BigQuery interoperability limitations apply: Subscribers must be given explicit permissions to read the source dataset to be able to query views within linked datasets. To grant access to views, as a best practice publishers shouldcreate authorized views. Authorized views can grant subscribers access to the view data without giving them access to the underlying"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-8",
    "content": "source data. Subscribers must be given explicit permissions to read the source dataset to be able to query views within linked datasets. To grant access to views, as a best practice publishers shouldcreate authorized views. Authorized views can grant subscribers access to the view data without giving them access to the underlying source data. Thequery planreveals the shared view query and the routine query, including project IDs, and other datasets involved in authorized views. Never include anything such as encryption keys that you consider sensitive in the shared view or routine query. Thequery planreveals the shared view query and the routine query, including project IDs, and other datasets involved in authorized views. Never include anything such as encryption keys that you consider sensitive in the shared view or routine query. Shared datasets are indexed inData Catalog(deprecated) andBigQuery universal catalog. Updates on a shared dataset, such as adding tables or views, are made available to subscribers without any delay. However, in certain scenarios, for example, when there are more than one hundred subscribers or tables in a shared dataset, the updates might take up to 18 hours to get indexed in these services. Due to the delay in indexing, subscribers cannot search for these updated resources in the Google Cloud console immediately. Shared datasets are indexed inData Catalog(deprecated) andBigQuery universal catalog. Updates on a shared dataset, such as adding tables or views, are made available to subscribers without any delay. However, in certain scenarios, for example, when there are more than one hundred subscribers or tables in a shared dataset, the updates might take up to 18 hours to get indexed in these services. Due to the delay in indexing,"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-9",
    "content": "subscribers cannot search for these updated resources in the Google Cloud console immediately. Shared topics are indexed in Data Catalog (deprecated) anduniversal catalog, but you cannot filter specifically for its resource type. Shared topics are indexed in Data Catalog (deprecated) anduniversal catalog, but you cannot filter specifically for its resource type. If you have set uprow-level securityordata maskingpolicies on the tables that are listed, then subscribers must be an Enterprise or Enterprise Plus customer to run the query job on the linked dataset. For information about editions, seeIntroduction to BigQuery editions. If you have set uprow-level securityordata maskingpolicies on the tables that are listed, then subscribers must be an Enterprise or Enterprise Plus customer to run the query job on the linked dataset. For information about editions, seeIntroduction to BigQuery editions. If you are a subscriber, the following BigQuery interoperability limitations apply:Materialized views that refer to tables in the linked dataset aren't supported.Takingsnapshotsof linked dataset tables isn't supported.Queries with linked datasets andJOINstatements that are larger than 1 TB (physical storage) might fail. You cancontact supportto resolve this issue.You cannot useregion qualifierswithINFORMATION_SCHEMAviews toview metadata for your linked dataset.When querying forroutinesin a linked dataset, you can only query forUser-defined functions(both SQL and Javascript UDFs) andTable functionsroutine types. Querying for an unsupported routine type results in the error message:Querying routine typetypeis not yet supported on linked datasetdataset.Caution:As a publisher, be cautious when sharing datasets with unsupported routines. Unsupported routines might receive support in"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-10",
    "content": "the future, causing previously inactive queries with unsupported routines to work. Only include routines in shared datasets that you intend to share to subscribers. If you are a subscriber, the following BigQuery interoperability limitations apply: Materialized views that refer to tables in the linked dataset aren't supported. Materialized views that refer to tables in the linked dataset aren't supported. Takingsnapshotsof linked dataset tables isn't supported. Takingsnapshotsof linked dataset tables isn't supported. Queries with linked datasets andJOINstatements that are larger than 1 TB (physical storage) might fail. You cancontact supportto resolve this issue. Queries with linked datasets andJOINstatements that are larger than 1 TB (physical storage) might fail. You cancontact supportto resolve this issue. You cannot useregion qualifierswithINFORMATION_SCHEMAviews toview metadata for your linked dataset. You cannot useregion qualifierswithINFORMATION_SCHEMAviews toview metadata for your linked dataset. When querying forroutinesin a linked dataset, you can only query forUser-defined functions(both SQL and Javascript UDFs) andTable functionsroutine types. Querying for an unsupported routine type results in the error message:Querying routine typetypeis not yet supported on linked datasetdataset.Caution:As a publisher, be cautious when sharing datasets with unsupported routines. Unsupported routines might receive support in the future, causing previously inactive queries with unsupported routines to work. Only include routines in shared datasets that you intend to share to subscribers. When querying forroutinesin a linked dataset, you can only query forUser-defined functions(both SQL and Javascript UDFs) andTable functionsroutine types. Querying for an unsupported"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-11",
    "content": "routine type results in the error message:Querying routine typetypeis not yet supported on linked datasetdataset.Caution:As a publisher, be cautious when sharing datasets with unsupported routines. Unsupported routines might receive support in the future, causing previously inactive queries with unsupported routines to work. Only include routines in shared datasets that you intend to share to subscribers. The following limitations apply for the usage metrics:You can't get the usage metrics for listings that were subscribed before July 20, 2023.External tableusage metrics for thenum_rows_processedandtotal_bytes_processedfields might contain inaccurate data.Usage metrics for consumption are supported only for usage usingBigQuery jobs. Consumption by using the following resources is not supported:BigQuery Storage Read APItabledata.listBigQuery BI Engine queriesUsage metrics forviewsare only populated for queries after April 22, 2024.Usage metrics aren't captured for linked Pub/Sub subscriptions in BigQuery (you can continue to see usage directly in Pub/Sub). The following limitations apply for the usage metrics: You can't get the usage metrics for listings that were subscribed before July 20, 2023. You can't get the usage metrics for listings that were subscribed before July 20, 2023. External tableusage metrics for thenum_rows_processedandtotal_bytes_processedfields might contain inaccurate data. External tableusage metrics for thenum_rows_processedandtotal_bytes_processedfields might contain inaccurate data. Usage metrics for consumption are supported only for usage usingBigQuery jobs. Consumption by using the following resources is not supported:BigQuery Storage Read APItabledata.listBigQuery BI Engine queries Usage metrics for consumption are supported only for usage"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-12",
    "content": "usingBigQuery jobs. Consumption by using the following resources is not supported: BigQuery Storage Read API tabledata.list BigQuery BI Engine queries Usage metrics forviewsare only populated for queries after April 22, 2024. Usage metrics forviewsare only populated for queries after April 22, 2024. Usage metrics aren't captured for linked Pub/Sub subscriptions in BigQuery (you can continue to see usage directly in Pub/Sub). Usage metrics aren't captured for linked Pub/Sub subscriptions in BigQuery (you can continue to see usage directly in Pub/Sub). The following limitations apply when subscribing to Salesforce Data Cloud data:Data Cloud data is shared as views. As a subscriber, you can't access the underlying tables that the views reference. The following limitations apply when subscribing to Salesforce Data Cloud data: Data Cloud data is shared as views. As a subscriber, you can't access the underlying tables that the views reference. BigQuery sharing is supported in the following regions and multi-regions. 1Data located in theEUmulti-region is not stored in theeurope-west2(London) oreurope-west6(Z\u00fcrich) data centers. This section shows an example of how you can use sharing in BigQuery. Suppose you are a retailer and your organization has real-time demand forecasting data in a Google Cloud project namedForecasting. You want to share this demand forecasting data with hundreds of vendors in your supply-chain system. Here's how you can share your data with vendors through BigQuery sharing: Administrators As the owner of theForecastingproject, you must first enable the Analytics Hub API and then assign theAnalytics Hub Admin roleto a user who administers the data exchange in the project. This administrator can perform the following tasks: Create, update, delete, and"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-13",
    "content": "share the data exchange in your organization'sForecastingproject. Create, update, delete, and share the data exchange in your organization'sForecastingproject. Manage otheradministratorswith the Analytics Hub Admin role. Manage otheradministratorswith the Analytics Hub Admin role. Managepublishersby granting the Analytics Hub Publisher role to your organization's employees. If you want some employees to only be able to update, delete, and share listings but not create them, then you can grant them the Analytics Hub Listing Admin role. Managepublishersby granting the Analytics Hub Publisher role to your organization's employees. If you want some employees to only be able to update, delete, and share listings but not create them, then you can grant them the Analytics Hub Listing Admin role. Managesubscribersby granting the Analytics Hub Subscriber role to a Google group consisting of all vendors. If you want some vendors to only have view access to the available exchanges and listings then you can grant them the Analytics Hub Viewer role. These vendors aren't able to subscribe to listings. Managesubscribersby granting the Analytics Hub Subscriber role to a Google group consisting of all vendors. If you want some vendors to only have view access to the available exchanges and listings then you can grant them the Analytics Hub Viewer role. These vendors aren't able to subscribe to listings. For more information, seeManage data exchanges. Publishers Publishers create the following listings for their datasets in theForecastingproject or in a different project: Listing A: Demand Forecast Dataset 1 Listing B: Demand Forecast Dataset 2 Listing C: Demand Forecast Dataset 3 As a data provider, you cantrack the usage metricsfor your shared dataset. The usage metrics include the"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-14",
    "content": "following details: Jobs that run against your shared dataset. The consumption details of your shared dataset by subscribers' projects and organization. The number of rows and bytes processed by the job. For more information, seeManage listings. Subscribers Subscribers can browse through listings that they have access to in data exchanges. They can also subscribe to these listings and add these datasets to their projects by creating a linked dataset. Vendors can then run queries on these linked datasets and retrieve results in real time. For more information, seeView and subscribe to listings. There is no additional cost for managing data exchanges or listings. For BigQuery datasets, publishers are charged for data storage, whereas subscribers pay for queries that run against the shared data based on either on-demand or capacity-based pricing model. For information about pricing, seeBigQuery pricing. For Pub/Sub, topic publishers are charged for the total number of bytes written (publish throughput) to the shared topic and network egress (if applicable). Subscribers are charged for the total number of bytes read (subscribe throughput) from the linked subscription and network egress (if applicable). SeePub/Sub pricingfor additional details. For information about BigQuery sharing quotas, seeQuotas and limits. Sharing, as part of BigQuery, is compliant with the following compliance programs: ISO 27001 ISO 27017 ISO 27018 SOC 1 SOC 2 SOC 3 PCI Penetration Testing HIPAA HITRUST You can set the ingress and egress rules needed to let publishers and subscribers access data from projects that have VPC Service Controls perimeters. For more information, seeSharing VPC Service Controls rules. Learn how toview and subscribe to listings. Learn how to grantAnalytics Hub roles. Except"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction",
    "title": "Introduction to BigQuery sharing",
    "chunk_id": "https://cloud.google.com/bigquery/docs/analytics-hub-introduction#chunk-15",
    "content": "as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-05 UTC."
  },
  {
    "source_url": "https://cloud.google.com/looker/docs/studio",
    "title": "Welcome to Looker StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/looker/docs/studio#chunk-0",
    "content": "Home Documentation Looker Studio Guides Visualize your data through highly configurable charts and tables. Easily connect to a variety of data sources. Share your insights with your team or with the world. Collaborate on reports with your team. Speed up your report creation process with built-in sample reports. Looker Studio is ano-costtool that turns your data into informative, easy to read, easy to share, and fully customizable dashboards and reports. Use the drag and drop report editor to: Tell your data story withcharts, including line, bar, and pie charts, geo maps, area and bubble graphs, paginated data tables, pivot tables, and more. Make your reportsinteractivewith viewerfiltersanddate rangecontrols. Thedata controlturns any report into a flexible template report that anyone can use to see their own data. Includelinksandclickable imagesto create product catalogs, video libraries, and other hyperlinked content. Annotate and brand your reports withtextandimages. Applystylesandcolor themesthat make your data stories works of data visualization art. Learn moreabout creating reports. Get started quickly by using theLooker Studio marketing templates. Templates make it easy to visualize your data in a finished report.Learn more. With Looker Studio, you can easily report on data from a wide variety of sources, without programming. In just a few moments, you can connect to data sets such as the following: Databases, including BigQuery, MySQL, and PostgreSQL Google Marketing Platform products, including Google Ads, Google Analytics, Display & Video 360, Search Ads 360 Google consumer products, such as Google Sheets, YouTube, and Search Console Flat files via CSV file upload and Cloud Storage Social media platforms such as Facebook, Reddit, and Twitter Blended data from"
  },
  {
    "source_url": "https://cloud.google.com/looker/docs/studio",
    "title": "Welcome to Looker StudioStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/looker/docs/studio#chunk-1",
    "content": "any combination of related sources Learn moreabout connecting to your data. It's easy to share your insights with individuals, teams, or the world. Invite others to view or edit your reports, or send them links in scheduled emails. To tell your data stories as broadly as possible, you can embed your reports in other pages, such as Google Sites, blog posts, marketing articles, and annual reports. When you share a Looker Studio file with another editor, you can work it together in real time as a team. Learn more aboutsharing. Looker Studio's enterprise features let Cloud Identity and Google Workspace administrators manage users and control access to Looker Studio assets. Learn more aboutenterprise administrator features. Get even more enterprise-grade collaboration and administration features, along with access to Google Cloud Customer Care. Learn more aboutLooker Studio Pro. Try Looker Studio now Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-06 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-0",
    "content": "Home Vertex AI Documentation To learn more, run the \"Vertex AI Pipelines: Lightweight Python function-based components, and component I/O\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub Vertex AI Pipelines lets you automate, monitor, and govern your machine learning (ML) systems in a serverless manner by using ML pipelines to orchestrate your ML workflows. You can batch run ML pipelines defined using the Kubeflow Pipelines or the TensorFlow Extended (TFX) framework. To learn how to choose a framework for defining your ML pipeline, seeInterfaces to define a pipeline. This page provides an overview of the following: What is an ML pipeline? What is an ML pipeline? Structure of an ML pipeline Structure of an ML pipeline Pipeline tasks and components Pipeline tasks and components Life cycle of an ML pipeline Life cycle of an ML pipeline Use Vertex ML Metadata to track the lineage of ML artifacts Use Vertex ML Metadata to track the lineage of ML artifacts Add pipeline runs to experiments Add pipeline runs to experiments An ML pipeline is a portable and extensible description of an MLOps workflow as a series of steps called pipeline tasks. Each task performs a specific step in the workflow to train and/or deploy an ML model. With ML pipelines, you can apply MLOps strategies to automate and monitor repeatable processes in your ML practice. For example, you can reuse a pipeline definition to continuously retrain a model on the latest production data. For more information about MLOps in Vertex AI, seeMLOps on Vertex AI. An ML pipeline is a directed acyclic graph (DAG) of containerized pipeline tasks that are interconnected using input-output dependencies. You can"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-1",
    "content": "author each task either in Python or as a prebuilt container images. You can define the pipeline as a DAG using either the Kubeflow Pipelines SDK or the TFX SDK, compile it to its YAML for intermediate representation, and then run the pipeline. By default, pipeline tasks run in parallel. You can link the tasks to execute them in series. For more information about pipeline tasks, seePipeline task. For more information about the workflow for defining, compiling, and running the pipeline, seeLife cycle of an ML pipeline. Consider an ML pipeline with the following steps: Prepare data: Prepare or preprocess training data.Input (from tasks within the same ML pipeline): None.Output: Prepared or preprocessed training data. Prepare data: Prepare or preprocess training data. Input (from tasks within the same ML pipeline): None. Input (from tasks within the same ML pipeline): None. Output: Prepared or preprocessed training data. Output: Prepared or preprocessed training data. Train model: Use the prepared training data to train a model.Input: Prepared or preprocessed training data from pipeline taskPrepare data.Output: Trained model. Train model: Use the prepared training data to train a model. Input: Prepared or preprocessed training data from pipeline taskPrepare data. Input: Prepared or preprocessed training data from pipeline taskPrepare data. Output: Trained model. Output: Trained model. Evaluate model: Evaluate the trained model.Input: Trained model from pipeline taskTrain model. Evaluate model: Evaluate the trained model. Input: Trained model from pipeline taskTrain model. Deploy: Deploy the trained model for predictions.Input: Trained model from pipeline taskTrain model. Deploy: Deploy the trained model for predictions. Input: Trained model from pipeline taskTrain model."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-2",
    "content": "When you compile your ML pipeline, the pipelines SDK you're using (Kubeflow Pipelines or TFX) analyzes the data dependencies between these tasks and creates the following workflow DAG: Prepare datadoesn't rely on other tasks within the same ML pipeline for inputs. Therefore, it can be the first step in the ML pipeline, or run concurrently with other tasks. Prepare datadoesn't rely on other tasks within the same ML pipeline for inputs. Therefore, it can be the first step in the ML pipeline, or run concurrently with other tasks. Train modelrelies onPrepare datafor inputs. Therefore, it occurs afterPrepare data. Train modelrelies onPrepare datafor inputs. Therefore, it occurs afterPrepare data. EvaluateandDeployboth depend on the trained model. Therefore, they can run concurrently, but afterTrain model. EvaluateandDeployboth depend on the trained model. Therefore, they can run concurrently, but afterTrain model. When you run your ML pipeline, Vertex AI Pipelines executes these tasks in the sequence described in the DAG. Apipeline taskis an instantiation of apipeline componentwith specific inputs. While defining your ML pipeline, you can interconnect multiple tasks to form a DAG, by routing the outputs of one pipeline task to the inputs for the next pipeline task in the ML workflow. You can also use the inputs for the ML pipeline as the inputs for a pipeline task. A pipeline component is a self-contained set of code that performs a specific step of an ML workflow, such as data preprocessing, model training, or model deployment. A component typically consists of the following: Inputs: A component might have one or more input parameters and artifacts. Inputs: A component might have one or more input parameters and artifacts. Outputs: Every component has one or more output"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-3",
    "content": "parameters or artifacts. Outputs: Every component has one or more output parameters or artifacts. Logic: This is the component's executable code. For containerized components, the logic also contains the definition of the environment, or container image, where the component runs. Logic: This is the component's executable code. For containerized components, the logic also contains the definition of the environment, or container image, where the component runs. Components are the basis of defining tasks in an ML pipeline. To define pipeline tasks, you can either use predefinedGoogle Cloud Pipeline Componentsor create your own custom components. Use predefined Google Cloud Pipeline Components if you want to use features of Vertex AI, such as AutoML, in your pipeline. To learn how to use Google Cloud Pipeline Components to define a pipeline, seeBuild a Pipeline. You can author your own custom components to use in your ML pipeline. For more information about authoring custom components, seeBuild your own pipeline components. To learn how to author custom Kubeflow Pipelines components, see the\"Pipelines with lightweight components based on Python functions\"Jupyter notebook on GitHub. To learn how to author custom TFX components, see theTFX Python function component tutorialon theTensorFlow Extended in Production tutorials. A pipeline task is the instantiation of a pipeline component and performs a specific step in your ML workflow. You can author ML pipeline tasks either using Python or as prebuilt container images. Within a task, you can build on the on-demand compute capabilities of Vertex AI with Kubernetes to scalably execute your code, or delegate your workload to another execution engine, such as BigQuery, Dataflow, or Dataproc Serverless. From definition to execution"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-4",
    "content": "and monitoring, the lifecycle of an ML pipeline comprises the following high-level stages: Define: The process of defining an ML pipeline and its task is also called building a pipeline. In this stage, you need to perform the following steps:Choose an ML framework: Vertex AI Pipelines supports ML pipelines defined using the TFX or Kubeflow Pipelines framework. To learn how to choose a framework for building your pipeline, seeInterfaces to define a pipeline.Define pipeline tasks and configure pipeline: For more information, seeBuild a Pipeline. Define: The process of defining an ML pipeline and its task is also called building a pipeline. In this stage, you need to perform the following steps: Choose an ML framework: Vertex AI Pipelines supports ML pipelines defined using the TFX or Kubeflow Pipelines framework. To learn how to choose a framework for building your pipeline, seeInterfaces to define a pipeline. Choose an ML framework: Vertex AI Pipelines supports ML pipelines defined using the TFX or Kubeflow Pipelines framework. To learn how to choose a framework for building your pipeline, seeInterfaces to define a pipeline. Define pipeline tasks and configure pipeline: For more information, seeBuild a Pipeline. Define pipeline tasks and configure pipeline: For more information, seeBuild a Pipeline. Compile: In this stage, you need to perform the following steps:Generate your ML pipeline definition in a compiled YAML file for intermediate representation, which you can use to run your ML pipeline.Optional: You can upload the compiled YAML file as apipeline templateto a repository and reuse it to create ML pipeline runs. Compile: In this stage, you need to perform the following steps: Generate your ML pipeline definition in a compiled YAML file for intermediate"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-5",
    "content": "representation, which you can use to run your ML pipeline. Generate your ML pipeline definition in a compiled YAML file for intermediate representation, which you can use to run your ML pipeline. Optional: You can upload the compiled YAML file as apipeline templateto a repository and reuse it to create ML pipeline runs. Optional: You can upload the compiled YAML file as apipeline templateto a repository and reuse it to create ML pipeline runs. Run: Create an execution instance of your ML pipeline using the compiled YAML file or a pipeline template. The execution instance of a pipeline definition is called apipeline run.You can create a one-time occurrence of a pipeline run or use thescheduler APIto create recurring pipeline runs from the same ML pipeline definition. You can also clone an existing pipeline run. To learn how to choose an interface to run an ML pipeline, seeInterfaces to run a pipeline. For more information about how to create a pipeline run, seeRun a pipeline. Run: Create an execution instance of your ML pipeline using the compiled YAML file or a pipeline template. The execution instance of a pipeline definition is called apipeline run. You can create a one-time occurrence of a pipeline run or use thescheduler APIto create recurring pipeline runs from the same ML pipeline definition. You can also clone an existing pipeline run. To learn how to choose an interface to run an ML pipeline, seeInterfaces to run a pipeline. For more information about how to create a pipeline run, seeRun a pipeline. Monitor, visualize, and analyze runs: After you create a pipeline run, you can do the following to monitor the performance, status, and costs of pipeline runs:Configure email notifications for pipeline failures. For more information, seeConfigure email"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-6",
    "content": "notifications.Use Cloud Logging to create log entries for monitoring events. For more information, seeView pipeline job logs.Visualize, analyze, and compare pipeline runs. For more information, seeVisualize and analyze pipeline results.Use Cloud Billing export to BigQuery to analyze pipeline run costs. For more information, seeUnderstand pipeline run costs. Monitor, visualize, and analyze runs: After you create a pipeline run, you can do the following to monitor the performance, status, and costs of pipeline runs: Configure email notifications for pipeline failures. For more information, seeConfigure email notifications. Configure email notifications for pipeline failures. For more information, seeConfigure email notifications. Use Cloud Logging to create log entries for monitoring events. For more information, seeView pipeline job logs. Use Cloud Logging to create log entries for monitoring events. For more information, seeView pipeline job logs. Visualize, analyze, and compare pipeline runs. For more information, seeVisualize and analyze pipeline results. Visualize, analyze, and compare pipeline runs. For more information, seeVisualize and analyze pipeline results. Use Cloud Billing export to BigQuery to analyze pipeline run costs. For more information, seeUnderstand pipeline run costs. Use Cloud Billing export to BigQuery to analyze pipeline run costs. For more information, seeUnderstand pipeline run costs. Optional: stop or delete pipeline runs: There is no restriction on how long you can keep a pipeline run active. You can optionally do the following:Stop a pipeline run.Pause or resume a pipeline run schedule.Delete an existing pipeline template, pipeline run, or pipeline run schedule. Optional: stop or delete pipeline runs: There is no restriction on how long you"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-7",
    "content": "can keep a pipeline run active. You can optionally do the following: Stop a pipeline run. Stop a pipeline run. Pause or resume a pipeline run schedule. Pause or resume a pipeline run schedule. Delete an existing pipeline template, pipeline run, or pipeline run schedule. Delete an existing pipeline template, pipeline run, or pipeline run schedule. A pipeline run is an execution instance of your ML pipeline definition. Each pipeline run is identified by a unique run name. Using Vertex AI Pipelines, you can create an ML pipeline run in the following ways: Use the compiled YAML definition of a pipeline Use the compiled YAML definition of a pipeline Use a pipeline template from the Template Gallery Use a pipeline template from the Template Gallery For more information about how to create a pipeline run, seeRun a pipeline. For more information about how to create a pipeline run from a pipeline template, seeCreate, upload, and use a pipeline template. For information about capturing and storing pipeline run metadata using Vertex ML Metadata, seeUse Vertex ML Metadata to track the lineage of ML artifacts. For information about using pipeline runs to experiment on your ML workflow using Vertex AI Experiments, seeAdd your pipeline runs to experiments. A pipeline run contains several artifacts and parameters, including pipeline metadata. To understand changes in the performance or accuracy of your ML system, you need to analyze the metadata and the lineage of ML artifacts from your ML pipeline runs. The lineage of an ML artifact includes all the factors that contributed to its creation, along with metadata and references to artifacts derived from it. Lineage graphs help you analyze upstream root cause and downstream impact. Each pipeline run produces a lineage graph of parameters"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-8",
    "content": "and artifacts that are input into the run, materialized within the run, and output from the run. Metadata that composes this lineage graph is stored in Vertex ML Metadata. This metadata can also be synced to Dataplex. Use Vertex ML Metadata to track pipeline artifact lineageWhen you run a pipeline using Vertex AI Pipelines, all parameters and artifact metadata consumed and generated by the pipeline are stored in Vertex ML Metadata. Vertex ML Metadata is a managed implementation of the ML Metadata library in TensorFlow, and supports registering and writing custom metadata schemas. When you create a pipeline run in Vertex AI Pipelines, metadata from the pipeline run is stored in the default metadata store for the project and region where you execute the pipeline. Use Vertex ML Metadata to track pipeline artifact lineage When you run a pipeline using Vertex AI Pipelines, all parameters and artifact metadata consumed and generated by the pipeline are stored in Vertex ML Metadata. Vertex ML Metadata is a managed implementation of the ML Metadata library in TensorFlow, and supports registering and writing custom metadata schemas. When you create a pipeline run in Vertex AI Pipelines, metadata from the pipeline run is stored in the default metadata store for the project and region where you execute the pipeline. Use Dataplex to track pipeline artifact lineageDataplexis a global and cross-project data fabric integrated with multiple systems within Google Cloud, such as Vertex AI, BigQuery, and Cloud Composer. Within Dataplex, you can search for a pipeline artifact and view its lineage graph. Note that to prevent artifact conflicts, any resource catalogued in Dataplex is identified with afully qualified name (FQN).Learn about Dataplex usage costs. Use Dataplex to track pipeline"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-9",
    "content": "artifact lineage Dataplexis a global and cross-project data fabric integrated with multiple systems within Google Cloud, such as Vertex AI, BigQuery, and Cloud Composer. Within Dataplex, you can search for a pipeline artifact and view its lineage graph. Note that to prevent artifact conflicts, any resource catalogued in Dataplex is identified with afully qualified name (FQN). Learn about Dataplex usage costs. For more information about tracking the lineage of ML artifacts using Vertex ML Metadata and Dataplex, seeTrack the lineage of pipeline artifacts. For more information about visualizing, analyzing, and comparing pipeline runs, seeVisualize and analyze pipeline results. For a list of first-party artifact types defined in Google Cloud Pipeline Components, seeML Metadata artifact types. Vertex AI Experiments lets you track and analyze various model architectures, hyperparameters, and training environments to find the best model for your ML use case. After you create an ML pipeline run, you can associate it with an experiment or experiment run. By doing so, you can experiment with different sets of variables, such as hyperparameters, number of training steps, or iterations. For more information about experimenting with ML workflows using Vertex AI Experiments, seeIntroduction to Vertex AI Experiments. Learn about theinterfaces you can use to define and run pipelines using Vertex AI Pipelines. Learn about theinterfaces you can use to define and run pipelines using Vertex AI Pipelines. Get started bylearning how to define a pipeline using the Kubeflow Pipelines SDK. Get started bylearning how to define a pipeline using the Kubeflow Pipelines SDK. Learn how to run a pipeline. Learn how to run a pipeline. Learn aboutbest practices for implementing custom-trained ML models"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction",
    "title": "Introduction to Vertex AI PipelinesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/pipelines/introduction#chunk-10",
    "content": "on Vertex AI. Learn aboutbest practices for implementing custom-trained ML models on Vertex AI. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/sqlserver",
    "title": "Cloud SQL for SQL Server documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/sqlserver#chunk-0",
    "content": "Home Cloud SQL Documentation SQL Server Cloud SQL for SQL Server is a managed database service that helps you set up, maintain, manage, and administer your SQL Server databases on Google Cloud. For information specific to Microsoft SQL Server, see theSQL Server documentation.Learn more Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create instances Create instances Connection Overview Connection Overview Enable and disable high availability on an instance Enable and disable high availability on an instance Create and manage SQL Server databases Create and manage SQL Server databases Create and manage SQL Server users Create and manage SQL Server users Exporting and importing using BAK files Exporting and importing using BAK files Export and import using SQL dump files Export and import using SQL dump files Create backups Create backups Configure database flags Configure database flags cloud sql command-line cloud sql command-line REST API REST API Use the Cloud SQL Admin API Use the Cloud SQL Admin API Best practices Best practices Performance tips Performance tips Authorize requests Authorize requests Configure VPC Service Controls Configure VPC Service Controls Client libraries and sample code for Cloud SQL Client libraries and sample code for Cloud SQL Cloud SQL Admin API error messages Cloud SQL Admin API error messages Pricing Pricing Quotas and limits Quotas and limits Troubleshoot Troubleshoot Cloud SQL feature support by database engine Cloud SQL feature support by database engine Release notes Release notes Billing questions Billing questions Get support Get support"
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/sqlserver",
    "title": "Cloud SQL for SQL Server documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/sqlserver#chunk-1",
    "content": "Google Cloud Fundamentals: Core Infrastructure These lectures, demos, and hands-on labs give you an overview of Google Cloud products and services so that you can learn the value of Google Cloud and how to incorporate cloud-based solutions into your business strategies. Architecting with Google Cloud: Design and Process This course features a combination of lectures, design activities, and hands-on labs to show you how to use proven design patterns on Google Cloud to build highly reliable and efficient solutions and operate deployments that are highly available and cost-effective. Creating a SQL Server instance integrated with Active Directory using Google Cloud SQL This blog post describes the basic steps required to create a SQL Server instance integrated with Managed Service for Microsoft Active Directory. Migrating data from SQL Server 2017 to Cloud SQL for SQL Server using snapshot replication Shows how to migrate data from Microsoft SQL Server 2017 Enterprise running on Compute Engine to Cloud SQL for SQL Server 2017 Enterprise.MigrationReplication Migrating data between SQL Server 2008 and Cloud SQL for SQL Server using backup files Shows how to migrate data from SQL Server 2008 to Cloud SQL for SQL Server 2017 Enterprise.Migration Data residency overview Learn how to use Cloud SQL to enforce data residency requirements for data.Data residencydata Use Secret Manager to handle secrets in Cloud SQL Learn how to use Secret Manager to store sensitive information about Cloud SQL instances and users as secrets.Secret Managersecret Python SQLAlchemy Use SQLAlchemy with your Cloud SQL for SQL Server database Node.js sample Connecting to your Cloud SQL for SQL Server database in Node.js PHP PDO Connecting your Cloud SQL for SQL Server database using PHP PDO Go web app"
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/sqlserver",
    "title": "Cloud SQL for SQL Server documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/sqlserver#chunk-2",
    "content": "sample Simple examples of connecting to Cloud SQL for SQL Server using Go Terraform for Cloud SQL networking Use Terraform to create Cloud SQL for SQL Server instances with private networking options. .NET sample This sample application demonstrates how to store data in Google Cloud SQL with a SQL Server database when running in Google App Engine Flexible Environment. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/workbench/managed",
    "title": "Vertex AI Workbench: Managed notebooks\n documentation",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/workbench/managed#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI Workbench Vertex AI Workbench managed notebooks isdeprecated. On April 14, 2025, support for managed notebooks will end and the ability to create managed notebooks instances will be removed. Existing instances will continue to function but patches, updates, and upgrades won't be available. To continue using Vertex AI Workbench, we recommend that youmigrate your managed notebooks instances to Vertex AI Workbench instances. Managed notebooks instances are Google-managed environments with integrations and capabilities that help you set up and work in an end-to-end Jupyter notebook-based production environment.Learn more. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create a managed notebooks instance Create a managed notebooks instance Introduction to managed notebooks Introduction to managed notebooks Query data in BigQuery tables from within JupyterLab Query data in BigQuery tables from within JupyterLab Run a managed notebooks instance on a Dataproc cluster Run a managed notebooks instance on a Dataproc cluster Run notebook files with the executor Run notebook files with the executor Add a custom container to a managed notebooks instance Add a custom container to a managed notebooks instance Pricing Pricing Release notes Release notes Get support Get support Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/tech-area-overviews",
    "title": "AI and ML",
    "chunk_id": "https://cloud.google.com/docs/tech-area-overviews#chunk-0",
    "content": "Home Documentation Leverage the power of AI/ML models and solutions to transform your organization and solve real-world problems. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Build AI applications with enterprise-grade scaling, security, and observability. Apply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs. Apply Google's state-of-the-art capabilities to handle your document management needs. Apply Google's state-of-the-art capabilities to handle your industry-specific needs. Apply Google's state-of-the-art capabilities to handle your video, images, vision, and augmented reality needs. Apply Google's state-of-the-art capabilities to handle your search and recommendations needs. Apply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs. Train ML models from your data using AutoML or your preferred ML framework. Apply operations best practices to monitor and improve your deployed ML models. Accelerate machine learning workloads. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/code/docs/intellij",
    "title": "Cloud Code for IntelliJ documentation",
    "chunk_id": "https://cloud.google.com/code/docs/intellij#chunk-0",
    "content": "Home Cloud Code Documentation IntelliJ Cloud Code Cloud Code for IntelliJ provides IDE support to help you develop and deploy Cloud Run, Kubernetes, and App Engine applications, manage your Google Cloud APIs and libraries, view your Cloud Storage content, and add new projects to Cloud Source Repositories, among a wealth of functionality, bringing speed, harmony, and efficiency to your development workflow.Learn more Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Set up the Cloud Code extension Set up the Cloud Code extension Quickstart: Deploy a Kubernetes app with Cloud Code for IntelliJ Quickstart: Deploy a Kubernetes app with Cloud Code for IntelliJ Quickstart: Deploy a Cloud Run service with Cloud Code for IntelliJ Quickstart: Deploy a Cloud Run service with Cloud Code for IntelliJ Get started with the Gemini Code Assist plugin Get started with the Gemini Code Assist plugin Manage Cloud APIs and Libraries Manage Cloud APIs and Libraries Work with Google Cloud and Kubernetes YAML files Work with Google Cloud and Kubernetes YAML files Explore Kubernetes development in Cloud Code Explore Kubernetes development in Cloud Code Run and developing a Kubernetes application Run and developing a Kubernetes application Deploy to the App Engine Flexible Environment Deploy to the App Engine Flexible Environment Debug a Kubernetes application Debug a Kubernetes application Develop a Cloud Run service locally Develop a Cloud Run service locally Troubleshoot common installation issues Troubleshoot common installation issues Getting support Getting support Release notes Release notes"
  },
  {
    "source_url": "https://cloud.google.com/code/docs/intellij",
    "title": "Cloud Code for IntelliJ documentation",
    "chunk_id": "https://cloud.google.com/code/docs/intellij#chunk-1",
    "content": "IntelliJ version support IntelliJ version support Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/memorystore/docs/memcached",
    "title": "Memorystore for Memcached documentation",
    "chunk_id": "https://cloud.google.com/memorystore/docs/memcached#chunk-0",
    "content": "Home Documentation Memorystore Memorystore for Memcached Memorystore for Memcached is a fully managed Memcached service for Google Cloud. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Memcached service without the burden of managing complex Memcached deployments.Learn more Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Quickstart: Create a Memorystore for Memcached instance by using the Google Cloud console Quickstart: Create a Memorystore for Memcached instance by using the Google Cloud console Quickstart: Create a Memorystore for Memcached instance by using the gcloud CLI Quickstart: Create a Memorystore for Memcached instance by using the gcloud CLI Overview of Memorystore for Memcached Overview of Memorystore for Memcached Monitor Memcached instances Monitor Memcached instances Connect to a Memcached instance Connect to a Memcached instance Create and manage Memcached instances Create and manage Memcached instances Configure Memcached instances Configure Memcached instances Use the Auto Discovery service Use the Auto Discovery service Establish a private services access connection Establish a private services access connection Setting up client libraries Setting up client libraries REST API REST API Pricing Pricing Release notes Release notes Billing questions Billing questions Quotas and limits Quotas and limits Getting support Getting support Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle"
  },
  {
    "source_url": "https://cloud.google.com/memorystore/docs/memcached",
    "title": "Memorystore for Memcached documentation",
    "chunk_id": "https://cloud.google.com/memorystore/docs/memcached#chunk-1",
    "content": "Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation To see an example of using Anthropic's Claude model on Vertex AI, run the \"Use Anthropic's Claude model on Vertex AI\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub The Anthropic Claude models on Vertex AI offer fully managed and serverless models as APIs. To use a Claude model on Vertex AI, send a request directly to the Vertex AI API endpoint. Because the Anthropic Claude models use a managed API, there's no need to provision or manage infrastructure. You can stream your Claude responses to reduce the end-user latency perception. A streamed response uses server-sent events (SSE) to incrementally stream the response. You pay for Claude models as you use them (pay as you go), or you pay a fixed fee when using [provisioned throughput][pt]. For pay-as-you-go pricing, seeAnthropic Claude models on the Vertex AI pricing page. The following models are available from Anthropic to use in Vertex AI. To access a Claude model, go to its Model Garden model card. Anthropic's Claude models support Vertex AI request-response logging. Enable 30-day request-response logging of your prompt and completion activity to track any model misuse by your users. For more information, seeLog requests and responses. Claude Opus 4 is Anthropic's most intelligent model and is state-of-the-art for coding and agent capabilities, especially agentic search. It excels for customers needing frontier intelligence: Advanced coding: Independently plan and execute complex development tasks end-to-end. It adapts to your style and maintains high code quality throughout. Long-horizon tasks and complex problem solving (virtual collaborator): Unlock new use"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-1",
    "content": "cases that involves long-horizon tasks that require memory, sustained reasoning, and long chains of actions. AI agents: Enable agents to tackle complex, multi-step tasks that require peak accuracy. Agentic search and research: Connect to multiple data sources to synthesize comprehensive insights across repositories. Content creation: Create human-quality content with natural prose. Produce long-form creative content, technical documentation, marketing copy, and frontend design mockups. Memory and context management: Incorporates memory capabilities that allow it to effectively summarize and reference previous interactions. Go to the Claude Opus 4 model card Claude Sonnet 4 balances impressive performance for coding with the right speed and cost for high-volume use cases: Coding: Handle everyday development tasks with enhanced performance\u2014power code reviews, bug fixes, API integrations, and feature development with immediate feedback loops. AI Assistants: Power production-ready assistants for real-time applications\u2014from customer support automation to operational workflows that require both intelligence and speed. Efficient research: Perform focused analysis across multiple data sources while maintaining fast response times. Ideal for rapid business intelligence, competitive analysis, and real-time decision support. Large-scale content: Generate and analyze content at scale with improved quality. Create customer communications, analyze user feedback, and produce marketing materials with the right balance of quality and throughput. Go to the Claude Sonnet 4 model card Claude 3.7 Sonnet is Anthropic's most intelligent model to date and the first Claude model to offer extended thinking\u2014the ability to solve complex problems with careful, step-by-step reasoning. Claude 3.7"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-2",
    "content": "Sonnet is a single model where you can balance speed and quality by choosing between standard thinking for near-instant responses or extended thinking for advanced reasoning. For more information about extended thinking, see Anthropic'sdocumentation. Claude 3.7 Sonnet is optimized for the following use cases: Agentic coding - Claude 3.7 Sonnet is state-of-the-art for agentic coding, and can complete tasks across the entire software development lifecycle\u2014from initial planning to bug fixes, maintenance to large refactors. It offers strong performance in both planning and solving for complex coding tasks, making Claude 3.7 Sonnet an ideal choice to power end-to-end software development processes. Customer-facing agents - Claude 3.7 Sonnet offers superior instruction following, tool selection, error correction, and advanced reasoning for customer-facing agents and complex AI workflows. Computer use - Claude 3.7 Sonnet is our most accurate model for computer use, enabling developers to direct Claude to use computers the way people do. Content generation and analysis - Claude 3.7 Sonnet excels at writing and is able to understand nuance and tone in content to generate more compelling content and analyze content on a deeper level. Visual data extraction - With Claude 3.7 Sonnet's robust vision skills, it is the right choice for teams that want to extract raw data from visuals like charts or graphs as part of their AI workflow. Go to the Claude 3.7 Sonnet model card Claude 3.5 Sonnet v2 is a state-of-the-art model for real-world software engineering tasks and agentic capabilities. Claude 3.5 Sonnet v2 delivers these advancements at the same price and speed as Claude 3.5 Sonnet. The upgraded Claude 3.5 Sonnet model is capable of interacting with tools that can manipulate a"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-3",
    "content": "computer desktop environment. For more information, see theAnthropic documentation. Claude 3.5 Sonnet is optimized for the following use cases: Agentic tasks and tool use - Claude 3.5 Sonnet offers superior instruction following, tool selection, error correction, and advanced reasoning for agentic workflows that require tool use. Coding - For software development tasks ranging from code migrations, code fixes, and translations, Claude 3.5 Sonnet offers strong performance in both planning and solving for complex coding tasks. Document Q&A - Claude 3.5 Sonnet combines strong context comprehension, advanced reasoning, and synthesis to deliver accurate and human-like responses. Visual data extraction - With Claude 3.5 Sonnet leading vision skills, Claude 3.5 Sonnet can extract raw data from visuals like charts or graphs as part of AI workflows. Content generation and analysis - Claude 3.5 Sonnet can understand nuance and tone in content, generating more compelling content and analyzing content on a deeper level. Go to the Claude 3.5 Sonnet v2 model card Claude 3.5 Haiku, the next generation of Anthropic's fastest and most cost-effective model, is optimal for use cases where speed and affordability matter. It improves on its predecessor across every skill set. Claude 3.5 Haiku is optimized for the following use cases: Code completions - With its rapid response time and understanding of programming patterns, Claude 3.5 Haiku excels at providing quick, accurate code suggestions and completions in real-time development workflows. Interactive chat bots - Claude 3.5 Haiku's improved reasoning and natural conversation abilities make it ideal for creating responsive, engaging chatbots that can handle high volumes of user interactions efficiently. Data extraction and labeling -"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-4",
    "content": "Leveraging its improved analysis skills, Claude 3.5 Haiku efficiently processes and categorizes data, making it useful for rapid data extraction and automated labeling tasks. Real-time content moderation - With strong reasoning skills and content understanding, Claude 3.5 Haiku provides fast, reliable content moderation for platforms that require immediate response times at scale. Go to the Claude 3.5 Haiku model card Anthropic's Claude 3 Opus is a powerful AI model with top-level performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Claude 3 Opus is optimized for the following use cases: Task automation, such as interactive coding and planning, or running complex actions across APIs and databases. Task automation, such as interactive coding and planning, or running complex actions across APIs and databases. Research and development tasks, such as research review, brainstorming and hypothesis generation, and product testing. Research and development tasks, such as research review, brainstorming and hypothesis generation, and product testing. Strategy tasks, such as advanced analysis of charts and graphs, financials and market trends, and forecasting. Strategy tasks, such as advanced analysis of charts and graphs, financials and market trends, and forecasting. Vision tasks, such as processing images to return text output. Also, analysis of charts, graphs, technical diagrams, reports, and other visual content. Vision tasks, such as processing images to return text output. Also, analysis of charts, graphs, technical diagrams, reports, and other visual content. Go to the Claude 3 Opus model card Anthropic's Claude 3 Haiku is Anthropic's fastest vision and text model for near-"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-5",
    "content": "instant responses to basic queries, meant for seamless AI experiences mimicking human interactions. Live customer interactions and translations. Live customer interactions and translations. Content moderation to catch suspicious behavior or customer requests. Content moderation to catch suspicious behavior or customer requests. Cost-saving tasks, such as inventory management and knowledge extraction from unstructured data. Cost-saving tasks, such as inventory management and knowledge extraction from unstructured data. Vision tasks, such as processing images to return text output, analysis of charts, graphs, technical diagrams, reports, and other visual content. Vision tasks, such as processing images to return text output, analysis of charts, graphs, technical diagrams, reports, and other visual content. Go to the Claude 3 Haiku model card Anthropic's Claude 3.5 Sonnet outperforms Claude 3 Opus on a wide range of Anthropic's evaluations, with the speed and cost of Anthropic's mid-tier Claude 3 Sonnet. Claude 3.5 Sonnet is optimized for the following use cases: Coding, such as writing, editing, and running code with sophisticated reasoning and troubleshooting capabilities. Coding, such as writing, editing, and running code with sophisticated reasoning and troubleshooting capabilities. Handle complex queries from customer support by understanding user context and orchestrating multi-step workflows. Handle complex queries from customer support by understanding user context and orchestrating multi-step workflows. Data science and analysis by navigating unstructured data and leveraging multiple tools to generate insights. Data science and analysis by navigating unstructured data and leveraging multiple tools to generate insights. Visual processing, such as interpreting"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",
    "title": "Anthropic's Claude modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#chunk-6",
    "content": "charts and graphs that require visual understanding. Visual processing, such as interpreting charts and graphs that require visual understanding. Writing content with a more natural, human-like tone. Writing content with a more natural, human-like tone. Go to the Claude 3.5 Sonnet model card Learn how to use Anthropic's models. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/observability",
    "title": "Observability and monitoring",
    "chunk_id": "https://cloud.google.com/docs/observability#chunk-0",
    "content": "Home Documentation Documentation, guides, and resources for observability and monitoring across Google Cloud products and services. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Integrated monitoring, logging, and trace managed services for applications and systems running on Google Cloud and beyond. Continuous profiling of production systems for large-scale operations. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-0",
    "content": "Home Cloud Billing Documentation Guides With the FinOps hub, you can monitor and communicate your current savings, explore new recommended opportunities to optimize costs, and plan your optimization goals. The FinOps hub presents all of your active savings and optimization opportunities in one dashboard. The FinOps hub automatically generates the dashboard based on historical usage metrics gathered by Cloud Billing andRecommender, including recent usage and current commitments. The FinOps hub uses Cloud Billing to retrieve cost data, and variousGoogle Cloud cost recommendersfor optimization and utilization metrics. To access the FinOps hub for a Cloud Billing account \u2014 to view all available recommendations, the FinOps score, CUDs optimization metrics, and utilization insights \u2014 you need Cloud Billing permissions. Depending on the type of recommendation, you might also need project permissions to view the details and apply a recommendation. To access the FinOps hub for a Cloud Billing account, and view all available recommendations, the FinOps score, CUDs optimization metrics, and utilization insights, you need one of the following predefinedCloud Billing IAM roleson your Cloud Billing account: Billing Account ViewerBilling Account AdministratorIf you prefer to use a custom role to access the FinOps hub, you need a role with the followingpermissionson your Cloud Billing account:billing.accounts.getbilling.accounts.getSpendingInformationbilling.finOpsBenchmarkInformation.getbilling.finOpsHealthInformation.getrecommender.costRecommendations.listAllView details of a recommendation and apply the recommendationTo view the details and apply a recommendation, you needrecommender-specific permissionson the billing account or project. Depending on thetype of recommendation, you"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-1",
    "content": "might need the following permissions:Project Viewerrole for each of the projects that you want to see recommendations for.Recommender Viewer roleon your Cloud Billing account.See thelist of FinOps hub cost recommendersfor the recommenders that contribute to hub metrics.To learn how to grant permissions to view recommendation updates, seeRecommender overview. Billing Account Viewer Billing Account Administrator If you prefer to use a custom role to access the FinOps hub, you need a role with the followingpermissionson your Cloud Billing account: billing.accounts.get billing.accounts.getSpendingInformation billing.finOpsBenchmarkInformation.get billing.finOpsHealthInformation.get recommender.costRecommendations.listAll To view the details and apply a recommendation, you needrecommender-specific permissionson the billing account or project. Depending on thetype of recommendation, you might need the following permissions: Project Viewerrole for each of the projects that you want to see recommendations for. Recommender Viewer roleon your Cloud Billing account. See thelist of FinOps hub cost recommendersfor the recommenders that contribute to hub metrics. To learn how to grant permissions to view recommendation updates, seeRecommender overview. For more information aboutCloud Billing permissions, see: Overview of Cloud Billing access control Create custom roles for Cloud Billing Understanding predefined Identity and Access Management roles for Cloud Billing For more information about Google Cloudproject permissions, see: Access control for projects with IAM IAM basic and predefined roles reference To view the FinOps hub: In the Google Cloud console, go to the FinOps hub.Go to FinOps hub In the Google Cloud console, go to the FinOps hub. Go to FinOps hub At the prompt, choose"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-2",
    "content": "the Cloud Billing account for which you want to view the FinOps hub. At the prompt, choose the Cloud Billing account for which you want to view the FinOps hub. TheFinOps hubdashboard summarizes your current cost optimizations and introduces Google Cloud-recommended optimizations. Information on the FinOps hub reflects historical data collected. The FinOps hub considers four optimization practices to create cost-saving recommendations: Turning off idle resources Right-sizing instances Other configuration changes for certain resources Purchasing committed use discounts (CUDs) Metrics throughout the hub reflect how well you're using those optimizations and identify additional optimization opportunities to reduce costs and improve FinOps practices. In a recommendation, the estimated savings for a resource is calculated using either a custom contract price or a list price. The pricing type used tocalculate the estimated cost savingsis based on the contract type you might have with Google Cloud and your specific role and cost view permissions on the Cloud Billing account. The estimated savings in a recommendation doesn't consider existing committed use discounts that you have purchased that might apply to the resource. TheOptimization summaryis a snapshot of how much you're saving with optimizations, current recommendations from Google Cloud, and how much more you could save by adopting additional optimizations. TheOptimization summaryincludes: Last month's realized savings- the total savings related to CUDs, right-sizing instances, and removing idle resources.Note:Realized savings don't calculate savings from turning off idle resources. Additionally, you might see a negative total if you've underutilized CUDs. Last month's realized savings- the total savings related to CUDs,"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-3",
    "content": "right-sizing instances, and removing idle resources. Active Recommendations- the total number of Google Cloud-recommended optimizations available to you. Recommendations include suggestions to turn off idle resources, right-size instances or other configuration changes, and purchase CUDs. Active Recommendations- the total number of Google Cloud-recommended optimizations available to you. Recommendations include suggestions to turn off idle resources, right-size instances or other configuration changes, and purchase CUDs. Potential savings per month- the estimated amount of money you can save by applying all available recommendations. If there are multiple potential opportunities to save on the same Compute Engine resource, such as purchasing a resource-based CUD and a Compute flexible CUD, the FinOps hub de-duplicates the opportunities and only includes the recommendation that brings you the higher savings. Potential savings per month- the estimated amount of money you can save by applying all available recommendations. If there are multiple potential opportunities to save on the same Compute Engine resource, such as purchasing a resource-based CUD and a Compute flexible CUD, the FinOps hub de-duplicates the opportunities and only includes the recommendation that brings you the higher savings. CUD optimization rate- the percentage of how much of your CUD-eligible usage is covered by committed use discounts. We determine your CUD optimization rate by calculating how much of your usage across all products over the last 30 days can be converted to CUDs. For example, in the past 30 days, you might have spent $10,000 on Google Cloud products eligible for CUDs, and received $4,500 worth of CUDs. In this example, you are using $4,500 out of $10,000 in CUDs opportunities, so"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-4",
    "content": "yourCUD optimization rateis 45%. CUD optimization rate- the percentage of how much of your CUD-eligible usage is covered by committed use discounts. We determine your CUD optimization rate by calculating how much of your usage across all products over the last 30 days can be converted to CUDs. For example, in the past 30 days, you might have spent $10,000 on Google Cloud products eligible for CUDs, and received $4,500 worth of CUDs. In this example, you are using $4,500 out of $10,000 in CUDs opportunities, so yourCUD optimization rateis 45%. Insights provided by Gemini Cloud Assist- If you enabledGemini Cloud Assist in Cloud Billing, then you will see Gemini-provided key optimization and utilization insights in the Optimization summary widget: Insights provided by Gemini Cloud Assist- If you enabledGemini Cloud Assist in Cloud Billing, then you will see Gemini-provided key optimization and utilization insights in the Optimization summary widget: TheFinOps scorecan help you gauge how well you're using Google Cloud tools to monitor and save costs, and how you can continue to optimize costs. The score is a calculation based on how you follow optimization best practices, including the following: Monitoring spend by actively logging in and using Cloud Billing tools. Using tools such as tags and labels to allocate costs for your resources. Optimizing resources by turning off idle resources and right-sizing instances. Purchasing CUDs, including CUDs opportunities recommended by Google Cloud. Creating and monitoring budgets frequently. Automating your cost management by using tools such as the billing BigQuery export and using the Budgets API. SelectImprove your scoreto review Google Cloud recommended cost-saving actions based on three stages of the cloud FinOps journey:"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-5",
    "content": "inform, operate, and optimize. The FinOps score also provides a peer benchmark score as a view of your optimization performance in the context of industry verticals based on aggregated usage data. All customers are automatically opted into peer benchmarking aggregation, but you can choose to opt out at any time. If you choose to opt out, you no longer see CUD recommendations, the FinOps score, or the peer benchmark score in the FinOps hub. The FinOps and peer benchmark scores are based on data collected two days prior, and are updated daily. Your score might change depending on factors such as new customers joining the peer benchmark score. To opt in or out of participating in peer benchmarking, you must have the Billing Account Administrator role on your Cloud Billing account, and thedataprocessing.groupcontrols.updatepermission on your Cloud Billing account, which is part of the Data Processing Controls Resource Admin role. In the Google Cloud console, open the Identity and Access Management (IAM) Transparency and Control Center for your Cloud Billing account.Go to Transparency and Control Center In the Google Cloud console, open the Identity and Access Management (IAM) Transparency and Control Center for your Cloud Billing account. Go to Transparency and Control Center Select your Cloud Billing account from the menu. Select your Cloud Billing account from the menu. To opt out of participating in peer benchmarking, in theData processing groupstable, for theBillinggroup, clickDisable.If you want to opt in, selectEnable. To opt out of participating in peer benchmarking, in theData processing groupstable, for theBillinggroup, clickDisable. If you want to opt in, selectEnable. With thePotential savings/monthchart, you can focus on savings byserviceor byproject. The chart"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-6",
    "content": "shows the total potential monthly savings from all cost optimization opportunities broken down by the associated service or project. From thePotential savings/monthchart, you can access theRecommendationsdashboard, where you can review all of the FinOps recommendations, view the recommendation details, send recommendations to others to review, and apply recommendations to optimize your cloud costs. To open theRecommendationsdashboard, selectView all recommendations. TheTop recommendationswidget shows you the top 10 recommendations by potential cost savings. Each recommendation displays the estimated monthly savings, the associated service, and a brief description of the recommendation. If you have theRecommender Viewer roleon your Cloud Billing account, you can select a recommendation to get further details and to apply the recommendation. Depending on the type of recommendation, you might also need project permissions to view the details and apply a recommendation. (Preview)Gain insights on resource utilization and the cost of potential waste. For resources that provide utilization metrics (such as Compute Engine, Google Kubernetes Engine, Cloud SQL, and Cloud Run), the chart shows the costs of provisioned resources with potentially wasted usage. To determine waste, utilization insights uses various recommenders to help you identify idle resources, right-sizing opportunities of over- or underprovisioned resources, and suboptimal configurations, and then summarizes the cost of the potential wasted usage for each service that returns utilization metrics. To view the details of the insights and access cost optimization recommendations, clickView utilization insightsto open theUtilization insightsdashboard. For a high-level view of your savings from commitments, use"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-7",
    "content": "theLast month's realized savingswidget, which breaks down your savings by the services that you have purchased commitments for. TheCarbon Footprint dashboardpresents the estimated greenhouse gas emissions from your Google Cloud usage, helping you optimize cloud spend while reducing carbon impact. For more information, visit theCarbon Footprint documentation. When you view theRecommendationsdashboard, in the list of recommendations, theRegioncolumn displays a green leaf next to locations that have the lowest carbon impact. For more information, seeLow CO2. Continue to optimize costs by taking advantage of Google's cost-saving recommendations. To access and apply recommendations, take any of the following actions: On theFinOps scorewidget, clickImprove your scoreto access and apply recommended actions. On thePotential savings/monthwidget, clickView all recommendationsto access theRecommendationsdashboard, then click any of the recommendations to view the details of the recommendation and take action. On thePotential wasted usagewidget, clickView utilization insightsto access theUtilization insightsdashboard. On theTop recommendationswidget, click any of the recommendations to view the details of the recommendation and take action. TheFinOps hubdashboard receives metrics from various Google Cloud cost recommenders. Use the following table to learn more about each recommender. Google Cloud Well-Architected Framework: Cost optimization Optimize costs with committed use discounts Purchasing spend-based committed use discounts Purchasing resource-based committed use discounts Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle"
  },
  {
    "source_url": "https://cloud.google.com/billing/docs/how-to/finops-hub",
    "title": "Optimize costs with FinOps hubStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/billing/docs/how-to/finops-hub#chunk-8",
    "content": "Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-29 UTC."
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect",
    "title": "CDN Interconnect overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect#chunk-0",
    "content": "Home Network Connectivity Documentation Guides CDN Interconnect enables select third-party Content Delivery Network (CDN) providers to establish direct peering links with Google's edge network at various locations, which enables you to direct your traffic from your Virtual Private Cloud (VPC) networks to a provider's network. CDN Interconnect enables you to optimize your CDN population costs and use direct connectivity to select CDN providers from Google Cloud. Your network traffic egressing from Google Cloud through one of these links benefits from the direct connectivity to supported CDN providers and is billed automatically with reduced pricing. If your CDN provider is already part of the program, you don't have to do anything. Traffic from supported Google Cloud locations to your CDN provider automatically takes advantage of the direct connection and reduced pricing. Work with your supported CDN provider to learn what locations are supported and how to correctly configure your deployment to use intra-region egress routes. CDN Interconnect does not require any configuration or integration with Cloud Load Balancing. If your CDN provider is not part of the program, contact your CDN provider and ask them to work with Google to get connected. High-volume egress traffic.If you're populating your CDN with large data files from Google Cloud, you can use the CDN Interconnect links between Google Cloud and selected providers to automatically optimize this traffic and save money. Frequent content updates.Cloud workloads that frequently update data stored in CDN locations benefit from using CDN Interconnect because the direct link to the CDN provider reduces latency for these CDN destinations.For example, if you have frequently updated data served by the CDN originally hosted"
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect",
    "title": "CDN Interconnect overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect#chunk-1",
    "content": "on Google Cloud, you might consider using CDN Interconnect. Frequent content updates.Cloud workloads that frequently update data stored in CDN locations benefit from using CDN Interconnect because the direct link to the CDN provider reduces latency for these CDN destinations. For example, if you have frequently updated data served by the CDN originally hosted on Google Cloud, you might consider using CDN Interconnect. The special pricing for your traffic egressing from Google Cloud to a CDN provider is automatic. Google works with approved CDN partners in supported locations to allowlist provider IP addresses. This means that any data that you send to your allowlisted CDN provider from Google Cloud is charged at the reduced price. This reduced price applies only to IPv4 traffic. It does not apply to IPv6 traffic. Traffic between Google Cloud and pre-approved CDN Interconnect locations is billed as follows: Ingress traffic is free for all regions. Egress traffic rates apply only to data leaving Compute Engine or Cloud Storage. Egress charges for CDN Interconnect appear on the invoice asCompute Engine Data Transfer Out via Carrier Peering Network.Forinter-regionCDN Interconnect traffic rates, seeInternet egress rates. Forinter-regionCDN Interconnect traffic rates, seeInternet egress rates. Intra-regionpricing for CDN Interconnect applies only to intra-region egress traffic that is sent toGoogle-approved CDN Interconnect providersat specific locations that Google approves for those providers. Consult with your CDN provider to verify that they are an approved provider, and if so, which of their CDN locations are approved for this program. They can help you set up your deployment to use intra-region egress routes when using Google Cloud as the origin source. Review the list"
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect",
    "title": "CDN Interconnect overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/cdn-interconnect#chunk-2",
    "content": "of CDN Interconnect service providers and choose the option that best suits your needs. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/code/docs/vscode",
    "title": "Cloud Code for VS Code documentation",
    "chunk_id": "https://cloud.google.com/code/docs/vscode#chunk-0",
    "content": "Home Cloud Code Documentation Cloud Code for VS Code Cloud Code for VS Code provides IDE support for the full development cycle of Kubernetes and Cloud Run applications, from creating a cluster to running your finished application, and for the development of APIs using Apigee. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Install the Cloud Code extension Install the Cloud Code extension Code with Gemini Code Assist Standard and Enterprise Code with Gemini Code Assist Standard and Enterprise Quickstart: Deploy a Kubernetes app with Cloud Code for VS Code Quickstart: Deploy a Kubernetes app with Cloud Code for VS Code Quickstart: Deploy a Cloud Run service with Cloud Code for VS Code Quickstart: Deploy a Cloud Run service with Cloud Code for VS Code Quickstart: Deploy a Kubernetes app by using remote development Quickstart: Deploy a Kubernetes app by using remote development Manage Cloud APIs and Libraries Manage Cloud APIs and Libraries Get started with Kubernetes in Cloud Code Get started with Kubernetes in Cloud Code Debug a Kubernetes application Debug a Kubernetes application Work with Google Cloud and Kubernetes YAML Work with Google Cloud and Kubernetes YAML Develop a Cloud Run service locally Develop a Cloud Run service locally Develop APIs using Apigee Develop APIs using Apigee Usage statistics Usage statistics Getting support Getting support Release notes Release notes Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle"
  },
  {
    "source_url": "https://cloud.google.com/code/docs/vscode",
    "title": "Cloud Code for VS Code documentation",
    "chunk_id": "https://cloud.google.com/code/docs/vscode#chunk-1",
    "content": "Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/ai-ml",
    "title": "AI and ML",
    "chunk_id": "https://cloud.google.com/docs/ai-ml#chunk-0",
    "content": "Home Documentation Leverage the power of AI/ML models and solutions to transform your organization and solve real-world problems. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Build AI applications with enterprise-grade scaling, security, and observability. Apply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs. Apply Google's state-of-the-art capabilities to handle your document management needs. Apply Google's state-of-the-art capabilities to handle your industry-specific needs. Apply Google's state-of-the-art capabilities to handle your video, images, vision, and augmented reality needs. Apply Google's state-of-the-art capabilities to handle your search and recommendations needs. Apply Google's state-of-the-art capabilities to handle your conversation, speech, and customer service needs. Train ML models from your data using AutoML or your preferred ML framework. Apply operations best practices to monitor and improve your deployed ML models. Accelerate machine learning workloads. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/terraform",
    "title": "Terraform on Google Cloud documentation",
    "chunk_id": "https://cloud.google.com/docs/terraform#chunk-0",
    "content": "Home Documentation Terraform on Google Cloud Learn how to use Terraform to reliably provision infrastructure on Google Cloud.Learn more Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. TutorialGet started with Terraform Get started with Terraform ReferenceBasic Terraform commands Basic Terraform commands TutorialStore Terraform state Store Terraform state TutorialHashiCorp tutorials HashiCorp tutorials ReferenceGoogle Cloud provider Google Cloud provider Best practiceTerraform best practices Terraform best practices TutorialManage infrastructure as code Manage infrastructure as code ReferenceGet support for Terraform issues Get support for Terraform issues TutorialExport resources into Terraform Export resources into Terraform TutorialImport resources into Terraform state Import resources into Terraform state TutorialCreate a configuration with Service Catalog Create a configuration with Service Catalog TechnicalUse policy validation Use policy validation Learn Terraform fundamentals In this lab, you install Terraform and create a VM instance using Terraform. Learn how to automate Infrastructure on Google Cloud with Terraform In this lab, you write infrastructure as code with Terraform. Learn how to build Cloud Infrastructure with Terraform In this lab, you learn how to describe and launch cloud resources with Terraform. Learn about managing state In this lab, you learn how to store Terraform state in Google Cloud Storage. Learn to use Terraform modules In this lab, you learn how modules can address problems of code complexity, duplication, and reuse. Learn to use policy"
  },
  {
    "source_url": "https://cloud.google.com/docs/terraform",
    "title": "Terraform on Google Cloud documentation",
    "chunk_id": "https://cloud.google.com/docs/terraform#chunk-1",
    "content": "validation In this lab, you learn how to enforce policies on Terraform configurations. Resource samples Find samples to build your infrastructure. Blueprints Find deployable, reusable Terraform modules. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/datasets/overview",
    "title": "Overview of creating managed datasets on Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/datasets/overview#chunk-0",
    "content": "Home Vertex AI Documentation You can use a managed dataset to provide the source data used to train AutoML and custom models on Vertex AI. A managed dataset is required for AutoML and is optional for custom training. You can create managed datasets for training AutoML models by using the Google Cloud console or the Vertex AI API. The instructions for how to do this slightly vary based on your data type and model objective. Start by preparing your training data. Learn how to create a managed dataset for the following types of image AutoML models: Image classification models Image object detection models Learn how to create a managed dataset for the following types of tabular AutoML models: Tabular classification and regression models Tabular forecasting models Learn how to create a managed dataset for the following types of text AutoML models: Text classification models Text entity extraction models Text sentiment analysis models Learn how to create a managed dataset for the following types of video AutoML models: Video action recognition models Video classification models Video object tracking models The instructions on how to create a managed dataset for training custom models are the same, regardless of your data type or model objective. For details, seeUse managed datasets. Data Catalog is a fully managed, scalable metadata management service within Dataplex which provides a centralized location to search for datasets across projects and regions. For details, seeUse Data Catalog to search for model and dataset resourcesoverview. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/datasets/overview",
    "title": "Overview of creating managed datasets on Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/datasets/overview#chunk-1",
    "content": "registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/postgres",
    "title": "Cloud SQL for PostgreSQL documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/postgres#chunk-0",
    "content": "Home Cloud SQL Documentation PostgreSQL Cloud SQL for PostgreSQL is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform. Learn more Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create instances Create instances Connection overview Connection overview Enable and disable high availability on an instance Enable and disable high availability on an instance Create and manage PostgreSQL databases Create and manage PostgreSQL databases Create and manage PostgreSQL users Create and manage PostgreSQL users Export and import using pg_dump, pg_dumpall, and pg_restore Export and import using pg_dump, pg_dumpall, and pg_restore Export and import using CSV files Export and import using CSV files Create backups Create backups Create read replicas Create read replicas Build generative AI applications using Cloud SQL Build generative AI applications using Cloud SQL Client libraries and sample code for Cloud SQL Client libraries and sample code for Cloud SQL gcloud sql command-line gcloud sql command-line Use the Cloud SQL Admin API Use the Cloud SQL Admin API REST API REST API Best practices Best practices Performance tips Performance tips Authorize requests Authorize requests Configure VPC Service Controls Configure VPC Service Controls Cloud SQL Admin API error messages Cloud SQL Admin API error messages Pricing Pricing Quotas and limits Quotas and limits Troubleshoot Troubleshoot Cloud SQL feature support by database engine Cloud SQL feature support by database engine Release notes Release notes Billing questions Billing questions"
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/postgres",
    "title": "Cloud SQL for PostgreSQL documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/postgres#chunk-1",
    "content": "Getting support Getting support Security bulletins Security bulletins Using Cloud SQL for PostgreSQL with Ruby on Rails 5 Learn how to connect a Ruby on Rails 5 app to Cloud SQL for PostgreSQL. Connecting to Cloud SQL with Cloud Functions Learn how to connect to Cloud SQL from Cloud Functions. Deploying Pega using Compute Engine and Cloud SQL Learn how to deploy Pega Platform, which is a business process management and customer relationship management (CRM) platform with Cloud SQL for PostgreSQL.CRMReference architecture Production launch checklist This checklist provides recommended activities to complete for launching a commercial application that uses Cloud SQL. This checklist focuses on Cloud SQL-specific activities.ProductionLaunch Data residency overview Learn how to use Cloud SQL to enforce data residency requirements for data.Data residencydata Use Secret Manager to handle secrets in Cloud SQL Learn how to use Secret Manager to store sensitive information about Cloud SQL instances and users as secrets.Secret Managersecret Python SQLAlchemy Use SQLAlchemy with your Cloud SQL for PostgreSQL database Node.js sample Connecting to your Cloud SQL for PostgreSQL database in Node.js PHP PDO Connecting your Cloud SQL for PostgreSQL database using PHP PDO Go web app sample Simple examples of connecting to Cloud SQL for PostgreSQL using Go .NET sample This sample application demonstrates how to store data in Google Cloud SQL with a PostgreSQL database when running in Google App Engine Flexible Environment. Terraform for Cloud SQL networking Use Terraform to create Cloud SQL for PostgreSQL instances with private networking options. Java servlet Connecting to Cloud SQL for PostgreSQL from a Java application Except as otherwise noted, the content of this page is licensed"
  },
  {
    "source_url": "https://cloud.google.com/sql/docs/postgres",
    "title": "Cloud SQL for PostgreSQL documentation",
    "chunk_id": "https://cloud.google.com/sql/docs/postgres#chunk-2",
    "content": "under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-0",
    "content": "Home Cloud Key Management Service Documentation Guides This topic provides an overview of Cloud HSM and shows you how to create and use HSM-protected encryption keys in Cloud Key Management Service. Cloud HSM is a cloud-hosted Hardware Security Module (HSM) service that allows you to host encryption keys and perform cryptographic operations in a cluster ofFIPS 140-2 Level 3certified HSMs. Google manages the HSM cluster for you, so you don't need to worry about clustering, scaling, or patching. Because Cloud HSM uses Cloud KMS as its front end, you can leverage all the conveniences and features that Cloud KMS provides. When you create a key, you add it to a key ring in a given Google Cloud location. You can create a new key ring or use an existing one. In this topic, you create a new key ring and add a new key to it. Create a key ring in a Google Cloudlocationthat supports Cloud HSM. Go to theKey Managementpage in the Google Cloud console.Go to Key Management Go to theKey Managementpage in the Google Cloud console. Go to Key Management ClickCreate key ring. ClickCreate key ring. ForKey ring name, enter a name for your key ring. ForKey ring name, enter a name for your key ring. ForKey ring location, select a location like\"us-east1\".Note:Choose a location that is near the resources you want to protect. For CMEK usage, your key ring should be in the same location as the resources you use it with. For Cloud EKM keys, the location must be physically close to your external key manager (EKM) vendor. ForKey ring location, select a location like\"us-east1\". ClickCreate. ClickCreate. In the Google Cloud console, activate Cloud Shell.Activate Cloud ShellIn your environment, run thegcloud kms keyrings createcommand:gcloudkmskeyringscreateKEY_RING\\--locationLOCATIONReplace the"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-1",
    "content": "following:KEY_RING: the name of the key ring that contains the key.LOCATION: the Cloud KMS location of the key ring.For information on all flags and possible values, run the command with the--helpflag. In the Google Cloud console, activate Cloud Shell. Activate Cloud Shell In your environment, run thegcloud kms keyrings createcommand:gcloudkmskeyringscreateKEY_RING\\--locationLOCATIONReplace the following:KEY_RING: the name of the key ring that contains the key.LOCATION: the Cloud KMS location of the key ring.For information on all flags and possible values, run the command with the--helpflag. Replace the following: KEY_RING: the name of the key ring that contains the key. LOCATION: the Cloud KMS location of the key ring. For information on all flags and possible values, run the command with the--helpflag. To run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK. To run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK. To run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK. To run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK. To run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK. To run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK. To run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK. These examples usecurlas an HTTP client to demonstrate using the API. For more information about access control, seeAccessing the Cloud KMS API. Replace the following: PROJECT_ID: the ID of the project that contains the key ring. KEY_RING: the name of the key ring that contains the key. LOCATION: the Cloud KMS"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-2",
    "content": "location of the key ring. See theKeyRing.createAPI documentationfor more information. Follow these steps to create a Cloud HSM key on the specified key ring and location. Go to theKey Managementpage in the Google Cloud console.Go to the Key Management page Go to theKey Managementpage in the Google Cloud console. Go to the Key Management page Click the name of the key ring for which you will create a key. Click the name of the key ring for which you will create a key. ClickCreate key. ClickCreate key. In theWhat type of key do you want to create?, chooseGenerated key. In theWhat type of key do you want to create?, chooseGenerated key. In theKey namefield, enter the name for your key. In theKey namefield, enter the name for your key. Click theProtection leveldropdown and selectHSM. Click theProtection leveldropdown and selectHSM. Click thePurposedropdown and selectSymmetric encrypt/decrypt. Click thePurposedropdown and selectSymmetric encrypt/decrypt. Accept the default values forRotation periodandStarting on. Accept the default values forRotation periodandStarting on. ClickCreate. ClickCreate. To use Cloud KMS on the command line, firstInstall or upgrade to the latest version of Google Cloud CLI. Replacekeywith a name for the new key. Replacekey-ringwith the name of the existing key ring where the key will be located. Replacelocationwith the Cloud KMS location for the key ring. For information on all flags and possible values, run the command with the--helpflag. To run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK. To run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK. To run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK. To run this code, firstset up a"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-3",
    "content": "Node.js development environmentandinstall the Cloud KMS Node.js SDK. To run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK. To run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK. To run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK. Now that you have a key, you can use that key to encrypt text or binary content. To use Cloud KMS on the command line, firstInstall or upgrade to the latest version of Google Cloud CLI. Replace the following: KEY_NAME: the name of the key that you want to use for encryption. KEY_RING: the name of the key ring that contains the key. LOCATION: the Cloud KMS location that contains the key ring. FILE_TO_ENCRYPT: the path to the file that you want to encrypt. ENCRYPTED_OUTPUT: the path where you want to save the encrypted output. For information on all flags and possible values, run the command with the--helpflag. To run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK. To run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK. To run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK. To run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK. To run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK. To run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK. To run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK. These examples usecurlas an HTTP client to demonstrate using the API. For more information about access control, seeAccessing the Cloud KMS API. When using JSON"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-4",
    "content": "and the REST API, content must be base64 encoded before it can be encrypted by Cloud KMS. Tip: You can base64-encode or decode data using thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting languages typically include libraries for base64-encoding. For command-line examples, seeBase64 Encodingin the Cloud Vision API documentation. Tip: You can base64-encode or decode data using thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting languages typically include libraries for base64-encoding. For command-line examples, seeBase64 Encodingin the Cloud Vision API documentation. To encrypt data, make aPOSTrequest and provide the appropriate project and key information and specify the base64 encoded text to be encrypted in theplaintextfield of the request body. Replace the following: PROJECT_ID: the ID of the project that contains the key ring and key that you want to use for encryption. LOCATION: the Cloud KMS location that contains the key ring. KEY_RING: the key ring that contains the key that you want to use for encryption. KEY_NAME: the name of the key that you want to use for encryption. PLAINTEXT_TO_ENCRYPT: the plaintext data that you want to encrypt. The plaintext must be base64 encoded before you call theencryptmethod. Here is an example payload with base64 encoded data: To decrypt encrypted content, you must use the same key that was used to encrypt the content. To use Cloud KMS on the command line, firstInstall or upgrade to the latest version of Google Cloud CLI. Replace the following: KEY_NAME: the name of the key that you want to use for decryption. KEY_RING: the name of the key ring that contains the key. LOCATION: the Cloud KMS location that contains the key ring."
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-5",
    "content": "FILE_TO_DECRYPT: the path to the file that you want to decrypt. DECRYPTED_OUTPUT: the path where you want to save the decrypted output. For information on all flags and possible values, run the command with the--helpflag. To run this code, firstset up a C# development environmentandinstall the Cloud KMS C# SDK. To run this code, firstset up a Go development environmentandinstall the Cloud KMS Go SDK. To run this code, firstset up a Java development environmentandinstall the Cloud KMS Java SDK. To run this code, firstset up a Node.js development environmentandinstall the Cloud KMS Node.js SDK. To run this code, first learn aboutusing PHP on Google Cloudandinstall the Cloud KMS PHP SDK. To run this code, firstset up a Python development environmentandinstall the Cloud KMS Python SDK. To run this code, firstset up a Ruby development environmentandinstall the Cloud KMS Ruby SDK. These examples usecurlas an HTTP client to demonstrate using the API. For more information about access control, seeAccessing the Cloud KMS API. Decrypted text that is returned in the JSON from Cloud KMS is base64 encoded. Tip: You can base64-encode or decode data using thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting languages typically include libraries for base64-encoding. For command-line examples, seeBase64 Encodingin the Cloud Vision API documentation. Tip: You can base64-encode or decode data using thebase64command on Linux or macOS, or theBase64.execommand on Windows. Programming and scripting languages typically include libraries for base64-encoding. For command-line examples, seeBase64 Encodingin the Cloud Vision API documentation. To decrypt encrypted data, make aPOSTrequest and provide the appropriate project and key information and"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-6",
    "content": "specify the encrypted text (also known asciphertext) to be decrypted in theciphertextfield of the request body. Replace the following: PROJECT_ID: the ID of the project that contains the key ring and key that you want to use for decryption. LOCATION: the Cloud KMS location that contains the key ring. KEY_RING: the key ring that contains the key that you want to use for decryption. KEY_NAME: the name of the key that you want to use for decryption. ENCRYPTED_DATA: the encrypted data that you want to decrypt. Here is an example payload with base64 encoded data: The encryption example in this topic used a symmetric key with theprotection levelof HSM. To encrypt using an asymmetric key with the protection level of HSM, follow the steps atEncrypting and decrypting data with an asymmetric keywith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM. The encryption example in this topic used a symmetric key with theprotection levelof HSM. To encrypt using an asymmetric key with the protection level of HSM, follow the steps atEncrypting and decrypting data with an asymmetric keywith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM. Create the key ring in one of thesupported regions for Cloud HSM. Create the key with protection level HSM. To use an asymmetric key with theprotection levelof HSM for elliptic curve signing or RSA signing, follow the steps atCreate and validate signatureswith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM. To use an asymmetric key with theprotection levelof HSM for elliptic curve signing or RSA signing, follow the steps atCreate and validate"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-7",
    "content": "signatureswith these changes:Create the key ring in one of thesupported regions for Cloud HSM.Create the key with protection level HSM. Create the key ring in one of thesupported regions for Cloud HSM. Create the key with protection level HSM. Startusing the API. Startusing the API. Take a look at theCloud KMS API Reference. Take a look at theCloud KMS API Reference. ReadHow-to guidesto get started with creating, rotating, and setting permissions on keys. ReadHow-to guidesto get started with creating, rotating, and setting permissions on keys. ReadConceptsto better understand object hierarchy, key states, and key rotation. ReadConceptsto better understand object hierarchy, key states, and key rotation. Learn aboutLoggingin Cloud KMS. Note that logging is based on operations, and applies to keys with both HSM and software protection levels. Learn aboutLoggingin Cloud KMS. Note that logging is based on operations, and applies to keys with both HSM and software protection levels. Learn more about how Cloud HSM protects your data in theCloud HSM architecture whitepaper. Learn more about how Cloud HSM protects your data in theCloud HSM architecture whitepaper. Message size is limited to 8 KiB (as opposed to 64 KiB for Cloud KMS software keys) for user-provided plaintext and ciphertext, including theadditional authenticated data. Message size is limited to 8 KiB (as opposed to 64 KiB for Cloud KMS software keys) for user-provided plaintext and ciphertext, including theadditional authenticated data. Cloud HSM may not be available in certain multi or dual regions. For details, seeSupported regions for Cloud HSM. Cloud HSM may not be available in certain multi or dual regions. For details, seeSupported regions for Cloud HSM. If you use Cloud HSM keys with customer-managed"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/hsm",
    "title": "Cloud HSMStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/hsm#chunk-8",
    "content": "encryption key (CMEK) integrations in other Google Cloud services, the locations you use for the services must match the locations of your Cloud HSM keys exactly. This applies to regional, dual-regional, and multi-regional locations.For more information about CMEK integrations, see the relevant section ofEncryption at rest. If you use Cloud HSM keys with customer-managed encryption key (CMEK) integrations in other Google Cloud services, the locations you use for the services must match the locations of your Cloud HSM keys exactly. This applies to regional, dual-regional, and multi-regional locations. For more information about CMEK integrations, see the relevant section ofEncryption at rest. Currently key operations for asymmetric keys stored in Cloud HSM may incur a noticeably greater latency compared to using Cloud KMS software keys. Currently key operations for asymmetric keys stored in Cloud HSM may incur a noticeably greater latency compared to using Cloud KMS software keys. Google Cloud offers additional HSM options, such as single-tenancy.Bare Metal Rack HSMis available for customers to host their own HSMs in Google-provided space. Inquire with your account representative for additional information. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-15 UTC."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/images#chunk-0",
    "content": "Home Compute Engine Documentation Guides Use operating system (OS) images to create boot disks for your virtual machine (VM) instances. You can use one of the following OS image types: Public OS imagesare provided and maintained by Google, open source communities, and third-party vendors. By default, all Google Cloud projects have access to these OS images and can use them tocreate VM instances. Custom OS imagesare available only to your Google Cloud project. You cancreate a custom OS imagefrom boot disks and other images. Then, use the custom OS image tocreate VM instances. Some OS images are also capable of runningcontainers on Compute Engine. Compute Engine offers many preconfigured public OS images that have compatible Linux or Windows operating systems. Use these OS images tocreate and start instances. Compute Engine uses your selected image to create a persistent boot disk for each VM. By default, the boot disk for a VM is the same size as the image that you selected. If your VM requires a larger boot disk than the image size,resize the boot disk. To see a full list of public OS images with each image's name, size, and version number, you can use the Google Cloud console or the Google Cloud CLI. Compute Engine updates public OS images regularly, or when a patch for a critical impact common vulnerability and exposure (CVE) is available. Compute Engine provides 64-bit versions of these public OS images. For more information about each OS, including how each OS is customized to run on Compute Engine, seeOperating system details. In the Google Cloud console, go to theImagespage.Go to ImagesBy default, the Google Cloud console list all OS images available in theCompute Engine images,Deep Learning VM Images, andHPC imagesprojects. In the Google Cloud console, go to"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/images#chunk-1",
    "content": "theImagespage. Go to Images By default, the Google Cloud console list all OS images available in theCompute Engine images,Deep Learning VM Images, andHPC imagesprojects. By default, the gcloud CLI list all OS images available in theCompute Engine imagesprojects. A custom OS image is a boot disk image that you own and control access to. Use custom OS images for the following tasks: Import a virtual diskto Compute Engine from your on-premises environment or from VMs that are running on your local workstation or on another cloud platform. You can manuallyimport boot disk imagesto Compute Engine, but one disk at a time.Note:If you are planning to migrate several VMs to Compute Engine, consider using theVM migration service. Import a virtual diskto Compute Engine from your on-premises environment or from VMs that are running on your local workstation or on another cloud platform. You can manuallyimport boot disk imagesto Compute Engine, but one disk at a time. Create an imagefrom the boot disks of your existing Compute Engine VM instances. Then use that image tocreate new boot disksfor your VMs. This process lets you create new VMs that are preconfigured with the apps that you need without having to configure apublic OS imagefrom scratch. Create an imagefrom the boot disks of your existing Compute Engine VM instances. Then use that image tocreate new boot disksfor your VMs. This process lets you create new VMs that are preconfigured with the apps that you need without having to configure apublic OS imagefrom scratch. Copy one image to another image by using either thegcloud CLIor theAPI. Use the same process that you use tocreate an image, but specify another image as the image source. You can also create an image from a custom image in a different project. Copy one image to"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/images#chunk-2",
    "content": "another image by using either thegcloud CLIor theAPI. Use the same process that you use tocreate an image, but specify another image as the image source. You can also create an image from a custom image in a different project. Some guest operating system features are available only on certain OS images. For example,multiqueue SCSIis enabled only on some public OS images. To enable these features on your custom OS images, specify one or more guest operating system features when youcreate a custom OS image. Premium OS images, whether public or custom, incur licensing fees to run on Compute Engine. You have two options: Attach an on-demand/pay-as-you-go (PAYG) license Bring your own license (BYOL)/Bring your own subscription (BYOS)For more information about licenses, seeLicense types and pricing. Bring your own license (BYOL)/Bring your own subscription (BYOS) For more information about licenses, seeLicense types and pricing. For custom OS images, you also incur animage storage chargewhile you keep your custom OS image in your project. Image families help you manage images in your project by grouping related images together, so that you can roll forward and roll back between specific image versions. An image family always points to the latest version of an OS image that is not deprecated. Mostpublic OS imagesare grouped into an image family. For example, thedebian-11image family in thedebian-cloudproject always points to the most recent Debian 11 image. If you regularly update yourcustom OS imageswith newer configurations and software, you can group those images into a custom image family. The image family always points to the most recent OS image in that family, so your instance templates and scripts can use that image without having to update references to a specific"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/images#chunk-3",
    "content": "image version. Also, because the image family never points to a deprecated image, you can roll the image family back to a previous OS image version by deprecating the most recent image in that family. Note that the rollback is possible only if the previous image version is not deprecated. For more information, seeSetting image versions in an image family. For best practices recommendations when working with image families, seeImage families best practices. These are operating systems that you can run on Google Cloud, but the partner or distributor is responsible for ensuring that these operating systems work with Google Cloud features and that security updates are maintained. For issues specific to the partner supported operating systems, you must use either community resources or get enterprise-level support from the partner. The following partner supported operating systems can run on Google Cloud. Oracle Linux is an operating system that is offered by Oracle. Oracle Linux images are available on Google Cloud provided by Oracle. You can also import Oracle Linux images to Google Cloud. If you require support that is specific to the Oracle Linux operating system, you can either consult community resources, or get enterprise-level support directly from Oracle. Import Oracle Linux OS images To import Oracle Linux OS image to Compute Engine, you can use the import tool available from Migrate to Virtual Machines. This tool ensures the imported OS images are set up correctly for working in a Google Cloud environment. For detailed instructions, seeImport virtual disk images. For a list of the Oracle Linux OS versions supported for import, seeOperating systems supported by partners. Community-supported OS images are not directly supported by Google Cloud. It is up to the"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/images#chunk-4",
    "content": "project community to ensure that these OS images work with Google Cloud features and that security updates are maintained. Community-supported images are provided as-is by the project communities that build and maintain them. The following community supported images can run on Google Cloud. AlmaLinux is an operating system offered by theAlmaLinux project. AlmaLinux images are available in thealmalinux-cloudproject. To list AlmaLinux OS images, use the followinggcloudcommand: Fedora Cloud is an operating system maintained by theFedora Cloud project. Fedora Cloud images are available in thefedora-cloudproject. To list Fedora Cloud OS images, use the followinggcloudcommand: FreeBSD is an operating system maintained by theFreeBSD project. FreeBSD images are available in thefreebsd-org-cloud-devproject. To list FreeBSD OS images, use the followinggcloudcommand: gVNIC support for FreeBSD (Preview) is available with release 14.0 and later. To use gVNIC with other releases, the driver can beinstalled manually. To create a VM that uses gVNIC with a FreeBSD release earlier than 14.0, you mustcreate a custom OS image that supports gVNICand then use that OS image when creating the VM. openSUSEis a Linux-based operating system sponsored by SUSE. openSUSE images are available in theopensuse-cloudproject. To list openSUSE OS images, use the followinggcloudcommand: The following OS images are available for creating VMs that are optimized to run high performance computing (HPC) workloads on Compute Engine: For CentOS 7: Image family:hpc-centos-7, Image project:cloud-hpc-image-public For Rocky Linux 8: Image family:hpc-rocky-linux-8, Image project:cloud-hpc-image-public For information about using this OS image, seeCreating an HPC-ready VM instance. View the source imagefor a VM"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/images",
    "title": "OS imagesStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/images#chunk-5",
    "content": "instance. ReadImage management best practices. Learn aboutSupport and maintenance policy for OS images. Create and start an instance. Read aboutCompute Engine instances. Create a custom image. Build an image from scratch. If you're new to Google Cloud, create an account to evaluate how Compute Engine performs in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/network-connectivity-center",
    "title": "Network Connectivity Center documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/network-connectivity-center#chunk-0",
    "content": "Home Network Connectivity Documentation Network Connectivity Center Network Connectivity Center lets you use a hub-and-spoke architecture for network connectivity management. With this architecture, you can conduct data transfer between your sites. Network Connectivity Center lets you create VPC spokes to connect VPC networks together for full mesh connectivity. It also includes the Router appliance feature, which lets you use a third-party network virtual appliance to establish site-to-site or site-to-cloud connectivity. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Working with hubs and spokes Working with hubs and spokes Viewing logs and metrics Viewing logs and metrics Creating router appliance instances Creating router appliance instances Deleting router appliance instances Deleting router appliance instances Network Connectivity Center overview Network Connectivity Center overview VPC spokes overview VPC spokes overview Router appliance overview Router appliance overview Topologies Topologies Locations Locations Access control Access control Audit logging Audit logging APIs and gcloud reference APIs and gcloud reference Troubleshooting Troubleshooting Quotas and limits Quotas and limits Partners Partners Pricing Pricing Release notes Release notes Getting support Getting support Billing questions Billing questions Connecting two branch offices using HA VPN spokes This tutorial describes how to use a Network Connectivity Center hub and Cloud VPN spokes to set up data transfer between two branch offices. Except as otherwise noted, the content of this page is licensed"
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/network-connectivity-center",
    "title": "Network Connectivity Center documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/network-connectivity-center#chunk-1",
    "content": "under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-0",
    "content": "Home Google Cloud Observability Logging Documentation Guides This document provides a conceptual overview of Cloud Audit Logs. Google Cloud services write audit logs that record administrative activities and accesses within your Google Cloud resources. Audit logs help you answer \"who did what, where, and when?\" within your Google Cloud resources with the same level of transparency as in on-premises environments. Enabling audit logs helps your security, auditing, and compliance entities monitor Google Cloud data and systems for possible vulnerabilities or external data misuse. For a list of Google Cloud services that provide audit logs, seeGoogle Cloud services with audit logs. All Google Cloud services will eventually provide audit logs. For an overview of Google Workspace audit logs, seeAudit logs for Google Workspace. To view audit logs, you must have the appropriateIdentity and Access Management (IAM)permissions and roles: To get the permissions that you need to get read-only access to Admin Activity, Policy Denied, and System Event audit logs, ask your administrator to grant you theLogs Viewer(roles/logging.viewer) IAM role on your project.If you have only the Logs Viewer role(roles/logging.viewer), then you cannot view Data Access audit logs that are in the_Defaultbucket.To get the permissions that you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs, ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer) IAM role on your project.The Private Logs Viewer role(roles/logging.privateLogViewer)includes the permissions contained in the Logs Viewer role (roles/logging.viewer), and those necessary to read Data Access audit logs in the_Defaultbucket.For more information about the IAM permissions"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-1",
    "content": "and roles that apply to audit logs data, seeAccess control with IAM.Types of audit logsCloud Audit Logs provides the following audit logs for each Google Cloud project, folder, and organization:Admin Activity audit logsData Access audit logsSystem Event audit logsPolicy Denied audit logsNote:Log entries written by Cloud Audit Logs are immutable.Admin Activity audit logsAdmin Activity audit logs contain log entries for API calls or other actions that modify the configuration or metadata of resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions.Admin Activity audit logs are always written; you can't configure, exclude, or disable them. Even if you disable the Cloud Logging API, Admin Activity audit logs are still generated.For a list of services that write Admin Activity audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logsData Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data.Publicly available resources that have the Identity and Access Management policiesallAuthenticatedUsersorallUsersdon't generate audit logs. Resources that can be accessed without logging into a Google Cloud, Google Workspace, Cloud Identity, or Drive Enterprise account don't generate audit logs. This helps protect end-user identities and information.Data Access audit logs\u2014except for BigQuery Data Access audit logs\u2014are disabled by default because audit logs can be quite large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable them. Enabling"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-2",
    "content": "the logs might result in your Google Cloud project being charged for the additional logs usage. For instructions on enabling and configuring Data Access audit logs, seeEnable Data Access audit logs.For a list of services that write Data Access audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logs are stored in the_Defaultlog bucket unless you've routed them elsewhere. For more information, see theStoring and routing audit logssection of this page.System Event audit logsSystem Event audit logs contain log entries for Google Cloud actions that modify the configuration of resources. System Event audit logs are generated by Google Cloud systems; they aren't driven by direct user action.System Event audit logs are always written; you can't configure, exclude, or disable them.For a list of services that write System Event audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Policy Denied audit logsPolicy Denied audit logs are recorded when a Google Cloud service denies access to a user orservice accountbecause of a security policy violation.Policy Denied audit logs are generated by default and your Google Cloud project is charged for the logs storage. You can't disable Policy Denied audit logs, but you can useexclusion filtersto prevent Policy Denied audit logs from being stored in Cloud Logging.For a list of services that write Policy Denied audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Audit log entry structureEvery audit log entry in Cloud Logging is an object of typeLogEntry. What distinguishes an audit log entry from other log entries is"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-3",
    "content": "theprotoPayloadfield; this field contains anAuditLogobject that stores the audit logging data.To understand how to read and interpret audit log entries, and for a sample of an audit log entry, seeUnderstanding audit logs.Log nameCloud Audit Logs log names include the following:Resource identifiers indicating the Google Cloud project or other Google Cloud entity that owns the audit logs.The stringcloudaudit.googleapis.com.A string that indicates whether the log contains Admin Activity, Data Access, Policy Denied, or System Event audit logging data.The following are the audit log names, including variables for the resource identifiers:projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivityprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fpolicyfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Factivityfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fdata_accessfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2FpolicybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2FactivitybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fpolicyorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Factivityorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fdata_accessorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2FpolicyCaller identities in audit logsAudit logs record the identity that"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-4",
    "content": "performed the logged operations on the Google Cloud resource. The caller's identity is held in theAuthenticationInfofield ofAuditLogobjects.Audit logging doesn't redact the caller's principal email address for any access that succeeds or for any write operation.For read-only operations that fail with a \"permission denied\" error, Audit Logging might redact the caller's principal email address unless the caller is a service account.In addition to the conditions listed above, the following applies to certain Google Cloud services:Legacy App Engine API: Identities aren't collected.BigQuery: Caller identities and IP addresses, as well as some resource names, are redacted from the audit logs, unless certain conditions are met.Cloud Storage: When Cloud Storage usage logs are enabled, Cloud Storage writes usage data to the Cloud Storage bucket, which generates Data Access audit logs for the bucket. The generated Data Access audit log has its caller identity redacted.Firestore: If a JSON Web Token (JWT) was used for third-party authentication, thethirdPartyPrincipalfield includes the token's header and payload. For example, audit logs for requests authenticated withFirebase Authenticationinclude that request'sauth token.VPC Service Controls: For Policy Denied audit logs, the following redaction occurs:Parts of the caller email addresses might be redacted and replaced by three period characters....Some caller email addresses belonging to the domaingoogle.comare redacted and replaced bygoogle-internal.Organization Policy: Parts of the caller email addresses might be redacted and replaced by three period characters....IP address of the caller in audit logsThe IP address of the caller is held in theRequestMetadata.callerIpfield of theAuditLogobject:For a caller from the internet,"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-5",
    "content": "the address is a public IPv4 or IPv6 address.For calls made from inside the internal production network from one Google Cloud service to another, thecallerIpis redacted to \"private\".For a caller from a Compute Engine VM with a external IP address, thecallerIpis the external address of the VM.For a caller from a Compute Engine VM without a external IP address, if the VM is in the same organization or project as the accessed resource, thencallerIpis the VM's internal IPv4 address. Otherwise, thecallerIpis redacted to \"gce-internal-ip\". For more information, seeVPC network overview.Viewing audit logsYou can query for all audit logs or you can query for logs by their audit log name. The audit log name includes theresource identifierof the Google Cloud project, folder, billing account, or organization for which you want to view audit logging information. Your queries can specify indexedLogEntryfields. For more information about querying your logs, seeBuild queries in the Logs ExplorerThe Logs Explorer lets you view filter individual log entries. If you want to use SQL to analyze groups of log entries, then use theLog Analyticspage. For more information, see:Query and view logs in Log Analytics.Sample queries for security insights.Chart query results.Most audit logs can be viewed in Cloud Logging by using the Google Cloud console, the Google Cloud CLI, or the Logging API. However, for audit logs related to billing, you can only use the Google Cloud CLI or the Logging API.ConsoleIn the Google Cloud console, you can use the Logs Explorer to retrieve your audit log entries for your Google Cloud project, folder, or organization:Note:You can't view audit logs for Cloud Billing accounts in the Google Cloud console. You must use the API or the gcloud CLI.In the Google Cloud console,"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-6",
    "content": "go to theLogs Explorerpage:Go toLogs ExplorerIf you use the search bar to find this page, then select the result whose subheading isLogging.Select an existing Google Cloud project, folder, or organization.To display all audit logs, enter either of the following queries into the query-editor field, and then clickRun query:logName:\"cloudaudit.googleapis.com\"protoPayload.\"@type\"=\"type.googleapis.com/google.cloud.audit.AuditLog\"To display the audit logs for a specific resource and audit log type, in theQuery builderpane, do the following:InResource type, select the Google Cloud resource whose audit logs you want to see.InLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.ClickRun query.If you don't see these options, then there aren't any audit logs of that type available in the Google Cloud project, folder, or organization.If you're experiencing issues when trying to view logs in the Logs Explorer, see thetroubleshootinginformation.For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer.gcloudThe Google Cloud CLI provides a command-line interface to the Logging API. Supply a valid resource identifier in each of the log names. For example, if your query includes aPROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project.To read your Google Cloud project-level audit log entries, run the following command:gcloud logging read \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" \\ --project=PROJECT_IDTo read your folder-level audit log entries, run the following command:gcloud logging"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-7",
    "content": "read \"logName : folders/FOLDER_ID/logs/cloudaudit.googleapis.com\" \\ --folder=FOLDER_IDTo read your organization-level audit log entries, run the following command:gcloud logging read \"logName : organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com\" \\ --organization=ORGANIZATION_IDTo read your Cloud Billing account-level audit log entries, run the following command:gcloud logging read \"logName : billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com\" \\ --billing-account=BILLING_ACCOUNT_IDAdd the--freshnessflagto your command to read logs that are more than 1 day old.For more information about using the gcloud CLI, seegcloud logging read.RESTWhen building your queries, supply a valid resource identifier in each of the log names. For example, if your query includes aPROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project.For example, to use the Logging API to view your project-level audit log entries, do the following:Go to theTry this APIsection in the documentation for theentries.listmethod.Put the following into theRequest bodypart of theTry this APIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.{ \"resourceNames\": [ \"projects/PROJECT_ID\" ], \"pageSize\": 5, \"filter\": \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" }ClickExecute.Storing and routing audit logsCloud Logging useslog bucketsas containers that store and organize your logs data. For each billing account, Google Cloud project, folder, and organization, Logging automatically creates two log buckets,_Requiredand_Default, and correspondingly namedsinks.Cloud Logging_Requiredbuckets store Admin Activity audit logs and System Event audit logs."
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-8",
    "content": "You can't prevent Admin Activity or System Event audit logs from being stored. You also can't configure the sink that routes log entries to the_Requiredbuckets.Admin Activity audit logs and System Event audit logs are always stored in the_Requiredbucket in the project where the logs were generated.If you route Admin Activity audit logs and System Event audit logs to a different project, then those logs don't pass through the_Defaultor_Requiredsink of the destination project. Therefore, these logs aren't stored in the_Defaultlog bucket or the_Requiredlog bucket of the destination project. To store these logs, create a log sink in the destination project. For more information, seeRoute logs to supported destinations.The_Defaultbuckets, by default, store any enabled Data Access audit logs as well as Policy Denied audit logs. To prevent Data Access audit logs from being stored in the_Defaultbuckets, you can disable them. To prevent any Policy Denied audit logs from being stored in the_Defaultbuckets, you can exclude them by modifying their sinks' filters.You can also route your audit log entries to user-defined Cloud Logging buckets at the Google Cloud project level or to supported destinations outside of Logging using sinks. For instructions on routing logs, seeRoute logs to supported destinations.When configuring your log sinks' filters, you need to specify the audit log types you want to route; for filtering examples, seeSecurity logging queries.If you want to route audit log entries for a Google Cloud organization, folder, or billing account, seeCollate and route organization-level logs to supported destinations.Audit log retentionFor details on how long log entries are retained by Logging, see the retention information inQuotas and limits: Logs retention periods.Access"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-9",
    "content": "controlIAM permissions and roles determine your ability to access audit logs data in theLogging API, theLogs Explorer, and theGoogle Cloud CLI.For detailed information about the IAM permissions and roles you might need, seeAccess control with IAM.Quotas and limitsFor details on logging usage limits, including the maximum sizes of audit logs, seeQuotas and limits.PricingCloud Logging doesn't charge to route logs to a supported destination; however, the destination might apply charges. With the exception of the_Requiredlog bucket, Cloud Logging charges to stream logs into log buckets and for storage longer than the default retention period of the log bucket.Cloud Logging doesn't charge for copying logs, for creatinglog scopesoranalytics views, or for queries issued through theLogs ExplorerorLog Analyticspages.For more information, see the following documents:Cloud Logging pricing summaryDestination costs:Cloud Storage pricingBigQuery pricingPub/Sub pricingCloud Logging pricingVPC flow log generation chargesapply when you send and then exclude your Virtual Private Cloud flow logs from Cloud Logging.What's nextLearn how toread and understand audit logs.Learn how toenable Data Access audit logs.Reviewbest practicesfor Cloud Audit Logs.Learn aboutAccess Transparency, which provides logs of actions taken by Google Cloud staff when accessing your Google Cloud content. To get the permissions that you need to get read-only access to Admin Activity, Policy Denied, and System Event audit logs, ask your administrator to grant you theLogs Viewer(roles/logging.viewer) IAM role on your project.If you have only the Logs Viewer role(roles/logging.viewer), then you cannot view Data Access audit logs that are in the_Defaultbucket. To get the permissions that you need to get read-only"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-10",
    "content": "access to Admin Activity, Policy Denied, and System Event audit logs, ask your administrator to grant you theLogs Viewer(roles/logging.viewer) IAM role on your project. If you have only the Logs Viewer role(roles/logging.viewer), then you cannot view Data Access audit logs that are in the_Defaultbucket. To get the permissions that you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs, ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer) IAM role on your project.The Private Logs Viewer role(roles/logging.privateLogViewer)includes the permissions contained in the Logs Viewer role (roles/logging.viewer), and those necessary to read Data Access audit logs in the_Defaultbucket.For more information about the IAM permissions and roles that apply to audit logs data, seeAccess control with IAM.Types of audit logsCloud Audit Logs provides the following audit logs for each Google Cloud project, folder, and organization:Admin Activity audit logsData Access audit logsSystem Event audit logsPolicy Denied audit logsNote:Log entries written by Cloud Audit Logs are immutable.Admin Activity audit logsAdmin Activity audit logs contain log entries for API calls or other actions that modify the configuration or metadata of resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions.Admin Activity audit logs are always written; you can't configure, exclude, or disable them. Even if you disable the Cloud Logging API, Admin Activity audit logs are still generated.For a list of services that write Admin Activity audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-11",
    "content": "logsData Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data.Publicly available resources that have the Identity and Access Management policiesallAuthenticatedUsersorallUsersdon't generate audit logs. Resources that can be accessed without logging into a Google Cloud, Google Workspace, Cloud Identity, or Drive Enterprise account don't generate audit logs. This helps protect end-user identities and information.Data Access audit logs\u2014except for BigQuery Data Access audit logs\u2014are disabled by default because audit logs can be quite large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable them. Enabling the logs might result in your Google Cloud project being charged for the additional logs usage. For instructions on enabling and configuring Data Access audit logs, seeEnable Data Access audit logs.For a list of services that write Data Access audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Data Access audit logs are stored in the_Defaultlog bucket unless you've routed them elsewhere. For more information, see theStoring and routing audit logssection of this page.System Event audit logsSystem Event audit logs contain log entries for Google Cloud actions that modify the configuration of resources. System Event audit logs are generated by Google Cloud systems; they aren't driven by direct user action.System Event audit logs are always written; you can't configure, exclude, or disable them.For a list of services that write System Event audit logs and detailed information about which activities generate those"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-12",
    "content": "logs, seeGoogle Cloud services with audit logs.Policy Denied audit logsPolicy Denied audit logs are recorded when a Google Cloud service denies access to a user orservice accountbecause of a security policy violation.Policy Denied audit logs are generated by default and your Google Cloud project is charged for the logs storage. You can't disable Policy Denied audit logs, but you can useexclusion filtersto prevent Policy Denied audit logs from being stored in Cloud Logging.For a list of services that write Policy Denied audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs.Audit log entry structureEvery audit log entry in Cloud Logging is an object of typeLogEntry. What distinguishes an audit log entry from other log entries is theprotoPayloadfield; this field contains anAuditLogobject that stores the audit logging data.To understand how to read and interpret audit log entries, and for a sample of an audit log entry, seeUnderstanding audit logs.Log nameCloud Audit Logs log names include the following:Resource identifiers indicating the Google Cloud project or other Google Cloud entity that owns the audit logs.The stringcloudaudit.googleapis.com.A string that indicates whether the log contains Admin Activity, Data Access, Policy Denied, or System Event audit logging data.The following are the audit log names, including variables for the resource"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-13",
    "content": "identifiers:projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivityprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventprojects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fpolicyfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Factivityfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fdata_accessfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventfolders/FOLDER_ID/logs/cloudaudit.googleapis.com%2FpolicybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2FactivitybillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fdata_accessbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventbillingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com%2Fpolicyorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Factivityorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fdata_accessorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2Fsystem_eventorganizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com%2FpolicyCaller identities in audit logsAudit logs record the identity that performed the logged operations on the Google Cloud resource. The caller's identity is held in theAuthenticationInfofield ofAuditLogobjects.Audit logging doesn't redact the caller's principal email address for any access that succeeds or for any write operation.For read-only operations that fail with a \"permission denied\" error, Audit Logging might redact the caller's principal email address unless the caller is a service account.In addition to the conditions listed above, the following applies to certain Google Cloud services:Legacy App Engine API: Identities aren't collected.BigQuery: Caller identities and IP addresses, as"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-14",
    "content": "well as some resource names, are redacted from the audit logs, unless certain conditions are met.Cloud Storage: When Cloud Storage usage logs are enabled, Cloud Storage writes usage data to the Cloud Storage bucket, which generates Data Access audit logs for the bucket. The generated Data Access audit log has its caller identity redacted.Firestore: If a JSON Web Token (JWT) was used for third-party authentication, thethirdPartyPrincipalfield includes the token's header and payload. For example, audit logs for requests authenticated withFirebase Authenticationinclude that request'sauth token.VPC Service Controls: For Policy Denied audit logs, the following redaction occurs:Parts of the caller email addresses might be redacted and replaced by three period characters....Some caller email addresses belonging to the domaingoogle.comare redacted and replaced bygoogle-internal.Organization Policy: Parts of the caller email addresses might be redacted and replaced by three period characters....IP address of the caller in audit logsThe IP address of the caller is held in theRequestMetadata.callerIpfield of theAuditLogobject:For a caller from the internet, the address is a public IPv4 or IPv6 address.For calls made from inside the internal production network from one Google Cloud service to another, thecallerIpis redacted to \"private\".For a caller from a Compute Engine VM with a external IP address, thecallerIpis the external address of the VM.For a caller from a Compute Engine VM without a external IP address, if the VM is in the same organization or project as the accessed resource, thencallerIpis the VM's internal IPv4 address. Otherwise, thecallerIpis redacted to \"gce-internal-ip\". For more information, seeVPC network overview.Viewing audit logsYou can query for all audit"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-15",
    "content": "logs or you can query for logs by their audit log name. The audit log name includes theresource identifierof the Google Cloud project, folder, billing account, or organization for which you want to view audit logging information. Your queries can specify indexedLogEntryfields. For more information about querying your logs, seeBuild queries in the Logs ExplorerThe Logs Explorer lets you view filter individual log entries. If you want to use SQL to analyze groups of log entries, then use theLog Analyticspage. For more information, see:Query and view logs in Log Analytics.Sample queries for security insights.Chart query results.Most audit logs can be viewed in Cloud Logging by using the Google Cloud console, the Google Cloud CLI, or the Logging API. However, for audit logs related to billing, you can only use the Google Cloud CLI or the Logging API.ConsoleIn the Google Cloud console, you can use the Logs Explorer to retrieve your audit log entries for your Google Cloud project, folder, or organization:Note:You can't view audit logs for Cloud Billing accounts in the Google Cloud console. You must use the API or the gcloud CLI.In the Google Cloud console, go to theLogs Explorerpage:Go toLogs ExplorerIf you use the search bar to find this page, then select the result whose subheading isLogging.Select an existing Google Cloud project, folder, or organization.To display all audit logs, enter either of the following queries into the query-editor field, and then clickRun query:logName:\"cloudaudit.googleapis.com\"protoPayload.\"@type\"=\"type.googleapis.com/google.cloud.audit.AuditLog\"To display the audit logs for a specific resource and audit log type, in theQuery builderpane, do the following:InResource type, select the Google Cloud resource whose audit logs you want to see.InLog"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-16",
    "content": "name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.ClickRun query.If you don't see these options, then there aren't any audit logs of that type available in the Google Cloud project, folder, or organization.If you're experiencing issues when trying to view logs in the Logs Explorer, see thetroubleshootinginformation.For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer.gcloudThe Google Cloud CLI provides a command-line interface to the Logging API. Supply a valid resource identifier in each of the log names. For example, if your query includes aPROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project.To read your Google Cloud project-level audit log entries, run the following command:gcloud logging read \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" \\ --project=PROJECT_IDTo read your folder-level audit log entries, run the following command:gcloud logging read \"logName : folders/FOLDER_ID/logs/cloudaudit.googleapis.com\" \\ --folder=FOLDER_IDTo read your organization-level audit log entries, run the following command:gcloud logging read \"logName : organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com\" \\ --organization=ORGANIZATION_IDTo read your Cloud Billing account-level audit log entries, run the following command:gcloud logging read \"logName : billingAccounts/BILLING_ACCOUNT_ID/logs/cloudaudit.googleapis.com\" \\ --billing-account=BILLING_ACCOUNT_IDAdd the--freshnessflagto your command to read logs that are more than 1 day old.For more information about using the"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-17",
    "content": "gcloud CLI, seegcloud logging read.RESTWhen building your queries, supply a valid resource identifier in each of the log names. For example, if your query includes aPROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project.For example, to use the Logging API to view your project-level audit log entries, do the following:Go to theTry this APIsection in the documentation for theentries.listmethod.Put the following into theRequest bodypart of theTry this APIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.{ \"resourceNames\": [ \"projects/PROJECT_ID\" ], \"pageSize\": 5, \"filter\": \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" }ClickExecute. To get the permissions that you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs, ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer) IAM role on your project.The Private Logs Viewer role(roles/logging.privateLogViewer)includes the permissions contained in the Logs Viewer role (roles/logging.viewer), and those necessary to read Data Access audit logs in the_Defaultbucket. To get the permissions that you need to get access to all logs in the_Requiredand_Defaultbuckets, including Data Access logs, ask your administrator to grant you thePrivate Logs Viewer(roles/logging.privateLogViewer) IAM role on your project. The Private Logs Viewer role(roles/logging.privateLogViewer)includes the permissions contained in the Logs Viewer role (roles/logging.viewer), and those necessary to read Data Access audit logs in the_Defaultbucket. For more information about the IAM permissions and roles that apply to audit logs"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-18",
    "content": "data, seeAccess control with IAM. Cloud Audit Logs provides the following audit logs for each Google Cloud project, folder, and organization: Admin Activity audit logs Data Access audit logs System Event audit logs Policy Denied audit logs Admin Activity audit logs contain log entries for API calls or other actions that modify the configuration or metadata of resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions. Admin Activity audit logs are always written; you can't configure, exclude, or disable them. Even if you disable the Cloud Logging API, Admin Activity audit logs are still generated. For a list of services that write Admin Activity audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs. Data Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Publicly available resources that have the Identity and Access Management policiesallAuthenticatedUsersorallUsersdon't generate audit logs. Resources that can be accessed without logging into a Google Cloud, Google Workspace, Cloud Identity, or Drive Enterprise account don't generate audit logs. This helps protect end-user identities and information. Data Access audit logs\u2014except for BigQuery Data Access audit logs\u2014are disabled by default because audit logs can be quite large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable them. Enabling the logs might result in your Google Cloud project being charged for the additional logs usage. For instructions on enabling and configuring Data"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-19",
    "content": "Access audit logs, seeEnable Data Access audit logs. For a list of services that write Data Access audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs. Data Access audit logs are stored in the_Defaultlog bucket unless you've routed them elsewhere. For more information, see theStoring and routing audit logssection of this page. System Event audit logs contain log entries for Google Cloud actions that modify the configuration of resources. System Event audit logs are generated by Google Cloud systems; they aren't driven by direct user action. System Event audit logs are always written; you can't configure, exclude, or disable them. For a list of services that write System Event audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs. Policy Denied audit logs are recorded when a Google Cloud service denies access to a user orservice accountbecause of a security policy violation. Policy Denied audit logs are generated by default and your Google Cloud project is charged for the logs storage. You can't disable Policy Denied audit logs, but you can useexclusion filtersto prevent Policy Denied audit logs from being stored in Cloud Logging. For a list of services that write Policy Denied audit logs and detailed information about which activities generate those logs, seeGoogle Cloud services with audit logs. Every audit log entry in Cloud Logging is an object of typeLogEntry. What distinguishes an audit log entry from other log entries is theprotoPayloadfield; this field contains anAuditLogobject that stores the audit logging data. To understand how to read and interpret audit log entries, and for a sample of an audit log entry, seeUnderstanding"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-20",
    "content": "audit logs. Cloud Audit Logs log names include the following: Resource identifiers indicating the Google Cloud project or other Google Cloud entity that owns the audit logs. Resource identifiers indicating the Google Cloud project or other Google Cloud entity that owns the audit logs. The stringcloudaudit.googleapis.com. The stringcloudaudit.googleapis.com. A string that indicates whether the log contains Admin Activity, Data Access, Policy Denied, or System Event audit logging data. A string that indicates whether the log contains Admin Activity, Data Access, Policy Denied, or System Event audit logging data. The following are the audit log names, including variables for the resource identifiers: Audit logs record the identity that performed the logged operations on the Google Cloud resource. The caller's identity is held in theAuthenticationInfofield ofAuditLogobjects. Audit logging doesn't redact the caller's principal email address for any access that succeeds or for any write operation. For read-only operations that fail with a \"permission denied\" error, Audit Logging might redact the caller's principal email address unless the caller is a service account. In addition to the conditions listed above, the following applies to certain Google Cloud services: Legacy App Engine API: Identities aren't collected. BigQuery: Caller identities and IP addresses, as well as some resource names, are redacted from the audit logs, unless certain conditions are met. BigQuery: Caller identities and IP addresses, as well as some resource names, are redacted from the audit logs, unless certain conditions are met. Cloud Storage: When Cloud Storage usage logs are enabled, Cloud Storage writes usage data to the Cloud Storage bucket, which generates Data Access audit logs for the bucket."
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-21",
    "content": "The generated Data Access audit log has its caller identity redacted. Cloud Storage: When Cloud Storage usage logs are enabled, Cloud Storage writes usage data to the Cloud Storage bucket, which generates Data Access audit logs for the bucket. The generated Data Access audit log has its caller identity redacted. Firestore: If a JSON Web Token (JWT) was used for third-party authentication, thethirdPartyPrincipalfield includes the token's header and payload. For example, audit logs for requests authenticated withFirebase Authenticationinclude that request'sauth token. VPC Service Controls: For Policy Denied audit logs, the following redaction occurs:Parts of the caller email addresses might be redacted and replaced by three period characters....Some caller email addresses belonging to the domaingoogle.comare redacted and replaced bygoogle-internal. VPC Service Controls: For Policy Denied audit logs, the following redaction occurs: Parts of the caller email addresses might be redacted and replaced by three period characters.... Parts of the caller email addresses might be redacted and replaced by three period characters.... Some caller email addresses belonging to the domaingoogle.comare redacted and replaced bygoogle-internal. Some caller email addresses belonging to the domaingoogle.comare redacted and replaced bygoogle-internal. Organization Policy: Parts of the caller email addresses might be redacted and replaced by three period characters.... The IP address of the caller is held in theRequestMetadata.callerIpfield of theAuditLogobject: For a caller from the internet, the address is a public IPv4 or IPv6 address. For calls made from inside the internal production network from one Google Cloud service to another, thecallerIpis redacted to \"private\". For a caller from a"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-22",
    "content": "Compute Engine VM with a external IP address, thecallerIpis the external address of the VM. For a caller from a Compute Engine VM without a external IP address, if the VM is in the same organization or project as the accessed resource, thencallerIpis the VM's internal IPv4 address. Otherwise, thecallerIpis redacted to \"gce-internal-ip\". For more information, seeVPC network overview. You can query for all audit logs or you can query for logs by their audit log name. The audit log name includes theresource identifierof the Google Cloud project, folder, billing account, or organization for which you want to view audit logging information. Your queries can specify indexedLogEntryfields. For more information about querying your logs, seeBuild queries in the Logs Explorer The Logs Explorer lets you view filter individual log entries. If you want to use SQL to analyze groups of log entries, then use theLog Analyticspage. For more information, see: Query and view logs in Log Analytics. Sample queries for security insights. Chart query results. Most audit logs can be viewed in Cloud Logging by using the Google Cloud console, the Google Cloud CLI, or the Logging API. However, for audit logs related to billing, you can only use the Google Cloud CLI or the Logging API. In the Google Cloud console, you can use the Logs Explorer to retrieve your audit log entries for your Google Cloud project, folder, or organization: In the Google Cloud console, go to theLogs Explorerpage:Go toLogs ExplorerIf you use the search bar to find this page, then select the result whose subheading isLogging. In the Google Cloud console, go to theLogs Explorerpage:Go toLogs Explorer Go toLogs Explorer If you use the search bar to find this page, then select the result whose subheading isLogging. Select an"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-23",
    "content": "existing Google Cloud project, folder, or organization. Select an existing Google Cloud project, folder, or organization. To display all audit logs, enter either of the following queries into the query-editor field, and then clickRun query:logName:\"cloudaudit.googleapis.com\"protoPayload.\"@type\"=\"type.googleapis.com/google.cloud.audit.AuditLog\" To display all audit logs, enter either of the following queries into the query-editor field, and then clickRun query: To display the audit logs for a specific resource and audit log type, in theQuery builderpane, do the following:InResource type, select the Google Cloud resource whose audit logs you want to see.InLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy.ClickRun query.If you don't see these options, then there aren't any audit logs of that type available in the Google Cloud project, folder, or organization.If you're experiencing issues when trying to view logs in the Logs Explorer, see thetroubleshootinginformation.For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer. To display the audit logs for a specific resource and audit log type, in theQuery builderpane, do the following: InResource type, select the Google Cloud resource whose audit logs you want to see. InResource type, select the Google Cloud resource whose audit logs you want to see. InLog name, select the audit log type that you want to see:For Admin Activity audit logs, selectactivity.For Data Access audit logs, selectdata_access.For System Event audit logs, selectsystem_event.For Policy Denied audit logs, selectpolicy."
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-24",
    "content": "InLog name, select the audit log type that you want to see: For Admin Activity audit logs, selectactivity. For Data Access audit logs, selectdata_access. For System Event audit logs, selectsystem_event. For Policy Denied audit logs, selectpolicy. ClickRun query. ClickRun query. If you don't see these options, then there aren't any audit logs of that type available in the Google Cloud project, folder, or organization. If you're experiencing issues when trying to view logs in the Logs Explorer, see thetroubleshootinginformation. For more information about querying by using the Logs Explorer, seeBuild queries in the Logs Explorer. The Google Cloud CLI provides a command-line interface to the Logging API. Supply a valid resource identifier in each of the log names. For example, if your query includes aPROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project. To read your Google Cloud project-level audit log entries, run the following command: To read your folder-level audit log entries, run the following command: To read your organization-level audit log entries, run the following command: To read your Cloud Billing account-level audit log entries, run the following command: Add the--freshnessflagto your command to read logs that are more than 1 day old. For more information about using the gcloud CLI, seegcloud logging read. When building your queries, supply a valid resource identifier in each of the log names. For example, if your query includes aPROJECT_ID, then the project identifier you supply must refer to the currently selected Google Cloud project. For example, to use the Logging API to view your project-level audit log entries, do the following: Go to theTry this APIsection in the documentation for"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-25",
    "content": "theentries.listmethod. Go to theTry this APIsection in the documentation for theentries.listmethod. Put the following into theRequest bodypart of theTry this APIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names.{ \"resourceNames\": [ \"projects/PROJECT_ID\" ], \"pageSize\": 5, \"filter\": \"logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com\" } Put the following into theRequest bodypart of theTry this APIform. Clicking thisprepopulated formautomatically fills the request body, but you need to supply a validPROJECT_IDin each of the log names. ClickExecute. ClickExecute. Cloud Logging useslog bucketsas containers that store and organize your logs data. For each billing account, Google Cloud project, folder, and organization, Logging automatically creates two log buckets,_Requiredand_Default, and correspondingly namedsinks. Cloud Logging_Requiredbuckets store Admin Activity audit logs and System Event audit logs. You can't prevent Admin Activity or System Event audit logs from being stored. You also can't configure the sink that routes log entries to the_Requiredbuckets. Admin Activity audit logs and System Event audit logs are always stored in the_Requiredbucket in the project where the logs were generated. If you route Admin Activity audit logs and System Event audit logs to a different project, then those logs don't pass through the_Defaultor_Requiredsink of the destination project. Therefore, these logs aren't stored in the_Defaultlog bucket or the_Requiredlog bucket of the destination project. To store these logs, create a log sink in the destination project. For more information, seeRoute logs to supported destinations. The_Defaultbuckets, by default, store any enabled Data"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-26",
    "content": "Access audit logs as well as Policy Denied audit logs. To prevent Data Access audit logs from being stored in the_Defaultbuckets, you can disable them. To prevent any Policy Denied audit logs from being stored in the_Defaultbuckets, you can exclude them by modifying their sinks' filters. You can also route your audit log entries to user-defined Cloud Logging buckets at the Google Cloud project level or to supported destinations outside of Logging using sinks. For instructions on routing logs, seeRoute logs to supported destinations. When configuring your log sinks' filters, you need to specify the audit log types you want to route; for filtering examples, seeSecurity logging queries. If you want to route audit log entries for a Google Cloud organization, folder, or billing account, seeCollate and route organization-level logs to supported destinations. For details on how long log entries are retained by Logging, see the retention information inQuotas and limits: Logs retention periods. IAM permissions and roles determine your ability to access audit logs data in theLogging API, theLogs Explorer, and theGoogle Cloud CLI. For detailed information about the IAM permissions and roles you might need, seeAccess control with IAM. For details on logging usage limits, including the maximum sizes of audit logs, seeQuotas and limits. Cloud Logging doesn't charge to route logs to a supported destination; however, the destination might apply charges. With the exception of the_Requiredlog bucket, Cloud Logging charges to stream logs into log buckets and for storage longer than the default retention period of the log bucket. Cloud Logging doesn't charge for copying logs, for creatinglog scopesoranalytics views, or for queries issued through theLogs ExplorerorLog Analyticspages. For"
  },
  {
    "source_url": "https://cloud.google.com/logging/docs/audit",
    "title": "Cloud Audit Logs overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/logging/docs/audit#chunk-27",
    "content": "more information, see the following documents: Cloud Logging pricing summary Destination costs:Cloud Storage pricingBigQuery pricingPub/Sub pricingCloud Logging pricingVPC flow log generation chargesapply when you send and then exclude your Virtual Private Cloud flow logs from Cloud Logging. Destination costs: Cloud Storage pricing BigQuery pricing Pub/Sub pricing Cloud Logging pricing VPC flow log generation chargesapply when you send and then exclude your Virtual Private Cloud flow logs from Cloud Logging. Learn how toread and understand audit logs. Learn how toenable Data Access audit logs. Reviewbest practicesfor Cloud Audit Logs. Learn aboutAccess Transparency, which provides logs of actions taken by Google Cloud staff when accessing your Google Cloud content. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-19 UTC."
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/vpn",
    "title": "Cloud VPN documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/vpn#chunk-0",
    "content": "Home Network Connectivity Documentation Cloud VPN Cloud VPN securely extends your peer network to Google's network through an IPsec VPN tunnel. Traffic is encrypted and travels between the two networks over the public internet. Cloud VPN is useful for low-volume data connections. For additional connection options, see theHybrid Connectivityproduct page. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Moving to HA VPN from Classic VPN Moving to HA VPN from Classic VPN Creating an HA VPN gateway to a peer VPN gateway Creating an HA VPN gateway to a peer VPN gateway Configuring firewall rules Configuring firewall rules Configuring the peer VPN gateway Configuring the peer VPN gateway Using third-party VPNs with Cloud VPN Using third-party VPNs with Cloud VPN Viewing logs and metrics Viewing logs and metrics Best practices for Cloud VPN Best practices for Cloud VPN Cloud VPN overview Cloud VPN overview Cloud VPN topologies Cloud VPN topologies Advanced configurations Advanced configurations APIs and gcloud reference APIs and gcloud reference Pricing Pricing Quotas and limits Quotas and limits Troubleshooting Troubleshooting Classic VPN partial deprecation Classic VPN partial deprecation Release notes Release notes SLA SLA Securely connecting to VM instances Learn how to build HA VPN connections between Google Cloud and AWS.Network Connectivity Deploying HA VPN with Terraform This tutorial demonstrates how to use Terraform to deploy the high-availability VPN resources on Google Cloud that are used in the VPN interoperability guides.Network Connectivity TCP optimization for network performance in Google Cloud and hybrid"
  },
  {
    "source_url": "https://cloud.google.com/network-connectivity/docs/vpn",
    "title": "Cloud VPN documentation",
    "chunk_id": "https://cloud.google.com/network-connectivity/docs/vpn#chunk-1",
    "content": "scenarios Learn about ways to improve connection latency between processes within Google Cloud, including how to compute correct settings for decreasing the latency of TCP connections.Network Connectivity Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/storage",
    "title": "Storage",
    "chunk_id": "https://cloud.google.com/docs/storage#chunk-0",
    "content": "Home Documentation Data storage, backup, and disaster recovery. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create and execute backup plans for databases, VMs, and file systems to protect all of your data resources consistently and efficiently. Choose your block, file, or object storage options. Transfer your data into, within, or from Google Cloud. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/networking",
    "title": "Networking",
    "chunk_id": "https://cloud.google.com/docs/networking#chunk-0",
    "content": "Home Documentation Connect your networks and workloads, load balance traffic, and secure your network. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Set up your Virtual Private Cloud network and connect it to your other networks. Make your services available at scale to your internal or external customers. Block unauthorized traffic and implement threat prevention and detection services. Monitor and troubleshoot your Google Cloud network. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-0",
    "content": "Home Google Cloud Observability Documentation Google Cloud Managed Service for Prometheus is Google Cloud's fully managed, multi-cloud, cross-project solution forPrometheusandOpenTelemetrymetrics. It lets you globally monitor and alert on your workloads, using Prometheus and OpenTelemetry, without having to manually manage and operate Prometheus at scale. Managed Service for Prometheus collects metrics from Prometheus exporters and lets you query the data globally using PromQL, meaning that you can keep using any existingGrafanadashboards, PromQL-based alerts, and workflows. It is hybrid- and multi-cloud compatible, can monitor Kubernetes, VMs, and serverless workloads on Cloud Run, retains data for 24 months, and maintains portability by staying compatible with upstream Prometheus. You can also supplement your Prometheus monitoring by queryingover 6,500 free metricsin Cloud Monitoring, includingfree GKE system metrics, using PromQL. This document gives an overview of the managed service, and further documents describe how to set up and run the service. To receive regular updates about new features and releases, submit the optionalsign-up form. Hear how The Home Depot uses Managed Service for Prometheus to get unified observability across 2,200 stores running on-prem Kubernetes clusters: Google Cloud Managed Service for Prometheus gives you the familiarity of Prometheus backed by the global, multi-cloud, and cross-project infrastructure of Cloud Monitoring. Managed Service for Prometheus is built on top of Monarch, the sameglobally scalable datastoreused for Google's own monitoring. Because Managed Service for Prometheus uses the same backend and APIs asCloud Monitoring, both Cloud Monitoring metrics and metrics ingested by Managed Service for Prometheus are queryable"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-1",
    "content": "by usingPromQL in Cloud Monitoring,Grafana, orany other tool that can read the Prometheus API. In a standard Prometheus deployment, data collection, query evaluation, rule and alert evaluation, and data storage are all handled within a single Prometheus server. Managed Service for Prometheus splits responsibilities for these functions into multiple components: Data collectionis handled by managed collectors, self-deployed collectors, the OpenTelemetry Collector, orthe Ops Agent, which scrape local exporters and forward the collected data to Monarch. These collectors can be used for Kubernetes, serverless, and traditional VM workloads and can run everywhere, including other clouds and on-prem deployments. Query evaluationis handled by Monarch, which executes queries and unions results across all Google Cloud regions and across up to 3,500 Google Cloud projects. Rule and alert evaluationis handled either by writingPromQL alerts in Cloud Monitoringwhich fully execute in the cloud, or by using locally run and locally configured rule evaluator components which execute rules and alerts against the global Monarch data store and forward any fired alerts toPrometheus AlertManager. Data storageis handled by Monarch, which stores all Prometheus data for 24 months at no additional cost. Grafana connects to the global Monarch data store instead of connecting to individual Prometheus servers. If you have Managed Service for Prometheus collectors configured in all your deployments, then this single Grafana instance gives you a unified view of all your metrics across all your clouds. You can use Managed Service for Prometheus in one of four modes: withmanaged data collection, withself-deployed data collection, withthe OpenTelemetry Collector, or withthe Ops Agent. Managed Service for"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-2",
    "content": "Prometheus offers an operator for managed data collection in Kubernetes environments. We recommend that you use managed collection; using it eliminates the complexity of deploying, scaling, sharding, configuring, and maintaining Prometheus servers. Managed collection is supported for both GKE and non-GKE Kubernetes environments. With self-deployed data collection, you manage your Prometheus installation as you always have. The only difference from upstream Prometheus is that you run the Managed Service for Prometheus drop-in replacement binary instead of the upstream Prometheus binary. The OpenTelemetry Collector can be used to scrape Prometheus exporters and send data to Managed Service for Prometheus. OpenTelemetry supports a single-agent strategy for all signals, where one collector can be used for metrics (including Prometheus metrics), logs, and traces in any environment. You can configure the Ops Agent on any Compute Engine instance to scrape and send Prometheus metrics to the global datastore. Using an agent greatly simplifies VM discovery and eliminates the need to install, deploy, or configure Prometheus in VM environments. If you have a Cloud Run service that writesPrometheus metricsorOTLP metrics, then you can use a sidecar and Managed Service for Prometheus to send the metrics to Cloud Monitoring. To collect Prometheus metrics from Cloud Run, use thePrometheus sidecar. To collect OTLP metrics from Cloud Run, use theOpenTelemetry sidecar. You can run managed, self-deployed, and OpenTelemetry collectors in on-prem deployments and on any cloud. Collectors running outside of Google Cloud send data to Monarch for long-term storage and global querying. When choosing between collection options, consider the following: Managed collection:Google's recommended"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-3",
    "content": "approach for all Kubernetes environments.Deployed by using the GKE UI, the gcloud CLI, thekubectlCLI, or Terraform.Operation of Prometheus\u2014generating scrape configurations, scaling ingestion, scoping rules to the right data, and so forth\u2014is fully handled by the Kubernetes operator.Scraping and rules are configured by using lightweight custom resources (CRs).Good for those who want a more hands-off, fully managed experience.Intuitive migration fromprometheus-operatorconfigs.Supports most current Prometheus use cases.Full assistance from Google Cloud technical support. Managed collection: Google's recommended approach for all Kubernetes environments. Deployed by using the GKE UI, the gcloud CLI, thekubectlCLI, or Terraform. Operation of Prometheus\u2014generating scrape configurations, scaling ingestion, scoping rules to the right data, and so forth\u2014is fully handled by the Kubernetes operator. Scraping and rules are configured by using lightweight custom resources (CRs). Good for those who want a more hands-off, fully managed experience. Intuitive migration fromprometheus-operatorconfigs. Supports most current Prometheus use cases. Full assistance from Google Cloud technical support. Self-deployed collection:A drop-in replacement for the upstream Prometheus binary.You can use your preferred deployment mechanism, likeprometheus-operatoror manual deployment.Scraping is configured by using your preferred methods, like annotations or prometheus-operator.Scaling and functional sharding is done manually.Good for quick integration into more complex existing setups. You can reuse your existing configs and run upstream Prometheus and Managed Service for Prometheus side by side.Rules and alerts typically run within individual Prometheus servers, which might be preferable for edge"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-4",
    "content": "deployments as local rule evaluation does not incur any network traffic.Might support long-tail use cases that aren't yet supported by managed collection, such aslocal aggregationsto reduce cardinality.Limited assistance from Google Cloud technical support. Self-deployed collection: A drop-in replacement for the upstream Prometheus binary. You can use your preferred deployment mechanism, likeprometheus-operatoror manual deployment. Scraping is configured by using your preferred methods, like annotations or prometheus-operator. Scaling and functional sharding is done manually. Good for quick integration into more complex existing setups. You can reuse your existing configs and run upstream Prometheus and Managed Service for Prometheus side by side. Rules and alerts typically run within individual Prometheus servers, which might be preferable for edge deployments as local rule evaluation does not incur any network traffic. Might support long-tail use cases that aren't yet supported by managed collection, such aslocal aggregationsto reduce cardinality. Limited assistance from Google Cloud technical support. The OpenTelemetry Collector:A single collector that can collect metrics (including Prometheus metrics) from any environment and send them to any compatible backend. Can also be used to collect logs and traces and send them to any compatible backend, including Cloud Logging and Cloud Trace.Deployed in any compute or Kubernetes environment either manually or by using Terraform. Can be used to send metrics from stateless environments such as Cloud Run.Scraping is configured using Prometheus-like configs in the collector's Prometheus receiver.Supports push-based metric collection patterns.Metadata is injected from any cloud using resource detector processors.Rules and"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-5",
    "content": "alerts can be executed using a Cloud Monitoring alerting policy or the stand-alone rule evaluator.Best supports cross-signal workflows and features such as exemplars.Limited assistance from Google Cloud technical support. The OpenTelemetry Collector: A single collector that can collect metrics (including Prometheus metrics) from any environment and send them to any compatible backend. Can also be used to collect logs and traces and send them to any compatible backend, including Cloud Logging and Cloud Trace. Deployed in any compute or Kubernetes environment either manually or by using Terraform. Can be used to send metrics from stateless environments such as Cloud Run. Scraping is configured using Prometheus-like configs in the collector's Prometheus receiver. Supports push-based metric collection patterns. Metadata is injected from any cloud using resource detector processors. Rules and alerts can be executed using a Cloud Monitoring alerting policy or the stand-alone rule evaluator. Best supports cross-signal workflows and features such as exemplars. Limited assistance from Google Cloud technical support. The Ops Agent:The easiest way to collect and send Prometheus metric data originating from Compute Engine environments, including both Linux and Windows distros.Deployed by using the gcloud CLI, the Compute Engine UI, or Terraform.Scraping is configured using Prometheus-like configs in the Agent's Prometheus receiver, powered by OpenTelemetry.Rules and alerts can be executed using Cloud Monitoring or the stand-alone rule evaluator.Comes bundled with optional Logging agents andprocess metrics.Full assistance from Google Cloud technical support. To get started, seeGet started with managed collection,Get started with self-deployed collection,Get started with the"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-6",
    "content": "OpenTelemetry Collector, orGet started with the Ops Agent. The Ops Agent: The easiest way to collect and send Prometheus metric data originating from Compute Engine environments, including both Linux and Windows distros. Deployed by using the gcloud CLI, the Compute Engine UI, or Terraform. Scraping is configured using Prometheus-like configs in the Agent's Prometheus receiver, powered by OpenTelemetry. Rules and alerts can be executed using Cloud Monitoring or the stand-alone rule evaluator. Comes bundled with optional Logging agents andprocess metrics. Full assistance from Google Cloud technical support. To get started, seeGet started with managed collection,Get started with self-deployed collection,Get started with the OpenTelemetry Collector, orGet started with the Ops Agent. If you use the managed service outside of Google Kubernetes Engine or Google Cloud, some additional configuration might be necessary; seeRun managed collection outside of Google Cloud,Run self-deployed collection outside of Google Cloud, orAdd OpenTelemetry processors. Managed Service for Prometheus supports any query UI that can call the Prometheus query API, including Grafana and the Cloud Monitoring UI. Existing Grafana dashboards continue to work when switching from local Prometheus to Managed Service for Prometheus, and you can continue using PromQL found in popular open-source repositories and on community forums. You can use PromQL to queryover 6,500 free metricsin Cloud Monitoring, even without sending data to Managed Service for Prometheus. You can also use PromQL to queryfree Kubernetes metrics,custom metricsandlog-based metrics. For information on how to configure Grafana to query Managed Service for Prometheus data, seeQuery using Grafana. For information on how to query Cloud"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-7",
    "content": "Monitoring metrics using PromQL, seePromQL in Cloud Monitoring. Managed Service for Prometheus provides both a fully cloud-based alerting pipeline and a stand-alone rule evaluator, both of which evaluate rules against all Monarch data accessible in ametrics scope. Evaluating rules against a multi-project metrics scope eliminates the need to co-locate all data of interest on a single Prometheus server or within a single Google Cloud project, and it lets you set IAM permissions on groups of projects. Because all rule evaluation options accept the standard Prometheusrule_filesformat, you can easily migrate to Managed Service for Prometheus by copy-pasting existing rules or by copy-pasting rules found in popular open source repositories. For those using self-deployed collectors, you can continue to evaluate recording rules locally in your collectors. The results of recording and alerting rules are stored in Monarch, just like directly collected metric data. You can also migrate your Prometheus alerting rules to PromQL-based alerting policies in Cloud Monitoring. For alert evaluation with Cloud Monitoring, seePromQL alerts in Cloud Monitoring. For rule evaluation with managed collection, seeManaged rule evaluation and alerting. For rule evaluation with self-deployed collection, the OpenTelemetry Collector, and the Ops Agent, seeSelf-deployed rule evaluation and alerting. For information on reducing cardinality using recording rules on self-deployed collectors, seeCost controls and attribution. All Managed Service for Prometheus data is stored for 24 months at no additional cost. Managed Service for Prometheus supports a minimum scrape interval of 5 seconds. Data is stored at full granularity for 1 week, then is downsampled to 1-minute points for the next 5 weeks, then is"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-8",
    "content": "downsampled to 10-minute points and stored for the remainder of the retention period. Managed Service for Prometheus has no limit on the number of active time series or total time series. For more information, seeQuotas and limits within the Cloud Monitoring documentation. Managed Service for Prometheus is a Google Cloud product, and billing and usage quotas apply. Billing for the service is based primarily on the number of metric samples ingested into storage. There is also a nominal charge for read API calls. Managed Service for Prometheus does not charge for storage or retention of metric data. For current pricing, seeGoogle Cloud Managed Service for Prometheus pricing summary. To estimate your bill based on your expected number of time series or your expected samples per second, see the Cloud Operations tab within theGoogle Cloud Pricing Calculator. For tips on how to lower your bill or determine the sources of high costs, seeCost controls and attribution. For information about the rationale for the pricing model, seePricing for controllability and predictability. For pricing examples, seePricing example based on samples ingested. Managed Service for Prometheus shares ingest and read quotas with Cloud Monitoring. The default ingest quota is 500 QPS per project with up to 200 samples in a single call, equivalent to 100,000 samples per second. The default read quota is 100 QPS permetrics scope. You can increase these quotas to support your metric and query volumes. For information about managing quotas and requesting quota increases, seeWorking with quotas. Managed Service for Prometheus is part of Cloud Monitoring and therefore inherits certain agreements and certifications from Cloud Monitoring, including (but not limited to): TheGoogle Cloud terms of service"
  },
  {
    "source_url": "https://cloud.google.com/stackdriver/docs/managed-prometheus",
    "title": "Google Cloud Managed Service for PrometheusStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/stackdriver/docs/managed-prometheus#chunk-9",
    "content": "TheOperations Service Level Agreement (SLA) US DISAandFedRAMPcompliance levels VPC-SC (VPC Service Controls)support Get started withmanaged collection. Get started withself-deployed collection. Get started withthe OpenTelemetry Collector. Get started withthe Ops Agent. Use PromQL in Cloud Monitoring to query Prometheus metrics. Use Grafana to query Prometheus metrics. Query Cloud Monitoring metricsusing PromQL. Read up onbest practices and view architecture diagrams. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-15 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models",
    "title": "Overview of self-deployed modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Model Garden offers both self-deploy open and partner models that you can deploy and serve on Vertex AI. These models are different from themodel-as-a-service (MaaS)offerings, which are serverless and require no manual deployment. When you self deploy models, you deploy them securely within your Google Cloud project and VPC network. Open models provide pretrained capabilities for various AI tasks, including Gemini models that excel in multimodal processing. An open model is freely available, you are free to publish its outputs, and it can be used anywhere as long as you adhere to its licensing terms.Vertex AIoffers both open (also known asopen weight) and open source models. When you use an open model with Vertex AI, you use Vertex AI for your infrastructure. You can also use open models with other infrastructure products, such as PyTorch or Jax. Many open models are considered open weight large language models (LLMs). Open models provide more transparency than models that aren't open weight. A model's weights are the numerical values stored in the model's neural network architecture that represent learned patterns and relationships from the data a model is trained on. The pretrained parameters, or weights, of open weight models are released. You can use an open weight model for inference and tuning while details such as the original dataset, model architecture, and training code aren't provided. Open models differ from open source AI models. While open models often expose the weights and the core numerical representation of learned patterns, they don't necessarily provide the full source code or training details. Providing weights offers a level of AI model transparency, allowing you to understand the model's capabilities"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models",
    "title": "Overview of self-deployed modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models#chunk-1",
    "content": "without needing to build it yourself. Preview This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. Model Garden helps you purchase and manage model licenses from partners who offer proprietary models as a self deploy option. After you purchase access to a model from Cloud Marketplace, you can choose to deploy on on-demand hardware or use your Compute Engine reservations and committed use discounts to meet your budget requirements. You are charged for model usage and for the Vertex AI infrastructure that you use. To request usage of a self-deploy partner model, find the relevant model in theModel Garden console, clickContact sales, and then complete the form, which initiates contact with a Google Cloud sales representative. For more information about deploying and using partner models, seeDeploy a partner model and make prediction requests. Consider the following limitations when using self-deploy partner models: Unlike with open models, you cannot export weights. If you VPC Service Controls set up for your project, you can't upload models, which prevents you from deploying partner models. For endpoints, only theshared public endpointtype is supported. For more information about Model Garden, seeOverview of Model Garden. For more information about deploying models, seeUse models in Model Garden. Use Gemma open models Use Llama open models Use Hugging Face open models Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models",
    "title": "Overview of self-deployed modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models#chunk-2",
    "content": "Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation To see an example of getting started with Responsible AI with Vertex AI Gemini API, run the \"Responsible AI with Vertex AI Gemini API: Safety ratings and thresholds\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub Google's generative AI models, like Gemini 2.0 Flash, are designed to prioritize safety. However, they can still generate harmful responses, especially when they're explicitly prompted. To further enhance safety and minimize misuse, you can configure content filters to block potentially harmful responses. This page describes each of the safety and content filter types and outlines key safety concepts. For configurable content filters, it shows you how to configure the blocking thresholds of each harm category to control how often prompts and responses are blocked. Safety and content filters act as a barrier, preventing harmful output, but they don't directly influence the model's behavior. To learn more about model steerability, seeSystem instructions for safety. The Vertex AI Gemini API provides one of the followingenumcodes to explain why a prompt was rejected: To learn more, seeBlockedReason. The following is an example of Vertex AI Gemini API output when a prompt is blocked for containingPROHIBITED_CONTENT: The following filters can detect and block potentially unsafe responses: Non-configurable safety filters, which block child sexual abuse material (CSAM) and personally identifiable information (PII). Configurable content filters, which block unsafe content based on a list of harm categories and their user-configured blocking thresholds. You can configure blocking thresholds for each of these harms"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-1",
    "content": "based on what is appropriate for your use case and business. To learn more, seeConfigurable content filters. Citation filters, which provide citations for source material. To learn more, seeCitation filter. An LLM generates responses in units of text called tokens. A model stops generating tokens because it reaches a natural stopping point or because one of the filters blocks the response. The Vertex AI Gemini API provides one of the followingenumcodes to explain why token generation stopped: To learn more, seeFinishReason. If a filter blocks the response, it voids the response'sCandidate.contentfield. It does not provide any feedback to the model. Content filters assess content against a list of harms. For each harm category, the content filters assign one score based on theprobabilityof the content being harmful and another score based on theseverityof harmful content. The configurable content filters don't have versioning independent of model versions. Google won't update the configurable content filter for a previously released version of a model. However, it may update the configurable content filter for a future version of a model. Content filters assess content based on the following harm categories: Theprobabilitysafety score reflects the likelihood that a model response is associated with the respective harm. It has an associated confidence score between0.0and1.0, rounded to one decimal place. The confidence score is discretized into four confidence levels:NEGLIGIBLE,LOW,MEDIUM, andHIGH. Theseverityscore reflects the magnitude of how harmful a model response might be. It has an associated severity score ranging from0.0to1.0, rounded to one decimal place. The severity score is discretized into four levels:NEGLIGIBLE,LOW,MEDIUM, andHIGH. Content can have a"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-2",
    "content": "lowprobabilityscore and a highseverityscore, or a highprobabilityscore and a lowseverityscore. You can use the Vertex AI Gemini API or the Google Cloud console to configure content filters. The Vertex AI Gemini API provides two \"harm block\" methods: SEVERITY: This method uses both probability and severity scores. PROBABILITY: This method uses the probability score only. The default method isSEVERITY. For models older thangemini-1.5-flashandgemini-1.5-pro, the default method isPROBABILITY. To learn more, seeHarmBlockMethodAPI reference. The Vertex AI Gemini API provides the following \"harm block\" thresholds: BLOCK_LOW_AND_ABOVE: Block when the probability score or the severity score isLOW,MEDIUMorHIGH. BLOCK_MEDIUM_AND_ABOVE: Block when the probability score or the severity score isMEDIUMorHIGH. Forgemini-2.0-flash-001and subsequent models,BLOCK_MEDIUM_AND_ABOVEis the default value. BLOCK_ONLY_HIGH: Block when the probability score or the severity score isHIGH. HARM_BLOCK_THRESHOLD_UNSPECIFIED: Block using the default threshold. OFF: No automated response blocking and no metadata is returned. Forgemini-2.0-flash-001and subsequent models,OFFis the default value. BLOCK_NONE: TheBLOCK_NONEsetting removes automated response blocking. Instead, you can configure your own content guidelines with the returned scores. This is a restricted field that isn't available to all users inGAmodel versions. For example, the following Python code demonstrates how you can set the harm block threshold toBLOCK_ONLY_HIGHfor the dangerous content category: This will block most of the content that is classified as dangerous content. To learn more, seeHarmBlockThresholdAPI reference. For end-to-end examples in Python, Node.js, Java, Go, C# and REST, seeExamples of content filter configuration. The"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-3",
    "content": "Google Cloud console lets you configure a threshold for each content attribute. The content filter uses only the probability scores. There is no option to use the severity scores. The Google Cloud console provides the following threshold values: Off(default): No automated response blocking. Block few: Block when the probability score isHIGH. Block some: Block when the probability score isMEDIUMorHIGH. Block most: Block when the probability score isLOW,MEDIUMorHIGH. For example, if you set the block setting toBlock fewfor theDangerous Content category, everything that has a high probability of being dangerous content is blocked. Anything with a lower probability is allowed. The default threshold isBlock some. To set the thresholds, see the following steps: In the Vertex AI section of the Google Cloud console, go to theVertex AI Studiopage.Go to Vertex AI Studio In the Vertex AI section of the Google Cloud console, go to theVertex AI Studiopage. Go to Vertex AI Studio UnderCreate a new prompt, click any of the buttons to open the prompt design page. UnderCreate a new prompt, click any of the buttons to open the prompt design page. ClickSafety settings.TheSafety settingsdialog window opens. ClickSafety settings. TheSafety settingsdialog window opens. For each harm category, configure the desired threshold value. For each harm category, configure the desired threshold value. ClickSave. ClickSave. The following is an example of Vertex AI Gemini API output when a response is blocked by the configurable content filter for containing dangerous content: The following examples demonstrate how you can configure the content filter using the Vertex AI Gemini API: To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:#"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-4",
    "content": "Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True After youset up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION: The region to process the request. Available options include the following:Click to expand a partial list of available regionsus-central1us-west4northamerica-northeast1us-east4us-west1asia-northeast3asia-southeast1asia-northeast1 Click to expand a partial list of available regions us-central1 us-west4 northamerica-northeast1 us-east4 us-west1 asia-northeast3 asia-southeast1 asia-northeast1 PROJECT_ID: Yourproject ID. MODEL_ID: The model ID of the multimodal model that you want to use, likegemini-2.0-flash. ROLE: The role in a conversation associated with the content. Specifying a role is required even in singleturn use cases. Acceptable values include the following:USER: Specifies content that's sent by you.MODEL: Specifies the model's response.TEXT: The text instructions to include in the prompt.SAFETY_CATEGORY: The safety category to configure a threshold for. Acceptable values include the following:Click to expand safety categoriesHARM_CATEGORY_SEXUALLY_EXPLICITHARM_CATEGORY_HATE_SPEECHHARM_CATEGORY_HARASSMENTHARM_CATEGORY_DANGEROUS_CONTENTTHRESHOLD: The threshold for blocking responses that could belong to the specified safety category based on probability. Acceptable values include the following:Click to expand blocking"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-5",
    "content": "thresholdsBLOCK_NONEBLOCK_ONLY_HIGHBLOCK_MEDIUM_AND_ABOVE(default)BLOCK_LOW_AND_ABOVEBLOCK_LOW_AND_ABOVEblocks the most whileBLOCK_ONLY_HIGHblocks the least. USER: Specifies content that's sent by you. MODEL: Specifies the model's response. TEXT: The text instructions to include in the prompt. SAFETY_CATEGORY: The safety category to configure a threshold for. Acceptable values include the following:Click to expand safety categoriesHARM_CATEGORY_SEXUALLY_EXPLICITHARM_CATEGORY_HATE_SPEECHHARM_CATEGORY_HARASSMENTHARM_CATEGORY_DANGEROUS_CONTENT Click to expand safety categories HARM_CATEGORY_SEXUALLY_EXPLICIT HARM_CATEGORY_HATE_SPEECH HARM_CATEGORY_HARASSMENT HARM_CATEGORY_DANGEROUS_CONTENT THRESHOLD: The threshold for blocking responses that could belong to the specified safety category based on probability. Acceptable values include the following:Click to expand blocking thresholdsBLOCK_NONEBLOCK_ONLY_HIGHBLOCK_MEDIUM_AND_ABOVE(default)BLOCK_LOW_AND_ABOVEBLOCK_LOW_AND_ABOVEblocks the most whileBLOCK_ONLY_HIGHblocks the least. Click to expand blocking thresholds BLOCK_NONE BLOCK_ONLY_HIGH BLOCK_MEDIUM_AND_ABOVE(default) BLOCK_LOW_AND_ABOVE HTTP method and URL: Request JSON body: To send your request, choose one of these options: Save the request body in a file namedrequest.json, and execute the following command: Save the request body in a file namedrequest.json, and execute the following command: You should receive a JSON response similar to the following. The generative code features of Vertex AI are intended to produce original content. By design, Gemini limits the likelihood that existing content is replicated at length. If a Gemini feature does make an extensive quotation from a web page, Gemini cites that page. Sometimes the same content can be found on multiple web"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-6",
    "content": "pages. Gemini attempts to point you to a popular source. In the case of citations to code repositories, the citation might also reference an applicable open source license. Complying with any license requirements is your own responsibility. To learn about the metadata of the citation filter, see theCitation API reference. Preview This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. The civic integrity filter detects and blocks prompts that mention or relate to political elections and candidates. This filter is disabled by default. To turn it on, set the blocking threshold forCIVIC_INTEGRITYto any of the following values. It doesn't make a difference which value you specify. BLOCK_LOW_AND_ABOVE BLOCK_MEDIUM_AND_ABOVE BLOCK_ONLY_HIGH The following Python code shows you how to turn on the civic integrity filter: For more details about the civic integrity filter, contact your Google Cloud representative. While content filters help prevent unsafe content, they might occasionally block benign content or miss harmful content. Advanced models like Gemini 2.0 Flash are designed to generate safe responses even without filters. Test different filter settings to find the right balance between safety and allowing appropriate content. Learn aboutsystem instructions for safety. Learn aboutabuse monitoring. Learn more aboutresponsible AI. Learn aboutdata governance. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes",
    "title": "Safety and content filtersStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#chunk-7",
    "content": "is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-15 UTC."
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/migration-intro",
    "title": "Introduction to BigQuery Migration Service",
    "chunk_id": "https://cloud.google.com/bigquery/docs/migration-intro#chunk-0",
    "content": "Home BigQuery Documentation Guides This document provides an overview of the BigQuery Migration Service. The BigQuery Migration Service is a comprehensive solution for migrating your data warehouse to BigQuery. It includes free-to-use tools that help you with each phase of migration, including assessment and planning, SQL translation for more than 10 dialects, data transfer, and data validation. Together, these tools help you accelerate migrations and reduce risk, shortening the time to value. In the assessment and planning phase, you can use theBigQuery migration assessmentfeature to understand your existing data warehouse. Then, you can use thebatch SQL translatorand theinteractive SQL translatorto prepare your SQL queries and scripts to work in BigQuery. The batch and interactive SQL translators support translation from a wide range of SQL dialects. When you're ready to move your data, you can use theBigQuery Data Transfer Serviceto automate and manage the migration from your data warehouse to BigQuery. After you migrate your data, you can use theData Validation Toolto validate that the migration succeeded. Quotas and limits apply to the number of jobs as well as the size of files. For more information on migration service quotas and limits, seeQuotas and limits. There is no charge to use the BigQuery Migration API. However, storage used for input and output files incurs the normal fees. For more information, seeStorage pricing. Additionally, you can use thecost estimation functionality in Google Cloud Migration Centerto generate a cost estimate of running your data warehouse setup that you migrate to BigQuery. For more information, seeStart a cost estimationandSpecify data warehousing requirements. For more information on batch SQL translator, seeBatch SQL"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/migration-intro",
    "title": "Introduction to BigQuery Migration Service",
    "chunk_id": "https://cloud.google.com/bigquery/docs/migration-intro#chunk-1",
    "content": "translator. For more information on using the interactive SQL translator, seeInteractive SQL translator. For more information on BigQuery migration assessment, seeBigQuery migration assessment. Learn about theData Validation Tool. For information about quotas and limits for the BigQuery Migration Service, seeQuotas and limits. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-05 UTC."
  },
  {
    "source_url": "https://cloud.google.com/memorystore/docs/redis",
    "title": "Memorystore for Redis documentation",
    "chunk_id": "https://cloud.google.com/memorystore/docs/redis#chunk-0",
    "content": "Home Documentation Memorystore Memorystore for Redis Memorystore for Redis is a fully managed Redis service for Google Cloud. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments. Memorystore for Redis is based on and is compatible with open-source Redis versions 7.2 and earlier and supports a subset of the total Redis command library. Not sure what database option is right for you? Learn more about ourdatabase services. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Quickstart: Create a Memorystore for Redis instance by using the gcloud CLI Quickstart: Create a Memorystore for Redis instance by using the gcloud CLI Quickstart: Create a Memorystore for Redis instance by using the Google Cloud console Quickstart: Create a Memorystore for Redis instance by using the Google Cloud console Overview of Memorystore for Redis Overview of Memorystore for Redis Connect to a Redis instance Connect to a Redis instance Create and manage Redis instances Create and manage Redis instances Monitor Redis instances Monitor Redis instances Configure a Redis instance Configure a Redis instance Export data from a Redis instance Export data from a Redis instance Scale Redis instances Scale Redis instances Setting up client libraries Setting up client libraries REST API REST API Supported environments Supported environments Pricing Pricing Quotas and limits Quotas and limits Troubleshoot issues Troubleshoot issues Release notes Release notes Getting support Getting support Billing questions Billing"
  },
  {
    "source_url": "https://cloud.google.com/memorystore/docs/redis",
    "title": "Memorystore for Redis documentation",
    "chunk_id": "https://cloud.google.com/memorystore/docs/redis#chunk-1",
    "content": "questions Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Redis is a trademark of Redis Ltd. All rights therein are reserved to Redis Ltd. Any use by Google is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and Google. Memorystore is based on and is compatible with open-source Redis versions 7.2 and earlier and supports a subset of the total Redis command library. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/flow-logs",
    "title": "VPC Flow Logs",
    "chunk_id": "https://cloud.google.com/vpc/docs/flow-logs#chunk-0",
    "content": "Home Virtual Private Cloud Documentation Guides VPC Flow Logs records a sample of packets sent from and received byvirtual machine (VM) instances, including instances used asGoogle Kubernetes Engine nodes, and packets sent through VLAN attachments forCloud InterconnectandCloud VPNtunnels. Flow logs are aggregated by IP connection (5-tuple). These logs can be used for network monitoring, forensics, security analysis, and expense optimization. You can view flow logs inCloud Logging, and you can export logs to any destination that Cloud Logging export supports. VPC Flow Logs provides you with visibility into network throughput and performance. You can: Monitor the VPC network Perform network diagnosis Filter the flow logs by VMs, VLAN attachments, and Cloud VPN tunnels to understand traffic changes Understand traffic growth for capacity forecasting You can analyze network usage with VPC Flow Logs to optimize network traffic expenses. For example, you can analyze the network flows for the following: Traffic between regions and zones Traffic to specific countries on the internet Traffic to on-premises and other cloud networks Top talkers in the network, including VMs, VLAN attachments, and Cloud VPN tunnels You can use VPC Flow Logs for network forensics. For example, if an incident occurs, you can examine the following: Which IPs talked with whom and when Any compromised IPs by analyzing all the incoming and outgoing network flows VPC Flow Logs is part of Andromeda, the software that powers VPC networks. VPC Flow Logs introduces no delay or performance penalty when enabled. VPC Flow Logs works with VPC networks, not legacy networks. You enable or disable VPC Flow Logs per subnet, VLAN attachment for Cloud Interconnect, or Cloud VPN tunnel. If enabled for a subnet, VPC Flow"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/flow-logs",
    "title": "VPC Flow Logs",
    "chunk_id": "https://cloud.google.com/vpc/docs/flow-logs#chunk-1",
    "content": "Logs collects data from all VM instances, including GKE nodes, in that subnet. VPC Flow LogssamplesTCP, UDP, ICMP, ESP, and GRE flows. Both inbound and outbound flows are sampled. These flows can be within Google Cloud or between Google Cloud and other networks. If a flow is captured by sampling, VPC Flow Logs generates a log for the flow. Each flow record includes the information described in theRecord formatsection. VPC Flow Logs interacts with firewall rules in the following ways:Egress packets are sampledbeforeegressfirewall rules. Even if an egress firewall rule denies outbound packets, those packets can be sampled by VPC Flow Logs.Ingress packets are sampledafteringressfirewall rules. If an ingress firewall rule denies inbound packets, those packets are not sampled by VPC Flow Logs. Egress packets are sampledbeforeegressfirewall rules. Even if an egress firewall rule denies outbound packets, those packets can be sampled by VPC Flow Logs. Ingress packets are sampledafteringressfirewall rules. If an ingress firewall rule denies inbound packets, those packets are not sampled by VPC Flow Logs. You can usefiltersin VPC Flow Logs to generate only certain logs. VPC Flow Logs supports VMs that have multiple network interfaces. You need to enable VPC Flow Logs for each subnet, in each VPC, that contains a network interface. To log flows between Pods on the same Google Kubernetes Engine (GKE) node, you must enableIntranode visibilityfor the cluster. VPC Flow Logs are not reported from Cloud Run resources. Packets are sampled within an aggregation interval. All packets collected for a given IP connection within the aggregation interval are aggregated into a single flow log entry. This data is then sent toLogging. Logs are stored in Logging for 30 days by default. If you want"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/flow-logs",
    "title": "VPC Flow Logs",
    "chunk_id": "https://cloud.google.com/vpc/docs/flow-logs#chunk-2",
    "content": "to keep logs longer than that, you can eitherset a custom retention periodorexport themto a supported destination. To generate flow logs, VPC Flow Logs samples packets that leave and enter a VM or pass through a gateway such as a VLAN attachment or Cloud VPN tunnel. After the flow logs are generated, VPC Flow Logs processes them by following the procedure described in this section. VPC Flow Logs samples packets using aprimary sampling rate. The primary sampling rate is dynamic and varies depending on the load of the physical host running the VM or gateway at the time of sampling. The probability of sampling any single IP connection increases with the volume of packets. You can't control the primary flow log sampling process or adjust the primary sampling rate. After the flow logs are generated, VPC Flow Logs processes them according to the following procedure: Filtering: You can specify that only logs that match specified criteria are generated. For example, you can filter so that only logs for a particular VM or only logs with a particular metadata value are generated and the rest are discarded. For more information, seeLog filtering. Aggregation: Information for sampled packets is aggregated over a configurableaggregation intervalto produce aflow log entry. Secondary flow log sampling: This is a second sampling process. Flow log entries are further sampled according to a configurablesecondary sampling rateparameter. The secondary sampling is performed on the flow logs generated by the primary flow log sampling process. For example, if the secondary sampling rate is set to 1.0, or 100%, VPC Flow Logs samples 100% of the flow logs generated by the primary flow log sampling. Metadata: If disabled, all metadata annotations are discarded. If you want to keep metadata, you"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/flow-logs",
    "title": "VPC Flow Logs",
    "chunk_id": "https://cloud.google.com/vpc/docs/flow-logs#chunk-3",
    "content": "can specify that all fields or a specified set of fields are retained. For more information, seeMetadata annotations. Write to Logging: The final log entries are written to Cloud Logging. Because VPC Flow Logs does not capture every packet, it compensates for missed packets by interpolating from the captured packets. This happens for packets missed because of initial and user-configurable sampling settings. Even though Google Cloud doesn't capture every packet, log record captures can be quite large. You can balance your traffic visibility and storage cost needs by adjusting the following aspects of logs collection: Aggregation interval: Sampled packets for a time interval are aggregated into a single log entry. This time interval can be 5 seconds (default), 30 seconds, 1 minute, 5 minutes, 10 minutes, or 15 minutes. Secondary sampling rate:For VMs, 50% of log entries are kept by default. You can set this parameter from1.0(100%, all log entries are kept) to0.0(0%, no logs are kept).For VLAN attachments and Cloud VPN tunnels, 100% of log entries are kept by default. You can set this parameter from1.0to greater than0.0. For VMs, 50% of log entries are kept by default. You can set this parameter from1.0(100%, all log entries are kept) to0.0(0%, no logs are kept). For VLAN attachments and Cloud VPN tunnels, 100% of log entries are kept by default. You can set this parameter from1.0to greater than0.0. Metadata annotations: By default, flow log entries are annotated with metadata information, such as the names of the source and destination within Google Cloud or the geographic region of external sources and destinations. Metadata annotations can be turned off, or you can specify only certain annotations, to save storage space. Filtering: By default, logs are generated for"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/flow-logs",
    "title": "VPC Flow Logs",
    "chunk_id": "https://cloud.google.com/vpc/docs/flow-logs#chunk-4",
    "content": "every sampled flow. You can set filters so that only logs that match certain criteria are generated. Standard pricing for Logging, BigQuery, or Pub/Sub apply. VPC Flow Logs pricing is described inNetwork Telemetry pricing. To learn more about the VPC Flow Logs record format and which metadata annotations are available, seeAbout VPC Flow Logs records. To see examples of VPC Flow Logs that are collected for various use cases, seeAbout traffic flows. To start reporting flows for a subnet, seeConfigure VPC Flow Logs. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-30 UTC."
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/dts-introduction",
    "title": "What is BigQuery Data Transfer Service?",
    "chunk_id": "https://cloud.google.com/bigquery/docs/dts-introduction#chunk-0",
    "content": "Home BigQuery Documentation Guides The BigQuery Data Transfer Service automates data movement intoBigQueryon a scheduled, managed basis. Your analytics team can lay the foundation for a BigQuery data warehouse without writing a single line of code. You can access the BigQuery Data Transfer Service using the: Google Cloud console bq command-line tool BigQuery Data Transfer Service API After you configure a data transfer, the BigQuery Data Transfer Service automatically loads data into BigQuery on a regular basis. You can also initiate data backfills to recover from any outages or gaps. You cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery. In addition to loading data into BigQuery, BigQuery Data Transfer Service is used for two BigQuery operations:dataset copiesandscheduled queries. The BigQuery Data Transfer Service supports loading data from the following data sources: Amazon S3 Amazon Redshift Azure Blob Storage Campaign Manager Cloud Storage Comparison Shopping Service (CSS) Center(Preview) Display & Video 360 Facebook Ads(Preview) Google Ad Manager Google Ads Google Analytics 4(Preview) Google Merchant Center(Preview) Google Play MySQL(Preview) Oracle(Preview) PostgreSQL(Preview) Salesforce(Preview) Salesforce Marketing Cloud(Preview) Search Ads 360 ServiceNow(Preview) Teradata YouTube Channel YouTube Content Owner Like BigQuery, the BigQuery Data Transfer Service is amulti-regional resource, with many additional single regions available. A BigQuery dataset's locality is specified when youcreate a destination datasetto store the data transferred by the BigQuery Data Transfer Service. When you set up a transfer, the transfer configuration itself is set to the same location as the destination dataset. The BigQuery Data Transfer Service"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/dts-introduction",
    "title": "What is BigQuery Data Transfer Service?",
    "chunk_id": "https://cloud.google.com/bigquery/docs/dts-introduction#chunk-1",
    "content": "processes and stages data in the same location as the destination dataset. The data you want to transfer to BigQuery can also have a region. In most cases, the region where your data is stored and the location of the destination dataset in BigQuery are irrelevant. In other kinds of transfers, the dataset and the source data must becolocatedin the same region, or a compatible region. For detailed information about transfers and region compatibility for BigQuery Data Transfer Service, seeDataset locations and transfers. For supported regions for BigQuery, seeDataset locations. For information on BigQuery Data Transfer Service pricing, see thePricingpage. Once data is transferred to BigQuery, standard BigQuerystorageandquerypricing applies. For information on BigQuery Data Transfer Service quotas, see theQuotas and limitspage. To learn how to create a transfer, see the documentation for yourdata source. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-05 UTC."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-0",
    "content": "Home Compute Engine Documentation Guides This document describes the features, types, performance and benefits of Persistent Disk volumes. If you need block storage for a virtual machine (VM) instance or container, such as for a boot disk or data disk, use Persistent Disk volumes if Google Cloud Hyperdisk isn't available for your compute instance. To learn about the other block storage options in Compute Engine, seeChoose a disk type. Persistent Disk volumes are durable network storage devices that your instances can access like physical disks in a desktop or a server. Persistent Disk volumes aren't attached to the physical machine hosting the instance. Instead, they are attached to the instance asnetwork block devices. When you read to or write from a Persistent Disk volume, data is transmitted over the network. The data on each Persistent Disk volume is distributed across several physical disks. Compute Engine manages the physical disks and the data distribution for you to ensure redundancy and optimal performance. You can detach or move the volumes to keep your data even after you delete your instances. Persistent Disk performance increases with size, so you can resize your existing Persistent Disk volumes or add more Persistent Disk volumes to a VM to meet your performance and storage space requirements. Add a non-boot disk to your instancewhen you need reliable and affordable storage with consistent performance characteristics. Add a Persistent Disk to your instance When you create a Persistent Disk volume, you can select one of the following disk types: Balanced Persistent Disk(pd-balanced)An alternative to SSD (Performance) Persistent Disk.Balance of performance and cost. For most Compute Engine machine types, these disks have the same maximum IOPS as SSD"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-1",
    "content": "Persistent Disk and lower IOPS per GiB. This disk type offers performance levels suitable for most general-purpose applications at a price point between that of standard and SSD Persistent Disk.Backed by solid-state drives (SSD). An alternative to SSD (Performance) Persistent Disk. Balance of performance and cost. For most Compute Engine machine types, these disks have the same maximum IOPS as SSD Persistent Disk and lower IOPS per GiB. This disk type offers performance levels suitable for most general-purpose applications at a price point between that of standard and SSD Persistent Disk. Backed by solid-state drives (SSD). SSD (Performance) Persistent Disk(pd-ssd)Suitable for enterprise applications and high-performance databases that require lower latency and more IOPS than standard Persistent Disk provides.Backed by solid-state drives (SSD). Suitable for enterprise applications and high-performance databases that require lower latency and more IOPS than standard Persistent Disk provides. Backed by solid-state drives (SSD). Standard Persistent Disk(pd-standard)Suitable for large data processing workloads that primarily use sequential I/Os.Backed by standard hard disk drives (HDD). Suitable for large data processing workloads that primarily use sequential I/Os. Backed by standard hard disk drives (HDD). Extreme Persistent Disk(pd-extreme)Offers consistently high performance for both random access workloads and bulk throughput.Designed for high-end database workloads.Lets you provision the target IOPS.Backed by solid-state drives (SSD).Available with a limited number ofmachine types. Offers consistently high performance for both random access workloads and bulk throughput. Designed for high-end database workloads. Lets you provision the target IOPS. Backed by solid-"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-2",
    "content": "state drives (SSD). Available with a limited number ofmachine types. If you create a disk in the Google Cloud console, the default disk type ispd-balanced. If you create a disk using the gcloud CLI or the Compute Engine API, the default disk type ispd-standard. For information about machine type support, refer to the following: Zonal Persistent Disk Regional Persistent Disk Disk durability represents the probability of data loss, by design, for a typical disk in a typical year, using a set of assumptions about hardware failures, the likelihood of catastrophic events, isolation practices and engineering processes in Google data centers, and the internal encodings used by each disk type. Persistent Disk data loss events are extremely rare and have historically been the result of coordinated hardware failures, software bugs, or a combination of the two. Google also takes many steps to mitigate the industry-wide risk ofsilent data corruption. Human error by a Google Cloud customer, such as when a customer accidentally deletes a disk, is outside the scope of Persistent Disk durability. There is a very small risk of data loss occurring with a regional Persistent Disk volume due to its internal data encodings and replication. Regional Persistent Disk provideshigh availabilityand can be used for disaster recovery if an entire data center is lost and can't be recovered. Regional Persistent Disk provides twice as many disk replicas as zonal Persistent Disk, with each replica distributed between two zones in the same region. If a primary zone becomes unavailable during an outage, the replica in the second zone can be accessed immediately. For more information about region-specific considerations, seeGeography and regions. The following table shows durability for each disk type's"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-3",
    "content": "design. 99.999% durability means that with 1,000 disks, you would likely go a hundred years without losing a single one. C4AC4C4D(Preview)C3C3DN4N2N2DN1T2DT2AE2Z3H3C2C2DX4M4M3M2M1N1+GPUA4A3 (H200)A3 (H100)A2G2 Select a machine series to see its supported Persistent Disk (PD) types. Persistent Disk volumes can be up to 64 TiB in size. You can add up to 127 secondary, non-boot zonal Persistent Disk volumes to a VM instance. However, the combined total capacity of all Persistent Disk volumes attached to a single VM can't exceed 257 TiB. You can create single logical volumes of up to 257 TiB using logical volume management inside your VM. For information about how to ensure maximum performance with large volumes, seeLogical volume size. A zonal Persistent Disk is a Persistent Disk that's accessible only within one specific zone, for example,europe-west-2. Compute Engine handles most disk management tasks for you so that you don't need to deal with partitioning, redundant disk arrays, or subvolume management. Generally, you don't need to create larger logical volumes. However, you can extend your secondary attached Persistent Disk capacity to 257 TiB per VM and apply these practices to your Persistent Disk volumes. You can save time and get the best performance if youformat your Persistent Disk volumeswith a single file system and no partition tables. If you need to separate your data into multiple unique volumes,create additional disksrather than dividing your existing disks into multiple partitions. When you require additional space on your Persistent Disk volumes,resize your disksrather than repartitioning and formatting. Persistent Disk performance is predictable and scales linearly with provisioned capacity until the limits for a VM's provisioned vCPUs are reached. For"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-4",
    "content": "more information about performance scaling limits and optimization, seeConfigure disks to meet performance requirements. Standard Persistent Disk volumes are efficient and economical for handling sequential read/write operations, but they aren't optimized to handle high rates of random input/output operations per second (IOPS). If your apps require high rates of random IOPS, use SSD or extreme Persistent Disk. SSD Persistent Disk is designed for single-digit millisecond latencies. Observed latency is application specific. Compute Engine optimizes performance and scaling on Persistent Disk volumes automatically. You don't need to stripe multiple disks together or pre-warm disks to get the best performance. When you need more disk space or better performance,resize your disksand possibly add more vCPUs to add more storage space, throughput, and IOPS. Persistent Disk performance is based on the total Persistent Disk capacity attached to a VM and the number of vCPUs that the VM has. For boot devices, you can reduce costs by using a standard Persistent Disk. Small, 10 GiB Persistent Disk volumes can work for basic boot and package management use cases. However, to ensure consistent performance for more general use of the boot device, use a balanced Persistent Disk as your boot disk. Because Persistent Disk write operations contribute to the cumulative network egress traffic for your VM, Persistent Disk write operations are capped by thenetwork egress capfor your VM. Persistent Disk has built-in redundancy to protect your data against equipment failure and to ensure data availability through data center maintenance events. Checksums are calculated for all Persistent Disk operations, so we can ensure that what you read is what you wrote. Additionally, you cancreate snapshots"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-5",
    "content": "of Persistent Diskto protect against data loss due to user error. Snapshots are incremental, and take only minutes to create even if you snapshot disks that are attached to running VMs. Regional Persistent Disk volumes have storage qualities that are similar to zonal Persistent Disk. However, regional Persistent Disk volumes provide durable storage and replication of data between two zones in the same region. When you create a new Persistent Disk, you can either create the disk in one zone, or replicate it across two zones within the same region. For example, if you create one disk in a zone, such as inus-west1-a, you have one copy of the disk. A disk created in only one zone is referred to as a zonal disk. You can increase the disk's availability by storing another copy of the disk in a different zone within the region, such as inus-west1-b. Persistent Disk replicated across two zones in the same region are called Regional Persistent Disk. You can also use Hyperdisk Balanced High Availability for cross-zonal synchronous replication of Google Cloud Hyperdisk. It's unlikely for a region to fail altogether, but zonal failures can happen. Replicating within the region to different zones, as shown in the following image, helps with availability and reduces disk latency. If both replication zones fail, it's considered a region-wide failure. Disk is replicated in two zones. In the replicated scenario, the data is available in the local zone (us-west1-a) which is the zone the virtual machine (VM) is running in. Then, the data is replicated to another zone (us-west1-b). One of the zones must be the same zone that the VM is running in. If a zonal outage occurs, you can usually failover your workload running on Regional Persistent Disk to another zone. To learn more, seeRegional"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-6",
    "content": "Persistent Disk failover. If you'redesigning robust systemsorhigh availability serviceson Compute Engine, use Regional Persistent Disk combined with other best practices such asbacking up your data using snapshots. Regional Persistent Disk volumes are also designed to work withregional managed instance groups. Regional Persistent Disk volumes are designed for workloads that require a lowerRecovery Point Objective (RPO)andRecovery Time Objective (RTO)compared to using Persistent Disk snapshots. Regional Persistent Disk are an option when write performance is less critical than data redundancy across multiple zones. Like zonal Persistent Disk, Regional Persistent Disk can achieve greater IOPS and throughput performance on VMs with a greater number of vCPUs. For more information about this and other limitations, seeConfigure disks to meet performance requirements. When you need more disk space or better performance, you canresize your regional disksto add more storage space, throughput, and IOPS. Compute Engine replicates data of your regional Persistent Disk to the zones you selected when you created your disks. The data of each replica is spread across multiple physical machines within the zone to ensure redundancy. Similar to zonal Persistent Disk, you cancreate snapshots of Persistent Diskto protect against data loss due to user error. Snapshots are incremental, and take only minutes to create even if you snapshot disks that are attached to running VMs. You can attach regional Persistent Disk only to VMs that useE2,N1,N2, andN2Dmachine types. You can attach Hyperdisk Balanced High Availability only tosupported machine types.You can't create a regional Persistent Disk from anOS image, or from a disk that was created from an OS image.You can't create a Hyperdisk Balanced"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-7",
    "content": "High Availability disk by cloning a zonal disk. To create a Hyperdisk Balanced High Availability disk from an zonal disk, complete the steps inChange a zonal disk to a Hyperdisk Balanced High Availability disk.When using read-only mode, you can attach a regional balanced Persistent Disk to a maximum of 10 VM instances.The minimum size of a regional standard Persistent Disk is 200 GiB.You can only increase the size of a regional Persistent Disk or Hyperdisk Balanced High Availability volume; you can't decrease its size.Regional Persistent Disk and Hyperdisk Balanced High Availability volumes have different performance characteristics than their corresponding zonal disks. For more information, seeAbout Persistent Disk performanceandHyperdisk Balanced High Availability performance limits.You can't use a Hyperdisk Balanced High Availability volume that's in multi-writer mode as a boot disk.If you create a replicated disk by cloning a zonal disk, then the two zonal replicas aren't fully in sync at the time of creation. After creation, you can use the regional disk clone within 3 minutes, on average. However, you might need to wait for tens of minutes before the disk reaches a fully replicated state and therecovery point objective (RPO)is close to zero. Learn how tocheck if your replicated disk is fully replicated. You can't create a regional Persistent Disk from anOS image, or from a disk that was created from an OS image. You can't create a Hyperdisk Balanced High Availability disk by cloning a zonal disk. To create a Hyperdisk Balanced High Availability disk from an zonal disk, complete the steps inChange a zonal disk to a Hyperdisk Balanced High Availability disk. When using read-only mode, you can attach a regional balanced Persistent Disk to a maximum of 10 VM"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-8",
    "content": "instances. The minimum size of a regional standard Persistent Disk is 200 GiB. You can only increase the size of a regional Persistent Disk or Hyperdisk Balanced High Availability volume; you can't decrease its size. Regional Persistent Disk and Hyperdisk Balanced High Availability volumes have different performance characteristics than their corresponding zonal disks. For more information, seeAbout Persistent Disk performanceandHyperdisk Balanced High Availability performance limits. You can't use a Hyperdisk Balanced High Availability volume that's in multi-writer mode as a boot disk. If you create a replicated disk by cloning a zonal disk, then the two zonal replicas aren't fully in sync at the time of creation. After creation, you can use the regional disk clone within 3 minutes, on average. However, you might need to wait for tens of minutes before the disk reaches a fully replicated state and therecovery point objective (RPO)is close to zero. Learn how tocheck if your replicated disk is fully replicated. The storage interface is chosen automatically for you when you create your instance or add Persistent Disk volumes to a VM. Tau T2A and third generation VMs (such as M3) use theNVMeinterface for Persistent Disk. Confidential VMinstances also use NVMe Persistent Disk. All other Compute Engine machine series use theSCSIdisk interface for Persistent Disk. Most public images include both NVMe and SCSI drivers. Most images include a kernel with optimized drivers that allow your VM to achieve the best performance using NVMe. Your imported Linux images achieve the best performance with NVMe if they include kernel version4.14.68or later. To determine if an operating system version supports NVMe, see theoperating system detailspage. Preview This feature is subject to the"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-9",
    "content": "\"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. You can attach an SSD Persistent Disk in multi-writer mode to up to two N2 VMs simultaneously so that both VMs can read and write to the disk. Persistent Disk in multi-writer mode provides a shared block storage capability and presents an infrastructural foundation for building highly available shared file systems and databases. These specialized file systems and databases should be designed to work with shared block storage and handle cache coherence between VMs by using tools such asSCSI Persistent Reservations. However, Persistent Disk with multi-writer mode should generally not be used directly. Many file systems such as EXT4, XFS, and NTFS aren't designed to be used with shared block storage. For more information about the best practices when sharing Persistent Disk between VMs, seeBest practices. If you require a fully managed file storage, you canmount a Filestore file share on your Compute Engine VMs. To enable multi-writer mode for new Persistent Disk volumes, create a new Persistent Disk and specify the--multi-writerflag in the gcloud CLI or themultiWriterproperty in the Compute Engine API. For more information, seeShare Persistent Disk volumes between VMs. Compute Engine automatically encrypts your data before it travels outside of your VM to the Persistent Disk storage space. Each Persistent Disk remains encrypted either with system-defined keys or withcustomer-supplied keys. Google distributes Persistent Disk data across multiple physical disks in a manner that users don't control. When you delete a Persistent Disk volume, Google discards the"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-10",
    "content": "cipher keys, rendering the data irretrievable. This process is irreversible. If you want to control the encryption keys that are used to encrypt your data,create your disks with your own encryption keys. You can't attach a Persistent Disk volume to an VM in another project. You can't attach a Persistent Disk volume to an VM in another project. You can attach a balanced Persistent Disk to a maximum of 10 VMs in read-only mode. You can attach a balanced Persistent Disk to a maximum of 10 VMs in read-only mode. Forcustom machine typesor predefined machine types with a minimum of 1 vCPU, you can attach up to 128 Persistent Disk volumes. Forcustom machine typesor predefined machine types with a minimum of 1 vCPU, you can attach up to 128 Persistent Disk volumes. Each Persistent Disk volume can be up to 64 TiB in size, so there is no need to manage arrays of disks to create large logical volumes. Each VM can attach only a limited amount of total Persistent Disk space and a limited number of individual Persistent Disk volumes. Predefined machine types and custom machine types have the same Persistent Disk limits. Each Persistent Disk volume can be up to 64 TiB in size, so there is no need to manage arrays of disks to create large logical volumes. Each VM can attach only a limited amount of total Persistent Disk space and a limited number of individual Persistent Disk volumes. Predefined machine types and custom machine types have the same Persistent Disk limits. Most VMs can have up to 128 Persistent Disk volumes and up to 257 TiB of total disk space attached. Total disk space for a VM includes the size of the boot disk. Most VMs can have up to 128 Persistent Disk volumes and up to 257 TiB of total disk space attached. Total disk space for a VM includes the size of the boot"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-11",
    "content": "disk. Shared-core machine typesare limited to 16 Persistent Disk volumes and 3 TiB of total Persistent Disk space. Shared-core machine typesare limited to 16 Persistent Disk volumes and 3 TiB of total Persistent Disk space. Creating logical volumes larger than 64 TiB might require special consideration. For more information about larger logical volume performance seelogical volume size. Creating logical volumes larger than 64 TiB might require special consideration. For more information about larger logical volume performance seelogical volume size. Persistent Disk is designed to run in tandem with Google's file system,Colossus, which is a distributed block storage system. Persistent Disk drivers automatically encrypt data on the VM before it's transmitted from the VM onto the network. Then, Colossus persists the data. When Colossus reads the data, the driver decrypts the incoming data. Persistent Disk volumes use Colossus for the storage backend. Having disks as a service is useful in a number of cases, for example: Resizing the disks while the instance is running becomes easier than stopping the instance first. You can increase the disk size without stopping the instance. Attaching and detaching disks becomes easier when disks and VMs don't have to share the same lifecycle or be co-located. It's possible to stop a VM and use its Persistent Disk boot disk to boot another VM. High availability features like replication become easier because the disk driver can hide replication details and provide automatic write-time replication. Learn how toadd a Persistent Disk volume to your VM. Learn how toadd a Persistent Disk volume to your VM. Reviewdisk and image pricinginformation. Reviewdisk and image pricinginformation. Learn how toclone a Persistent Disk volume. Learn how"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/persistent-disks",
    "title": "About Persistent DiskStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/persistent-disks#chunk-12",
    "content": "toclone a Persistent Disk volume. Learn how toshare Persistent Disk volumes between VMs. Learn how toshare Persistent Disk volumes between VMs. Learn how tooptimize Persistent Disk performance. Learn how tooptimize Persistent Disk performance. Learn how toview your Persistent Disk volumes' actual and forecasted usage. Seebest practices for disk snapshots. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/gitlab",
    "title": "GitLab on Google Cloud documentation",
    "chunk_id": "https://cloud.google.com/docs/gitlab#chunk-0",
    "content": "Home Documentation GitLab on Google Cloud The GitLab on Google Cloud integration simplifies deploying GitLab source code to Google Cloud runtimes, and is available for the Free, Premium, and Ultimate tier of the GitLab.com offering. To get started, try the end-to-end GitLab tutorial,Set up the GitLab on Google Cloud integration. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Set up the GitLab on Google Cloud integration Set up the GitLab on Google Cloud integration Overview Overview GitLab on Google Cloud Identity and Access Management GitLab on Google Cloud Identity and Access Management GitLab on Google Cloud Artifact management GitLab on Google Cloud Artifact management Provisioning runners Provisioning runners Create a GitLab pipeline to push to Google Artifact Registry Create a GitLab pipeline to push to Google Artifact Registry Create and deploy a web service with the Google Cloud Run component Create and deploy a web service with the Google Cloud Run component GitLab Pricing GitLab Pricing Artifact Registry Pricing Artifact Registry Pricing Google Cloud Pricing Google Cloud Pricing Release notes Release notes Artifact Registry GitLab component Artifact Registry is a single place to manage container images. It is fully integrated with Google Cloud\u2019s tooling and runtimes. This makes it simple to integrate it with your CI/CD tooling to set up automated pipelines.Once you have connected GitLab to Artifact Registry and pushed a container image to your repository, you can view the container image in GitLab orArtifact Registry, and you can access metadata for each artifact inGoogle Cloud.Component Cloud Deploy component"
  },
  {
    "source_url": "https://cloud.google.com/docs/gitlab",
    "title": "GitLab on Google Cloud documentation",
    "chunk_id": "https://cloud.google.com/docs/gitlab#chunk-1",
    "content": "Thecreate-cloud-deploy-releaseGitLab Component creates a Cloud Deploy release to manage the deployment of an application to one or more Google Kubernetes Engine (GKE) Enterprise edition or Cloud Run targets.Component Cloud Run component Thedeploy-cloud-runGitLab Component automates the deployment of your Cloud Run services within your GitLab CI/CD pipeline. The component offers flexible deployment behavior, creating a brand-new service if one doesn't already exist in your project and region. Conversely, if a Cloud Run service with the same name is already present, the component updates it to a new revision using your inputs.Component Google Cloud SDK component Therun-gcloudGitLab component executes Google Cloud CLI commands. The component uses a customized Google Cloud CLI image instead of thegoogle/cloud-sdkimage to reduce the image size and avoid security vulnerabilities.Component Cloud Storage component Cloud Storage is a managed service for storing unstructured data. Store any amount of data and retrieve it as often as you like. To upload to Cloud Storage, add the component to your CICD pipeline in GitLab.Component App Engine component Thedeploy-app-enginecomponent deploys container images stored in Artifact Registry, or App Engine flexible environmentsource code, to App Engine as part of your GitLab CI/CD pipeline.Component Google Kubernetes Engine (GKE) component Thedeploy-gkecomponent deploys a container image to a GKE cluster. It also performs horizontal pod autoscaling up to 3 nodes and creates a Service if the application needs a port exposed.Component Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers"
  },
  {
    "source_url": "https://cloud.google.com/docs/gitlab",
    "title": "GitLab on Google Cloud documentation",
    "chunk_id": "https://cloud.google.com/docs/gitlab#chunk-2",
    "content": "Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini",
    "title": "Text generationStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation To see an example of getting started with Chat with the Gemini Pro model, run the \"Getting Started with Chat with the Gemini Pro model\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub This page shows you how to send chat prompts to a Gemini model by using the Google Cloud console, REST API, and supported SDKs. To learn how to add images and other media to your request, seeImage understanding. For a list of languages supported by Gemini, seeLanguage support. To explore the generative AI models and APIs that are available on Vertex AI, go to Model Garden in the Google Cloud console. Go to Model Garden If you're looking for a way to use Gemini directly from your mobile and web apps, see theFirebase AI Logic client SDKsfor Swift, Android, Web, Flutter, and Unity apps. For testing and iterating on chat prompts, we recommend using the Google Cloud console. To send prompts programmatically to the model, you can use the REST API, Google Gen AI SDK, Vertex AI SDK for Python, or one of the other supported libraries and SDKs. You can use system instructions to steer the behavior of the model based on a specific need or use case. For example, you can define a persona or role for a chatbot that responds to customer service requests. For more information, see thesystem instructions code samples. You can use theGoogle Gen AI SDKto send requests if you're usingGemini 2.0 Flash. Here is a simple text generation example. To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini",
    "title": "Text generationStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini#chunk-1",
    "content": "your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True Learn how to install or update theGen AI SDK for Go. To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True Learn how to install or update theGen AI SDK for Java. To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True You can choose whether the model generatesstreamingresponses ornon-streamingresponses. For streaming responses, you receive each response as soon as its output token is generated. For non-streaming responses, you receive all responses after all of the output tokens are generated. Here is a streaming text generation example. Before trying this sample, follow thePythonsetup instructions in theVertex AI quickstart using client libraries. For more information, see theVertex AIPythonAPI reference documentation. To"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini",
    "title": "Text generationStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini#chunk-2",
    "content": "authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment. Learn how to send multimodal prompt requests:Image understandingVideo understandingAudio understandingDocument understanding Learn how to send multimodal prompt requests: Image understanding Video understanding Audio understanding Document understanding Learn aboutresponsible AI best practices and Vertex AI's safety filters. Learn aboutresponsible AI best practices and Vertex AI's safety filters. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/samples",
    "title": "Google Cloud samples",
    "chunk_id": "https://cloud.google.com/docs/samples#chunk-0",
    "content": "Home Documentation Search for samples demonstrating the usage of Google Cloud products. For Terraform samples, seeResource samplesandBlueprints."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments",
    "title": "Introduction to Vertex AI ExperimentsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments#chunk-0",
    "content": "Home Vertex AI Documentation To see an example of getting started with Vertex AI Experiments, run the \"Get started with Vertex AI Experiments\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub Vertex AI Experiments is a tool that helps you track and analyze different model architectures, hyperparameters, and training environments, letting you track the steps, inputs, and outputs of an experiment run. Vertex AI Experiments can also evaluate how your model performed in aggregate, against test datasets, and during the training run. You can then use this information to select the best model for your particular use case. Experiment runs don't incur additional charges. You're only charged for resources that you use during your experiment as described inVertex AI pricing. Vertex AI Experiments lets you track: steps of anexperiment run, for example, preprocessing, training, inputs, for example, algorithm, parameters, datasets, outputs of those steps, for example, models, checkpoints, metrics. You can then figure out what worked and what didn't, and identify further avenues for experimentation. For user journey examples, check out: Model training Compare models Vertex AI Experiments lets you track and evaluate how the model performed in aggregate, against test datasets, and during the training run. This ability helps to understand the performance characteristics of the models -- how well a particular model works overall, where it fails, and where the model excels. For user journey examples, check out: Compare pipeline runs Compare models Vertex AI Experiments lets you group and compare multiple models acrossexperiment runs. Each model has its own specified parameters,"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments",
    "title": "Introduction to Vertex AI ExperimentsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments#chunk-1",
    "content": "modeling techniques, architectures, and input. This approach helps select the best model. For user journey examples, check out: Compare pipeline runs Compare models The Google Cloud console provides a centralized view of experiments, a cross-sectional view of the experiment runs, and the details for each run. The Vertex AI SDK for Python provides APIs to consume experiments, experiment runs, experiment run parameters, metrics, and artifacts. Vertex AI Experiments, along withVertex ML Metadata, provides a way to find the artifacts tracked in an experiment. This lets you quickly view the artifact's lineage and the artifacts consumed and produced by steps in a run. Vertex AI Experiments supports development of models using Vertex AI custom training, Vertex AI Workbench notebooks, Notebooks, and all Python ML Frameworks across most ML Frameworks. For some ML frameworks, such as TensorFlow, Vertex AI Experiments provides deep integrations into the framework that makes the user experience automagical. For other ML frameworks, Vertex AI Experiments provides a framework neutral Vertex AI SDK for Python that you can use. (see:Prebuilt containersfor TensorFlow, scikit-learn, PyTorch, XGBoost). Vertex AI Experiments is acontextinVertex ML Metadatawhere an experiment can containnexperiment runs in addition tonpipeline runs. An experiment run consists of parameters, summary metrics, time series metrics, andPipelineJob,Artifact, andExecutionVertex AI resources.Vertex AI TensorBoard, a managed version of open source TensorBoard, is used for time-series metrics storage. Executions andartifactsof a pipeline run are viewable in theGoogle Cloud console. experimentAn experiment is a context that can contain a set of n experiment runs in addition to pipeline runs where a user can"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments",
    "title": "Introduction to Vertex AI ExperimentsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments#chunk-2",
    "content": "investigate, as a group, different configurations such as input artifacts or hyperparameters.SeeCreate an experiment. An experiment is a context that can contain a set of n experiment runs in addition to pipeline runs where a user can investigate, as a group, different configurations such as input artifacts or hyperparameters. experiment runAn experiment run can contain user-defined metrics, parameters, executions, artifacts, and Vertex resources (for example, PipelineJob).SeeCreate and manage experiment runs. An experiment run can contain user-defined metrics, parameters, executions, artifacts, and Vertex resources (for example, PipelineJob). pipeline runOne or more Vertex PipelineJobs can be associated with an experiment where each PipelineJob is represented as a single run. In this context, the parameters of the run are inferred by the parameters of the PipelineJob. The metrics are inferred from the system.Metric artifacts produced by that PipelineJob. The artifacts of the run are inferred from artifacts produced by that PipelineJob.One or more Vertex AIPipelineJobresource can be associated with anExperimentRunresource. In this context, the parameters, metrics, and artifacts are not inferred. One or more Vertex PipelineJobs can be associated with an experiment where each PipelineJob is represented as a single run. In this context, the parameters of the run are inferred by the parameters of the PipelineJob. The metrics are inferred from the system.Metric artifacts produced by that PipelineJob. The artifacts of the run are inferred from artifacts produced by that PipelineJob. SeeAssociate a pipeline with an experiment. SeeLog parameters. summary metricsSummary metrics are a single value for each metric key in an experiment run. For example, the test accuracy of an"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments",
    "title": "Introduction to Vertex AI ExperimentsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments#chunk-3",
    "content": "experiment is the accuracy calculated against a test dataset at the end of training that can be captured as a single value summary metric. Summary metrics are a single value for each metric key in an experiment run. For example, the test accuracy of an experiment is the accuracy calculated against a test dataset at the end of training that can be captured as a single value summary metric. SeeLog summary metrics. time series metricsTime series metrics are longitudinal metric values where each value represents a step in the training routine portion of a run. Time series metrics are stored in Vertex AI TensorBoard. Vertex AI Experiments stores a reference to the Vertex TensorBoard resource. Time series metrics are longitudinal metric values where each value represents a step in the training routine portion of a run. Time series metrics are stored in Vertex AI TensorBoard. Vertex AI Experiments stores a reference to the Vertex TensorBoard resource. SeeLog time series metrics. pipeline jobA pipeline job or a pipeline run corresponds to the PipelineJob resource in the Vertex AI API. It's an execution instance of your ML pipeline definition, which is defined as a set of ML tasks interconnected by input-output dependencies. A pipeline job or a pipeline run corresponds to the PipelineJob resource in the Vertex AI API. It's an execution instance of your ML pipeline definition, which is defined as a set of ML tasks interconnected by input-output dependencies. artifactAn artifact is a discrete entity or piece of data produced and consumed by a machine learning workflow. Examples of artifacts include datasets, models, input files, and training logs. An artifact is a discrete entity or piece of data produced and consumed by a machine learning workflow. Examples of artifacts include"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments",
    "title": "Introduction to Vertex AI ExperimentsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments#chunk-4",
    "content": "datasets, models, input files, and training logs. Vertex AI Experiments lets you use a schema to define the type of artifact. For example, supported schema types includesystem.Dataset,system.Model, andsystem.Artifact. For more information, seeSystem schemas. Get started with Vertex AI Experiments Set up to get started with Vertex AI Experiments Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/devtools",
    "title": "Google Cloud SDK, languages, frameworks, and tools",
    "chunk_id": "https://cloud.google.com/docs/devtools#chunk-0",
    "content": "Home Documentation Use Google Cloud SDK, languages, frameworks, and tools effectively in cloud development. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/iac",
    "title": "Infrastructure as code",
    "chunk_id": "https://cloud.google.com/docs/iac#chunk-0",
    "content": "Home Documentation Configure your infrastructure using code instead of graphical interfaces or command-line scripts. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview",
    "title": "Grounding overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation To see an example of grounding, run the \"Intro to grounding\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub In generative AI, grounding is the ability to connect model output to verifiable sources of information. If you provide models with access to specific data sources, then grounding tethers their output to these data and reduces the chances of inventing content. This is particularly important in situations where accuracy and reliability are significant. Grounding provides the following benefits: Reduces model hallucinations, which are instances where the model generates content that isn't factual. Anchors model responses to your data sources. Provides auditability by providing grounding support, which are links to sources. You can ground supported-model output in Vertex AI in the following ways: For language support, seeSupported languages for prompts. To learn more about responsible AI best practices and Vertex AI's safety filters, seeResponsible AI. To ground with your Google Search API, seeGrounding with Google Search API. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-20 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/buildpacks",
    "title": "Buildpacks documentation",
    "chunk_id": "https://cloud.google.com/docs/buildpacks#chunk-0",
    "content": "Home Documentation Google Cloud's buildpacks Use Google Cloud's Buildpacks to create and run containers on Google Cloud.Learn more. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. GuideBuild an application with buildpacks Build an application with buildpacks GuideBuild a function with buildpacks Build a function with buildpacks ReferenceLanguages supported by buildpacks Languages supported by buildpacks ReferenceBuildpacks configurations Buildpacks configurations ReferenceBuildpacks build and run image configurations Buildpacks build and run image configurations GitHubGoogle Cloud's buildpacks Google Cloud's buildpacks Sample apps Find samples to build your functions and applications with buildpacks. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/generative-ai",
    "title": "No title",
    "chunk_id": "https://cloud.google.com/docs/generative-ai#chunk-0",
    "content": "Home Documentation Google Cloud provides a set of state-of-the-art foundation models through Vertex AI, including Gemini. You can also deploy a third-party model to either Vertex AI Model Garden or self-host on GKE or Compute Engine. Prompt design is the process of authoring prompt and response pairs to give language models additional context and instructions. After you author prompts, you feed them to the model as a prompt dataset for pretraining. When a model serves predictions, it responds with your instructions built in. Groundingconnects AI models to data sources to improve the accuracy of responses and reduce hallucinations.RAG, a common grounding technique, searches for relevant information and adds it to the model's prompt, ensuring output is based on facts and up-to-date information. Agents make it easy to design and integrate a conversational user interface into your mobile app, while function calling extends the capabilities of a model. Specialized tasks, such as training a language model on specific terminology, might require more training than you can do with prompt design or grounding alone. In that scenario, you can use model tuning to improve performance, or train your own model. C# and .NET C++ Go Java JavaScript and Node.js Python Ruby Python (LangChain) JavaScript (LangChain.js) Java (LangChain4j) Go (LangChainGo) Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-17 UTC."
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/bigquery/overview",
    "title": "Gemini in BigQuery overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/bigquery/overview#chunk-0",
    "content": "Home Gemini for Google Cloud Documentation Guides This document describes how Gemini in BigQuery, which is part of theGemini for Google Cloudproduct suite, provides AI-powered assistance to help you work with your data. Gemini in BigQuery provides AI assistance to help you do the following: Explore and understand your data with data insights. Data insights offers an automated, intuitive way to uncover patterns and perform statistical analysis by using insightful queries that are generated from the metadata of your tables. This feature is especially helpful in addressing the cold-start challenges of early data exploration. For more information, seeGenerate data insights in BigQuery. Discover, transform, query, and visualize data with BigQuery data canvas. You can use natural language with Gemini in BigQuery, to find, join, and query table assets, visualize results, and seamlessly collaborate with others throughout the entire process. For more information, seeAnalyze with data canvas. Get assisted SQL and Python data analysis. You can use Gemini in BigQuery to generate or suggest code in either SQL or Python, and to explain an existing SQL query. You can also use natural language queries to begin data analysis. To learn how to generate, complete, and summarize code, see the following documentation:SQL code assistUse the SQL generation toolPrompt to generate SQL queriesGenerate SQL queries with Gemini Cloud Assist(Preview)Complete a SQL query(Preview)Explain a SQL queryPython code assistGenerate Python code with the code generation toolGenerate Python code with Gemini Cloud Assist(Preview)Python code completionGenerate BigQuery DataFrames Python code(Preview)Prepare data for analysis. Data preparation in BigQuery gives you context aware, AI-generated transformation"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/bigquery/overview",
    "title": "Gemini in BigQuery overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/bigquery/overview#chunk-1",
    "content": "recommendations to cleanse data for analysis. For more information, seePrepare data with Gemini.Customize your SQL translations with translation rules. (Preview) Create Gemini-enhanced translation rules to customize your SQL translations when using theinteractive SQL translator. You can describe changes to the SQL translation output using natural language prompts or specify SQL patterns to find and replace. For more information, seeCreate a translation rule. SQL code assistUse the SQL generation toolPrompt to generate SQL queriesGenerate SQL queries with Gemini Cloud Assist(Preview)Complete a SQL query(Preview)Explain a SQL query Use the SQL generation tool Prompt to generate SQL queries Generate SQL queries with Gemini Cloud Assist(Preview) Complete a SQL query(Preview) Explain a SQL query Python code assistGenerate Python code with the code generation toolGenerate Python code with Gemini Cloud Assist(Preview)Python code completionGenerate BigQuery DataFrames Python code(Preview) Generate Python code with the code generation tool Generate Python code with Gemini Cloud Assist(Preview) Python code completion Generate BigQuery DataFrames Python code(Preview) Prepare data for analysis. Data preparation in BigQuery gives you context aware, AI-generated transformation recommendations to cleanse data for analysis. For more information, seePrepare data with Gemini. Customize your SQL translations with translation rules. (Preview) Create Gemini-enhanced translation rules to customize your SQL translations when using theinteractive SQL translator. You can describe changes to the SQL translation output using natural language prompts or specify SQL patterns to find and replace. For more information, seeCreate a translation rule. Gemini for Google Cloud doesn't use your prompts or"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/bigquery/overview",
    "title": "Gemini in BigQuery overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/bigquery/overview#chunk-2",
    "content": "its responses as data to train its models without your express permission. For more information about how Google uses your data, seeHow Gemini for Google Cloud uses your data. Learn how and when Gemini for Google Cloud uses your data. As an early-stage technology, Gemini for Google Cloud products can generate output that seems plausible but is factually incorrect. We recommend that you validate all output from Gemini for Google Cloud products before you use it. For more information, seeGemini for Google Cloud and responsible AI. SeeGemini for Google Cloud pricing. For quotas and limits that apply to Gemini in BigQuery, seeGemini for Google Cloud quotas and limits. After youset up Gemini in BigQuery, you can use Gemini in BigQuery to do the following in BigQuery Studio: Togenerate data insights, go to theInsightstab for a table entry, where you can identify patterns, assess quality, and run statistical analysis across your BigQuery data. To use data canvas,create a data canvas or use data canvasfrom a table or query to explore data assets with natural language and share your canvases. To use natural language to generate SQL or Python code, or receive suggestions with autocomplete while typing, use theSQL generation toolfor yourSQL queriesorPython code. Gemini in BigQuery can also explain your SQL code in natural language. To prepare data for analysis, in theCreate newlist, selectData preparation. For more information, seeOpen the data preparation editor in BigQuery. For detailed setup steps, seeSet up Gemini in BigQuery. In order to provide accurate results, Gemini in BigQuery requires access to both your Customer Data and metadata in BigQuery for enhanced features. Enabling Gemini in BigQuery grants Gemini permission to access this data, which includes your tables and"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/bigquery/overview",
    "title": "Gemini in BigQuery overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/bigquery/overview#chunk-3",
    "content": "query history. Gemini in BigQuery doesn't use your data to train or fine-tune its models. For more information on how Gemini uses your data, seehow Gemini for Google Cloud uses your data. Enhanced features in Gemini in BigQuery are the following: SQL generation tool Prompt to generate SQL queries Complete a SQL query Explain a SQL query Generate python code Python code completion Data canvas Data preparation Data insights For information about where Gemini processes your data, seeGemini serving locations. See the latest enhancements and fixes inrelease notes. Learn how toset up Gemini in BigQuery. Learn how towrite queries with Gemini assistance. Learn more aboutGoogle Cloud compliance. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-14 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/predictions/overview",
    "title": "Overview of getting predictions on Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/predictions/overview#chunk-0",
    "content": "Home Vertex AI Documentation A prediction is the output of a trained machine learning model. This page provides an overview of the workflow for getting predictions from your models on Vertex AI. Vertex AI offers two methods for getting prediction: Online predictionsare synchronous requests made to a model that is deployed to anEndpoint. Therefore, before sending a request, you must first deploy theModelresource to an endpoint. This associatescompute resourceswith the model so that the model can serve online predictions with low latency. Use online predictions when you are making requests in response to application input or in situations that require timely inference. Batch predictionsare asynchronous requests made to a model that isn't deployed to an endpoint. You send the request (as aBatchPredictionJobresource) directly to theModelresource. Use batch predictions when you don't require an immediate response and want to process accumulated data by using a single request. To get predictions, you must firstimport your model. After it's imported, it becomes aModelresource that is visible inVertex AI Model Registry. Then, read the following documentation to learn how to get predictions: Get batch predictionsOr Get batch predictions Or Deploy model to endpointandget online predictions. Deploy model to endpointandget online predictions. Unlike custom trained models, AutoML models are automatically imported into the Vertex AI Model Registry after training. Other than that, the workflow for AutoML models is similar, but varies slightly based on your data type and model objective. The documentation for getting AutoML predictions is located alongside the other AutoML documentation. Here are links to the documentation: Learn how to get predictions from the following types of image"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/predictions/overview",
    "title": "Overview of getting predictions on Vertex AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/predictions/overview#chunk-1",
    "content": "AutoML models: Image classification models Image object detection models Learn how to get predictions from the following types of tabular AutoML models: Tabular classification and regression modelsOnline predictionsBatch predictions Tabular classification and regression models Online predictions Batch predictions Tabular forecasting models(batch predictions only) Tabular forecasting models(batch predictions only) Learn how to get predictions from the following types of text AutoML models: Text classification models Text entity extraction models Text sentiment analysis models Learn how to get predictions from the following types of video AutoML models: Video action recognition models(batch predictions only) Video classification models(batch predictions only) Video object tracking models(batch predictions only) There are two ways to get predictions from BigQuery ML models: You can request batch predictions directly from the model in BigQuery ML. You can register the models directly with the Model Registry, without exporting them from BigQuery ML or importing them into the Model Registry. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction",
    "title": "Introduction to Vertex AI TensorBoardStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI TensorBoard is an enterprise-ready managed version ofOpen Source TensorBoard(TB), which is a Google Open Source project for machine learning experiment visualization. With Vertex AI TensorBoard, you can track, visualize, and compare ML experiments and share them with your team. Vertex AI TensorBoard provides various detailed visualizations, that includes: tracking and visualizing metrics such as loss and accuracy over time, visualizing model computational graphs (ops and layers), viewing histograms of weights, biases, or other tensors as they change over time, projecting embeddings to a lower dimensional space, and displaying image, text, and audio samples. In addition to the powerful visualizations from TensorBoard, Vertex AI TensorBoard provides: a persistent, shareable link to your experiment's, Vertex AI TensorBoard experiment, tight integrations with Vertex AI services for model training, enterprise-grade security, privacy, and compliance. Integration with Vertex AI Experiments lets you: use a searchable and compare list of all experiments in a project, view time series metrics in the Google Cloud console, compare scalars across experiments and experiment runs, have direct access to Vertex AI TensorBoard. The Google Cloud console is used to: create or delete Vertex AI TensorBoard instances, createVertex AI Experiments, view Vertex AI TensorBoard instance storage size \u2192 associated costs, view associatedVertex AI Experiments, Custom Jobs, and Pipeline Runs, visualize sometime series metrics. Setup Vertex AI TensorBoard Check out theTensorBoard API documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction",
    "title": "Introduction to Vertex AI TensorBoardStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction#chunk-1",
    "content": "License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/application-development",
    "title": "Application development",
    "chunk_id": "https://cloud.google.com/docs/application-development#chunk-0",
    "content": "Home Documentation Create applications with a comprehensive set of tools and services. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Design, build, productize, and manage APIs for your internal and external customers. Automate code deployment using CI/CD processes and products that assist in efficient, seamless, and secure deployment of code. Write, deploy, and debug your applications faster with powerful developer tools. Orchestrate decoupled services and build message-based and event-driven solutions. Expand this section to see relevant products and documentation. Automate your business workflows with integrations that connect to enterprise applications, databases, and much more. Track modifications to source code. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/looker/overview#chunk-0",
    "content": "Home Gemini for Google Cloud Documentation Guides Preview This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA products and features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. Gemini in Looker is a product in theGemini for Google Cloudportfolio that provides generative AI-powered assistance to help you analyze and gain valuable insights from your data. Gemini in Looker can provide assistance for tasks in Looker (original) instances, Looker (Google Cloud core) instances, and in Looker Studio. For more information about available features, see theGemini in Looker overviewdocumentation page. Learnhow and when Gemini for Google Cloud uses your data. When Gemini in Looker is enabled, Looker users can perform the following tasks in a Looker (Google Cloud core) instance or a Looker (original) instance: Ask questions about and converse with your data using Conversational Analytics. Gemini in Looker lets you ask questions about your data source by using natural language. Gemini returns Looker Studio charts or data tables that are based on your query. You can learn more about how your response was generated and save your conversation for future reference. Generate custom Looker visualizationsGemini in Looker lets you customize formatting options for Looker visualizations by using natural language. Gemini generates JSON formatting options from text-based prompts, which you can apply to your visualization. You can also use prompts as a starting point for creating templates and patterns for more complex customizations and thenmanually update the visualization formatting options. Generate LookML. Gemini in Looker assists you in"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/looker/overview#chunk-1",
    "content": "generating LookML parameters. Gemini suggests LookML parameters based on a natural language prompt, which you can add to your project files. To access these features in a Looker (original) instance, a Looker admin mustenable Gemini in Lookerin the Looker (original) instance settings. The instance must be on Looker 25.2 or later and be Looker hosted. Conversational Analytics is available on Looker instances on 25.0 or later. To access these features in a Looker (Google Cloud core) instance, a user with theLooker Admin(roles/looker.admin) IAM role mustenable Gemini in Lookerin the Looker (Google Cloud core) instance settings in the Google Cloud console. To use any of the aforementioned Gemini in Looker features in a Looker instance, users must be granted a Looker role that contains thegemini_in_lookerpermissionfor the models that they're applying Gemini assistance to. This permission is available as part of the defaultGemini role. The following Gemini in Looker features require additional permissions: Tocreate custom visualizations with Gemini assistance, you must be assigned a Looker role that contains thecan_override_vis_configpermission. Towrite LookML with Gemini assistance, you must be assigned a Looker role that contains thedeveloppermission for at least one model in a LookML project. To query data or create a data agent withConversational Analytics, you must be assigned a Looker role that contains theaccess_datapermission for the model that you are querying. When Gemini in Looker is enabled for Looker Studio, Looker Studio users can perform the following tasks in Looker Studio: Ask questions about and converse with your data using Conversational Analytics. Gemini in Looker lets you ask questions about your data source by using natural language. Gemini returns"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/looker/overview#chunk-2",
    "content": "Looker Studio charts or data tables that are based on your query. You can learn more about how your response was generated and save your conversation for future reference. Create calculated fields by using natural language. Gemini in Looker lets you createcalculated fieldsin Looker Studio by prompting you to describe the kinds of fields that you'd like to create. Based on your input, Gemini suggests a formula for a calculated field by using fields from your data source along with Looker Studio functions and operators. Add Looker Studio content to your Slides presentation. Gemini in Looker lets you importcomponentsfrom your Looker Studio Pro reports into your Slides presentations. Gemini inserts report charts as images, generates a textual summary of each image, and inserts the summary as a text element. You can generate a new Slides presentation by using all or selected visualizations in a Looker Studio report, or you can add or update Looker Studio content to an existing Slides presentation. You can also update the Looker Studio data that has been imported in a Slides presentation. To access these features, a user with the appropriate IAM or Google Workspace role mustenable Gemini in Lookerin Looker Studio. To use any of the aforementioned Gemini in Looker features in Looker Studio, users must be granted the following roles or privileges: To create calculated fields, users must be assigned anEditorrole. To add Looker Studio content to your Slides presentation, users must be assigned a Viewer or Editor role in Looker Studio and have theEditor or Ownerpermission level for the Slides presentation. To useConversational Analytics with a Looker data sourcein Looker Studio users must be granted thegemini_in_lookerpermissionfor the Looker models that they're applying Gemini"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/looker/overview#chunk-3",
    "content": "assistance to. Looker users can also interact with the Gemini in Looker features that appear in Looker Studioas part of a Looker Studio Pro subscription. If your Looker admin has accepted the complimentary Looker Studio Pro licenses for yourLooker (Google Cloud core) instanceorLooker (original) instance, as a Looker user, you can access the Gemini in Looker features that appear in Looker Studio when Gemini in Looker is enabled for your Looker Studio Pro subscription. After you enable Gemini in Looker for the assistants that appear in your Looker product, you can seek Gemini assistance in the places that are described in the following sections. To access Conversational Analytics, follow these steps: Navigate to themain navigation menu. Selectchat_sparkConversations. To access Conversational Analytics, follow these steps: Navigate to the Looker instance homepage. Select theCreatemenu. Selectchat_sparkConversation. To access Conversational Analytics, follow these steps: Navigate to theLooker Explorethat contains the data you would like to chat with. SelectStart a conversation. TheVisualization Assistantis available for visualizations that use the HighCharts API, which includes mostCartesian charts, such as thecolumn chart,bar chart, andline chart. To access theVisualization Assistant, follow these steps: View a supported visualization in an Explore, or edit a visualization in a Look or dashboard. Open theEditmenu in the visualization. ClickVisualization Assistantto open the prompt menu. For more information, seeCreate visualizations with Gemini assistance. To use Gemini to create LookML in your Looker project, follow these steps: On your Looker instance, enableDevelopment Mode. Open your project in the Looker IDE. Use the IDEfile browserto open a LookML view file in which"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/looker/overview#chunk-4",
    "content": "you want to insert LookML. Select theHelp Me Codeicon from the side panel selector. For more information, seeWrite LookML with Gemini assistance. To use Gemini assistance to query your data in natural language, selectConversational Analyticsfrom the left navigation in Looker Studio. To create a custom data agent, selectConversational Analyticsfrom the left navigation in Looker Studio, and then selectManage agents. If you want to query data within yourpersonal sandbox, follow these steps: From the left navigation, select theSandboxproject. ClickCreate. SelectConversation. For more information about querying your data in natural language, seeConversational Analytics: Query your data in natural language with Gemini assistance. For more information about creating and managing a custom data agent, seeConversational Analytics: Data Agents. To use Gemini assistance to write formulas for calculated fields, follow these steps from a Looker Studio report: Edit the data source. ClickAdd a field. SelectAdd calculated field. Click theHelp me writeicon. For more information, seeCreate calculated fields with Gemini assistance. To use Gemini assistance to create a Slides presentation that includes all or selected visualizations in a Looker Studio report, follow these steps: Open a Looker Studio report in either view or edit mode. Select the Gemini panel in the panel manager. SelectGenerate Slides. For more information, seeAdding Looker Studio content to your Slides presentation with Gemini assistance. To use Gemini assistance to add Looker Studio content to an existing Slides presentation, follow these steps: Open a Slides presentation. Click the Looker Studio icon on the right-hand toolbar to open the Looker Studio Pro panel. For more information, seeAdding Looker Studio content to"
  },
  {
    "source_url": "https://cloud.google.com/gemini/docs/looker/overview",
    "title": "Gemini in LookerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/gemini/docs/looker/overview#chunk-5",
    "content": "your Slides presentation with Gemini assistance. See the latest enhancements and fixes inrelease notes. Assign the Gemini role to Gemini in Looker users Learn howGemini for Google Cloud uses your data. Learn more aboutGoogle Cloud compliance. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/spanner/docs/graph",
    "title": "Spanner Graph documentation",
    "chunk_id": "https://cloud.google.com/spanner/docs/graph#chunk-0",
    "content": "Documentation Spanner Graph unites purpose-built graph database capabilities withSpanner, which offers industry-leading scalability, availability, and consistency. Spanner Graph supports a graph query interface compatible with the ISO GQL (Graph Query Language) standards. It also supports interoperability between relational and graph models and combines the well-established SQL capabilities with the expressiveness of graph pattern matching from GQL. To learn more, see theSpanner Graph overview. Not sure what database option is right for you? Learn more about ourdatabase services. Spanner Graph overview Spanner Graph overview Set up and query Spanner Graph Set up and query Spanner Graph Spanner Graph schema overview Spanner Graph schema overview Insert, update, or delete Spanner Graph data Insert, update, or delete Spanner Graph data Spanner Graph queries overview Spanner Graph queries overview Migrate to Spanner Graph Migrate to Spanner Graph Spanner Graph reference for openCypher users Spanner Graph reference for openCypher users Troubleshoot Spanner Graph Troubleshoot Spanner Graph Graph Query Language Graph Query Language Schema statements Schema statements Query statements Query statements GQL within SQL GQL within SQL gcloud command-line tool gcloud command-line tool Pricing Pricing Quotas and limits Quotas and limits Release notes Release notes Get support Get support Get started with the Spanner Graph codelab In this Codelab, learn how to set up a Spanner Graph database using a pre-populated dataset, query the graph using GQL, and access both graph and relational data together by combining GQL and SQL. Spanner Graph reference for openCypher users Learn the differences between Spanner Graph and openCypher.MigrationOpenCypherApache Create database with a property"
  },
  {
    "source_url": "https://cloud.google.com/spanner/docs/graph",
    "title": "Spanner Graph documentation",
    "chunk_id": "https://cloud.google.com/spanner/docs/graph#chunk-1",
    "content": "graph Create a Spanner database using a property graph. Insert graph data Insert data into a Spanner Graph database. Query data in a graph Query data in a Spanner Graph database. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-0",
    "content": "Home Vertex AI Documentation With Vertex AI Neural Architecture Search, you can search for optimal neural architectures in terms of accuracy, latency, memory, a combination of these, or a custom metric. Vertex AI Neural Architecture Search is a high-end optimization tool used to find best neural architectures in terms of accuracy with or without constraints such as latency, memory, or a custom metric. The search space of possible neural architecture choices can be as large as 10^20. It is based on a technique, which has successfully generated several state of the art computer vision models in the past years, includingNasnet,MNasnet,EfficientNet,NAS-FPN, andSpineNet. Neural Architecture Search isn't a solution where you can just bring your data and expect a good result without experimentation. It is an experimentation tool. Neural Architecture Search isn't for hyperparameter tuning such as for tuning the learning rate or optimizer settings. It is only meant for an architecture search. You shouldn't combine hyper-parameter tuning with Neural Architecture Search. Neural Architecture Search is not recommended with limited training data or for highly imbalanced datasets where some classes are very rare. If you are already using heavy augmentations for your baseline training due to lack of data, then Neural Architecture Search is not recommended. You should first try other traditional and conventional machine learning methods and techniques such as hyperparameter tuning. You should use Neural Architecture Search only if you don't see further gain with such traditional methods. You should have an in-house team for model tuning, which has some basic idea about architecture parameters to modify and try. These architecture parameters can include the kernel size, number of"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-1",
    "content": "channels or connections among many other possibilities. If you have a search space in mind to explore, then Neural Architecture Search is highly valuable and can reduce at least approximately six months of engineering time in exploring a large search space: up to 10^20 architecture choices. Neural Architecture Search is meant for enterprise customers who can spend several thousand dollars on an experiment. Neural Architecture Search isn't limited to vision only use case. Currently, only vision-based prebuilt search spaces and prebuilt trainers are provided, but customers can bring their own non-vision search spaces and trainers as well. Neural Architecture Search doesn't use asupernet(oneshot-NAS or weight-sharing based NAS) approach where you just bring your own data, and use it as a solution. It is non-trivial (months of effort) to customize a supernet. Unlike a supernet, Neural Architecture Search is highly customizable to define custom search spaces and rewards. The customization can be done in approximately one to two days. Neural Architecture Search is supported in 8 regions across the world. Check theavailability in your region. You should also read the following section on expected cost, result gains, and GPU quota requirements before using Neural Architecture Search. The figure shows a typical Neural Architecture Search curve. TheY-axisshows the trial rewards, and theX-axisshows the number of trials launched. As the number of trials increase, the controller starts finding better models. Therefore, the reward starts increasing, and later, the reward variance and the reward growth start decreasing and show the convergence. At the point of convergence, the number of trials can vary based on the search-space size, but it is of the order of approximately 2000"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-2",
    "content": "trials. Each trial is designed to be a smaller version of full training calledproxy-taskwhich runs for approximately one to two hours on two Nvidia V100 GPUs. The customer can stop the search manually at any point and might find higher reward models compared to their baseline before the point of convergence occurs. It might be better to wait until the point of convergence occurs to choose the better results. After the search, the next stage is to pick top 10 trials (models) and run a full training on them. In this mode, observe the search curve or a few trials, approximately 25, and do a test drive with a prebuilt MNasNet search space and trainer. In the figure, the best stage-1 reward starts to climb up from ~0.30 at trial-1 to ~0.37 at trial-17. Your exact run may look slightly different due to sampling randomness but you should see some small increase in the best reward.Note that this is still a toy run and doesn't represent any proof-of-concept or a public benchmark validation. The cost for this run is detailed as follows: Stage-1:Number of trials: 25Number of GPUs per trial: 2GPU type: TESLA_T4Number of CPUs per trial: 1CPU type: n1-highmem-16Avg single trial training time: 3 hoursNumber of parallel trials: 6GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 12 GPUs. Useus-central1 regionfor the test drive and host training data in the same region.No extra quota needed.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials) = 12 hoursGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 150 T4 GPU hoursCPU hours: (total-trials * training-time-per-trial * num-cpus-per-trial) = 75 n1-highmem-16 hoursCost: Approximately $185. You can stop the job earlier to reduce the cost. Refer to thepricing pageto calculate exact"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-3",
    "content": "price. Number of trials: 25 Number of GPUs per trial: 2 GPU type: TESLA_T4 Number of CPUs per trial: 1 CPU type: n1-highmem-16 Avg single trial training time: 3 hours Number of parallel trials: 6 GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 12 GPUs. Useus-central1 regionfor the test drive and host training data in the same region.No extra quota needed. Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials) = 12 hours GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 150 T4 GPU hours CPU hours: (total-trials * training-time-per-trial * num-cpus-per-trial) = 75 n1-highmem-16 hours Cost: Approximately $185. You can stop the job earlier to reduce the cost. Refer to thepricing pageto calculate exact price. Because this is a toy run, there is no need to run a full stage-2 training for models from stage-1. To learn more about running stage-2, seetutorial 3. TheMnasNet notebookis used for this run. In case you are interested in almost replicating a publishedMNasnetresult, you can use this mode. According to the paper, MnasNet achieves 75.2% top-1 accuracy with 78 ms latency on a Pixel phone, which is 1.8x faster than the MobileNetV2 with 0.5% higher accuracy and 2.3x faster than NASNet with 1.2% higher accuracy. However, this example uses GPUs instead of TPUs for training and uses cloud-CPU (n1-highmem-8) to evaluate latency. With this example, the expected Stage2 top-1 accuracy on MNasNet is 75.2% with 50ms latency on cloud-CPU (n1-highmem-8). The cost for this run is detailed as follows: Stage-1 search:Number of trials: 2000Number of GPUs per trial: 2GPU type: TESLA_T4Avg single trial training time: 3 hoursNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Since"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-4",
    "content": "this number is above the default quota, create a quota request from your project UI. For more information, seesetting_up_path.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 25 days. Note: The job terminates after 14 days. After that time, you canresume the search jobeasily with one command for another 14 days. If you have higher GPU quota, then the runtime decreases proportionately.GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 12000 T4 GPU hours.Cost: ~$15,000 Stage-1 search: Number of trials: 2000 Number of GPUs per trial: 2 GPU type: TESLA_T4 Avg single trial training time: 3 hours Number of parallel trials: 10 GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Since this number is above the default quota, create a quota request from your project UI. For more information, seesetting_up_path. Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 25 days. Note: The job terminates after 14 days. After that time, you canresume the search jobeasily with one command for another 14 days. If you have higher GPU quota, then the runtime decreases proportionately. GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 12000 T4 GPU hours. Cost: ~$15,000 Stage-2 full-training with top 10 models:Number of trials: 10Number of GPUs per trial: 4GPU type: TESLA_T4Avg single trial training time: ~9 daysNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 40 T4 GPUs.Because this number is above the default quota, create a quota request from your project UI. For more information, seesetting_up_path.You can also run this with 20 T4 GPUs by running the job twice with five models at a time instead of all 10 in parallel.Time to"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-5",
    "content": "run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = ~9 daysGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 8960 T4 GPU hours.Cost: ~$8,000 Stage-2 full-training with top 10 models: Number of trials: 10 Number of GPUs per trial: 4 GPU type: TESLA_T4 Avg single trial training time: ~9 days Number of parallel trials: 10 GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 40 T4 GPUs.Because this number is above the default quota, create a quota request from your project UI. For more information, seesetting_up_path.You can also run this with 20 T4 GPUs by running the job twice with five models at a time instead of all 10 in parallel. Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = ~9 days GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 8960 T4 GPU hours. Cost: ~$8,000 Total cost: Approximately $23,000. Refer to thepricing pageto calculate exact price. Note: This example isn't an average regular training job. The full training runs for approximately nine days on four TESLA_T4 GPUs. TheMnasNet notebookis used for this run. We provide an approximate cost for an average custom user. Your needs can vary depending on your training task and GPUs and CPUs used. You need at least 20 GPUs quota for an end-to-end runas documented here. Note: The performance gain is completely dependent on your task. We can only provide examples like MNasnet as referenced examples for performance gain. The cost for this hypothetical custom run is detailed as follows: Stage-1 search:Number of trials: 2,000Number of GPUs per trial: 2GPU type: TESLA_T4Avg single trial training time: 1.5 hoursNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-6",
    "content": "GPUs.Because this number is above the default quota, you need to create a quota request from your project UI. For more information, seeRequest additional device quota for the project.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 12.5 daysGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 6000 T4 GPU hours.Cost: approximately $7,400 Stage-1 search: Number of trials: 2,000 Number of GPUs per trial: 2 GPU type: TESLA_T4 Avg single trial training time: 1.5 hours Number of parallel trials: 10 GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs.Because this number is above the default quota, you need to create a quota request from your project UI. For more information, seeRequest additional device quota for the project. Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = 12.5 days GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 6000 T4 GPU hours. Cost: approximately $7,400 Stage-2 full training with top 10 models:Number of trials: 10Number of GPUs per trial: 2GPU type: TESLA_T4Average single trial training time: approximately 4 daysNumber of parallel trials: 10GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs. **Since this number is above the default quota, you need to create a quota request from your project UI. For more information, seeRequest additional device quota for the project. Refer to the same documentation for custom quota needs.Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = approximately 4 daysGPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 1920 T4 GPU hours.Cost: approximately $2,400 Stage-2 full training with top 10 models: Number of trials: 10"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-7",
    "content": "Number of GPUs per trial: 2 GPU type: TESLA_T4 Average single trial training time: approximately 4 days Number of parallel trials: 10 GPU quota used: (num-gpus-per-trial * num-parallel-trials) = 20 T4 GPUs. **Since this number is above the default quota, you need to create a quota request from your project UI. For more information, seeRequest additional device quota for the project. Refer to the same documentation for custom quota needs. Time to run: (total-trials * training-time-per-trial)/(num-parallel-trials)/24 = approximately 4 days GPU hours: (total-trials * training-time-per-trial * num-gpus-per-trial) = 1920 T4 GPU hours. Cost: approximately $2,400 For more information on proxy-task design cost, seeProxy task designThe cost is similar to training 12 models (stage-2 in the figure uses 10 models):GPU quota used: Same as stage-2 run in the figure.Cost: (12/10) * stage-2-cost-for-10-models = ~$2,880 For more information on proxy-task design cost, seeProxy task designThe cost is similar to training 12 models (stage-2 in the figure uses 10 models): GPU quota used: Same as stage-2 run in the figure. Cost: (12/10) * stage-2-cost-for-10-models = ~$2,880 Total cost: approximately $12,680. Refer to thepricing pageto calculate exact price. These stage-1 search cost are for the search until the convergence point is reached and for maximum performance gain. However, don't wait until the search converges. You can expect to see a smaller amount of performance gain with a smaller search cost by running stage-2 full training with the best model so far if the search-reward curve has started growing. For example, for thesearch-plot shown earlier, don't wait until the 2,000 trials for convergence are reached. You might have found better models at 700 or 1,200 trials and can run"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-8",
    "content": "stage-2 full training for those. You can always stop the search earlier to reduce the cost. You might also do stage-2 full training in parallel while the search is running, but make sure you have GPU quota to support an extra parallel job. The following table summarizes some data points with different use cases and associated performance and cost. Neural Architecture Search features are both flexible and easy to use. A novice user can use prebuilt search spaces, prebuilt-trainer, and notebooks without any further setup to start exploring Vertex AI Neural Architecture Search for their dataset. At the same time, an expert user can use Neural Architecture Search with their custom trainer, custom search space, and custom inference device and even extend architecture-search for non-vision use cases as well. Neural Architecture Search offers prebuilt trainers and search spaces to be run on GPUs for the following use cases: Tensorflow trainers with public dataset based results published in a notebookImage Object Detection with end to end (SpineNet) search spacesClassification with prebuilt backbone (MnasNet) search spacesLiDAR 3D Point Cloud Object Detection with prebuilt end to end search spacesLatency and memory constrained search for targeting devices Image Object Detection with end to end (SpineNet) search spaces Classification with prebuilt backbone (MnasNet) search spaces LiDAR 3D Point Cloud Object Detection with prebuilt end to end search spaces Latency and memory constrained search for targeting devices PyTorch trainers to be used only as a tutorial examplePyTorch 3D medical image segmentation search space examplePyTorch-based MNasNet classificationLatency and memory constrained search for targeting devices PyTorch 3D medical image segmentation search space example"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-9",
    "content": "PyTorch-based MNasNet classification Latency and memory constrained search for targeting devices Additional Tensorflow based prebuilt state-of-the-art search spaces with codeModel ScalingData Augmentation Model Scaling Data Augmentation The full set of features that Neural Architecture Search offers can be used easily for customized architectures and use cases as well: A Neural Architecture Search language to define a custom search space over possible neural-architectures and integrate this search space with custom trainer code. Ready-to-use prebuilt state-of-the-art search spaces with code. Ready-to-use prebuilt Trainer, with code, which runs on GPU. A Managed Service for architecture-search includingA Neural Architecture Search controller which samples the search space to find the best architecture.Prebuilt docker/libraries, with code, to calculate latency/FLOPs/Memory on custom hardware. A Neural Architecture Search controller which samples the search space to find the best architecture. Prebuilt docker/libraries, with code, to calculate latency/FLOPs/Memory on custom hardware. Tutorials to teach NAS usage. A set of tools to design proxy-tasks. Guidance and example for efficient PyTorch training with Vertex AI. Library support for custom metrics reporting and analysis. Google Cloud console UI to monitor and manage jobs. Easy to use notebooks to kick-start the search. Library support for GPU/CPU resource usage management on per project or per job level of granularity. Python-based Nas-client to build dockers, launch NAS jobs, and resume a previous search job. Google Cloud console UI-based customer support. Neural Architecture Searchis a technique for automating the design ofneural networks. It has successfully generated several state of the art computer vision models"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-10",
    "content": "in the past years, including: Nasnet, MNasnet, EfficientNet, NAS-FPN, SpineNet These resulting models are leading the way in all 3 key classes of computer vision problems: image classification, object detection, and segmentation. With Neural Architecture Search, engineers can optimize models foraccuracy,latency, andmemoryin the same trial, reducing the time needed to deploy models. Neural Architecture Search explores many different types of models: thecontrollerproposes ML models, then trains and evaluates models and iterates 1k+ times to find the best solutionswith latency and/or memory constraint on targeting devices. The following figure shows the key components of the architecture search framework: Model: A neural architecture with operations and connections. Search space: The space of possible models (operations and connections) that can be designed and optimized. Trainer docker: User customizable trainer code to train and evaluate a model and compute accuracy of the model. Inference device: A hardware device such as CPU/GPU on which the model latency and memory usage is computed. Reward: A combination of model metrics such as the accuracy, latency, and memory used for ranking the models as better or worse. Neural Architecture Search Controller: The orchestrating algorithm that (a) samples the models from the search space, (b) receives the model-rewards, and (c) provides next set of model suggestions to evaluate to find the most optimal models. Neural Architecture Search offers prebuilt trainer integrated with prebuilt search spaces which can be easily used with provided notebooks without any further setup. However, most users need to use their custom trainer, custom search spaces, custom metrics (memory, latency, and training time, for examples), and custom reward"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-11",
    "content": "(combination of things such as accuracy and latency). For this, you need to: Define a custom search space using the provided Neural Architecture Search language. Integrate the search space definition into the trainer code. Add custom metrics reporting to the trainer code. Add custom reward to the trainer code. Build training container and use it to start Neural Architecture Search jobs. The following diagram illustrates this: After you set up the training container to use, the Neural Architecture Search service then launches multiple training-containers in parallel on multiple GPU devices. You can control how many trials to use in parallel for training and how many total trials to launch. Each training-container is provided a suggested architecture from the search space. The training-container builds the suggested model, does train/eval, and then reports rewards back to the Neural Architecture Search service. As this process progresses, the Neural Architecture Search service uses the reward feedback to find better and better model-architectures. After the search, you have access to the reported metrics for further analysis. The high level steps for performing an Neural Architecture Search experiment are as follows: Setups and definitions:Identify the labeled dataset and specify the task type (detection or segmentation, for example).Customize trainer code:Use a prebuilt search space or define a custom search space using the Neural Architecture Search language.Integrate the search-space definition into the trainer code.Add custom metrics reporting to the trainer code.Add custom reward to the trainer code.Build a trainer container.Set up search trial parameters for partial training (proxy task). The search training should ideally finish fast (for example, 30-60 minutes) to"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-12",
    "content": "partially train the models:Minimum epochs needed for sampled models to gather reward (the minimum epochsdon'tneed to ensure model convergence).Hyperparameters (for example, learning rate). Setups and definitions: Identify the labeled dataset and specify the task type (detection or segmentation, for example). Customize trainer code:Use a prebuilt search space or define a custom search space using the Neural Architecture Search language.Integrate the search-space definition into the trainer code.Add custom metrics reporting to the trainer code.Add custom reward to the trainer code. Use a prebuilt search space or define a custom search space using the Neural Architecture Search language. Integrate the search-space definition into the trainer code. Add custom metrics reporting to the trainer code. Add custom reward to the trainer code. Build a trainer container. Set up search trial parameters for partial training (proxy task). The search training should ideally finish fast (for example, 30-60 minutes) to partially train the models:Minimum epochs needed for sampled models to gather reward (the minimum epochsdon'tneed to ensure model convergence).Hyperparameters (for example, learning rate). Minimum epochs needed for sampled models to gather reward (the minimum epochsdon'tneed to ensure model convergence). Hyperparameters (for example, learning rate). Run search locally to ensure the search space integrated container can run properly. Run search locally to ensure the search space integrated container can run properly. Start the Google Cloud search (stage-1) job with fivetest trialsand verify that the search trials meet the runtime and accuracy goals. Start the Google Cloud search (stage-1) job with fivetest trialsand verify that the search trials meet the runtime and accuracy"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-13",
    "content": "goals. Start the Google Cloud search (stage-1) job with +1k trials.As part of the search, also set a regular interval to train (stage-2) top N models:Hyperparameters and algorithm for hyperparameter search. stage-2 normally uses the similar configuration as stage-1, but with higher settings for certain parameters, such as training steps/epochs, and number of channels.Stop criteria (the number of epochs). Start the Google Cloud search (stage-1) job with +1k trials. As part of the search, also set a regular interval to train (stage-2) top N models:Hyperparameters and algorithm for hyperparameter search. stage-2 normally uses the similar configuration as stage-1, but with higher settings for certain parameters, such as training steps/epochs, and number of channels.Stop criteria (the number of epochs). As part of the search, also set a regular interval to train (stage-2) top N models: Hyperparameters and algorithm for hyperparameter search. stage-2 normally uses the similar configuration as stage-1, but with higher settings for certain parameters, such as training steps/epochs, and number of channels. Stop criteria (the number of epochs). Analyze the reported metrics and/or visualize architectures for insights. Analyze the reported metrics and/or visualize architectures for insights. An architecture-search experiment can be followed up by a scaling-search experiment, followed up by an augmentation search experiment as well. (Required)Set up your environment (Required)Tutorials (Required only for PyTorch customers)PyTorch efficient training with cloud data (Required)Best practices and suggested workflow (Required)Proxy task design (Required only when using prebuilt trainers)How to use prebuilt search spaces and a prebuilt trainer Using Machine Learning to Explore Neural"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview",
    "title": "About Vertex AI Neural Architecture SearchStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/overview#chunk-14",
    "content": "Network Architecture MnasNet: Towards Automating the Design of Mobile Machine Learning Models EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization RandAugment Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/workbench/user-managed",
    "title": "Vertex AI Workbench: User-managed notebooks\n documentation",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/workbench/user-managed#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI Workbench Vertex AI Workbench user-managed notebooks isdeprecated. On April 14, 2025, support for user-managed notebooks will end and the ability to create user-managed notebooks instances will be removed. Existing instances will continue to function but patches, updates, and upgrades won't be available. To continue using Vertex AI Workbench, we recommend that youmigrate your user-managed notebooks instances to Vertex AI Workbench instances. User-managed notebooks instances offer an integrated and secure JupyterLab environment for data scientists and machine learning developers to experiment, develop, and deploy models into production. User-managed notebooks instances come preinstalled with the latest data science and machine learning frameworks.Learn more. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Create a user-managed notebooks instance Create a user-managed notebooks instance Introduction to user-managed notebooks Introduction to user-managed notebooks Install dependencies Install dependencies Change machine type and configure GPUs of a user-managed notebooks instance Change machine type and configure GPUs of a user-managed notebooks instance Use R with BigQuery Use R with BigQuery Save a notebook to GitHub Save a notebook to GitHub Pricing Pricing Release notes Release notes Get support Get support Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/workbench/user-managed",
    "title": "Vertex AI Workbench: User-managed notebooks\n documentation",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/workbench/user-managed#chunk-1",
    "content": "a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/dhm-cloud",
    "title": "Distributed, hybrid, and multicloud",
    "chunk_id": "https://cloud.google.com/docs/dhm-cloud#chunk-0",
    "content": "Home Documentation Extend your Google Cloud topology to the edge, on-premises, and other cloud platforms. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Extend Google Cloud infrastructure and services to the edge and into your data centers. Create and manage Kubernetes clusters from Google Cloud in both AWS and Azure cloud environments. Simplify managing multi-cluster deployments, including clusters outside Google Cloud. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/migration",
    "title": "Migration",
    "chunk_id": "https://cloud.google.com/docs/migration#chunk-0",
    "content": "Home Documentation Use tools and information to help you on your journey to Google Cloud. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Tools and information to guide your migration to Google Cloud. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke",
    "title": "Backup for GKE documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke#chunk-0",
    "content": "Home Google Kubernetes Engine (GKE) Documentation Documentation Backup for GKE lets you protect, manage, and restore your containerized applications and data for workloads running on Google Kubernetes Engine clusters.Learn more. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Install Backup for GKE Install Backup for GKE Using VPC service controls Using VPC service controls Define custom backup and restore logic Define custom backup and restore logic Plan a set of backups Plan a set of backups Back up your workloads Back up your workloads Plan a set of restores Plan a set of restores Modify resources during restoration Modify resources during restoration Restore a backup Restore a backup gcloud CLI commands gcloud CLI commands REST API REST API IAM roles and permissions IAM roles and permissions Pricing Pricing Quotas and limits Quotas and limits Release notes Release notes Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-0",
    "content": "Home Compute Engine Documentation Guides If your workloads need high performance, low latency, temporary storage, consider using Local solid-state drive (Local SSD) disks when you create your compute instance. Local SSD disks are always-encrypted temporary solid-state storage for Compute Engine. To learn about the other disks available in Compute Engine, seeChoose a disk type. Local SSD disks are ideal when you need storage for any of the following use cases: Caches or storage for transient data Scratch processing space for high performance computing or data analytics Temporary data storage like for thetempdbsystem database for Microsoft SQL Server Local SSD disks offer superior I/O operations per second (IOPS), and very low latency compared to the persistent storage provided byGoogle Cloud HyperdiskandPersistent Disk. This low latency is because Local SSD disks are physically attached to the server that hosts your instance. For this same reason, Local SSD disks canprovide only temporary storage. Because Local SSD is suitable only for temporary storage, you must store data that isn't temporary or ephemeral in nature on a Hyperdisk or Persistent Disk volume. To use Local SSD disks with a compute instance,add Local SSD disks when you create the instance. You can't add Local SSD disks to an instance after you create it. Local SSD disks come in two types: Titanium SSD: Titanium SSD is a custom-designed local SSD disk that usesTitanium I/O offload processingand offers enhanced SSD security, performance, and management. Titanium offers higher storage IOPS, throughput, and lower latency than the previous generation of Local SSD. The following machine series offer local SSD storage using Titanium SSD:Storage-optimizedZ3 machine seriesGeneral-purposeC4AandC4D(Preview) machine"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-1",
    "content": "seriesTitanium SSD disks are directly attached to the compute instances inside their host server. Titanium SSD: Titanium SSD is a custom-designed local SSD disk that usesTitanium I/O offload processingand offers enhanced SSD security, performance, and management. Titanium offers higher storage IOPS, throughput, and lower latency than the previous generation of Local SSD. The following machine series offer local SSD storage using Titanium SSD: Storage-optimizedZ3 machine series General-purposeC4AandC4D(Preview) machine series Titanium SSD disks are directly attached to the compute instances inside their host server. Local SSD: Local SSD is the original local SSD feature for Google Cloud. Each Local SSD disk attached to an instance provides 375 GiB of capacity. These disks provide higher performance than Hyperdisk or Persistent Disk. You can use either the NVMe or SCSI interface to mount Local SSD disks.Local SSD disks are directly attached to the instances inside their host server. Local SSD: Local SSD is the original local SSD feature for Google Cloud. Each Local SSD disk attached to an instance provides 375 GiB of capacity. These disks provide higher performance than Hyperdisk or Persistent Disk. You can use either the NVMe or SCSI interface to mount Local SSD disks. Local SSD disks are directly attached to the instances inside their host server. Unless Titanium SSD is specifically mentioned, the term \"Local SSD\" applies to both Local SSD and Titanium SSD when describing features of local SSD disks. Local SSD performance depends on several factors, including the number of attached Local SSD disks, the selected disk interface (NVMeorSCSI), and the instance's machine type. The available performance increases as you attach more Local SSD disks to your instance. The"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-2",
    "content": "following tables list the maximum IOPS and throughput for NVMe- and SCSI-attached Local SSD disks. The metrics are listed by the total capacity of Local SSD disks attached to the instance. The following table lists the performance limits for Titanium SSD disks on C4A,C4D(Preview), andZ3instances. The following table lists the performance limits for Local SSD disks that are attached to instances using NVMe. The following table lists the performance limits for Local SSD disks that are attached to instances using SCSI. To reach the stated performance levels, you must configure your compute instance as follows: Attach the Local SSD disks with the NVMe interface. Disks attached with the SCSI interface have lower performance. Attach the Local SSD disks with the NVMe interface. Disks attached with the SCSI interface have lower performance. The following machine types also require a minimum number of vCPUs to reach these maximums:N2,N2D, orA2machine types require at least 24 vCPUs.N1machine types require at least 32 vCPUs. The following machine types also require a minimum number of vCPUs to reach these maximums: N2,N2D, orA2machine types require at least 24 vCPUs. N1machine types require at least 32 vCPUs. If your instance uses a custom Linux image, the image must use version 4.14.68 or later of the Linux kernel. If you use the public images provided by Compute Engine, you don't have to take any further action. If your instance uses a custom Linux image, the image must use version 4.14.68 or later of the Linux kernel. If you use the public images provided by Compute Engine, you don't have to take any further action. For additional instance and disk configuration settings that can improve Local SSD performance, seeOptimizing Local SSD performance. For more information about"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-3",
    "content": "selecting a disk interface, seeChoose a disk interface. Compute Engine preserves the data on Local SSD disks in certain scenarios, and in other cases, Compute Engine does not guarantee Local SSD data persistence. The following information describes these scenarios and applies to each Local SSD disk attached to an instance. Data on Local SSD disks persist only through the following events: If you reboot the guest operating system. If you configure your instance forlive migrationand the instance goes through a host maintenance event. If youopt to preserve the Local SSD datawhen you stop or suspend the instance. This feature is inPreview. Data on Local SSD disks might be lost if ahost erroroccurs on the instance and Compute Engine can't reconnect the instance to the Local SSD disk within a specified time. You can control how much time, if any, is spent attempting to recover the data with the Local SSD recovery timeout. If Compute Engine can't reconnect to the disk before the timeout expires, the instance is restarted. When the instance restarts, the Local SSD data is unrecoverable. Compute Engine attaches a blank Local SSD disk to the restarted instance. The Local SSD recovery timeout is part of an instance's host maintenance policy. For more information, seeLocal SSD recovery timeout. Data on Local SSD disks does not persist through the following events: If you shut down the guest operating system and force the instance to stop. If you create aSpot VMorpreemptible VMand the VM goes through the preemption process. If you configure the instance tostop on host maintenance eventsand the instance goes through a host maintenance event. If you misconfigure the Local SSD so that it becomes unreachable. If you disable project billing, causing the instance to stop. If Compute"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-4",
    "content": "Engine was unable to recover an instance's Local SSD data, Compute Engine restarts the instance with a mounted and attached Local SSD disk for each previously attached Local SSD disk. You can use Local SSD disks with the following machine series. C4AC4C4D(Preview)C3C3DN4N2N2DN1T2DT2AE2Z3H3C2C2DX4M4M3M2M1N1+GPUA4A3 (H200)A3 (H100)A2G2 Select a machine series to display its support for Local SSD. However, there are constraints around how many Local SSD disks you can attach based on each machine type. For more information, seeChoose a valid number of Local SSD disks. Local SSD has the following limitations: You can't use Local SSD disks with VMs withshared-coremachine types. You can't use Local SSD disks with VMs withshared-coremachine types. You can't attach Local SSD disks to instances that use N4, H3, M4, M2, E2, Tau T2A, or Tau T2D machine types. You can't attach Local SSD disks to instances that use N4, H3, M4, M2, E2, Tau T2A, or Tau T2D machine types. You can't use customer-supplied encryption keys or customer-managed encryption keys with Local SSD disks. Compute Engine automatically encrypts your data when it's written to Local SSD storage. You can't use customer-supplied encryption keys or customer-managed encryption keys with Local SSD disks. Compute Engine automatically encrypts your data when it's written to Local SSD storage. You can't back up Local SSD disks with snapshots, clones, machine images, or images. Store important data on Hyperdisk or Persistent Disk volumes. You can't back up Local SSD disks with snapshots, clones, machine images, or images. Store important data on Hyperdisk or Persistent Disk volumes. Compute Engine automatically encrypts your data when it is written to Local SSD disks. You can't usecustomer-supplied encryption keyswith Local SSD"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-5",
    "content": "disks. Since you can't back up Local SSD data with disk images, standard snapshots, or disk clones, Google recommends that you always store valuable data on adurable storage option. If you need to preserve the data on a Local SSD disk, attach a Persistent Disk or Google Cloud Hyperdisk to the instance. After you mount the Persistent Disk or Hyperdisk copy the data from the Local SSD disk to the newly attached disk. To achieve the highest Local SSD performance, you must attach your disks to the instance with the NVMe interface. Performance is lower if you use the SCSI interface. The disk interface you choose also depends on the machine type and OS that your instance uses. Some of the available machine types in Compute Engine allow you to choose between NVMe and SCSI interfaces, while others support either only NVMe or only SCSI. Similarly, some of the public OS images provided by Compute Engine might support both NVMe and SCSI, or only one of the two. The following pages provide more information about available machine types and supported public images, as well as performance details. Supported interfaces by machine types: SeeMachine series comparison. In theChoose VM properties to comparelist, selectDisk interface type. Supported interfaces by machine types: SeeMachine series comparison. In theChoose VM properties to comparelist, selectDisk interface type. OS image: For a list of which public OS images provided by Compute Engine support SCSI or NVMe, see theInterfacestab for each table in the operating system details documentation. OS image: For a list of which public OS images provided by Compute Engine support SCSI or NVMe, see theInterfacestab for each table in the operating system details documentation. If your instance uses a custom Linux image, you must use"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-6",
    "content": "version 4.14.68 or later of the Linux kernel for optimal NVMe performance. If you have an existing setup that requires using a SCSI interface, consider using multi-queue SCSI to achieve better performance over the standard SCSI interface. If you are using a custom image that you imported, seeEnable multi-queue SCSI. Most machine types available on Compute Engine support Local SSD disks. Some machine types always include a fixed number of Local SSD disks by default, while others allow you to add specific numbers of disks. You can only add Local SSD disks when you create the instance. You can't add Local SSD disks to an instance after you create it. For instances created using a storage-optimized Z3 machine type, each attached Titanium SSD disk has 3,000 GiB of capacity. For all other machine series, each Local SSD disk that you attach has 375 GiB of capacity. The following table lists the machine types that come with Local SSD disks by default. The table also shows the number of these disks that are attached when you create the instance. The machine types listed in the following table don't attach Local SSD disks to a newly created instance unless you specify how many disks to attach. Because you can add Local SSD disks only during instance creation, use the information in this section to determine how many Local SSD disks to attach when you create an instance. For each Local SSD disk you create, you are billed for the total capacity of the disk for the lifetime of the instance that it is attached to. For detailed information about Local SSD pricing and available discounts, seeLocal SSD pricing. If you start aSpot VMor preemptible VM with a Local SSD disk, Compute Engine charges discountedspot pricesfor the Local SSD usage. Local SSD disks that are attached to Spot VMs"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-7",
    "content": "or preemptible VMs work like normal Local SSD disks, retain the samedata persistence characteristics, and remain attached for the life of the VM. Compute Engine doesn't charge you for Local SSD disk usage on a Spot VM or preemptible VM if the VM is preempted within a minute after it starts running. To reserve Local SSD resources in a specific zone, seeChoose a reservation type. To receive committed use discounts for Local SSD disks in a specific zone, you must purchase resource-based commitments for the Local SSD resources and also attach reservations that specify matching Local SSD resources to your commitments. For more information, seeAttach reservations to resource-based commitments. To use a Local SSD disk with a compute instance, you must complete the following steps: Add Local SSD disks when you create an instance. Format and mount Local SSD disksthat you added to your instance. The Linux device names for the disks attached to your instance depend on the interface that you choose when creating the disks. When you use thelsblkoperating system command to view your disk devices, it displays the prefixnvmefor disks attached with the NVMe interface, and the prefixsdfor disks attached with the SCSI interface. The ordering of the disk numbers or NVMe controllers is not predictable or consistent across instance restarts. On the first boot, a disk might benvme0n1(orsdafor SCSI). On the second boot, the device name for the same disk might benvme2n1ornvme0n3(orsdcfor SCSI). When accessing attached disks, you should use the symbolic links created in/dev/disk/by-id/instead. These names persist across reboots. For more information about symlinks, seeSymbolic links for disks attached to an instance. For more information about device names, seeDevice naming on Linux instances."
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-8",
    "content": "When youstoporsuspend a VM, Compute Engine discards the data of any Local SSD disks attached to the VM by default. When you resume the VM, all Local SSD disks attached to the VM are blank. Preview \u2014 Preserving Local SSD data This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. When you stop or suspend a VM, you can optionally preserve the data on the Local SSD disks attached to the VM. When the stop or suspend operation starts, Compute Engine performs a managed migration of the Local SSD disk data to durable storage. When you resume or restart the VM, Compute Engine copies the preserved data to Local SSD disks attached to the VM. After you resume or restart the VM, you might have toremount the Local SSD disk into the file system. You're billed for the storage space used to preserve the Local SSD data until you restart or resume the VM. The used storage space consumes your project'sPersistent disk standard GBquota. Preserving Local SSD data is inPreviewonly and is not covered under the GA terms for Compute Engine. You can't use this feature with Z3 instances. You can't preserve the Local SSD data if you stop or suspend a VM that has more than 32 Local SSD disks attached. You can't preserve Local SSD data if you stop or suspend a VM from the Google Cloud console. Saving the Local SSD data is a slow process and begins only after the suspend or stop operation starts. If you're using Spot VMs or preemptible VMs and you opt to preserve Local SSD data during a suspend or stop operation, then theLocal SSD data is lostif Compute Engine preempts the VM during the stop or suspend"
  },
  {
    "source_url": "https://cloud.google.com/compute/docs/disks/local-ssd",
    "title": "About Local SSD disksStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/compute/docs/disks/local-ssd#chunk-9",
    "content": "operation. To learn how to preserve Local SSD data when you stop or suspend a VM, seeStop an instance with Local SSD disksandSuspend an instance with Local SSD disks, respectively. To remove or delete Local SSD disks, you must delete the VM the disks are attached to. You can't delete Local SSD disks unless you delete the VM. Before youdelete a VMthat has Local SSD disks attached, make sure that you migrate any critical data on the Local SSD disks to a Persistent Disk, Hyperdisk, or to another VM. Otherwise, the data on the Local SSD disks is permanently lost. Learn how toCreate a VM with Local SSD disks. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Hex-LLM, a high-efficiency large language model (LLM) serving with XLA, is the Vertex AI LLM serving framework that's designed and optimized forCloud TPUhardware. Hex-LLM combines LLM serving technologies such as continuous batching andPagedAttentionwith Vertex AI optimizations that are tailored forXLAand Cloud TPU. It's a high-efficiency and low-cost LLM serving on Cloud TPU for open source models. Hex-LLM is available inModel Gardenthrough model playground, one-click deployment, and notebook. Hex-LLM is based on open source projects with Google's own optimizations for XLA and Cloud TPU. Hex-LLM achieves high throughput and low latency when serving frequently used LLMs. Hex-LLM includes the following optimizations: Token-based continuous batching algorithm to help ensure models are fully utilizing the hardware with a large number of concurrent requests. A complete rewrite of the attention kernels that are optimized for XLA. Flexible and composable data parallelism and tensor parallelism strategies with highly optimized weight sharding methods to efficiently run LLMs on multiple Cloud TPU chips. Hex-LLM supports a wide range of dense and sparse LLMs: Gemma 2B and 7B Gemma 2 9B and 27B Llama 2 7B, 13B and 70B Llama 3 8B and 70B Llama 3.1 8B and 70B Llama 3.2 1B and 3B Llama Guard 3 1B and 8B Mistral 7B Mixtral 8x7B and 8x22B Phi-3 mini and medium Qwen2 0.5B, 1.5B and 7B Qwen2.5 0.5B, 1.5B, 7B, 14B and 32B AWQ Hex-LLM also provides a variety of features, such as the following: Hex-LLM is included in a single container. Hex-LLM packages the API server, inference engine, and supported models into a single Docker image to be deployed. Compatible with theHugging Face modelsformat. Hex-LLM can load a Hugging Face model from local"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-1",
    "content": "disk, the Hugging Face Hub, and a Cloud Storage bucket. Quantization usingbitsandbytesandAWQ. DynamicLoRAloading. Hex-LLM is able to load the LoRA weights through reading the request argument during serving. Hex-LLM supports the following advanced features: Multi-host serving Disaggregated serving [experimental] Prefix caching 4-bit quantization support Hex-LLM now supports serving models with amulti-host TPU slice. This feature lets you serve large models that can't be loaded into a single host TPU VM, which contains at most eight v5e cores. To enable this feature, set--num_hostsin the Hex-LLM container arguments and set--tpu_topologyin the Vertex AI SDK model upload request. The following example shows how to deploy the Hex-LLM container with a TPU 4x4 v5e topology that serves the Llama 3.1 70B bfloat16 model: For an end-to-end tutorial for deploying the Hex-LLM container with a multi-host TPU topology, see theVertex AI Model Garden - Llama 3.1 (Deployment) notebook. In general, the only changes needed to enable multi-host serving are: Set argument--tensor_parallel_sizeto the total number of cores within the TPU topology. Set argument--num_hoststo the number of hosts within the TPU topology. Set--tpu_topologywith the Vertex AI SDK model upload API. Hex-LLM now supports disaggregated serving as an experimental feature. It can only be enabled on the single host setup and the performance is under tuning. Disaggregated serving is an effective method for balancing Time to First Token (TTFT) and Time Per Output Token (TPOT) for each request, and the overall serving throughput. It separates the prefill phase and the decode phase into different workloads so that they don't interfere with each other. This method is especially useful for scenarios that set strict latency"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-2",
    "content": "requirements. To enable this feature, set--disagg_topoin the Hex-LLM container arguments. The following is an example that shows how to deploy the Hex-LLM container on TPU v5e-8 that serves the Llama 3.1 8B bfloat16 model: The--disagg_topoargument accepts a string in the format\"number_of_prefill_workers,number_of_decode_workers\". In the earlier example, it is set to\"3,1\"to configure three prefill workers and 1 decode worker. Each worker uses two TPU v5e cores. Prefix caching reduces Time to First Token (TTFT) for prompts that have identical content at the beginning of the prompt, such as company-wide preambles, common system instructions, and multi-turn conversation history. Instead of processing the same input tokens repeatedly, Hex-LLM can retain a temporary cache of the processed input token computations to improve TTFT. To enable this feature, set--enable_prefix_cache_hbmin the Hex-LLM container arguments. The following is an example that shows how to deploy the Hex-LLM container on TPU v5e-8 that serves the Llama 3.1 8B bfloat16 model: Hex-LLM employs prefix caching to optimize performance for prompts exceeding a certain length (512 tokens by default, configurable usingprefill_len_padding). Cache hits occur in increments of this value, ensuring the cached token count is always a multiple ofprefill_len_padding. Thecached_tokensfield ofusage.prompt_tokens_detailsin the chat completion API response indicates how many of the prompt tokens were a cache hit. Quantization is a technique for reducing the computational and memory costs of running inference by representing the weights or activations with low-precision data types like INT8 or INT4 instead of the usual BF16 or FP32. Hex-LLM supports INT8 weight-only quantization. Extended support includes models with INT4"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-3",
    "content": "weights quantized using AWQ zero-point quantization. Hex-LLM supports INT4 variants of Mistral, Mixtral and Llama model families. There is no additional flag required for serving quantized models. The Hex-LLM Cloud TPU serving container is integrated into Model Garden. You can access this serving technology through the playground, one-click deployment, and Colab Enterprise notebook examples for a variety of models. Model Garden playground is a pre-deployed Vertex AI endpoint that is reachable by sending requests in the model card. Enter a prompt and, optionally, include arguments for your request. Enter a prompt and, optionally, include arguments for your request. ClickSUBMITto get the model response quickly. ClickSUBMITto get the model response quickly. Try it out with Gemma! You can deploy a custom Vertex AI endpoint with Hex-LLM by using a model card. Navigate to themodel card pageand clickDeploy. Navigate to themodel card pageand clickDeploy. For the model variation that you want to use, select theCloud TPU v5e machine typefor deployment. For the model variation that you want to use, select theCloud TPU v5e machine typefor deployment. ClickDeployat the bottom to begin the deployment process. You receive two email notifications; one when the model is uploaded and another when the endpoint is ready. ClickDeployat the bottom to begin the deployment process. You receive two email notifications; one when the model is uploaded and another when the endpoint is ready. For flexibility and customization, you can use Colab Enterprise notebook examples to deploy a Vertex AI endpoint with Hex-LLM by using the Vertex AI SDK for Python. Navigate to the model card page and clickOpen notebook. Navigate to the model card page and clickOpen notebook. Select the Vertex Serving"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-4",
    "content": "notebook. The notebook is opened in Colab Enterprise. Select the Vertex Serving notebook. The notebook is opened in Colab Enterprise. Run through the notebook to deploy a model by using Hex-LLM and send prediction requests to the endpoint. The code snippet for the deployment is as follows: Run through the notebook to deploy a model by using Hex-LLM and send prediction requests to the endpoint. The code snippet for the deployment is as follows: Example Colab Enterprise notebooks include: Gemma 2 deployment CodeGemma deployment Llama 3.2 deployment Llama 3.1 deployment Phi-3 deployment Qwen2 deployment You can set the following arguments to launch the Hex-LLM server. You can tailor the arguments to best fit your intended use case and requirements. Note that the arguments are predefined for one-click deployment for enabling the easiest deployment experience. To customize the arguments, you can build off of the notebook examples for reference and set the arguments accordingly. Model --model: The model to load. You can specify a Hugging Face model ID, a Cloud Storage bucket path (gs://my-bucket/my-model), or a local path. The model artifacts are expected to follow the Hugging Face format and usesafetensorsfiles for the model weights.BitsAndBytesint8 andAWQquantized model artifacts are supported for Llama, Gemma 2 and Mistral/Mixtral. --tokenizer: Thetokenizerto load. This can be a Hugging Face model ID, aCloud Storagebucket path (gs://my-bucket/my-model), or a local path. If this argument is not set, it defaults to the value for--model. --tokenizer_mode: The tokenizer mode. Possible choices are[\"auto\", \"slow\"]. The default value is\"auto\". If this is set to\"auto\", the fast tokenizer is used if available. The slow tokenizers are written in Python and provided in the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-5",
    "content": "Transformers library, while the fast tokenizers offering performance improvement are written in Rust and provided in the Tokenizers library. For more information, see theHugging Face documentation. --trust_remote_code: Whether to allow remote code files defined in the Hugging Face model repositories. The default value isFalse. --load_format: Format of model checkpoints to load. Possible choices are[\"auto\", \"dummy\"]. The default value is\"auto\". If this is set to\"auto\", the model weights are loaded in safetensors format. If this is set to\"dummy\", the model weights are randomly initialized. Setting this to\"dummy\"is useful for experimentation. --max_model_len: The maximum context length (input length plus the output length) to serve for the model. The default value is read from the model configuration file in Hugging Face format:config.json. A larger maximum context length requires more TPU memory. --sliding_window: If set, this argument overrides the model's window size forsliding window attention. Setting this argument to a larger value makes the attention mechanism include more tokens and approaches the effect of standard self attention. This argument is meant for experimental usage only. In general use cases, we recommend using the model's original window size. --seed: The seed for initializing all random number generators. Changing this argument might affect the generated output for the same prompt through changing the tokens that are sampled as next tokens. The default value is0. Inference engine --num_hosts: The number of hosts to run. The default value is1. For more details, refer to the documentation onTPU v5e configuration. --disagg_topo: Defines the number of prefill workers and decode workers with the experimental feature disaggregated serving. The default value"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-6",
    "content": "isNone. The argument follows the format:\"number_of_prefill_workers,number_of_decode_workers\". --data_parallel_size: The number of data parallel replicas. The default value is1. Setting this toNfrom1approximately improves the throughput byN, while maintaining the same latency. --tensor_parallel_size: The number of tensor parallel replicas. The default value is1. Increasing the number of tensor parallel replicas generally improves latency, because it speeds up matrix multiplication by reducing the matrix size. --worker_distributed_method: The distributed method to launch the worker. Usempfor themultiprocessingmodule orrayfor theRaylibrary. The default value ismp. --enable_jit: Whether to enableJIT (Just-in-Time Compilation)mode. The default value isTrue. Setting--no-enable_jitdisables it. Enabling JIT mode improves inference performance at the cost of requiring additional time spent on initial compilation. In general, the inference performance benefits overweigh the overhead. --warmup: Whether to warm up the server with sample requests during initialization. The default value isTrue. Setting--no-warmupdisables it. Warmup is recommended, because initial requests trigger heavier compilation and therefore will be slower. --max_prefill_seqs: The maximum number of sequences that can be scheduled for prefilling per iteration. The default value is1. The larger this value is, the higher throughput the server can achieve, but with potential adverse effects on latency. --prefill_seqs_padding: The server pads the prefill batch size to a multiple of this value. The default value is8. Increasing this value reduces model recompilation times, but increases wasted computation and inference overhead. The optimal setting depends on the request traffic. --prefill_len_padding: The server"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-7",
    "content": "pads the sequence length to a multiple of this value. The default value is512. Increasing this value reduces model recompilation times, but increases wasted computation and inference overhead. The optimal setting depends on the data distribution of the requests. --max_decode_seqs/--max_running_seqs: The maximum number of sequences that can be scheduled for decoding per iteration. The default value is256. The larger this value is, the higher throughput the server can achieve, but with potential adverse effects on latency. --decode_seqs_padding: The server pads the decode batch size to a multiple of this value. The default value is8. Increasing this value reduces model recompilation times, but increases wasted computation and inference overhead. The optimal setting depends on the request traffic. --decode_blocks_padding: The server pads the number of memory blocks used for a sequence's Key-Value cache (KV cache) to a multiple of this value during decoding. The default value is128. Increasing this value reduces model recompilation times, but increases wasted computation and inference overhead. The optimal setting depends on the data distribution of the requests. --enable_prefix_cache_hbm: Whether to enableprefix cachingin HBM. The default value isFalse. Setting this argument can improve performance by reusing the computations of shared prefixes of prior requests. Memory management --hbm_utilization_factor: The percentage of freeCloud TPU High Bandwidth Memory (HBM)that can be allocated for KV cache after model weights are loaded. The default value is0.9. Setting this argument to a higher value increases the KV cache size and can improve throughput, but it increases the risk of running out of Cloud TPU HBM during initialization and at runtime. --num_blocks: Number of device"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-8",
    "content": "blocks to allocate for KV cache. If this argument is set, the server ignores--hbm_utilization_factor. If this argument is not set, the server profiles HBM usage and computes the number of device blocks to allocate based on--hbm_utilization_factor. Setting this argument to a higher value increases the KV cache size and can improve throughput, but it increases the risk of running out of Cloud TPU HBM during initialization and at runtime. --block_size: Number of tokens stored in a block. Possible choices are[8, 16, 32, 2048, 8192]. The default value is32. Setting this argument to a larger value reduces overhead in block management, at the cost of more memory waste. The exact performance impact needs to be determined empirically. Dynamic LoRA --enable_lora: Whether to enable dynamicLoRA adaptersloading from Cloud Storage. The default value isFalse. This is supported for the Llama model family. --max_lora_rank: The maximum LoRA rank supported for LoRA adapters defined in requests. The default value is16. Setting this argument to a higher value allows for greater flexibility in the LoRA adapters that can be used with the server, but increases the amount of Cloud TPU HBM allocated for LoRA weights and decreases throughput. --enable_lora_cache: Whether to enable caching of dynamic LoRA adapters. The default value isTrue. Setting--no-enable_lora_cachedisables it. Caching improves performance because it removes the need to re-download previously used LoRA adapter files. --max_num_mem_cached_lora: The maximum number of LoRA adapters stored in TPU memory cache.The default value is16. Setting this argument to a larger value improves the chance of a cache hit, but it increases the amount of Cloud TPU HBM usage. You can also configure the server using the following environment"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-9",
    "content": "variables: HEX_LLM_LOG_LEVEL: Controls the amount of logging information generated. The default value isINFO. Set this to one of the standard Python logging levels defined in thelogging module. HEX_LLM_VERBOSE_LOG: Whether to enable detailed logging output. Allowed values aretrueorfalse. Default value isfalse. The server arguments are interrelated and have a collective effect on the serving performance. For example, a larger setting of--max_model_len=4096leads to higher TPU memory usage, and therefore requires larger memory allocation and less batching. In addition, some arguments are determined by the use case, while others can be tuned. Here is a workflow for configuring the Hex-LLM server. Determine the model family and model variant of interest. For example, Llama 3.1 8B Instruct. Estimate the lower bound of TPU memory needed based on the model size and precision:model_size * (num_bits / 8). For an 8B model and bfloat16 precision, the lower bound of TPU memory needed would be8 * (16 / 8) = 16 GB. Estimate the number of TPU v5e chips needed, where each v5e chip offers 16GB:tpu_memory / 16. For an 8B model and bfloat16 precision, you need more than 1 chip. Among the1-chip, 4-chip and 8-chip configurations, the smallest configuration that offers more than 1 chip is the 4-chip configuration:ct5lp-hightpu-4t. You can subsequently set--tensor_parallel_size=4. Determine the maximum context length (input length + output length) for the intended use case. For example, 4096. You can subsequently set--max_model_len=4096. Tune the amount of free TPU memory allocated for KV cache to the maximum value achievable given the model, hardware and server configurations (--hbm_utilization_factor). Start with0.95. Deploy the Hex-LLM server and test the server with long prompts and high"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm",
    "title": "Serve open models using Hex-LLM premium container on Cloud TPUStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#chunk-10",
    "content": "concurrency. If the server runs out-of-memory, reduce the utilization factor accordingly. A sample set of arguments for deploying Llama 3.1 8B Instruct is: A sample set of arguments for deploying Llama 3.1 70B Instruct AWQ onct5lp-hightpu-4tis: In Model Garden, your default quota is 4 Cloud TPU v5e chips in theus-west1region. This quotas applies to one-click deployments and Colab Enterprise notebook deployments. To request additional quotas, seeRequest a higher quota. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
    "title": "Google modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation 2.5 Flashpreview A preview of the latest version of our Flash line of models Input audio, images, video, and text, get text responses See the model's thinking process as part of the response Balances price and performance 2.0 Flashspark Our newest multimodal model, with next generation features and improved capabilities Input audio, images, video, and text, get text responses Generate code and images, extract data, analyze files, generate graphs, and more Low latency, enhanced performance, built to power agentic experiences 2.0 Flash-Lite A Gemini 2.0 Flash model optimized for cost efficiency and low latency Input audio, images, video, and text, get text responses Outperforms 1.5 Flash on the majority of benchmarks A 1 million token context window and multimodal input, like Flash 2.0 All the Gemini models can understand and respond in the following languages: Arabic (ar), Bengali (bn), Bulgarian (bg), Chinese (Simplified and Traditional) (zh), Croatian (hr), Czech (cs), Danish (da), Dutch (nl), English (en), Estonian (et), Finnish (fi), French (fr), German (de), Greek (el), Hebrew (iw), Hindi (hi), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Latvian (lv), Lithuanian (lt), Norwegian (no), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Serbian (sr), Slovak (sk), Slovenian (sl), Spanish (es), Swahili (sw), Swedish (sv), Thai (th), Turkish (tr), Ukrainian (uk), Vietnamese (vi)Gemini 2.0 Flash, Gemini 1.5 Pro and Gemini 1.5 Flash models can understand and respond in the following additional languages:Afrikaans (af), Amharic (am), Assamese (as), Azerbaijani (az), Belarusian (be), Bosnian (bs), Catalan (ca), Cebuano (ceb), Corsican (co), Welsh (cy), Dhivehi (dv), Esperanto (eo), Basque"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
    "title": "Google modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#chunk-1",
    "content": "(eu), Persian (fa), Filipino (Tagalog) (fil), Frisian (fy), Irish (ga), Scots Gaelic (gd), Galician (gl), Gujarati (gu), Hausa (ha), Hawaiian (haw), Hmong (hmn), Haitian Creole (ht), Armenian (hy), Igbo (ig), Icelandic (is), Javanese (jv), Georgian (ka), Kazakh (kk), Khmer (km), Kannada (kn), Krio (kri), Kurdish (ku), Kyrgyz (ky), Latin (la), Luxembourgish (lb), Lao (lo), Malagasy (mg), Maori (mi), Macedonian (mk), Malayalam (ml), Mongolian (mn), Meiteilon (Manipuri) (mni-Mtei), Marathi (mr), Malay (ms), Maltese (mt), Myanmar (Burmese) (my), Nepali (ne), Nyanja (Chichewa) (ny), Odia (Oriya) (or), Punjabi (pa), Pashto (ps), Sindhi (sd), Sinhala (Sinhalese) (si), Samoan (sm), Shona (sn), Somali (so), Albanian (sq), Sesotho (st), Sundanese (su), Tamil (ta), Telugu (te), Tajik (tg), Uyghur (ug), Urdu (ur), Uzbek (uz), Xhosa (xh), Yiddish (yi), Yoruba (yo), Zulu (zu) Gemini 2.0 Flash, Gemini 1.5 Pro and Gemini 1.5 Flash models can understand and respond in the following additional languages:Afrikaans (af), Amharic (am), Assamese (as), Azerbaijani (az), Belarusian (be), Bosnian (bs), Catalan (ca), Cebuano (ceb), Corsican (co), Welsh (cy), Dhivehi (dv), Esperanto (eo), Basque (eu), Persian (fa), Filipino (Tagalog) (fil), Frisian (fy), Irish (ga), Scots Gaelic (gd), Galician (gl), Gujarati (gu), Hausa (ha), Hawaiian (haw), Hmong (hmn), Haitian Creole (ht), Armenian (hy), Igbo (ig), Icelandic (is), Javanese (jv), Georgian (ka), Kazakh (kk), Khmer (km), Kannada (kn), Krio (kri), Kurdish (ku), Kyrgyz (ky), Latin (la), Luxembourgish (lb), Lao (lo), Malagasy (mg), Maori (mi), Macedonian (mk), Malayalam (ml), Mongolian (mn), Meiteilon (Manipuri) (mni-Mtei), Marathi (mr), Malay (ms), Maltese (mt), Myanmar (Burmese) (my), Nepali (ne), Nyanja (Chichewa) (ny), Odia (Oriya) (or), Punjabi"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
    "title": "Google modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#chunk-2",
    "content": "(pa), Pashto (ps), Sindhi (sd), Sinhala (Sinhalese) (si), Samoan (sm), Shona (sn), Somali (so), Albanian (sq), Sesotho (st), Sundanese (su), Tamil (ta), Telugu (te), Tajik (tg), Uyghur (ug), Urdu (ur), Uzbek (uz), Xhosa (xh), Yiddish (yi), Yoruba (yo), Zulu (zu) Gemini 2.0 Flash, Gemini 1.5 Pro and Gemini 1.5 Flash models can understand and respond in the following additional languages: Afrikaans (af), Amharic (am), Assamese (as), Azerbaijani (az), Belarusian (be), Bosnian (bs), Catalan (ca), Cebuano (ceb), Corsican (co), Welsh (cy), Dhivehi (dv), Esperanto (eo), Basque (eu), Persian (fa), Filipino (Tagalog) (fil), Frisian (fy), Irish (ga), Scots Gaelic (gd), Galician (gl), Gujarati (gu), Hausa (ha), Hawaiian (haw), Hmong (hmn), Haitian Creole (ht), Armenian (hy), Igbo (ig), Icelandic (is), Javanese (jv), Georgian (ka), Kazakh (kk), Khmer (km), Kannada (kn), Krio (kri), Kurdish (ku), Kyrgyz (ky), Latin (la), Luxembourgish (lb), Lao (lo), Malagasy (mg), Maori (mi), Macedonian (mk), Malayalam (ml), Mongolian (mn), Meiteilon (Manipuri) (mni-Mtei), Marathi (mr), Malay (ms), Maltese (mt), Myanmar (Burmese) (my), Nepali (ne), Nyanja (Chichewa) (ny), Odia (Oriya) (or), Punjabi (pa), Pashto (ps), Sindhi (sd), Sinhala (Sinhalese) (si), Samoan (sm), Shona (sn), Somali (so), Albanian (sq), Sesotho (st), Sundanese (su), Tamil (ta), Telugu (te), Tajik (tg), Uyghur (ug), Urdu (ur), Uzbek (uz), Xhosa (xh), Yiddish (yi), Yoruba (yo), Zulu (zu) Gemma supports only the English language. Multilingual text embedding models support the following languages: Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French,"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
    "title": "Google modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#chunk-3",
    "content": "Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu. Imagen 3 supports the following languages: English, Chinese, Hindi, Japanese, Korean, Portuguese, and Spanish. The MedLM model supports the English language. Model Garden is a platform that helps you discover, test, customize, and deploy Google proprietary and select OSS models and assets. To explore the generative AI models and APIs that are available on Vertex AI, go to Model Garden in the Google Cloud console. Go to Model Garden To learn more about Model Garden, including available models and capabilities, seeExplore AI models in Model Garden. To see all model versions, including legacy and retired models, seeModel versions and lifecycle. Try a quickstart tutorial usingVertex AI Studioor theVertex AI API. Explore pretrained models inModel Garden. Learn how to control access to specific models in Model Garden by using aModel Garden organization policy. Learn aboutpricing. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
    "title": "Google modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#chunk-4",
    "content": "Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-21 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview",
    "title": "Imagen on Vertex AI | AI Image GeneratorStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation API reference overview: To view an overview of the API options for image generation and editing, see theimagegenerationmodel API reference. Imagen on Vertex AI brings Google's state of the art image generative AI capabilities to application developers. With Imagen on Vertex AI, application developers can build next-generation AI products that transform their user's imagination into high quality visual assets using AI generation, in seconds. Try image generation (Vertex AI Studio) Try Imagen in a Colab With Imagen, you can do the following: Generate novel images using only a text prompt (text-to-image AI generation). Edit or expand an uploaded or generated image using a mask area you define. Upscale existing, generated, or edited images. These images are generated using the general Imagen 3 image generation model (imagen-3.0-generate-002) and the following prompts: Claymation scene. A medium wide shot of an elderly woman. She is wearing flowing clothing. She is standing in a lush garden watering the plants with an orange watering can Shot in the style of DSLR camera with the polarizing filter. A photo of two hot air balloons over the unique rock formations in Cappadocia, Turkey. The colors and patterns on these balloons contrast beautifully against the earthy tones of the landscape below. This shot captures the sense of adventure that comes with enjoying such an experience. A weathered, wooden mech robot covered in flowering vines stands peacefully in a field of tall wildflowers, with a a small blue bird resting on its outstrecteched hand. Digital Cartoon, with warm colors and soft lines. A large cliff with a waterfall looms behind. A view of a person's hand as they hold a little clay figurine of a bird in their hand and"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview",
    "title": "Imagen on Vertex AI | AI Image GeneratorStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview#chunk-1",
    "content": "sculpt it with a modeling tool in their other hand. You can see the sculptor's scarf. Their hands are covered in clay dust. A macro DSLR image highlighting the texture and craftsmanship. A large, colorful bouquet of flowers in an old blue glass vase on the table. In front is one beautiful peony flower surrounded by various other blossoms like roses, lilies, daisies, orchids, fruits, berries, green leaves. The background is dark gray. Oil painting in the style of the Dutch Golden Age. A single comic book panel of a boy and his father on a grassy hill, staring at the sunset. A speech bubble points from the boy's mouth and says: The sun will rise again. Muted, late 1990s coloring style You can generate novel images using only descriptive text as an input. The following samples show a simplified case for generating images, but you can useadditional parametersto tailor the generated images to your needs. Sign in to your Google Cloud account. If you're new to Google Cloud,create an accountto evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads. In the Google Cloud console, on the project selector page, select or create a Google Cloud project.Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.Go to project selector In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API.Enable the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview",
    "title": "Imagen on Vertex AI | AI Image GeneratorStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview#chunk-2",
    "content": "API Enable the Vertex AI API. Enable the API In the Google Cloud console, on the project selector page, select or create a Google Cloud project.Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.Go to project selector In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API.Enable the API Enable the Vertex AI API. Enable the API Set up authentication for your environment.Select the tab for how you plan to use the samples on this page:PythonTo use the Python samples on this page in a local development environment, install and initialize the gcloud CLI, and then set up Application Default Credentials with your user credentials.Installthe Google Cloud CLI. Set up authentication for your environment. Select the tab for how you plan to use the samples on this page: To use the Python samples on this page in a local development environment, install and initialize the gcloud CLI, and then set up Application Default Credentials with your user credentials. Installthe Google Cloud CLI. Installthe Google Cloud CLI. If you're using an external identity provider (IdP), you must firstsign in to the gcloud CLI with your federated identity.Toinitializethe gcloud CLI, run the following command:gcloudinitIf you're using a local shell, then create local authentication credentials for your user account:gcloudauthapplication-defaultloginYou don't need to do this if you're using"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview",
    "title": "Imagen on Vertex AI | AI Image GeneratorStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview#chunk-3",
    "content": "Cloud Shell.If an authentication error is returned, and you are using an external identity provider (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity.For more information, seeSet up ADC for a local development environmentin the Google Cloud authentication documentation. If you're using an external identity provider (IdP), you must firstsign in to the gcloud CLI with your federated identity. Toinitializethe gcloud CLI, run the following command:gcloudinitIf you're using a local shell, then create local authentication credentials for your user account:gcloudauthapplication-defaultloginYou don't need to do this if you're using Cloud Shell.If an authentication error is returned, and you are using an external identity provider (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity.For more information, seeSet up ADC for a local development environmentin the Google Cloud authentication documentation. Toinitializethe gcloud CLI, run the following command: If you're using a local shell, then create local authentication credentials for your user account:gcloudauthapplication-defaultloginYou don't need to do this if you're using Cloud Shell.If an authentication error is returned, and you are using an external identity provider (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity. If you're using a local shell, then create local authentication credentials for your user account: You don't need to do this if you're using Cloud Shell. If an authentication error is returned, and you are using an external identity provider (IdP), confirm that you havesigned in to the gcloud CLI with your federated identity. For more information, seeSet up ADC for a local development environmentin the Google"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview",
    "title": "Imagen on Vertex AI | AI Image GeneratorStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview#chunk-4",
    "content": "Cloud authentication documentation."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation To see an example of function calling, run the \"Intro to Function Calling with the Gemini API\" Jupyter notebook in one of the following environments: Open in Colab|Open in Colab Enterprise|Open in Vertex AI Workbench user-managed notebooks|View on GitHub Large Language Models (LLMs) are powerful at solving many types of problems. However, they are constrained by the following limitations: They are frozen after training, leading to stale knowledge. They can't query or modify external data. Function calling can address these shortcomings. Function calling is sometimes referred to astool usebecause it allows the model to use external tools such as APIs and functions. When submitting a prompt to the LLM, you also provide the model with a set of tools that it can use to respond to the user's prompt. For example, you could provide a functionget_weatherthat takes a location parameter and returns information about the weather conditions at that location. While processing a prompt, the model can choose to delegate certain data processing tasks to the functions that you identify. The model does not call the functions directly. Instead, the model provides structured data output that includes the function to call and parameter values to use. For example, for a promptWhat is the weather like in Boston?, the model can delegate processing to theget_weatherfunction and provide the location parameter valueBoston, MA. You can use the structured output from the model to invoke external APIs. For example, you could connect to a weather service API, provide the locationBoston, MA, and receive information about temperature, cloud cover, and wind conditions. You can then provide the API output back to the model, allowing it to complete its"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-1",
    "content": "response to the prompt. For the weather example, the model may provide the following response:It is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies. The following models provide support for function calling: Vertex AI Model Optimizer Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.0 Flash Gemini 2.0 Flash-Lite You can use function calling for the following tasks: Here are some more use cases: Interpret voice commands: Create functions that correspond with in-vehicle tasks. For example, you can create functions that turn on the radio or activate the air conditioning. Send audio files of the user's voice commands to the model, and ask the model to convert the audio into text and identify the function that the user wants to call. Interpret voice commands: Create functions that correspond with in-vehicle tasks. For example, you can create functions that turn on the radio or activate the air conditioning. Send audio files of the user's voice commands to the model, and ask the model to convert the audio into text and identify the function that the user wants to call. Automate workflows based on environmental triggers: Create functions to represent processes that can be automated. Provide the model with data from environmental sensors and ask it to parse and process the data to determine whether one or more of the workflows should be activated. For example, a model could process temperature data in a warehouse and choose to activate a sprinkler function. Automate workflows based on environmental triggers: Create functions to represent processes that can be automated. Provide the model with data from environmental sensors and ask it to parse and process the data to determine whether one or more of the workflows should be activated. For example, a model could"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-2",
    "content": "process temperature data in a warehouse and choose to activate a sprinkler function. Automate the assignment of support tickets: Provide the model with support tickets, logs, and context-aware rules. Ask the model to process all of this information to determine who the ticket should be assigned to. Call a function to assign the ticket to the person suggested by the model. Automate the assignment of support tickets: Provide the model with support tickets, logs, and context-aware rules. Ask the model to process all of this information to determine who the ticket should be assigned to. Call a function to assign the ticket to the person suggested by the model. Retrieve information from a knowledge base: Create functions that retrieve academic articles on a given subject and summarize them. Enable the model to answer questions about academic subjects and provide citations for its answers. Retrieve information from a knowledge base: Create functions that retrieve academic articles on a given subject and summarize them. Enable the model to answer questions about academic subjects and provide citations for its answers. To enable a user to interface with the model and use function calling, you must create code that performs the following tasks: Set up your environment. Define and describe a set of available functionsusingfunction declarations. Submit a user's prompt and the function declarations to the model. Invoke a functionusing the structured data output from the model. Provide the function output to the model. You can create an application that manages all of these tasks. This application can be a text chatbot, a voice agent, an automated workflow, or any other program. You can use function calling to generate a single text response or to support a chat session. Ad hoc text"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-3",
    "content": "responses are useful for specific business tasks, including code generation. Chat sessions are useful in freeform, conversational scenarios, where a user is likely to ask follow-up questions. If you use function calling to generate a single response, you must provide the model with the full context of the interaction. On the other hand, if you use function calling in the context of a chat session, the session stores the context for you and includes it in every model request. In both cases, Vertex AI stores the history of the interaction on the client side. This guide demonstrates how you can use function calling to generate a single text response. For an end-to-end sample, seeText examples. To learn how to use function calling to support a chat session, seeChat examples. Import the required modules and initialize the model: Declare aToolthat contains up to 128FunctionDeclarations. You will later pass this tool to the model when submitting the prompt. The model can use the functions in the tool to process the prompt. At most one tool can be provided with the request. You must provide function declarations in a schema format that's compatible with theOpenAPI schema. Vertex AI offers limited support of the OpenAPI schema. The following attributes are supported from the OpenAPI schema:type,nullable,required,format,description,properties,items,enum,anyOf. Remaining attributes are not supported. For best practices related to the function declarations, including tips for names and descriptions, seeBest practices. If you use the REST API, specify the schema using JSON. If you use the Vertex AI SDK for Python, you can specify the schema either manually using a Python dictionary or automatically with thefrom_funchelper function. The following function declaration takes a"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-4",
    "content": "singlestringparameter: The following function declaration takes both object and array parameters: The following function declaration takes an integerenum: The following code sample declares a function that multiplies an array of numbers and usesfrom_functo generate theFunctionDeclarationschema. When the user provides a prompt, the application must provide the model with the user prompt and thefunction declarations. To configure how the model generates results, the application can provide the model with ageneration configuration. To configure how the model uses the function declarations, the application can provide the model with atool configuration. The following is an example of a user prompt: \"What is the weather like in Boston?\" The following is an example of how you can define the user prompt: For best practices related to the user prompt, seeBest practices - User prompt. The model can generate different results for different parameter values. The temperature parameter controls the degree of randomness in this generation. Lower temperatures are good for functions that require deterministic parameter values, while higher temperatures are good for functions with parameters that accept more diverse or creative parameter values. A temperature of0is deterministic. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible. To learn more, seeGemini API. To set this parameter, submit a generation configuration (generation_config) along with the prompt and the function declarations. You can update thetemperatureparameter during a chat conversation using the Vertex AI API and an updatedgeneration_config. For an example of setting thetemperatureparameter, seeHow to submit the prompt and the function declarations. For"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-5",
    "content": "best practices related to the generation configuration, seeBest practices - Generation configuration. You can place some constraints on how the model should use the function declarations that you provide it with. For example, instead of allowing the model to choose between a natural language response and a function call, you can force it to only predict function calls. This is known as \"forced function calling\". You can also choose to provide the model with a full set of function declarations, but restrict its responses to a subset of these functions. To place these constraints, submit a tool configuration (tool_config) along with the prompt and the function declarations. In the configuration, you can specify one of the following modes: For a list of models that support theANYmode (\"forced function calling\"), seesupported models. To learn more, seeFunction Calling API. The following is an example of how can you submit the prompt and the function declarations to the model, and constrain the model to predict onlyget_current_weatherfunction calls. If the model determines that it needs the output of a particular function, the response that the application receives from the model contains the function name and the parameter values that the function should be called with. The following is an example of a model response to the user prompt \"What is the weather like in Boston?\". The model proposes calling theget_current_weatherfunction with the parameterBoston, MA. For prompts such as \"Get weather details in New Delhi and San Francisco?\", the model may propose several parallel function calls. To learn more, seeParallel function calling example. If the application receives a function name and parameter values from the model, the application must connect to an external API and"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-6",
    "content": "call the function. The following example uses synthetic data to simulate a response payload from an external API: For best practices related to API invocation, seeBest practices - API invocation. After an application receives a response from an external API, the application must provide this response to the model. The following is an example of how you can do this using Python: If the model had proposed several parallel function calls, the application must provide all of the responses back to the model. To learn more, seeParallel function calling example. The model may determine that the output of another function is necessary for responding to the prompt. In this case, the response that the application receives from the model contains another function name and another set of parameter values. If the model determines that the API response is sufficient for responding to the user's prompt, it creates a natural language response and returns it to the application. In this case, the application must pass the response back to the user. The following is an example of a response: You can use function calling to generate a single text response. Ad hoc text responses are useful for specific business tasks, including code generation. If you use function calling to generate a single response, you must provide the model with the full context of the interaction. Vertex AI stores the history of the interaction on the client side. This example demonstrates a text scenario with one function and one prompt. It uses theGenerativeModelclass and its methods. For more information about using the Vertex AI SDK for Python with Gemini multimodal models, seeIntroduction to multimodal classes in the Vertex AI SDK for Python. Gen AI SDK for PythonInstallpip install --upgrade google-genaiTo learn"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-7",
    "content": "more, see theSDK reference documentation.Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=Truefromgoogleimportgenaifromgoogle.genai.typesimport(FunctionDeclaration,GenerateContentConfig,HttpOptions,Tool,)client=genai.Client(http_options=HttpOptions(api_version=\"v1\"))model_id=\"gemini-2.0-flash-001\"get_album_sales=FunctionDeclaration(name=\"get_album_sales\",description=\"Gets the number of albums sold\",# Function parameters are specified in JSON schema formatparameters={\"type\":\"OBJECT\",\"properties\":{\"albums\":{\"type\":\"ARRAY\",\"description\":\"List of albums\",\"items\":{\"description\":\"Album and its sales\",\"type\":\"OBJECT\",\"properties\":{\"album_name\":{\"type\":\"STRING\",\"description\":\"Name of the music album\",},\"copies_sold\":{\"type\":\"INTEGER\",\"description\":\"Number of copies sold\",},},},},},},)sales_tool=Tool(function_declarations=[get_album_sales],)response=client.models.generate_content(model=model_id,contents='At Stellar Sounds, a music label, 2024 was a rollercoaster. \"Echoes of the Night,\" a debut synth-pop album, ''surprisingly sold 350,000 copies, while veteran rock band \"Crimson Tide\\'s\" latest, \"Reckless Hearts,\" ''lagged at 120,000. Their up-and-coming indie artist, \"Luna Bloom\\'s\" EP, \"Whispers of Dawn,\" ''secured 75,000 sales. The biggest disappointment was the highly-anticipated rap album \"Street Symphony\" '\"only reaching 100,000 units. Overall, Stellar Sounds moved over 645,000 units this year, revealing unexpected \"\"trends in music"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-8",
    "content": "consumption.\",config=GenerateContentConfig(tools=[sales_tool],temperature=0,),)print(response.function_calls)# Example response:# [FunctionCall(# id=None,# name=\"get_album_sales\",# args={# \"albums\": [# {\"album_name\": \"Echoes of the Night\", \"copies_sold\": 350000},# {\"copies_sold\": 120000, \"album_name\": \"Reckless Hearts\"},# {\"copies_sold\": 75000, \"album_name\": \"Whispers of Dawn\"},# {\"copies_sold\": 100000, \"album_name\": \"Street Symphony\"},# ]# },# )] To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True This example demonstrates a text scenario with one function and one prompt. Node.jsBefore trying this sample, follow theNode.jssetup instructions in theVertex AI quickstart using client libraries. For more information, see theVertex AINode.jsAPI reference documentation.To authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment.const{VertexAI,FunctionDeclarationSchemaType,}=require('@google-cloud/vertexai');constfunctionDeclarations=[{function_declarations:[{name:'get_current_weather',description:'get weather in a given location',parameters:{type:FunctionDeclarationSchemaType.OBJECT,properties:{location:{type:FunctionDeclarationSchemaType.STRING},unit:{type:FunctionDeclarationSchemaType.STRING,enum:['celsius','fahrenheit'],},},required:['location'],},},],},];constfunctionResponseParts=[{functionResponse:{name:'get_current_weather',response:{name:'get_current_weather',content:{weather:'super"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-9",
    "content": "nice'}},},},];/*** TODO(developer): Update these variables before running the sample.*/asyncfunctionfunctionCallingStreamContent(projectId='PROJECT_ID',location='us-central1',model='gemini-2.0-flash-001'){// Initialize Vertex with your Cloud project and locationconstvertexAI=newVertexAI({project:projectId,location:location});// Instantiate the modelconstgenerativeModel=vertexAI.getGenerativeModel({model:model,});constrequest={contents:[{role:'user',parts:[{text:'What is the weather in Boston?'}]},{role:'ASSISTANT',parts:[{functionCall:{name:'get_current_weather',args:{location:'Boston'},},},],},{role:'USER',parts:functionResponseParts},],tools:functionDeclarations,};conststreamingResp=awaitgenerativeModel.generateContentStream(request);forawait(constitemofstreamingResp.stream){console.log(item.candidates[0].content.parts[0].text);}} Before trying this sample, follow theNode.jssetup instructions in theVertex AI quickstart using client libraries. For more information, see theVertex AINode.jsAPI reference documentation. To authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment. This example demonstrates a text scenario with one function and one prompt. Learn how to install or update theGen AI SDK for Go. To learn more, see theSDK reference documentation. Set environment variables to use the Gen AI SDK with Vertex AI:# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values# with appropriate values for your project.exportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECTexportGOOGLE_CLOUD_LOCATION=globalexportGOOGLE_GENAI_USE_VERTEXAI=True This example demonstrates a text scenario with one function and one prompt. Before trying this sample, follow theC#setup instructions in"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-10",
    "content": "theVertex AI quickstart using client libraries. For more information, see theVertex AIC#API reference documentation. To authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment. This example demonstrates a text scenario with three functions and one prompt. In this example, you call the generative AI model twice. In thefirst call, you provide the model with the prompt and the function declarations. In thesecond call, you provide the model with the API response. The request must define a prompt in thetextparameter. This example defines the following prompt: \"Which theaters in Mountain View show the Barbie movie?\". The request must also define a tool (tools) with a set of function declarations (functionDeclarations). These function declarations must be specified in a format that's compatible with theOpenAPI schema. This example defines the following functions: find_moviesfinds movie titles playing in theaters. find_theatresfinds theaters based on location. get_showtimesfinds the start times for movies playing in a specific theater. To learn more about the parameters of the model request, seeGemini API. Replacemy-projectwith the name of your Google Cloud project. For the prompt \"Which theaters in Mountain View show the Barbie movie?\", the model might return the functionfind_theatreswith parametersBarbieandMountain View, CA. This example uses synthetic data instead of calling the external API. There are two results, each with two parameters (nameandaddress): name:AMC Mountain View 16,address:2000 W El Camino Real, Mountain View, CA 94040 name:Regal Edwards 14,address:245 Castro St, Mountain View, CA 94040 Replacemy-projectwith the name of your Google Cloud project. The model's"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-11",
    "content": "response might be similar to the following: You can use function calling to support a chat session. Chat sessions are useful in freeform, conversational scenarios, where a user is likely to ask follow-up questions. If you use function calling in the context of a chat session, the session stores the context for you and includes it in every model request. Vertex AI stores the history of the interaction on the client side. Before trying this sample, follow theJavasetup instructions in theVertex AI quickstart using client libraries. For more information, see theVertex AIJavaAPI reference documentation. To authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment. Before trying this sample, follow theGosetup instructions in theVertex AI quickstart using client libraries. For more information, see theVertex AIGoAPI reference documentation. To authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment. Before trying this sample, follow theNode.jssetup instructions in theVertex AI quickstart using client libraries. For more information, see theVertex AINode.jsAPI reference documentation. To authenticate to Vertex AI, set up Application Default Credentials. For more information, seeSet up authentication for a local development environment. For prompts such as \"Get weather details in New Delhi and San Francisco?\", the model may propose several parallel function calls. For a list of models that support parallel function calling, seeSupported models. This example demonstrates a scenario with oneget_current_weatherfunction. The user prompt is \"Get weather details in New Delhi and San Francisco?\". The"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-12",
    "content": "model proposes two parallelget_current_weatherfunction calls: one with the parameterNew Delhiand the other with the parameterSan Francisco. To learn more about the parameters of the model request, seeGemini API. The following command demonstrates how you can provide the function output to the model. Replacemy-projectwith the name of your Google Cloud project. The natural language response created by the model is similar to the following: Function name should start with a letter or an underscore and contains only characters a-z, A-Z, 0-9, underscores, dots or dashes with a maximum length of 64. Write function descriptions clearly and verbosely. For example, for abook_flight_ticketfunction: The following is an example of a good function description:book flight tickets after confirming users' specific requirements, such as time, departure, destination, party size and preferred airline The following is an example of a bad function description:book flight ticket Function parameter and nested attribute names should start with a letter or an underscore and contains only characters a-z, A-Z, 0-9, or underscores with a maximum length of 64. Don't use period (.), dash (-), or space characters in the function parameter names and nested attributes. Instead, use underscore (_) characters or any other characters. Write clear and verbose parameter descriptions, including details such as your preferred format or values. For example, for abook_flight_ticketfunction: The following is a good example of adepartureparameter description:Use the 3 char airport code to represent the airport. For example, SJC or SFO. Don't use the city name. The following is a bad example of adepartureparameter description:the departure airport If possible, use strongly typed parameters to reduce model"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-13",
    "content": "hallucinations. For example, if the parameter values are from a finite set, add anenumfield instead of putting the set of values into the description. If the parameter value is always an integer, set the type tointegerrather thannumber. When using functions with date, time, or location parameters, include the current date, time, or relevant location information (for example, city and country) in the system instruction. This ensures the model has the necessary context to process the request accurately, even if the user's prompt lacks details. For best results, prepend the user prompt with the following details: Additional context for the model-for example,You are a flight API assistant to help with searching flights based on user preferences. Details or instructions on how and when to use the functions-for example,Don't make assumptions on the departure or destination airports. Always use a future date for the departure or destination time. Instructions to ask clarifying questions if user queries are ambiguous-for example,Ask clarifying questions if not enough information is available. For the temperature parameter, use0or another low value. This instructs the model to generate more confident results and reduces hallucinations. If the model proposes the invocation of a function that would send an order, update a database, or otherwise have significant consequences, validate the function call with the user before executing it. The pricing for function calling is based on the number of characters within the text inputs and outputs. To learn more, seeVertex AI pricing. Here, text input (prompt) refers to the user prompt for the current conversation turn, the function declarations for the current conversation turn, and the history of the conversation. The history of the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
    "title": "Introduction to function callingStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chunk-14",
    "content": "conversation includes the queries, the function calls, and the function responses of previous conversation turns. Vertex AI truncates the history of the conversation at 32,000 characters. Text output (response) refers to the function calls and the text responses for the current conversation turn. See theAPI reference for function calling. See theAPI reference for function calling. Learn aboutVertex AI extensions. Learn aboutVertex AI extensions. Learn aboutLangChain on Vertex AI. Learn aboutLangChain on Vertex AI. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-21 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-0",
    "content": "Home Vertex AI Documentation Machine learning models are often seen as \"black boxes\", where even its designers can't explain how or why a model produced a specific prediction. Vertex Explainable AI offersFeature-basedandExample-basedexplanations to provide better understanding of model decision making. Knowing how a model behaves, and how it is influenced by its training dataset, gives anyone who builds or uses ML new abilities to improve models, build confidence in their predictions, and understand when and why things go awry. With example-based explanations, Vertex AI usesnearest neighbor searchto return a list of examples (typically from the training set) that are most similar to the input. Because we generally expect similar inputs to yield similar predictions, we can use these explanations to explore and explain our model's behavior. Example-based explanations can be useful in several scenarios: Improve your data or model: One of the core use cases for example-based explanations is helping you understand why your model made certain mistakes in its predictions, and using those insights to improve your data or model. To do so, first select test data that is of interest to you. This could be either driven by business needs or heuristics like data where the model made the most egregious mistakes.For example, suppose we have a model that classifies images as either a bird or a plane, and that it is misclassifying the following bird as a plane with high confidence. You can use Example-based explanations to retrieve similar images from the training set to figure out what is happening.Since all of its explanations are dark silhouettes from the plane class, it's a signal to get more bird silhouettes.However, if the explanations were mainly from the bird class, it's a signal"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-1",
    "content": "that our model is unable to learn relationships even when the data is rich, and we should consider increasing model complexity (for example, adding more layers). Improve your data or model: One of the core use cases for example-based explanations is helping you understand why your model made certain mistakes in its predictions, and using those insights to improve your data or model. To do so, first select test data that is of interest to you. This could be either driven by business needs or heuristics like data where the model made the most egregious mistakes. For example, suppose we have a model that classifies images as either a bird or a plane, and that it is misclassifying the following bird as a plane with high confidence. You can use Example-based explanations to retrieve similar images from the training set to figure out what is happening. Since all of its explanations are dark silhouettes from the plane class, it's a signal to get more bird silhouettes. However, if the explanations were mainly from the bird class, it's a signal that our model is unable to learn relationships even when the data is rich, and we should consider increasing model complexity (for example, adding more layers). Interpret novel data: Assume, for the moment, that your model was trained to classify birds and planes, but in the real world, the model also encounters images of kites, drones, and helicopters. If your nearest neighbor dataset includes some labeled images of kites, drones, and helicopters, you can use example-based explanations to classify novel images by applying the most frequently occurring label of its nearest neighbors. This is possible because we expect the latent representation of kites to be different from that of birds or planes and more similar to the labeled kites in"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-2",
    "content": "the nearest neighbor dataset. Interpret novel data: Assume, for the moment, that your model was trained to classify birds and planes, but in the real world, the model also encounters images of kites, drones, and helicopters. If your nearest neighbor dataset includes some labeled images of kites, drones, and helicopters, you can use example-based explanations to classify novel images by applying the most frequently occurring label of its nearest neighbors. This is possible because we expect the latent representation of kites to be different from that of birds or planes and more similar to the labeled kites in the nearest neighbor dataset. Detect anomalies: Intuitively, if an instance is far away from all of the data in the training set, then it is likely an outlier. Neural networks are known to be overconfident in their mistakes, thus masking their errors. Monitoring your models using example-based explanations helps identify the most serious outliers. Detect anomalies: Intuitively, if an instance is far away from all of the data in the training set, then it is likely an outlier. Neural networks are known to be overconfident in their mistakes, thus masking their errors. Monitoring your models using example-based explanations helps identify the most serious outliers. Active learning: Example-based explanations can help you identify the instances that might benefit from human labeling. This is particularly useful if labeling is slow or expensive, ensuring that you get the richest possible dataset from limited labeling resources.For example, suppose we have a model that classifies a medical patient as either having a cold or a flu. If a patient is classified as having the flu, and all of her example-based explanations are from the flu class, then the doctor can be more"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-3",
    "content": "confident in the model's prediction without having to take a closer look. However, if some of the explanations are from the flu class, and some others from cold class, it would be worthwhile to get a doctor's opinion. This will lead to a dataset where difficult instances have more labels, making it easier for downstream models to learn complex relationships. Active learning: Example-based explanations can help you identify the instances that might benefit from human labeling. This is particularly useful if labeling is slow or expensive, ensuring that you get the richest possible dataset from limited labeling resources. For example, suppose we have a model that classifies a medical patient as either having a cold or a flu. If a patient is classified as having the flu, and all of her example-based explanations are from the flu class, then the doctor can be more confident in the model's prediction without having to take a closer look. However, if some of the explanations are from the flu class, and some others from cold class, it would be worthwhile to get a doctor's opinion. This will lead to a dataset where difficult instances have more labels, making it easier for downstream models to learn complex relationships. To create a Model that supports example-based explanations, seeConfiguring example-based explanations. Any TensorFlow model that can provide an embedding (latent representation) for inputs is supported. Tree-based models, such as decision trees, are not supported. Models from other frameworks, such as PyTorch or XGBoost, are not supported yet. For deep neural networks, we generally assume that the higher layers (closer to the output layer) have learned something \"meaningful\", and thus, the penultimate layer is often chosen for embeddings. You can experiment"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-4",
    "content": "with a few different layers, investigate the examples you are getting, and choose one based on some quantitative (class match) or qualitative (looks sensible) measures. For a demonstration on how to extract embeddings from a TensorFlow model and perform nearest neighbor search, see theexample-based explanation notebook. Vertex Explainable AI integratesfeature attributionsinto Vertex AI. This section provides a brief conceptual overview of the feature attribution methods available with Vertex AI. Feature attributions indicate how much each feature in your model contributed to the predictions for each given instance. When you request predictions, you get predicted values as appropriate for your model. When you requestexplanations, you get the predictions along with feature attribution information. Feature attributions work on tabular data, and include built-in visualization capabilities for image data. Consider the following examples: A deep neural network is trained to predict the duration of a bike ride, based on weather data and previous ride sharing data. If you request only predictions from this model, you get predicted durations of bike rides in number of minutes. If you requestexplanations, you get the predicted bike trip duration, along with an attribution score for each feature in your explanations request. The attribution scores show how much the feature affected the change in prediction value, relative to the baseline value that you specify. Choose a meaningful baseline that makes sense for your model - in this case, the median bike ride duration. You can plot the feature attribution scores to see which features contributed most strongly to the resulting prediction: A deep neural network is trained to predict the duration of a bike ride, based on weather data"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-5",
    "content": "and previous ride sharing data. If you request only predictions from this model, you get predicted durations of bike rides in number of minutes. If you requestexplanations, you get the predicted bike trip duration, along with an attribution score for each feature in your explanations request. The attribution scores show how much the feature affected the change in prediction value, relative to the baseline value that you specify. Choose a meaningful baseline that makes sense for your model - in this case, the median bike ride duration. You can plot the feature attribution scores to see which features contributed most strongly to the resulting prediction: An image classification model is trained to predict whether a given image contains a dog or a cat. If you request predictions from this model on a new set of images, then you receive a prediction for each image (\"dog\" or \"cat\"). If you requestexplanations, you get the predicted class along with an overlay for the image, showing which pixels in the image contributed most strongly to the resulting prediction:A photo of a cat with feature attribution overlayA photo of a dog with feature attribution overlay An image classification model is trained to predict whether a given image contains a dog or a cat. If you request predictions from this model on a new set of images, then you receive a prediction for each image (\"dog\" or \"cat\"). If you requestexplanations, you get the predicted class along with an overlay for the image, showing which pixels in the image contributed most strongly to the resulting prediction: An image classification model is trained to predict the species of a flower in the image. If you request predictions from this model on a new set of images, then you receive a prediction for each image (\"daisy\" or"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-6",
    "content": "\"dandelion\"). If you requestexplanations, you get the predicted class along with an overlay for the image, showing which areas in the image contributed most strongly to the resulting prediction:A photo of a daisy with feature attribution overlay An image classification model is trained to predict the species of a flower in the image. If you request predictions from this model on a new set of images, then you receive a prediction for each image (\"daisy\" or \"dandelion\"). If you requestexplanations, you get the predicted class along with an overlay for the image, showing which areas in the image contributed most strongly to the resulting prediction: Feature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML models, and modalities (images, text, tabular, video). To use feature attribution,configure your model for feature attributionwhen you upload or register the model to the Vertex AI Model Registry. Additionally, for the following types of AutoML models, feature attribution is integrated into the Google Cloud console: AutoML image models (classification models only) AutoML tabular models (classification and regression models only) For AutoML model types that are integrated, you can enable feature attribution in the Google Cloud console during training and seemodel feature importancefor the model overall, andlocal feature importancefor bothonlineandbatchpredictions. For AutoML model types that are not integrated, you can still enable feature attribution by exporting the model artifacts and configuring feature attribution when you upload the model artifacts to the Vertex AI Model Registry. If you inspect specific instances, and also aggregate feature attributions across your training"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-7",
    "content": "dataset, you can get deeper insight into how your model works. Consider the following advantages: Debugging models: Feature attributions can help detect issues in the data that standard model evaluation techniques would usually miss.For example, an image pathology model achieved suspiciously good results on a test dataset of chest X-Ray images. Feature attributions revealed that the model's high accuracy depended on the radiologist's pen marks in the image. For more details about this example, see theAI Explanations Whitepaper. Debugging models: Feature attributions can help detect issues in the data that standard model evaluation techniques would usually miss. For example, an image pathology model achieved suspiciously good results on a test dataset of chest X-Ray images. Feature attributions revealed that the model's high accuracy depended on the radiologist's pen marks in the image. For more details about this example, see theAI Explanations Whitepaper. Optimizing models: You can identify and remove features that are less important, which can result in more efficient models. Optimizing models: You can identify and remove features that are less important, which can result in more efficient models. Each feature attribution method is based onShapley values- a cooperative game theory algorithm that assigns credit to each player in a game for a particular outcome. Applied to machine learning models, this means that each model feature is treated as a \"player\" in the game. Vertex Explainable AI assigns proportional credit to each feature for the outcome of a particular prediction. Thesampled Shapleymethod provides a sampling approximation of exact Shapley values. AutoML tabular models use the sampled Shapley method for feature importance. Sampled Shapley works well for"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-8",
    "content": "these models, which are meta-ensembles of trees and neural networks. For in-depth information about how the sampled Shapley method works, read the paperBounding the Estimation Error of Sampling-based Shapley Value Approximation. In theintegrated gradientsmethod, the gradient of the prediction output is calculated with respect to the features of the input, along an integral path. The gradients are calculated at different intervals of a scaling parameter. The size of each interval is determined by using theGaussian quadraturerule. (For image data, imagine this scaling parameter as a \"slider\" that is scaling all pixels of the image to black.) The gradients are integrated as follows:The integral is approximated by using a weighted average.The element-wise product of the averaged gradients and the original input is calculated. The integral is approximated by using a weighted average. The element-wise product of the averaged gradients and the original input is calculated. For an intuitive explanation of this process as applied to images, refer to the blog post,\"Attributing a deep network's prediction to its input features\". The authors of the original paper about integrated gradients (Axiomatic Attribution for Deep Networks) show in the preceding blog post what the images look like at each step of the process. TheXRAImethod combines the integrated gradients method with additional steps to determine whichregionsof the image contribute the most to a given class prediction. Pixel-level attribution: XRAI performs pixel-level attribution for the input image. In this step, XRAI uses the integrated gradients method with a black baseline and a white baseline. Oversegmentation: Independently of pixel-level attribution, XRAI oversegments the image to create a patchwork of small"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-9",
    "content": "regions. XRAI usesFelzenswalb's graph-based methodto create the image segments. Region selection: XRAI aggregates the pixel-level attribution within each segment to determine its attribution density. Using these values, XRAI ranks each segment and then orders the segments from most to least positive. This determines which areas of the image are most salient, or contribute most strongly to a given class prediction. Vertex Explainable AI offers three methods to use for feature attributions:sampled Shapley,integrated gradients, andXRAI. Classification and regression on tabular data Any custom-trained model (running in any prediction container) AutoML tabular models Classification and regression on tabular data Classification on image data Custom-trained TensorFlow models that use aTensorFlow prebuilt containerto serve predictions AutoML image models Classification on image data Custom-trained TensorFlow models that use aTensorFlow prebuilt containerto serve predictions AutoML image models For a more thorough comparison of attribution methods, see theAI Explanations Whitepaper. Indifferentiablemodels, you can calculate the derivative of all the operations in your TensorFlow graph. This property helps to make backpropagation possible in such models. For example, neural networks are differentiable. To get feature attributions for differentiable models, use the integrated gradients method. The integrated gradients method doesnotwork for non-differentiable models. Learn more aboutencoding non-differentiable inputsto work with the integrated gradients method. Non-differentiablemodels include non-differentiable operations in the TensorFlow graph, such as operations that perform decoding and rounding tasks. For example, a model built as an ensemble of trees and neural networks is"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-10",
    "content": "non-differentiable. To get feature attributions for non-differentiable models, use the sampled Shapley method. Sampled Shapley also works on differentiable models, but in that case, it is more computationally expensive than necessary. Consider the following limitations of feature attributions: Feature attributions, including local feature importance for AutoML, are specific to individual predictions. Inspecting the feature attributions for an individual prediction may provide good insight, but the insight may not be generalizable to the entire class for that individual instance, or the entire model.To get more generalizable insight for AutoML models, refer to the model feature importance. To get more generalizable insight for other models, aggregate attributions over subsets over your dataset, or the entire dataset. Feature attributions, including local feature importance for AutoML, are specific to individual predictions. Inspecting the feature attributions for an individual prediction may provide good insight, but the insight may not be generalizable to the entire class for that individual instance, or the entire model. To get more generalizable insight for AutoML models, refer to the model feature importance. To get more generalizable insight for other models, aggregate attributions over subsets over your dataset, or the entire dataset. Although feature attributions can help with model debugging, they do not always indicate clearly whether an issue arises from the model or from the data that the model is trained on. Use your best judgment, and diagnose common data issues to narrow the space of potential causes. Although feature attributions can help with model debugging, they do not always indicate clearly whether an issue arises from the model or from the data that"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview",
    "title": "Introduction to Vertex Explainable AIStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#chunk-11",
    "content": "the model is trained on. Use your best judgment, and diagnose common data issues to narrow the space of potential causes. Feature attributions are subject to similar adversarial attacks as predictions in complex models. Feature attributions are subject to similar adversarial attacks as predictions in complex models. For more information about limitations, refer to thehigh-level limitations listand theAI Explanations Whitepaper. For feature attribution, the implementations of sampled Shapley, integrated gradients, and XRAI are based on the following references, respectively: Bounding the Estimation Error of Sampling-based Shapley Value Approximation Axiomatic Attribution for Deep Networks XRAI: Better Attributions Through Regions Learn more about the implementation of Vertex Explainable AI by reading theAI Explanations Whitepaper. To get started using Vertex Explainable AI, use these notebooks: The following resources provide further useful educational material: Explainable AI for Practitioners Interpretable Machine Learning: Shapley values Ankur Taly'sIntegrated Gradients GitHub repository. Introduction to Shapley values Configure your model for feature-based explanations Configure your model for example-based explanations Viewfeature importance for AutoML tabular models. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-0",
    "content": "Home Document AI Warehouse Documentation Guides Caution: Document AI Warehouse is deprecated and will no longer be available on Google Cloud after January 16, 2025. To safeguard your data, migrate any documents currently saved in Document AI Warehouse to an alternative like Cloud Storage. Verify that your data migration is completed before the discontinuation date to prevent any data loss. SeeDeprecationsfor details. Document AI Warehouse is an integrated, cloud-based platform to store, search, organize, govern and analyze documents and their structured metadata (called Properties). Documents include structured (e.g. forms, invoices) and unstructured (e.g. contracts, research papers) and their Properties (metadata) includes AI-extracted data from documents and manually or AI-assigned tags (for example, account number, loan ID, document type). Document AI Warehouse offers several advantages over legacy repositories. Following are some features and benefits: API-first: single integrated APIto manage documents and their properties (extracted or tagged metadata), that integrates into your workflows and applications. Metadata Management: to manage extracted and tagged metadata. Governance: integrated with IAM and corporate directoriesFine-grained Access Control (permissions) at the document and folder levels can be assigned to users and groups to view, edit, manage (share, delete) documents.Document AI Warehouse is integrated with IAM (Cloud Identity), so that users and groups can be provisioned into Cloud IdentityUsers/groups can also be federated/synced into Cloud Identity from an enterprise LDAP / identity provider such Azure AD, Active Directory and Keycloak. Fine-grained Access Control (permissions) at the document and folder levels can be assigned to users and groups"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-1",
    "content": "to view, edit, manage (share, delete) documents. Document AI Warehouse is integrated with IAM (Cloud Identity), so that users and groups can be provisioned into Cloud Identity Users/groups can also be federated/synced into Cloud Identity from an enterprise LDAP / identity provider such Azure AD, Active Directory and Keycloak. Search: the product supports rich semantic search, including the following features:Full-text searchFiltering search results by Properties (date, numeric, enum, text). Filters can be combined withANDandORoperatorsSemantic search - support common synonyms and misspellings, stemmings. Quotes (\" \") may be used in the query to specify exact matching keywordsCustom synonyms - industry-specific or company-specific terms, for example.Search within a root-folder hierarchyOperators for search keywords:\"\"exact match,|or,+and,-exclude Full-text search Filtering search results by Properties (date, numeric, enum, text). Filters can be combined withANDandORoperators Semantic search - support common synonyms and misspellings, stemmings. Quotes (\" \") may be used in the query to specify exact matching keywords Custom synonyms - industry-specific or company-specific terms, for example. Search within a root-folder hierarchy Operators for search keywords:\"\"exact match,|or,+and,-exclude Organization:Flexible Folder managementDocuments can be cataloged into one or more folders, based on application (for example, an ID card is placed in a KYC folder, Loan folder, Bank Account folder), without replicating the document.These folders have their own Properties and Access Control, independent from Document properties and access control.The folders can be nested in one or more hierarchies [for example, AllLoans->State->Branch->Loans or LoanTypes->Loans].Users can search for"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-2",
    "content": "documents within a folder hierarchy e.g. search within AllLoans->State Documents can be cataloged into one or more folders, based on application (for example, an ID card is placed in a KYC folder, Loan folder, Bank Account folder), without replicating the document. These folders have their own Properties and Access Control, independent from Document properties and access control. The folders can be nested in one or more hierarchies [for example, AllLoans->State->Branch->Loans or LoanTypes->Loans]. Users can search for documents within a folder hierarchy e.g. search within AllLoans->State UI*- the product includes Web-accessible UI with the following features:Doc Explorer: search documents, filter search results, select documents to bulk-update properties or deleteDoc Viewer: view documents, view/update its properties,assign ACLs, add to foldersUpload: upload documents and run them through a DocAI**extractor (either OCR or a supported specialized parser such as Invoice DocAI).Folder Explorer: add documents to one or more folders, explore folder hierarchy.Embeddable UI: the Doc Explorer and the Doc Viewer (for PDFs) components can be integrated in customer's applications Doc Explorer: search documents, filter search results, select documents to bulk-update properties or delete Doc Viewer: view documents, view/update its properties,assign ACLs, add to folders Upload: upload documents and run them through a DocAI**extractor (either OCR or a supported specialized parser such as Invoice DocAI). Folder Explorer: add documents to one or more folders, explore folder hierarchy. Embeddable UI: the Doc Explorer and the Doc Viewer (for PDFs) components can be integrated in customer's applications Connectors***to common on-premise and cloud repositories: We provide a Cloud Storage to"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-3",
    "content": "Document AI Warehouse connector (as a separate template based onGoogle Workflows) that can be customized/extended to other repositories. We also work with partners to provide out-of-box connectors to repositories such as Sharepoint, Amazon S3, IBM FileNet and others, to ingest and index documents. Migrate vs Federate flexibility: The product supports a flexible architecture such that your document content can be migrated to Document AI Warehouse or stay-in-place if there are constraints in migrating content (we simply index the content and metadata) Integrated with Document Workflows- this integrates with Google Workflows and other document processing workflows by supporting:Properties- that represent the state of a document in a workflow and APIs that workflows can use to update the state of documentsDoc Explorer interface- to track the progress of documents through a workflow pipeline, enabling a human to inspect, manage failures and stalled documents in the workflow pipeline.Conditional Notifications- where documents meeting a certain conditions can trigger/notify a workflow via a Pub/Sub topic or a Web API call: for example, Trigger: OnUpdate; Condition: (DocType=Invoice and TotalAmount>$1000) -> send Pub/Sub Notification Properties- that represent the state of a document in a workflow and APIs that workflows can use to update the state of documents Doc Explorer interface- to track the progress of documents through a workflow pipeline, enabling a human to inspect, manage failures and stalled documents in the workflow pipeline. Conditional Notifications- where documents meeting a certain conditions can trigger/notify a workflow via a Pub/Sub topic or a Web API call: for example, Trigger: OnUpdate; Condition: (DocType=Invoice and TotalAmount>$1000) -> send Pub/Sub"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-4",
    "content": "Notification Policy Management and Compliance Enforcement: conditional notifications and scheduled notifications can be used to trigger workflows that enforce policies (for example, records management, retention and disposition, legal holds) on specific documents in Document AI Warehouse. Files supported- Text PDFs, Images (scanned PDFs, TIFF files, JPEG files), Office (DOCX, PPTX, XLSX) files - run through OCR and indexed.Note - while the product focus is documents, it is also used to manage associated images (e.g. in verticals such as Insurance, Engg, Construction, Research, etc). Note - while the product focus is documents, it is also used to manage associated images (e.g. in verticals such as Insurance, Engg, Construction, Research, etc). Integrated with DocAI: Document AI Warehouse is integrated with Document AI processors at several levels:Document AI processing in UI: Document AI Warehouse UI enables users to upload either scanned PDFs/TIFFs or special document types, both of which are automatically extracted by Document AI OCR or specialized processors respectively before the document is indexed into Document AI Warehouse.Managing batch Document AI pipelines***: Document AI Warehouse integrates with Workflows to provide templates that process batch pipelines of documents through Document AI extraction and classification. This is non-trivial because it entails long-running (LRO) operations and asynchronous API calls that need to be managed for failures and retries. The Workflows template orchestrates such pipelines. Document AI Warehouse UI may be used to search and track the document flow through such pipelines, visualize the Document AI output for failures in each step of the pipeline and take action on stalled/failed documents. Integrated with DocAI: Document"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-5",
    "content": "AI Warehouse is integrated with Document AI processors at several levels: Document AI processing in UI: Document AI Warehouse UI enables users to upload either scanned PDFs/TIFFs or special document types, both of which are automatically extracted by Document AI OCR or specialized processors respectively before the document is indexed into Document AI Warehouse. Managing batch Document AI pipelines***: Document AI Warehouse integrates with Workflows to provide templates that process batch pipelines of documents through Document AI extraction and classification. This is non-trivial because it entails long-running (LRO) operations and asynchronous API calls that need to be managed for failures and retries. The Workflows template orchestrates such pipelines. Document AI Warehouse UI may be used to search and track the document flow through such pipelines, visualize the Document AI output for failures in each step of the pipeline and take action on stalled/failed documents. *The UI is in Preview and expected to go GA soon. **OCR and other document extractors are available in Document AI products but not included in Document AI Warehouse. ***These features are not part of Document AI Warehouse. These features are enabled by external open source components and scripts that customers can deploy or customize and are not implemented within Document AI Warehouse. For more information about Disclaimers and Known Limitations, seeDisclaimers and Known Limitations Following are terms used in Document AI Warehouse. [Images stored in Document AI Warehouse are also referred to as \"Documents\"] Document AI Specialized parsers (for Procurement forms, Lending forms, others)OCR, AutoML, Forms parser (for images such as TIFF/PNG/etc.)Other custom modelsText extracting tools for specialized"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-6",
    "content": "document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents. OCR, AutoML, Forms parser (for images such as TIFF/PNG/etc.)Other custom modelsText extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents. Other custom modelsText extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents. Text extracting tools for specialized document formats such as PDFs, Office documents and others.Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents. Note that Document AI Warehouse can work with any extraction pipeline that calls Document AI Warehouse APIs to ingest/update documents. A user needs Edit permission to the Folder and View permission to the Document), in order to add a Document to a Folder A Trigger, for example, upon DocUpdate/DocCreateCondition, for example, Invoice.Amount <$1000Action, for example, Update Doc Metadata, Return Condition Evaluation, Add Doc to Folder, etc.A policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later) Condition, for example, Invoice.Amount <$1000Action, for example, Update Doc Metadata, Return Condition Evaluation, Add Doc to Folder, etc.A policy is typically associated with a Document Type.It is expressed in a low-code"
  },
  {
    "source_url": "https://cloud.google.com/document-warehouse/docs/overview",
    "title": "Document AI Warehouse overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/document-warehouse/docs/overview#chunk-7",
    "content": "Common Expression Language (JSON format, specified later) Action, for example, Update Doc Metadata, Return Condition Evaluation, Add Doc to Folder, etc.A policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later) A policy is typically associated with a Document Type.It is expressed in a low-code Common Expression Language (JSON format, specified later) It is expressed in a low-code Common Expression Language (JSON format, specified later) API: Admin API used to create/update/read/delete policies. Facet is typically an enumerated field.. We will support Date and Numeric facets in future releases.Facets for a Document type are specified in the Document Schema by Admins (via Admin API) Facets for a Document type are specified in the Document Schema by Admins (via Admin API) Universal access - any user can access any document in the project. The API is access-controlled to user accounts or service accounts but no document-level permissionsDoc-level ACL - users are granted document-level permissions. Each document has R/U/D permissions assigned to users/groups. Doc-level ACL - users are granted document-level permissions. Each document has R/U/D permissions assigned to users/groups. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-0",
    "content": "Home Apigee Documentation Guides Supported versions: v1.14 (latest) v1.13 List of supported versions Unsupported versions: v1.12 v1.11 v1.10 v1.9 v1.8 v1.7 v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 Apigee hybrid is a platform for developing and managing API proxies that features a hybrid deployment model. The hybrid model includes a management plane hosted by Apigee in the Cloud and a runtime plane that you install and manage on one of thesupported Kubernetes platforms. Apigee hybrid helps you manage internal and external APIs with Google Cloud. With unified API management, you can provide your developers, partners, and customers a consistent API program experience. If your compliance and security considerations make on-premises deployment a must for your applications, with an enterprise-grade hybrid gateway, you can host and manage the Apigee hybrid runtime plane on your premises. You manage and control the runtime, enabling you to leverage your existing compliance, governance, and security infrastructure. Balancing cost and performance may lead you to a hybrid strategy. Whether you are just exploring different cloud providers or have already chosen a hybrid strategy, your API management platform should give you the flexibility you need. Host and manage enterprise-grade hybrid gateways across your data center, Google Cloud, or both. Tolearn moreabout hybrid: Keep Reading Toinstallhybrid: Let's go! Apigee hybrid consists of a management plane maintained by Google and a runtime plane that you install on asupported Kubernetes platform. Both planes use Google Cloud Platform services, as the following image shows: As you can see, hybrid consists of the following primary components: Apigee-runmanagement plane: A set of services hosted in the cloud and maintained by Google. These"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-1",
    "content": "services include the UI, management API, and analytics. Customer-managedruntime plane: A set of containerized runtime services that you set up and maintain in your own Kubernetes cluster. All API traffic passes through and is processed within the runtime plane.You manage the containerized runtime on your Kubernetes cluster for greater agility with staged rollouts, auto-scaling, and other operational benefits of containers. Customer-managedruntime plane: A set of containerized runtime services that you set up and maintain in your own Kubernetes cluster. All API traffic passes through and is processed within the runtime plane. You manage the containerized runtime on your Kubernetes cluster for greater agility with staged rollouts, auto-scaling, and other operational benefits of containers. Google Cloud: A suite of Cloud services hosted by Google. One key thing to know about hybrid is that all API traffic is processed within the boundaries of your network and under your control, while management services such as the UI and API analytics run in the cloud and are maintained by Google. For more information, seeWhere is your data stored? The following video provides a deep dive into the hybrid architecture: The runtime plane is a set of containerized runtime services that you set up and maintain in your own Kubernetes cluster running on asupported Kubernetes platform. All API traffic passes through and is processed within the runtime plane. The runtime plane includes the following major components: Message Processors Synchronizer Cassandra MART UDCA The runtime plane runs in a Kubernetes cluster running on asupported Kubernetes platformthat you maintain. The following image shows the primary services that execute on the runtime plane: For general information about the runtime"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-2",
    "content": "components, see the sections that follow. In addition, seeRuntime service configuration overview. The following sections describe each of these primary runtime plane services in more detail. Hybrid Message Processors (MPs) provide API request processing and policy execution on the runtime plane. MPs load all of the deployed proxies, resources, target servers, certificates, and keystores from local storage. You configure an Apigee ingress gateway to expose the MPs to requests that come from outside the cluster. The Synchronizer fetches configuration data about an API environment from the management plane and propagates it across the runtime plane. This downloaded data is also called thecontractand is stored on the local file system. The Synchronizer periodically polls the Management Server for changes and downloads a new configuration whenever changes are detected. The configuration data is retrieved and stored locally as a JSON file on the local file system, where the Message Processors can access it. The downloaded configuration data allows the runtime plane to function independently from the management plane. With the contract, Message Processors on the runtime plane use the locally stored data as their configuration. If the connection between the management and runtime plane goes down, services on the runtime plane continue to function. The configuration data downloaded by the Synchronizer includes: Proxy bundles and shared flow deployments Flow hooks Environment information Shared API resources Target server definitions TLS settings Key Value Map (KVM) names Data masks Apache Cassandrais the runtime datastore that provides data persistence for the runtime plane. Cassandra is a distributed data system that provides data persistence on the runtime plane. You deploy"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-3",
    "content": "the Cassandra database as aStatefulSetnode pool on your Kubernetes cluster. Locating these entities close to the runtime processing services helps support requirements for security and high scalability. The Cassandra database stores information about the following entities: Key management system (KMS) Key Value Map (KVM) OAuth Management API for RunTime data (MART) Monetization data Quotas Response cache Data that belongs to your organization and is accessed during runtime API calls are stored by Cassandra in the runtime plane.This data includes:Application configurationsKey Management System (KMS) dataCacheKey Value Maps (KVMs)API productsDeveloper appsTo access and update that data\u2014for example, to add a new KVM or to remove an environment\u2014you can use the Apigee hybrid UI or theApigee APIs. The MART server (Management API for Runtime data) processes the API calls against the runtime datastore.This section describes the role that MART plays when you call the Apigee APIs to access the runtime datastore.What MART isTo call an Apigee API, you send an authenticated request to the Management Server (MS) on the management plane. The MS authenticates and authorizes the request, and then forwards the request to MART on the runtime plane. Attached to that request is a token that the MS generated using a pre-configured service account.MART receives the request, authenticates and authorizes it, and then performs business validation on it. (For example, if the app is part of an API product, MART ensures it's a valid request.) After determining that a request is valid, MART then processes it.Cassandra stores the runtime data that MART processes (it is, after all, aruntime datastore). MART might read data from the Cassandra or it might update that data, depending on the type of"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-4",
    "content": "request.Like most hybrid services, MART is stateless: it does not persist its own state at runtime.What MART is notThe Management plane communicates with MART via the Apigee Connect agent, which uses a service account with the Apigee Connect Agent role (the MART service account in most installations). You do not call MART directly.Furthermore, MART does not receive API proxy requests; those calls are handled by the Runtime ingress controller and are routed to your cluster's Message Processors.It's worth pointing out that both MART and the Message Processors have access to the same runtime datastore (Cassandra), which is how data such as KMS, KVMs, and caches are shared.The following image shows the flow of an Apigee API call:UDCAThe Universal Data Collection Agent (UDCA) is a service running within the data collection pod in the runtime plane that extracts analytics, debug, and deployment status data and sends it to the UAP.For more information, seeDebug, analytics, and deployment status data collection.About the management planeThe management plane runs on Google Cloud. It includes administrative services such as:Apigee hybrid UI:Provides a UI for developers to create and deploy API proxies, configure policies, create API products, and create developer apps. Administrators can use the Apigee hybrid UI to monitor deployment status.Apigee APIs:Provide a programmatic interface for managing your organization and environments.Unified Analytics Platform (UAP):Receives and processes analytics and deployment status data from the runtime plane.The following image shows the primary services that execute on the management plane:Management plane data and data residencyIf your installation is usingdata residencyyou can specify the region in which this data is stored. SeeUsing data"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-5",
    "content": "residency with Apigee hybrid.About the Google Cloud servicesThe following table describes the key Google Cloud services that hybrid leverages:Google Cloud ServiceDescriptionIdentityUser account authentication uses Google Cloud accounts. Authorization uses Google Cloud service accounts.RolesAccess management for hybrid uses Google's roles engine, IAM, and supports default Apigee roles.Resource HierarchyResources are organized in Google Cloud projects (linked to Apigee organizations).Cloud OperationsProvides logging and metrics data analysis.Types of usersApigee has identified the following primary types of hybrid users:RoleTypical responsibilities/tasksAreas of interestSystem administrators/operatorsInstall and configure services on hybrid's runtime planeSet up Google Cloud, Apigee, and service accountsCreate Google Cloud services and projectsManage the Kubernetes clusterMaintain all of the aboveTroubleshoot the API proxiesCreate and administer a Kubernetes clusterDownload the Apigee Helm chartsUpgrade Apigee hybridConfigure Google Cloud services and Apigee hybrid UIInstall hybrid overviewAdminister hybridData collection on hybridHybrid service configurationKubernetesDevelopersBuild API proxies using the Apigee hybrid UI or Apigee APIsDeploy API proxies to the runtime planeTroubleshoot the API proxiesTest the API proxiesCreate and deploy API proxiesData collection on hybridApigee PoliciesAdvantagesApigee hybrid has the following advantages:Increased agilityBecause hybrid is delivered and runs in containers, you can achieve staged rollouts, auto-scaling, and other operational benefits of a containerized system.Reduced latencyAll communication with the hybrid management plane is asynchronous and does not happen as part of processing client API requests.Increased API"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-6",
    "content": "adoptionAlthough it is possible to process internal APIs using Apigee, the reduced latency and efficiency you can achieve with hybrid makes processing internal APIs with hybrid an attractive option. Part of this efficiency is achieved because your API gateway runs on-premises, in close proximity to your backend services. Also, if you are on Apigee, you can increase your adoption of Apigee by processing internal APIs through hybrid.Greater controlMany enterprises are embarking on a hybrid strategy. The ability to manage API runtimes deployed in private data centers is a key requirement for large enterprises. Currently, the hybrid runtime plane can be deployed to Google Cloud or in your own data center.Next stepSeethe Big Picture\u2014an overview of the hybrid installation process. This data includes: Application configurations Key Management System (KMS) data Cache Key Value Maps (KVMs) API products Developer apps To access and update that data\u2014for example, to add a new KVM or to remove an environment\u2014you can use the Apigee hybrid UI or theApigee APIs. The MART server (Management API for Runtime data) processes the API calls against the runtime datastore. This section describes the role that MART plays when you call the Apigee APIs to access the runtime datastore. To call an Apigee API, you send an authenticated request to the Management Server (MS) on the management plane. The MS authenticates and authorizes the request, and then forwards the request to MART on the runtime plane. Attached to that request is a token that the MS generated using a pre-configured service account.MART receives the request, authenticates and authorizes it, and then performs business validation on it. (For example, if the app is part of an API product, MART ensures it's a valid request.) After"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-7",
    "content": "determining that a request is valid, MART then processes it.Cassandra stores the runtime data that MART processes (it is, after all, aruntime datastore). MART might read data from the Cassandra or it might update that data, depending on the type of request.Like most hybrid services, MART is stateless: it does not persist its own state at runtime. MART receives the request, authenticates and authorizes it, and then performs business validation on it. (For example, if the app is part of an API product, MART ensures it's a valid request.) After determining that a request is valid, MART then processes it. Cassandra stores the runtime data that MART processes (it is, after all, aruntime datastore). MART might read data from the Cassandra or it might update that data, depending on the type of request. Like most hybrid services, MART is stateless: it does not persist its own state at runtime. The Management plane communicates with MART via the Apigee Connect agent, which uses a service account with the Apigee Connect Agent role (the MART service account in most installations). You do not call MART directly. Furthermore, MART does not receive API proxy requests; those calls are handled by the Runtime ingress controller and are routed to your cluster's Message Processors. It's worth pointing out that both MART and the Message Processors have access to the same runtime datastore (Cassandra), which is how data such as KMS, KVMs, and caches are shared. The following image shows the flow of an Apigee API call: The Universal Data Collection Agent (UDCA) is a service running within the data collection pod in the runtime plane that extracts analytics, debug, and deployment status data and sends it to the UAP. For more information, seeDebug, analytics, and deployment status data"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-8",
    "content": "collection. The management plane runs on Google Cloud. It includes administrative services such as: Apigee hybrid UI:Provides a UI for developers to create and deploy API proxies, configure policies, create API products, and create developer apps. Administrators can use the Apigee hybrid UI to monitor deployment status. Apigee APIs:Provide a programmatic interface for managing your organization and environments. Unified Analytics Platform (UAP):Receives and processes analytics and deployment status data from the runtime plane. The following image shows the primary services that execute on the management plane: If your installation is usingdata residencyyou can specify the region in which this data is stored. SeeUsing data residency with Apigee hybrid. The following table describes the key Google Cloud services that hybrid leverages: Apigee has identified the following primary types of hybrid users: Install and configure services on hybrid's runtime plane Set up Google Cloud, Apigee, and service accounts Create Google Cloud services and projects Manage the Kubernetes cluster Maintain all of the above Troubleshoot the API proxies Create and administer a Kubernetes cluster Download the Apigee Helm charts Upgrade Apigee hybrid Configure Google Cloud services and Apigee hybrid UI Install hybrid overview Administer hybrid Data collection on hybrid Hybrid service configuration Kubernetes Build API proxies using the Apigee hybrid UI or Apigee APIs Deploy API proxies to the runtime plane Troubleshoot the API proxies Test the API proxies Create and deploy API proxies Data collection on hybrid Apigee Policies Apigee hybrid has the following advantages: Seethe Big Picture\u2014an overview of the hybrid installation process. Except as otherwise noted, the content of this page is licensed"
  },
  {
    "source_url": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid",
    "title": "What is Apigee hybrid?Stay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/apigee/docs/hybrid/latest/what-is-hybrid#chunk-9",
    "content": "under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/attached",
    "title": "GKE attached clusters documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/attached#chunk-0",
    "content": "Home Google Kubernetes Engine (GKE) GKE Enterprise Clusters Documentation GKE Multi-Cloud GKE attached clusters GKE attached clusters With GKE attached clusters you can manage any standard,CNCF-compliantKubernetes installation, including clusters already in production. Then addGoogle Kubernetes Engine (GKE) Enterprise editionfeatures to standardize and secure your clusters across multiple cloud environments and Kubernetes vendors. GKE attached clusters can automatically installConfig SyncandPolicy Controlleron your clusters, so your platform team can maintain consistent configurations and security policies across all your Kubernetes environments. App teams are free to use their continuous delivery tool of choice with policies in place to provide necessary safeguards. You can usePolicy ControllerandCloud Service Meshto apply security and network configurations across all your cluster configurations. Additionally, you can enable valuable intra-cluster telemetry. Use theConnect gatewayto connect to clusters across environments without proxies, inbound firewall rules, or bastion hosts. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Attach your EKS cluster Attach your EKS cluster Connect to your EKS cluster Connect to your EKS cluster Install Config Management Install Config Management Install Cloud Service Mesh Install Cloud Service Mesh Install logging for EKS attached clusters Install logging for EKS attached clusters Install monitoring for EKS attached clusters Install monitoring for EKS attached clusters Enable Binary Authorization for EKS attached clusters Enable Binary"
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/attached",
    "title": "GKE attached clusters documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/attached#chunk-1",
    "content": "Authorization for EKS attached clusters Get support Get support Attach your AKS cluster Attach your AKS cluster Connect to your AKS cluster Connect to your AKS cluster Install Config Management Install Config Management Install Cloud Service Mesh Install Cloud Service Mesh Install logging for AKS attached clusters Install logging for AKS attached clusters Install monitoring for AKS attached clusters Install monitoring for AKS attached clusters Enable Binary Authorization for AKS attached clusters Enable Binary Authorization for AKS attached clusters Get support Get support Attach other Kubernetes clusters Attach other Kubernetes clusters Connect to your cluster Connect to your cluster Install Config Management Install Config Management Install Cloud Service Mesh Install Cloud Service Mesh Install logging for attached clusters Install logging for attached clusters Install monitoring for attached clusters Install monitoring for attached clusters Get support Get support Impact of temporary disconnection from Google Cloud Impact of temporary disconnection from Google Cloud Pricing Pricing Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models",
    "title": "Introduction to tuningStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Model tuningis a crucial process in adapting Gemini to perform specific tasks with greater precision and accuracy. Model tuning works by providing a model with a training dataset that contains a set of examples of specific downstream tasks. This page provides an overview of model tuning for Gemini, describes the tuning options available for Gemini, and helps you determine when each tuning option should be used. Model tuning is an effective way to customize large models to your tasks. It's a key step to improve the model's quality and efficiency. Model tuning provides the following benefits: Higher quality for your specific tasks Increased model robustness Lower inference latency and cost due to shorter prompts Prompting with pre-trained Gemini models: Prompting is the art of crafting effective instructions to guide AI models like Gemini in generating the outputs you want. It involves designing prompts that clearly convey the task, format you want, and any relevant context. You can use Gemini's capabilities with minimal setup. It's best suited for:Limited labeled data: If you have a small amount of labeled data or can't afford a lengthy fine-tuning process.Rapid prototyping: When you need to quickly test a concept or get a baseline performance without heavy investment in fine-tuning. Limited labeled data: If you have a small amount of labeled data or can't afford a lengthy fine-tuning process. Rapid prototyping: When you need to quickly test a concept or get a baseline performance without heavy investment in fine-tuning. Customized fine-tuning of Gemini models: For more tailored results, Gemini lets you fine-tune its models on your specific datasets. To create an AI model that excels in your specific domain, consider fine-"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models",
    "title": "Introduction to tuningStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models#chunk-1",
    "content": "tuning. This involves retraining the base model on your own labeled dataset, adapting its weights to your task and data. You can adapt Gemini to your use cases. Fine-tuning is most effective when:You have labeled data: A sizable dataset to train on (think 100 examples or more), which allows the model to deeply learn your task's specifics.Complex or unique tasks: For scenarios where advanced prompting strategies are not sufficient, and a model tailored to your data is essential. You have labeled data: A sizable dataset to train on (think 100 examples or more), which allows the model to deeply learn your task's specifics. Complex or unique tasks: For scenarios where advanced prompting strategies are not sufficient, and a model tailored to your data is essential. We recommend starting with prompting to find the optimal prompt. Then, move on to fine-tuning (if required) to further boost performances or fix recurrent errors. While adding more examples might be beneficial, it is important to evaluate where the model makes mistakes before adding more data. High-quality, well-labeled data is crucial for good performance and better than quantity. Also, the data you use for fine-tuning should reflect the prompt distribution, format and context the model will encounter in production. Tuning provides the following benefits over prompt design: Allows deep customization on the model and results in better performance on specific tasks. Align the model with custom syntax, instructions, domain specific semantic rules. Offers more consistent and reliable results. Capable of handling more examples at once. Save cost at inference by removing few-shot examples, long instructions in the prompts Parameter-efficient tuning and full fine-tuning are two approaches to customizing large models."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models",
    "title": "Introduction to tuningStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models#chunk-2",
    "content": "Both methods have their advantages and implications in terms of model quality and resource efficiency. Parameter-efficient tuning, also called adapter tuning, enables efficient adaptation of large models to your specific tasks or domain. Parameter-efficient tuning updates a relatively small subset of the model's parameters during the tuning process. To understand how Vertex AI supports adapter tuning and serving, you can find more details in the following whitepaper,Adaptation of Large Foundation Models. Full fine-tuning updates all parameters of the model, making it suitable for adapting the model to highly complex tasks, with the potential of achieving higher quality. However full fine tuning demands higher computational resources for both tuning and serving, leading to higher overall costs. Parameter-efficient tuning is more resource efficient and cost effective compared to full fine-tuning. It uses significantly lower computational resources to train. It's able to adapt the model faster with a smaller dataset. The flexibility of parameter-efficient tuning offers a solution for multi-task learning without the need for extensive retraining. Vertex AI supports supervised fine-tuning to customize foundational models. Supervised fine-tuning improves the performance of the model by teaching it a new skill. Data that contains hundreds of labeled examples is used to teach the model to mimic a desired behavior or task. Each labeled example demonstrates what you want the model to output during inference. When you run a supervised fine-tuning job, the model learns additional parameters that help it encode the necessary information to perform the desired task or learn the desired behavior. These parameters are used during inference. The output of the tuning job is a new model"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models",
    "title": "Introduction to tuningStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models#chunk-3",
    "content": "that combines the newly learned parameters with the original model. Supervised fine-tuning of a text model is a good option when the output of your model isn't complex and is relatively easy to define. Supervised fine-tuning is recommended for classification, sentiment analysis, entity extraction, summarization of content that's not complex, and writing domain-specific queries. For code models, supervised tuning is the only option. gemini-2.0-flash-001 gemini-2.0-flash-lite-001 For more information on using supervised fine-tuning with each respective model, see the following pages: Tunetext,image,audio, anddocumentdata types. To learn more about the document understanding capability of Gemini models, see theDocument understandingoverview. To start tuning, seeTune Gemini models by using supervised fine-tuning To learn how supervised fine-tuning can be used in a solution that builds a generative AI knowledge base, seeJump Start Solution: Generative AI knowledge base. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-0",
    "content": "Home Cloud Key Management Service Documentation Guides This page provides an overview of Cloud External Key Manager (Cloud EKM).TerminologyExternal key manager (EKM)The key manager used outside of Google Cloud to manage your keys.Cloud External Key Manager (Cloud EKM)A Google Cloud service for using your external keys that are managed within a supported EKM.Cloud EKM through the internetA version of Cloud EKM where Google Cloud communicates with your external key manager over the internet.Cloud EKM through a VPCA version of Cloud EKM where Google Cloud communicates with your external key manager over a Virtual Private Cloud (VPC). For more information, seeVPC network overview.EKM key management from Cloud KMSWhen using Cloud EKM through a VPC with an external key management partner that supports the Cloud EKM control plane, you can use theCloud KMSEKM management mode to simplify the process of maintaining external keys in your external key management partner and in Cloud EKM. For more information, seeCoordinated external keysandEKM key management from Cloud KMSon this page.Crypto spaceA container for your resources within your external key management partner. Your crypto space is identified by a unique crypto space path. The format of the crypto space path varies by external key management partner\u2014for example,v0/cryptospaces/YOUR_UNIQUE_PATH.Partner-managed EKMAn arrangement where your EKM is managed for you by a trusted partner. For more information, seePartner-managed EKMon this page.Key Access JustificationsWhen you use Cloud EKM with Key Access Justifications, each request to your external key management partner includes a field that identifies the reason for each request. You can configure your external key management partner to allow or deny requests based on the"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-1",
    "content": "Key Access Justifications code provided. For more information about Key Access Justifications, seeKey Access Justifications overview.OverviewWith Cloud EKM, you can use keys that you manage within asupported external key management partnerto protect data within Google Cloud. You can protect data at rest insupported CMEK integration services, or by calling the Cloud Key Management Service API directly.Cloud EKM provides several benefits:Key provenance:You control the location and distribution of your externally managed keys. Externally managed keys are never cached or stored within Google Cloud. Instead, Cloud EKM communicates directly with the external key management partner for each request.Access control:You manage access to your externally managed keys in your external key manager. You can't use an externally managed key in Google Cloud without first granting the Google Cloud project access to the key in your external key manager. You can revoke this access at any time.Centralized key management:You can manage your keys and access policies from a single user interface, whether the data they protect resides in the cloud or on your premises.In all cases, the key resides on the external system, and is never sent to Google.You can communicate with your external key managerover the internetorover a Virtual Private Cloud (VPC). This page provides an overview of Cloud External Key Manager (Cloud EKM). External key manager (EKM)The key manager used outside of Google Cloud to manage your keys. External key manager (EKM) The key manager used outside of Google Cloud to manage your keys. Cloud External Key Manager (Cloud EKM)A Google Cloud service for using your external keys that are managed within a supported EKM. Cloud External Key Manager (Cloud EKM) A Google Cloud service"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-2",
    "content": "for using your external keys that are managed within a supported EKM. Cloud EKM through the internetA version of Cloud EKM where Google Cloud communicates with your external key manager over the internet. Cloud EKM through the internet A version of Cloud EKM where Google Cloud communicates with your external key manager over the internet. Cloud EKM through a VPCA version of Cloud EKM where Google Cloud communicates with your external key manager over a Virtual Private Cloud (VPC). For more information, seeVPC network overview. Cloud EKM through a VPC A version of Cloud EKM where Google Cloud communicates with your external key manager over a Virtual Private Cloud (VPC). For more information, seeVPC network overview. EKM key management from Cloud KMSWhen using Cloud EKM through a VPC with an external key management partner that supports the Cloud EKM control plane, you can use theCloud KMSEKM management mode to simplify the process of maintaining external keys in your external key management partner and in Cloud EKM. For more information, seeCoordinated external keysandEKM key management from Cloud KMSon this page. EKM key management from Cloud KMS When using Cloud EKM through a VPC with an external key management partner that supports the Cloud EKM control plane, you can use theCloud KMSEKM management mode to simplify the process of maintaining external keys in your external key management partner and in Cloud EKM. For more information, seeCoordinated external keysandEKM key management from Cloud KMSon this page. Crypto spaceA container for your resources within your external key management partner. Your crypto space is identified by a unique crypto space path. The format of the crypto space path varies by external key management partner\u2014for"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-3",
    "content": "example,v0/cryptospaces/YOUR_UNIQUE_PATH. Crypto space A container for your resources within your external key management partner. Your crypto space is identified by a unique crypto space path. The format of the crypto space path varies by external key management partner\u2014for example,v0/cryptospaces/YOUR_UNIQUE_PATH. Partner-managed EKMAn arrangement where your EKM is managed for you by a trusted partner. For more information, seePartner-managed EKMon this page. Partner-managed EKM An arrangement where your EKM is managed for you by a trusted partner. For more information, seePartner-managed EKMon this page. Key Access JustificationsWhen you use Cloud EKM with Key Access Justifications, each request to your external key management partner includes a field that identifies the reason for each request. You can configure your external key management partner to allow or deny requests based on the Key Access Justifications code provided. For more information about Key Access Justifications, seeKey Access Justifications overview. Key Access Justifications When you use Cloud EKM with Key Access Justifications, each request to your external key management partner includes a field that identifies the reason for each request. You can configure your external key management partner to allow or deny requests based on the Key Access Justifications code provided. For more information about Key Access Justifications, seeKey Access Justifications overview. With Cloud EKM, you can use keys that you manage within asupported external key management partnerto protect data within Google Cloud. You can protect data at rest insupported CMEK integration services, or by calling the Cloud Key Management Service API directly. Cloud EKM provides several benefits: Key provenance:You control the"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-4",
    "content": "location and distribution of your externally managed keys. Externally managed keys are never cached or stored within Google Cloud. Instead, Cloud EKM communicates directly with the external key management partner for each request. Key provenance:You control the location and distribution of your externally managed keys. Externally managed keys are never cached or stored within Google Cloud. Instead, Cloud EKM communicates directly with the external key management partner for each request. Access control:You manage access to your externally managed keys in your external key manager. You can't use an externally managed key in Google Cloud without first granting the Google Cloud project access to the key in your external key manager. You can revoke this access at any time. Access control:You manage access to your externally managed keys in your external key manager. You can't use an externally managed key in Google Cloud without first granting the Google Cloud project access to the key in your external key manager. You can revoke this access at any time. Centralized key management:You can manage your keys and access policies from a single user interface, whether the data they protect resides in the cloud or on your premises. Centralized key management:You can manage your keys and access policies from a single user interface, whether the data they protect resides in the cloud or on your premises. In all cases, the key resides on the external system, and is never sent to Google. Cloud EKM key versions consist of these parts: External key material: The external key material of a Cloud EKM key is cryptographic material created and stored in your EKM. This material does not leave your EKM and it is never shared with Google. Key reference: Each Cloud EKM key version contains"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-5",
    "content": "either a key URI or a key path. This is a unique identifier for the external key material that Cloud EKM uses when requesting cryptographic operations using the key. Internal key material: When a symmetric Cloud EKM key is created, Cloud KMS creates additional key material in Cloud KMS, which never leaves Cloud KMS. This key material is used as an extra layer of encryption when communicating with your EKM. This internal key material does not apply to asymmetric signing keys. To use your Cloud EKM keys, Cloud EKM sends requests for cryptographic operations to your EKM. For example, to encrypt data with a symmetric encryption key, Cloud EKM first encrypts the data using the internal key material. The encrypted data is included in a request to the EKM. The EKM wraps the encrypted data in another layer of encryption using the external key material, and then returns the resulting ciphertext. Data encrypted using a Cloud EKM key can't be decrypted without both the external key material and the internal key material. Creating and managing Cloud EKM keys requires corresponding changes in both Cloud KMS and the EKM. These corresponding changes are handled differently formanually managed external keysand forcoordinated external keys. All external keys accessed over the internet are manually managed. External keys accessed over a VPC network can be manually managed or coordinated, depending on the EKM management mode of the EKM connection. TheManualEKM management mode is used for manually managed keys. TheCloud KMSEKM management mode is used for coordinated external keys. For more information about EKM management modes, seeManually managed external keysandCoordinated external keyson this page. The following diagram shows how Cloud KMS fits into the key management model. This"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-6",
    "content": "diagram uses Compute Engine and BigQuery as two examples; you can also seethe full list of services that support Cloud EKM keys. You can learn about theconsiderationsandrestrictionswhen using Cloud EKM. This section provides a broad overview of how Cloud EKM works with a manually managed external key. You create or use an existing key in asupported external key management partner system. This key has a unique URI or key path. You grant your Google Cloud project access to use the key, in the external key management partner system. In your Google Cloud project, you create a Cloud EKM key version, using the URI or key path for the externally managed key. Maintenance operations like key rotation must be manually managed between your EKM and Cloud EKM. For example, key version rotation or key version destruction operations need to be completed both directly in your EKM and in Cloud KMS. Within Google Cloud, the key appears alongside your other Cloud KMS and Cloud HSM keys, with protection levelEXTERNALorEXTERNAL_VPC. The Cloud EKM key and the external key management partner key work together to protect your data. The external key material is never exposed to Google. This section provides an overview of how Cloud EKM works with coordinated external keys. Youset up an EKM connection, setting theEKM management modetoCloud KMS. During setup, you must authorize your EKM to access your VPC network and authorize your Google Cloud project service account to access yourcrypto spacein your EKM. Your EKM connection uses the hostname of your EKM and acrypto space paththat identifies your resources within your EKM. Youset up an EKM connection, setting theEKM management modetoCloud KMS. During setup, you must authorize your EKM to access your VPC network and authorize your Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-7",
    "content": "project service account to access yourcrypto spacein your EKM. Your EKM connection uses the hostname of your EKM and acrypto space paththat identifies your resources within your EKM. Youcreate an external keyin Cloud KMS. When you create a Cloud EKM key using an EKM over VPC connection with theCloud KMSEKM management mode enabled, the following steps take place automatically:Cloud EKM sends a key creation request to your EKM.Your EKM creates the requested key material. This external key material remains in the EKM and is never sent to Google.Your EKM returns a key path to Cloud EKM.Cloud EKM creates your Cloud EKM key version using the key path provided by your EKM. Youcreate an external keyin Cloud KMS. When you create a Cloud EKM key using an EKM over VPC connection with theCloud KMSEKM management mode enabled, the following steps take place automatically: Cloud EKM sends a key creation request to your EKM. Your EKM creates the requested key material. This external key material remains in the EKM and is never sent to Google. Your EKM returns a key path to Cloud EKM. Cloud EKM creates your Cloud EKM key version using the key path provided by your EKM. Maintenance operations on coordinated external keys can be initiated from Cloud KMS. For example, coordinated external keys used for symmetric encryption can be automatically rotated on a set schedule. The creation of new key versions is coordinated in your EKM by Cloud EKM. You can also trigger the creation or destruction of key versions in your EKM from Cloud KMS using the Google Cloud console, the gcloud CLI, the Cloud KMS API, or Cloud KMS client libraries. Maintenance operations on coordinated external keys can be initiated from Cloud KMS. For example, coordinated external keys used for symmetric encryption can be"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-8",
    "content": "automatically rotated on a set schedule. The creation of new key versions is coordinated in your EKM by Cloud EKM. You can also trigger the creation or destruction of key versions in your EKM from Cloud KMS using the Google Cloud console, the gcloud CLI, the Cloud KMS API, or Cloud KMS client libraries. Within Google Cloud, the key appears alongside your other Cloud KMS and Cloud HSM keys, with protection levelEXTERNAL_VPC. The Cloud EKM key and the external key management partner key work together to protect your data. The external key material is never exposed to Google. Coordinated external keys are made possible by EKM connections that use EKM key management from Cloud KMS. If your EKM supports the Cloud EKM control plane, then you can enable EKM key management from Cloud KMS for your EKM connections to create coordinated external keys. With EKM key management from Cloud KMS enabled, Cloud EKM can request the following changes in your EKM: Create a key: When you create an externally managed key in Cloud KMS using a compatible EKM connection, Cloud EKM sends your key creation request to your EKM. When successful, your EKM creates the new key and key material and returns the key path for Cloud EKM to use to access the key. Create a key: When you create an externally managed key in Cloud KMS using a compatible EKM connection, Cloud EKM sends your key creation request to your EKM. When successful, your EKM creates the new key and key material and returns the key path for Cloud EKM to use to access the key. Rotate a key: When you rotate an externally-managed key in Cloud KMS using a compatible EKM connection, Cloud EKM sends your rotation request to your EKM. When successful, your EKM creates new key material and returns the key path for Cloud EKM to use to access the"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-9",
    "content": "new key version. Rotate a key: When you rotate an externally-managed key in Cloud KMS using a compatible EKM connection, Cloud EKM sends your rotation request to your EKM. When successful, your EKM creates new key material and returns the key path for Cloud EKM to use to access the new key version. Destroy a key: When you destroy a key version for an externally-managed key in Cloud KMS using a compatible EKM connection, Cloud KMS schedules the key version for destruction in Cloud KMS. If the key version is not restored before thescheduled for destructionperiod ends, Cloud EKM destroys its part of the key's cryptographic material and sends a destruction request to your EKM.Data encrypted with this key version cannot be decrypted after the key version is destroyed in Cloud KMS, even if the EKM has not yet destroyed the key version. You can see whether the EKM has successfully destroyed the key version by viewing the key's details in Cloud KMS. Destroy a key: When you destroy a key version for an externally-managed key in Cloud KMS using a compatible EKM connection, Cloud KMS schedules the key version for destruction in Cloud KMS. If the key version is not restored before thescheduled for destructionperiod ends, Cloud EKM destroys its part of the key's cryptographic material and sends a destruction request to your EKM. Data encrypted with this key version cannot be decrypted after the key version is destroyed in Cloud KMS, even if the EKM has not yet destroyed the key version. You can see whether the EKM has successfully destroyed the key version by viewing the key's details in Cloud KMS. When keys in your EKM are managed from Cloud KMS, the key material still resides in your EKM. Google can't make any key management requests to your EKM without explicit permission. Google"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-10",
    "content": "can't change permissions or Key Access Justifications policies in your external key management partner system. If you revoke Google's permissions in your EKM, key management operations attempted in Cloud KMS fail. You can store external keys in the following external key management partner systems: Supported today:FortanixFuturexThales Fortanix Futurex Thales The following services support integration with Cloud KMS for external (Cloud EKM) keys: Agent Assist AlloyDB for PostgreSQL Apigee API hub Application Integration Artifact Registry Backup for GKE BigQuery Bigtable Cloud Composer Cloud Data Fusion Cloud Healthcare API Cloud Logging:Data in the Log RouterandData in Logging storage Cloud Run Cloud Run functions Cloud SQL Cloud Storage Cloud Tasks Cloud Workstations Compute Engine:Persistent disks,Snapshots,Custom images, andMachine images Conversational Insights Database Migration Service:MySQL migrations - data written to databases,PostgreSQL migrations - Data written to databases,PostgreSQL to AlloyDB migrations - Data written to databases,SQL Server migrations - Data written to databases, andOracle to PostgreSQL data at rest Dataflow Dataform Dataplex Dataproc:Dataproc clusters data on VM disksandDataproc serverless data on VM disks Dataproc Metastore Dialogflow CX Document AI Eventarc Standard Filestore Firestore Google Cloud Managed Service for Apache Kafka Google Distributed Cloud Google Kubernetes Engine:Data on VM disksandApplication-layer secrets Integration Connectors(Preview) Looker (Google Cloud core) Memorystore for Redis Migrate to Virtual Machines:Data migrated from VMware, AWS, and Azure VM sourcesandData migrated from disk and machine image sources Parameter Manager Pub/Sub Secret Manager Secure Source Manager Spanner Speaker ID (Restricted GA)"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-11",
    "content": "Speech-to-Text Vertex AI Vertex AI Workbench instances Workflows When you use a Cloud EKM key, Google has no control over the availability of your externally managed key in the external key management partner system. If you lose keys that you manage outside of Google Cloud, Google can't recover your data. When you use a Cloud EKM key, Google has no control over the availability of your externally managed key in the external key management partner system. If you lose keys that you manage outside of Google Cloud, Google can't recover your data. Review the guidelines aboutexternal key management partners and regionswhen choosing the locations for your Cloud EKM keys. Review the guidelines aboutexternal key management partners and regionswhen choosing the locations for your Cloud EKM keys. Review theCloud EKM Service Level Agreement (SLA). Review theCloud EKM Service Level Agreement (SLA). Communicating with an external service over the internet can lead to problems with reliability, availability, and latency. For applications with low tolerance for these types of risks, consider using Cloud HSM or Cloud KMS to store your key material.If an external key is unavailable, Cloud KMS returns aFAILED_PRECONDITIONerror and provides details in thePreconditionFailureerror detail.Enable data audit loggingto maintain a record of all errors related to Cloud EKM. Error messages contain detailed information to help pinpoint the source of the error. An example of a common error is when an external key management partner does not respond to a request within a reasonable timeframe.You need a support contract with the external key management partner. Google Cloud support can only help with issues in Google Cloud services and cannot directly assist with issues on external systems. Sometimes,"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-12",
    "content": "you must work with support on both sides to troubleshoot interoperability issues. Communicating with an external service over the internet can lead to problems with reliability, availability, and latency. For applications with low tolerance for these types of risks, consider using Cloud HSM or Cloud KMS to store your key material. If an external key is unavailable, Cloud KMS returns aFAILED_PRECONDITIONerror and provides details in thePreconditionFailureerror detail.Enable data audit loggingto maintain a record of all errors related to Cloud EKM. Error messages contain detailed information to help pinpoint the source of the error. An example of a common error is when an external key management partner does not respond to a request within a reasonable timeframe. If an external key is unavailable, Cloud KMS returns aFAILED_PRECONDITIONerror and provides details in thePreconditionFailureerror detail. Enable data audit loggingto maintain a record of all errors related to Cloud EKM. Error messages contain detailed information to help pinpoint the source of the error. An example of a common error is when an external key management partner does not respond to a request within a reasonable timeframe. You need a support contract with the external key management partner. Google Cloud support can only help with issues in Google Cloud services and cannot directly assist with issues on external systems. Sometimes, you must work with support on both sides to troubleshoot interoperability issues. You need a support contract with the external key management partner. Google Cloud support can only help with issues in Google Cloud services and cannot directly assist with issues on external systems. Sometimes, you must work with support on both sides to troubleshoot interoperability"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-13",
    "content": "issues. Cloud EKM can be used withBare Metal Rack HSMto create a single-tenant HSM solution integrated with Cloud KMS. To learn more, choose a Cloud EKM partner that supports single-tenant HSMs and review therequirements for Bare Metal Rack HSMs. Cloud EKM can be used withBare Metal Rack HSMto create a single-tenant HSM solution integrated with Cloud KMS. To learn more, choose a Cloud EKM partner that supports single-tenant HSMs and review therequirements for Bare Metal Rack HSMs. Enable audit logging in your external key manager to capture access and usage to your EKM keys. Enable audit logging in your external key manager to capture access and usage to your EKM keys. When you create a Cloud EKM key using the API or the Google Cloud CLI, it must not have an initial key version. This does not apply to Cloud EKM keys created using the Google Cloud console. Automatic rotation is not supported for manually-managed external keys. Cloud EKM operations are subject tospecific quotasin addition to the quotas on Cloud KMS operations. Symmetric encryption keys are only supported for the following:Customer managed encryption keys (CMEK) insupported integration services.Symmetric encryption and decryption using Cloud KMS directly. Customer managed encryption keys (CMEK) insupported integration services. Symmetric encryption and decryption using Cloud KMS directly. Data that is encrypted by Cloud EKM using an externally managed key cannot be decrypted without using Cloud EKM. Asymmetric signing keys are limited to a subset of Cloud KMS algorithms. Asymmetric signing keys are only supported for the following use cases:Asymmetric signing using Cloud KMS directly.Custom signing key with Access Approval. Asymmetric signing using Cloud KMS directly. Custom signing key with Access"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-14",
    "content": "Approval. Once an asymmetric signing algorithm is set on a Cloud EKM key, it cannot be modified. Signing must be done on thedatafield. Cloud EKM needs to be able to reach your keys quickly to avoid an error. When creating a Cloud EKM key, choose a Google Cloud location that is geographically near the location of the external key management partner key. See your external key management partner's documentation to determine which locations they support. Cloud EKM over the internet: available in most Google Cloud locations where Cloud KMS is available, including regional and multi-regional locations. Cloud EKM over a VPC: available in mostregional locationswhere Cloud KMS is available. Cloud EKM over a VPC isn't available in multi-regional locations. Some locations includingglobalandnam-eur-asia1aren't available for Cloud EKM. To learn which locations support Cloud EKM, seeCloud KMS locations. When you use an externally managed key with a multi-region, the metadata of the key is available in multiple data centers within the multi-region. This metadata includes the information needed to communicate with the external key management partner. If your application fails over from one data center to another within the multi-region, the new data center initiates key requests. The new data center may have different network characteristics from the previous data center, including distance from the external key management partner and the likelihood of timeouts. We recommend only using a multi-region with Cloud EKM if your chosen external key manager provides low latency to all areas of that multi-region. Partner-managed EKM lets you use Cloud EKM through a trusted sovereign partner that manages your EKM system for you. With partner-managed EKM, your partner creates and manages the"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-15",
    "content": "keys that you use in Cloud EKM. The partner ensures that your EKM complies with sovereignty requirements. When you onboard with your sovereign partner, the partner provisions resources in the Google Cloud and your EKM. These resources include a Cloud KMS project to manage your Cloud EKM keys and an EKM connection configured for EKM key management from Cloud KMS. Your partner creates resources in Google Cloud locations according to your data residency requirements. Each Cloud EKM key includes Cloud KMS metadata, which lets Cloud EKM send requests to your EKM to perform cryptographic operations using the external key material that never leaves your EKM. Symmetric Cloud EKM keys also include Cloud KMS internal key material that never leaves Google Cloud. For more information about the internal and external sides of Cloud EKM keys, seeHow Cloud EKM workson this page. For more information about partner-managed EKM, seeConfigure partner-managed Cloud KMS. Preview This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. You can use Cloud Monitoring to monitor your EKM connection. The following metrics can help you understand your EKM usage: cloudkms.googleapis.com/ekm/external/request_latencies cloudkms.googleapis.com/ekm/external/request_count For more information about these metrics, seecloudkms metrics. You can create a dashboard to track these metrics. To learn how to set up a dashboard to monitor your EKM connection, seeMonitor EKM usage. If you experience an issue with Cloud EKM, contactSupport. Startusing the API. Startusing the API. Set up Cloud EKM over the internet. Set up Cloud"
  },
  {
    "source_url": "https://cloud.google.com/kms/docs/ekm",
    "title": "Cloud External Key ManagerStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/kms/docs/ekm#chunk-16",
    "content": "EKM over the internet. Create an EKM connectionto use EKM over VPC. Create an EKM connectionto use EKM over VPC. Read through theCloud KMS API Reference. Read through theCloud KMS API Reference. Learn aboutLoggingin Cloud KMS. Logging is based on operations, and applies to keys with both HSM and software protection levels. Learn aboutLoggingin Cloud KMS. Logging is based on operations, and applies to keys with both HSM and software protection levels. SeeReference architectures for reliable deployment of Cloud EKM servicesfor recommendations on configuring an External Key Manager (EKM) service deployment integrated with Cloud EKM. SeeReference architectures for reliable deployment of Cloud EKM servicesfor recommendations on configuring an External Key Manager (EKM) service deployment integrated with Cloud EKM. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-0",
    "content": "Home Documentation This document provides information about committed use discounts (CUDs) for Google Cloud and how they apply for various services. CUDs provide discounted prices for eligible Google Cloud resources in exchange for purchasing committed use contracts (also known as commitments). When you purchase Google Cloud commitments, you commit to either using a minimum level of resources or spending a minimum amount, for a term duration of one or three years. CUDs for Google Cloud are broadly available as spend-based or resource-based CUDs and cover a wide range of services and resources. You can choose the type of commitment that you want to purchase depending on the service that you use and whether you have predictable or unpredictable resource needs. Spend-based CUDs provide a discount in exchange for your commitment to spend a minimum amount for any of the services listed in this section. The discount applies to the set of eligible resources for each service. With spend-based commitments, you commit to spending on eligible resources or services that are worth a specified minimum amount of on-demand price, per hour, throughout the commitment's term of one or three years. In return, you receive discounted rate on the minimum spend amount that you commit to. Any overage usage that takes your hourly spend amount over your committed amount is charged at the on-demand rate. Spend-based CUDs apply to eligible usage in any projects that the Cloud Billing account pays for. Depending on the Google Cloud service that you use, you can purchase spend-based commitments in one of the following ways: Compute flexible commitments Service-specific spend-based commitments Compute flexible commitments are ideal for scenarios where you have predictable Google Cloud spend needs that"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-1",
    "content": "span usage beyond a single service. Compute Flexible CUDs apply to your spend across one or more of the following services: Compute Engine Google Kubernetes Engine Cloud Run You can purchase a single flexible commitment to cover your eligible spend across all three services. For more information about how Compute flexible CUDs apply to each service, see the service-specific CUDs documentation: CUDs documentation for Compute Engine CUDs documentation for GKE CUDs documentation for Cloud Run These commitments are ideal for scenarios where you have predictable spend needs within a Google Cloud service. You must purchase these commitments separately for each Google Cloud service and CUDs from these commitments apply only to spend within that service. You can get spend-based CUDs for the following Google Cloud services: AlloyDB for PostgreSQL Backup and DR Service Backup for GKE BigQuery Bigtable Cloud Run Dataflow Firestore Spanner Cloud SQL Google Cloud Managed Service for Apache Kafka Google Cloud NetApp Volumes Google Cloud VMware Engine Google Kubernetes Engine (Autopilot) Memorystore AlloyDB for PostgreSQL committed use discounts are spend-based CUDs that apply to all AlloyDB for PostgreSQL instance vCPU and memory usage. The discount applies to AlloyDB for PostgreSQL instances in any project or region that is associated with a single Cloud Billing account. AlloyDB for PostgreSQL commitmentsdo not applyto storage, backup, and network egress. For current rates, seeAlloyDB for PostgreSQL pricing. Backup and DR (for VMware Engine) committed use discountsapply to the combined node usage in a region. This gives you low, predictable costs, without the need to make any manual changes or updates yourself. They apply to VMware Engine backups in the regions where the service is"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-2",
    "content": "available and to which you have committed. Backup and DR (for protecting Oracle databases into a backup vault) committed use discountsapply to aggregate Oracle database protection into backup vault usage. This gives you low, predictable costs, without the need to make any manual changes or updates yourself. This flexibility saves you time and helps you to save more by achieving high utilization rates across your commitments. For current rates, seeBackup and DR pricing. Backup for GKE committed use discountsare spend-based CUDs that apply to Backup for GKE backup management, that is Pods per plan in a region. Committed use discounts don't apply to backup storage (GiB) or inter-region data transfer (GiB). For current rates, seeBackup for GKE CUDs pricing. BigQuery CUDsprovide discounted prices in exchange for your commitment to use BigQuery PAYG compute capacity for a one- or three-year term, with the commitment fee billed monthly. BigQuery CUDs apply to all compute types. For current rates, seeBigQuery pricing. As a Bigtable customer, you can purchase a commitment to receive a committed use discount on the price of Bigtable nodes. The discount applies to nodes in any project or region that is associated with a single Cloud Billing account. The discount does not apply to the cost of storage, backup storage, or network egress. For details, seeBigtable committed use discounts. For current rates, seeBigtable pricing. Cloud Run committed use discountsapply to all aggregated Cloud Run CPU, memory, and request usage in a region, giving you low, predictable costs when your code is running in one of the supported container ecosystems. Cloud Run commitmentsdo not applyto networking changes. For current rates, see theCloud Run pricing details. Dataflow committed use discountsapply"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-3",
    "content": "to your spending on the Dataflow compute capacity used by streaming jobs across projects. The discount applies to any eligible usage in Dataflow projects associated with the Cloud Billing account used to purchase the commitment, regardless of instance configuration or region. All CUDs apply to both regional and multi-region configurations. The discount doesn't apply to the cost of worker CPU and memory for batch and FlexRS jobs, Dataflow Shuffle data processed, Data Compute Units (DCUs) for batch jobs, Persistent Disk storage, GPUs, snapshots, and confidential VMs. For current rates and other details, seeDataflow pricing. As a Firestore customer, you can purchase a commitment to receive a committed use discount on the price of Firestore Read/Write/Delete operations. The discount applies to Read/Write/Delete operations in any project or region that is associated with a single Cloud Billing account. The discount doesn't apply to any other Firestore resource. For details, seeFirestore committed use discounts. For current rates, seeFirestore pricing. Spanner committed use discountsapply to all Spanner compute capacity associated with a single Cloud Billing account, regardless of region. This includes all instances in all projects, whether configured as single-region or multi-region instances. Spanner CUDs do not apply to Spanner storage, backup, or network egress. For current rates and other details, seeSpanner pricing. Cloud SQL committed use discountsprovide you the flexibility to use any machine shapes with the supported Cloud SQL database engines, without having to modify your commitments. They apply to all Cloud SQL database instance vCPU and memory usage for the service in the region you purchased the commitments,exceptshared CPU machine types (such as db-f1-micro and"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-4",
    "content": "db-g1-small). The commitments apply to usage from all supported database engines, such as MySQL, PostgreSQL, and SQL Server. Cloud SQL commitmentsdo not applyto persistent disk snapshots, storage, IP addresses, network egress, or licensing. For current rates, seeCloud SQL pricing. Managed Service for Apache Kafka committed use discounts (CUDs)are discounts that apply to the Kafka compute (vCPU and RAM) costs for running Managed Service for Apache Kafka clusters across projects. You can apply the CUDs to any Managed Service for Apache Kafka project that is associated with the Cloud Billing account used to purchase the commitment. You can apply CUDs to any available region. You cannot apply the Managed Service for Apache Kafka CUDs to the cost of storage, networking or Private Service Connect. For current rates and other details, seeManaged Service for Apache Kafka pricing. Google Cloud NetApp Volumes committed use discountsapply to the aggregate storage capacity on Flex, Standard, Premium, and Extreme service levels and regions at your billing account level. CUDs can keep your storage costs low when you have predictable storage needs. NetApp Volumes CUDs are'nt available for volume replications and backups. For current rates, seeNetApp Volumes pricing. VMware Engine committed use discountsapply to aggregate VMware Engine node usage in a region, giving you low, predictable costs, without the need to make any manual changes or updates yourself. They apply to VMware Engine node CPU and memory usage in the regions where the service is available and you have committed. Current rates and supported regions for Google Cloud VMware Engine CUDs are detailed on theVMware Engine pricing page. Google Kubernetes Engine (Autopilot Mode) committed use discountsapply to all Autopilot Pod"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-5",
    "content": "workload vCPU, memory, and ephemeral storage usage in the region in which you have purchased commitments. Google Kubernetes Engine (Autopilot Mode) CUDs don't apply to the cluster management fee or to GKE Standard mode compute nodes. For current rates, see theGoogle Kubernetes Engine pricing details. Memorystore CUDs apply to Memorystore for Valkey, Memorystore for Redis Cluster, Memorystore for Redis, and Memorystore for Memcached usage. A Memorystore CUD gives you the flexibility to use Valkey, Redis Cluster, Redis, or Memcached instance spending toward a commitment on a single Cloud Billing account. Memorystore commitmentsdon't applyto Cloud Storage storage for backups, persistence, network egress, or Memorystore for Redis M1 capacity tier instances (less than 5 GB). For current rates, see the following pages: Memorystore for Valkey committed use discounts Memorystore for Redis Cluster committed use discounts Memorystore for Redis committed use discounts Memorystore for Memcached committed use discounts Resource-based CUDs are available only for Compute Engine.Compute Engine resource-based CUDsprovide discounts in exchange for committing to using a minimum amount of Compute Engine resources in a specific region and a project. These CUDs are ideal when you have predictable resource needs. You can purchase Compute Engine resource-based commitments for the following hardware and software resources for a term duration of one or three years: vCPUs Memory GPUs Local SSD disks Sole-tenant nodes Operating system (OS) licenses Commitments for hardware resources are separate from the ones for OS licenses. You can purchase both categories of commitments for the same virtual machine (VM) instance, but you cannot purchase a single commitment that covers both hardware resources"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-6",
    "content": "and licenses. For current rates, seeVM instance pricing. The following table demonstrates the differences betweenspend-basedandresource-basedCUDs, using Compute Engine as the example service. Resource-basedcommitments are purchased in the context of an individual project, rather than that of a Cloud Billing account. You can enablediscount sharingso that Compute Engine resource-based CUDs are shared across all projects that are paid for by the same Cloud Billing account. You can change the Cloud Billing account that pays for the project where you purchased the resource-based commitments.Learn about changing the Cloud Billing account for projects. The following table summarizes the differences between resource-based CUDs and spend-based CUDs. Cloud Run Compute Engine Google Kubernetes Engine AlloyDB for PostgreSQL Backup and DR Service Backup for GKE Bigtable Cloud Run Cloud SQL Dataflow Firestore Google Cloud NetApp Volumes Google Cloud VMware Engine Google Kubernetes Engine (Autopilot) Memorystore Spanner Resource-based (available for vCPUs, memory, Local SSD disks and GPUs). You commit to purchasing a minimum amount of eligible resources. Spend-based (for example, $100/hour). You commit to spending a minimum dollar amount per hour of equivalent on-demand spend on eligible services. These CUDs apply to a specific project by default, but you canshare themacross all projects linked to the same billing account. You can buy spend-based commitments measured in dollars per hour of equivalent on-demand spend. You buy these commitments at the billing account level and they apply to eligible usage in any project linked to that billing account, across all regions. Compute Engine AlloyDB for PostgreSQL Backup and DR Service Backup for GKE Bigtable Cloud Run Cloud SQL Compute"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-7",
    "content": "Engine Dataflow Firestore Google Cloud NetApp Volumes Google Cloud VMware Engine Google Kubernetes Engine (Autopilot) Memorystore Spanner To view your spend-based and resource-based CUDs in the dashboard, complete the following steps: In the Google Cloud console, open theCommitted use discounts (CUDs)page forBilling.Go to Committed use discounts (CUDs) In the Google Cloud console, open theCommitted use discounts (CUDs)page forBilling. Go to Committed use discounts (CUDs) At the prompt, choose the Cloud Billing account for which you want to view commitments. At the prompt, choose the Cloud Billing account for which you want to view commitments. TheCommitted use discounts (CUDs)dashboard displays a list of all the commitments, across services, that are associated with your Cloud Billing account. You can view which of your commitments are expiring in the next 30 days. You can also specify yourresource-basedcommitments to automatically renew at the end of their ongoing terms by changing the setting in theAuto-renewcolumn. For information about viewingspend-basedCUDs, seeViewing spend-based commitments. For information about viewingresource-basedCUDs, seeViewing resource-based commitments. You are billed a monthly fee for the commitments you purchase. This fee is calculated when you purchase the commitments, based on the list price on the date you made the purchase. This monthly fee applies to your purchased commitments for the entire duration of the commitment period. Future changes to the list prices don't affect your commitment fee during the commitment period. To understand how your commitment fees and credits are applied to your Cloud Billing account and projects, seeAttribution of committed use discount fees and credits. Pricing for CUDs is unique for each Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/docs/cuds",
    "title": "Committed use discountsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/docs/cuds#chunk-8",
    "content": "product: Backup and DR Service Backup for GKE Bigtable CUD pricing Cloud Run pricing details Cloud SQL pricing Firestore CUD pricing Spanner CUD pricing Compute Engine pricing Google Cloud VMware Engine pricing Google Kubernetes Engine (Autopilot Mode) pricing details Memorystore for Memcached CUD pricing Memorystore for Redis CUD pricing To purchasespend-basedcommitments, seePurchasing spend-based commitments. To purchaseresource-basedcommitments for Compute Engine, see one of the following, depending on your use case: Purchase commitments without attached reservations Purchase commitments with attached reservations Purchase commitments for software licenses Learn how topurchase spend-based CUDs. Learn more aboutresource-based CUDs for Compute Engine. Learn how toview your CUDs reports. Learn how toview your Cloud Billing reports and cost trends. Understand your savings with cost breakdown reports. Learn how toexport Cloud Billing data to BigQuery. Learn how toview your cost and payment history. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-30 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/databases",
    "title": "Databases",
    "chunk_id": "https://cloud.google.com/docs/databases#chunk-0",
    "content": "Home Documentation Migrate and manage enterprise data with security, reliability, high availability, and fully-managed data services. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Expand this section to see relevant products and documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/aws",
    "title": "GKE on AWS documentation",
    "chunk_id": "https://cloud.google.com/kubernetes-engine/multi-cloud/docs/aws#chunk-0",
    "content": "Home Google Kubernetes Engine (GKE) GKE Enterprise Clusters Documentation GKE Multi-Cloud GKE on AWS GKE on AWS lets you manage GKE clusters running on AWS infrastructure through the GKE Multi-Cloud API. Combined withConnect, GKE on AWS lets you manage GKE clusters on both Google Cloud and AWS from the Google Cloud console.Learn more about how GKE on AWS works. When you create a cluster with GKE on AWS, Google creates the AWS resources you need and brings up a cluster on your behalf. You can then deploy your workloads with thegcloudandkubectlcommand-line tools. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. GKE on AWS architecture GKE on AWS architecture Quickstart: Deploy an application Quickstart: Deploy an application Prerequisites Prerequisites Create a cluster Create a cluster Create a cluster with Terraform Create a cluster with Terraform Create AWS IAM roles Create AWS IAM roles Supported AWS instance types Supported AWS instance types Supported AWS regions Supported AWS regions gcloud commands gcloud commands AWS IAM role list AWS IAM role list Troubleshooting GKE on AWS Troubleshooting GKE on AWS Get support Get support Release notes Release notes Quotas and limits Quotas and limits Contact us for GKE on AWS pricing information Contact us for GKE on AWS pricing information Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models",
    "title": "Vertex AI partner models for MaaSStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Vertex AI supports a curated list of models developed by Google partners. Partner models can be used withVertex AIas a model as a service (MaaS) and are offered as a managed API. When you use a partner model, you continue to send your requests to Vertex AI endpoints. Partner models are serverless so there's no need to provision or manage infrastructure. Partner models can be discovered using Model Garden. You can also deploy models using Model Garden. For more information, seeExplore AI models in Model Garden. While information about each available partner model can be found on its model card in Model Garden, only third-party models that perform as a MaaS with Vertex AI are documented in this guide. Anthropic's Claude and Mistral models are examples of third-party managed models that are available to use on Vertex AI. Google offers provisioned throughput for some partner models that reserves throughput capacity for your models for a fixed fee. You decide on the throughput capacity and in which regions to reserve that capacity. Because provisioned throughput requests are prioritized over the standard pay-as-you-go requests, provisioned throughput provides increased availability. When the system is overloaded, your requests can still be completed as long as the throughput remains under your reserved throughput capacity. For more information or to subscribe to the service,Contact sales. For regional endpoints, requests are served from your specified region. In cases where you have data residency requirements or if a model doesn't support the global endpoint, use the regional endpoints. When you use the global endpoint (Preview), Google can process and serve your requests from any region that is supported by the model that you"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models",
    "title": "Vertex AI partner models for MaaSStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#chunk-1",
    "content": "are using, which might result in higher latency in some cases. The global endpoint helps improve overall availability and helps reduce errors. There is no price difference with the regional endpoints when you use the global endpoint. However, the global endpoint quotas and supported model capabilities can differ from the regional endpoints. For more information, view the related third-party model page. To use the global endpoint, set the region toglobal. For example, the request URL for a curl command uses the following format:https://aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/global/publishers/PUBLISHER_NAME/models/MODEL_NAME For the Vertex AI SDK, a regional endpoint is the default. Set the region toGLOBALto use the global endpoint. The global endpoint is available for the following models: Claude Sonnet 4 Claude 3.7 Sonnet Claude 3.5 Sonnet v2 For you to enable partner models and make a prompt request, a Google Cloud administrator mustset the required permissionsandverify the organization policy allows the use of required APIs. The following roles and permissions are required to use partner models: You must have the Consumer Procurement Entitlement Manager Identity and Access Management (IAM) role. Anyone who's been granted this role can enable partner models in Model Garden. You must have the Consumer Procurement Entitlement Manager Identity and Access Management (IAM) role. Anyone who's been granted this role can enable partner models in Model Garden. You must have theaiplatform.endpoints.predictpermission. This permission is included in the Vertex AI User IAM role. For more information, seeVertex AI UserandAccess control. You must have theaiplatform.endpoints.predictpermission. This permission is included in the Vertex AI User IAM role. For more"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models",
    "title": "Vertex AI partner models for MaaSStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#chunk-2",
    "content": "information, seeVertex AI UserandAccess control. To grant the Consumer Procurement Entitlement Manager IAM roles to a user, go to theIAMpage.Go to IAM To grant the Consumer Procurement Entitlement Manager IAM roles to a user, go to theIAMpage. Go to IAM In thePrincipalcolumn, find the userprincipalfor which you want to enable access to partner models, and then clickeditEdit principalin that row. In thePrincipalcolumn, find the userprincipalfor which you want to enable access to partner models, and then clickeditEdit principalin that row. In theEdit accesspane, clickaddAdd another role. In theEdit accesspane, clickaddAdd another role. InSelect a role, selectConsumer Procurement Entitlement Manager. InSelect a role, selectConsumer Procurement Entitlement Manager. In theEdit accesspane, clickaddAdd another role. In theEdit accesspane, clickaddAdd another role. InSelect a role, selectVertex AI User. InSelect a role, selectVertex AI User. ClickSave. ClickSave. In the Google Cloud console, activate Cloud Shell.Activate Cloud Shell In the Google Cloud console, activate Cloud Shell. Activate Cloud Shell Grant the Consumer Procurement Entitlement Manager role that's required to enable partner models in Model Gardengcloudprojectsadd-iam-policy-bindingPROJECT_ID\\--member=PRINCIPAL--role=roles/consumerprocurement.entitlementManager Grant the Consumer Procurement Entitlement Manager role that's required to enable partner models in Model Garden Grant the Vertex AI User role that includes theaiplatform.endpoints.predictpermission which is required to make prompt requests:gcloudprojectsadd-iam-policy-bindingPROJECT_ID\\--member=PRINCIPAL--role=roles/aiplatform.userReplacePRINCIPALwith the identifier for the principal. The identifier takes the"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models",
    "title": "Vertex AI partner models for MaaSStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#chunk-3",
    "content": "formuser|group|serviceAccount:emailordomain:domain\u2014for example,user:cloudysanfrancisco@gmail.com,group:admins@example.com,serviceAccount:test123@example.domain.com, ordomain:example.domain.com.The output is a list of policy bindings that includes the following:- members: - user:PRINCIPALrole: roles/roles/consumerprocurement.entitlementManagerFor more information, seeGrant a single roleandgcloud projects add-iam-policy-binding. Grant the Vertex AI User role that includes theaiplatform.endpoints.predictpermission which is required to make prompt requests: ReplacePRINCIPALwith the identifier for the principal. The identifier takes the formuser|group|serviceAccount:emailordomain:domain\u2014for example,user:cloudysanfrancisco@gmail.com,group:admins@example.com,serviceAccount:test123@example.domain.com, ordomain:example.domain.com. The output is a list of policy bindings that includes the following: For more information, seeGrant a single roleandgcloud projects add-iam-policy-binding. To enable partner models, your organization policy must allow the following API: Cloud Commerce Consumer Procurement API -cloudcommerceconsumerprocurement.googleapis.com If your organization sets an organization policy torestrict service usage, then an organization administrator must verify thatcloudcommerceconsumerprocurement.googleapis.comis allowed bysetting the organization policy. Also, if you have an organization policy that restricts model usage in Model Garden, the policy must allow access to partner models. For more information, seeControl model access. ThecertificationsforGenerative AI on Vertex AIcontinue to apply when partner models are used as a managed API using Vertex AI. If you need details about the models themselves, additional information can be found in the respective Model Card,"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models",
    "title": "Vertex AI partner models for MaaSStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#chunk-4",
    "content": "or you can contact the respective model publisher. Your data is stored at rest within the selected region or multi-region for partner models on Vertex AI, but the regionalization of data processing may vary. For a detailed list of partner models' data processing commitments, seeData residency for partner models. Customer prompts and model responses are not shared with third-parties when using the Vertex AI API, including partner models. Google only processes Customer Data as instructed by the Customer, which is further described in ourCloud Data Processing Addendum. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-22 UTC."
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-0",
    "content": "Home Document AI Documentation Guides This page contains detailed information on all processors offered by Document AI. You can see a list of all processors by solution type. All Document AI processors adhere to theData Processing and Security Terms. Refer to theManaging processor versionsdocumentation for more details. Also, specific processor limits apply in addition to overall productquotas and limits. Identify and extract text in different types of documents. This processor allows you to identify and extract text, including handwritten text, from documents in more than 200 languages. The processor also uses machine learning to perform a quality assessment of a document based on the readability of its content. None None None None None None None None For more information, seeManaging processor versions. asia-south1 asia-southeast1 australia-southeast1 eu europe-west2 europe-west3 northamerica-northeast1 us Refer toSample datasetsfor sample labeled and unlabeled datasets to use for training. Extract fields from documents using generative AI or custom models; fine tune models to accurately extract data from your documents. If using generative AI for extraction, then:Only the English language is officially supported.Region availability is in theUS,EU,northamerica-northeast1andasia-southeast1. If using generative AI for extraction, then:Only the English language is officially supported.Region availability is in theUS,EU,northamerica-northeast1andasia-southeast1. Only the English language is officially supported. Region availability is in theUS,EU,northamerica-northeast1andasia-southeast1. None None None None None None None None For more information, seeManaging processor versions. You can find more information in theEnrichment & normalization, andCreate datasetpages."
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-1",
    "content": "dateTime asSTRING currency asSTRING money asgoogle.type.Money number asFLOATorINTEGER asia-south1 asia-southeast1 australia-southeast1 eu europe-west2 europe-west3 northamerica-northeast1 us Extract general key-value pairs (entity and checkbox), tables, and generic entities from documents in addition to OCR text. This processor applies advanced machine learning technologies to extract key-value pairs, checkboxes, and tables from documents more than 200 languages. This processor also leverages deep learning models to extract 11 generic entities that are common in various document types. None None email phone url date_time address person organization quantity price id page_number None None None For more information, seeManaging processor versions. asia-south1 asia-southeast1 australia-southeast1 eu europe-west2 europe-west3 northamerica-northeast1 us Extracts document content elements (text, tables, and lists) and creates context-aware chunks. Layout Parser extracts document content elements like text, tables, and lists, and creates context-aware chunks that facilitate information retrieval in generative AI and discovery applications. This parser supports PDF, HTML and DOCX files. None None For more information, seeManaging processor versions. eu us Extract from bank statements including name, account, transactions, etc. If a page of a multi-page input file is the correct document type and one of the supported versions, the processor performs entity extraction on the first supported document. If the processor doesn't find any applicable documents in the input file, the processor returns an error message. None None None None None None None None None None None None For more information, seeManaging processor versions. You can also find this information in theField"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-2",
    "content": "detectedpage. account_number account_type bank_address bank_name client_address client_name ending_balance starting_balance statement_date statement_end_date statement_start_date table_item table_item/transaction_deposit table_item/transaction_deposit_date table_item/transaction_deposit_description table_item/transaction_withdrawal table_item/transaction_withdrawal_date table_item/transaction_withdrawal_description You can find more information in theEnrichment & normalizationpage. bank_address bank_name You can find more information in theEnrichment & normalizationpage. ending_balance starting_balance statement_date statement_end_date statement_start_date table_item/transaction_deposit table_item/transaction_deposit_date table_item/transaction_withdrawal table_item/transaction_withdrawal_date eu us Extract from Form W2, including employee, employer, wages, etc. If a page of a multi-page input file is the correct document type and one of the supported versions, the processor performs entity extraction on the first supported document. If the processor doesn't find any applicable documents in the input file, the processor returns an error message. 2020 (standard and customized versions) 2019 (standard and customized versions) 2018 (standard and customized versions) None None None None AllocatedTips ControlNumber DependentCareBenefits EIN EmployeeAddress EmployeeName EmployerNameAndAddress EmployerStateIdNumber_Line1 FederalIncomeTaxWithheld FormYear LocalIncomeTax_Line1 LocalityName_Line1 LocalWagesTipsEtc_Line1 MedicareTaxWithheld MedicareWagesAndTips NonqualifiedPlans SocialSecurityTaxWithheld SocialSecurityTips SocialSecurityWages SSN State_Line1 StateIncomeTax_Line1 StateWagesTipsEtc_Line1 WagesTipsOtherCompensation None Quality improvements and supporting new fields;"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-3",
    "content": "does not include splitter. AllocatedTips ControlNumber DependentCareBenefits EIN EmployeeAddress_AdditionalStreetAddressOrPostalBox EmployeeAddress_City EmployeeAddress_State EmployeeAddress_StreetAddressOrPostalBox EmployeeAddress_Zip EmployeeName_FirstName EmployeeName_LastName EmployeeName_MiddleNameOrInitial EmployerAddress_AdditionalStreetAddressOrPostalBox EmployerAddress_City EmployerAddress_State EmployerAddress_StreetAddressOrPostalBox EmployerAddress_Zip EmployerName EmployerStateIdNumber_Line1 FederalIncomeTaxWithheld FormYear LocalIncomeTax_Line1 LocalWagesTipsEtc_Line1 LocalityName_Line1 MedicareTaxWithheld MedicareWagesAndTips NonqualifiedPlans SSN SocialSecurityTaxWithheld SocialSecurityTips SocialSecurityWages StateIncomeTax_Line1 StateWagesTipsEtc_Line1 State_Line1 WagesTipsOtherCompensation a_Code a_Value b_Code b_Value c_Code c_Value d_Code d_Value None Quality improvements and support for box 12 fields and fine-grained predictions ofEmployeeName,EmployeeAddress, andEmployerNameAndAddress, all of which are no longer part of the output and are replaced with additional fields. AllocatedTips ControlNumber DependentCareBenefits EIN EmployeeAddress_AdditionalStreetAddressOrPostalBox EmployeeAddress_City EmployeeAddress_State EmployeeAddress_StreetAddressOrPostalBox EmployeeAddress_Zip EmployeeName_FirstName EmployeeName_LastName EmployeeName_MiddleNameOrInitial EmployeeName_Suffix EmployerAddress_AdditionalStreetAddressOrPostalBox EmployerAddress_City EmployerAddress_State EmployerAddress_StreetAddressOrPostalBox EmployerAddress_Zip EmployerName EmployerStateIdNumber_Line1 FederalIncomeTaxWithheld FormYear LocalIncomeTax_Line1 LocalWagesTipsEtc_Line1 LocalityName_Line1 MedicareTaxWithheld MedicareWagesAndTips NonqualifiedPlans SSN SocialSecurityTaxWithheld"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-4",
    "content": "SocialSecurityTips SocialSecurityWages StateIncomeTax_Line1 StateWagesTipsEtc_Line1 State_Line1 WagesTipsOtherCompensation a_Code a_Value b_Code b_Value c_Code c_Value d_Code d_Value None Similar to versionpretrained-w2-v2.0-2022-03-30with further quality enhancements and introducing one more entityEmployeeName_Suffix. For more information, seeManaging processor versions. You can also find this information in theField detectedpage. ControlNumber EIN EmployeeAddress EmployeeName EmployerNameAndAddress FederalIncomeTaxWithheld MedicareTaxWithheld MedicareWagesAndTips SSN SocialSecurityTaxWithheld SocialSecurityWages WagesTipsOtherCompensation You can find more information in theEnrichment & normalizationpage. EmployerNameAndAddress EIN eu us Extract fields such as names, document ID, date of birth, etc. None None For more information, seeManaging processor versions. You can also find this information in theField detectedpage. Family Name Given Names Document Id Expiration Date Date Of Birth Issue Date MRZ Code Portrait You can find more information in theEnrichment & normalizationpage. Date Of Birth Expiration Date Issue Date eu us Extract text and values from utility bills such as supplier name and previous paid amount. None None None None For more information, seeManaging processor versions. You can also find this information in theField detectedpage. adjusted_amount amount_due balance_transfer_amount carrier currency currency_exchange_rate delivery_date deposit_credited_amount due_date freight_amount invoice_date invoice_id late_fee_amount line_item line_item/amount line_item/description line_item/frequency line_item/product_code line_item/purchase_order line_item/quantity line_item/service_address line_item/service_end_date line_item/service_id_1"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-5",
    "content": "line_item/service_id_2 line_item/service_start_date line_item/supplier_account_number line_item/tax_amount line_item/unit_number line_item/unit_of_measure line_item/unit_price line_item/usage net_amount payment_terms prior_amount_due prior_paid_amount purchase_order receiver_address receiver_email receiver_name receiver_phone receiver_tax_id receiver_website reclaimed_water remit_to_address remit_to_name service service/service_end_date service/service_id service/service_start_date service/unit_of_measure service/usage service_address service_end_date service_id service_start_date ship_from_address ship_from_name ship_to_address ship_to_name supplier_account_number supplier_address supplier_email supplier_iban supplier_name supplier_payment_ref supplier_phone supplier_registration supplier_tax_id supplier_website tampering total_amount total_tax_amount usage vat vat/amount vat/category_code vat/tax_amount vat/tax_rate You can find more information in theEnrichment & normalizationpage. adjusted_amount amount_due balance_transfer_amount currency currency_exchange_rate delivery_date due_date invoice_date late_fee_amount line_item/amount line_item/quantity line_item/tax_amount line_item/unit_price net_amount prior_amount_due prior_paid_amount total_amount total_tax_amount eu us Predict the validity of ID documents using multiple signals. Identity Document Proofing Processor is designed to help predict the validity of ID documents with four different signals.The processor currently returns information from the following signals:fraud_signals_is_identity_documentdetection: Predicts whether an image contains a recognized identity document.fraud_signals_suspicious_wordsdetection: Predicts whether words are present that aren't typical on"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-6",
    "content": "IDs.fraud_signals_image_manipulationdetection: Predicts whether the image was altered or tampered with an image editing tool.fraud_signals_online_duplicatedetection: Predicts whether the image can be found online (US only). Identity Document Proofing Processor is designed to help predict the validity of ID documents with four different signals. The processor currently returns information from the following signals: fraud_signals_is_identity_documentdetection: Predicts whether an image contains a recognized identity document. fraud_signals_suspicious_wordsdetection: Predicts whether words are present that aren't typical on IDs. fraud_signals_image_manipulationdetection: Predicts whether the image was altered or tampered with an image editing tool. fraud_signals_online_duplicatedetection: Predicts whether the image can be found online (US only). The Online Duplicate Detection feature is currently processed in US data centers. Regional and multi-regional support is unavailable for this feature outside of the US. This processor is supported by algorithms that are updated more frequently than new processor versions are released. For this reason, the processor might return different outputs over time even when using the same processor version. For example, the Online Duplicate Detection system monitors images present on the web. The system's behavior can then change more quickly than can be tracked in processor versions. Refer to notes on Responsible AI[\u2020]and Human review.[\u2021] Support for US passports, passcards and driver's licenses. None None fraud_signals_photocopy_detection None Additional photocopy detection signal fraud_signals_photocopy_detection None For more information, seeManaging processor versions. You can also find this information in theField detectedpage."
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-7",
    "content": "fraud_signals_is_identity_document fraud_signals_suspicious_words evidence_suspicious_word evidence_inconclusive_suspicious_word fraud_signals_image_manipulation fraud_signals_online_duplicate (US only) fraud_signals_photocopy_detection evidence_hostname (US only) evidence_thumbnail_url (US only) You can find more information in theEnrichment & normalizationpage. fraud_signals_image_manipulation fraud_signals_online_duplicate (US only) fraud_signals_is_identity_document fraud_signals_suspicious_words eu us Extract from pay slips, including name, business, amounts, etc. If the multi-page input document contains more than one valid pay slips, the processor extracts entities from only the first valid pay slip. If no pay slips are found in the input file, the processor returns an error message. None None net_pay net_pay_ytd employee_account_number None None None deduction_item deduction_item/deduction_type deduction_item/deduction_this_period deduction_item/deduction_ytd direct_deposit_item direct_deposit_item/direct_deposit direct_deposit_item/employee_account_number earning_item earning_item/earning_type earning_item/earning_rate earning_item/earning_hours earning_item/earning_this_period earning_item/earning_ytd page_number tax_item tax_item/tax_type tax_item/tax_this_period tax_item/tax_ytd federal_additional_tax federal_allowance federal_marital_status state_additional_tax state_allowance state_marital_status None This version assumes that the input file contains a single pay slip. Unlike the default version, this version does not check the input file for pay slips and will not return an error if no pay slips are found.Quality improvement, new fields support and new schema. Bonus, Commissions, Holiday, Overtime, Regular Pay and Vacation are now part of"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-8",
    "content": "earning_item/earning_this_period, and their year-to-date versions are in earning_item/earning_ytd. Direct Deposit and Employee Account Number are now nested under direct_deposit_item.Async page limit is 10. Quality improvement, new fields support and new schema. Bonus, Commissions, Holiday, Overtime, Regular Pay and Vacation are now part of earning_item/earning_this_period, and their year-to-date versions are in earning_item/earning_ytd. Direct Deposit and Employee Account Number are now nested under direct_deposit_item. Async page limit is 10. None None Quality improvement and uptraining enhancements. None None For more information, seeManaging processor versions. You can also find this information in theField detectedpage. bonus bonus_ytd commissions commissions_ytd direct_deposit employee_account_number (Added in \"pretrained-paystub-v1.1-2021-08-13\") employee_address employee_name employer_address employer_name end_date gross_earnings gross_earnings_ytd holiday holiday_ytd net_pay (Added in \"pretrained-paystub-v1.1-2021-08-13\") net_pay_ytd (Added in \"pretrained-paystub-v1.1-2021-08-13\") overtime overtime_ytd pay_date regular_pay regular_pay_ytd ssn start_date vacation vacation_ytd You can find more information in theEnrichment & normalizationpage. employer_address employer_name You can find more information in theEnrichment & normalizationpage. bonus bonus_ytd commissions commissions_ytd direct_deposit end_date gross_earnings gross_earnings_ytd holiday holiday_ytd net_pay net_pay_ytd overtime overtime_ytd pay_date regular_pay regular_pay_ytd start_date vacation vacation_ytd eu us Extract fields such as names, document ID, date of birth, etc. Supports all 50 States and D.C. None None For more information, seeManaging processor versions. You can also find this"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-9",
    "content": "information in theField detectedpage. Family Name Given Names Document Id Expiration Date Date Of Birth Issue Date Address Portrait You can find more information in theEnrichment & normalizationpage. Date Of Birth Expiration Date Issue Date eu us Extract text and values from expense documents such as expense date, supplier name, total amount, and currency. None None credit_card_last_four_digits line_item/quantity payment_type ja: Japanese traveler_name reservation_id line_item/transaction_date ja: Japanese it: Italian pt: Portuguese (Portugal & Brazil) traveler_name reservation_id line_item/transaction_date ja: Japanese it: Italian pt: Portuguese (Portugal & Brazil) For more information, seeManaging processor versions. You can also find this information in theField detectedpage. credit_card_last_four_digits currency end_date net_amount payment_type purchase_time receipt_date start_date supplier_address supplier_city supplier_name tip_amount total_amount total_tax_amount line_item line_item/amount line_item/description line_item/product_code You can find more information in theEnrichment & normalizationpage. supplier_address supplier_name supplier_phone You can find more information in theEnrichment & normalizationpage. currency total_amount total_tax_amount net_amount receipt_date purchase_time start_date end_date line_item/amount line_item/payment_date line_item/payment_amount asia-southeast1 australia-southeast1 eu northamerica-northeast1 us Extract text and values from invoices such as invoice number, supplier name, invoice amount, tax amount, invoice date, due date. The invoice Parser extracts both header and line item fields, such as invoice number, supplier name, invoice amount, tax amount, invoice date, due date, and line item amounts. None None None None None"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-10",
    "content": "it: Italian pt: Portuguese (Portugal & Brazil) ro: Romanian sv: Swedish et: Estonian lv: Latvian lt: Lithuanian None None None None None None For more information, seeManaging processor versions. You can also find this information in theField detectedpage. amount_paid_since_last_invoice carrier currency currency_exchange_rate delivery_date due_date freight_amount invoice_date invoice_id line_item line_item/amount line_item/description line_item/product_code line_item/purchase_order line_item/quantity line_item/unit line_item/unit_price net_amount payment_terms purchase_order receiver_address receiver_email receiver_name receiver_phone receiver_tax_id receiver_website remit_to_address remit_to_name ship_from_address ship_from_name ship_to_address ship_to_name supplier_address supplier_email supplier_iban supplier_name supplier_payment_ref supplier_phone supplier_registration supplier_tax_id supplier_website total_amount total_tax_amount vat vat/amount vat/category_code vat/tax_amount vat/tax_rate You can find more information in theEnrichment & normalizationpage. supplier_address supplier_name supplier_phone You can find more information in theEnrichment & normalizationpage. amount_paid_since_last_invoice currency currency_exchange_rate delivery_date due_date freight_amount invoice_date net_amount total_amount total_tax_amount line_item/amount line_item/quantity line_item/unit_price vat/amount vat/tax_amount vat/tax_rate asia-south1 asia-southeast1 australia-southeast1 eu northamerica-northeast1 us Train a model to classify a document type from a set of classes. asia-south1 asia-southeast1 australia-southeast1 eu europe-west2 europe-west3 northamerica-northeast1 us Train a model to split a file containing multiple documents into individual, classified documents. i18n can"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-11",
    "content": "be supported through custom training options only. asia-south1 asia-southeast1 australia-southeast1 eu europe-west2 europe-west3 northamerica-northeast1 us Get abstract and bullet point summaries for short and long documents. None None For more information, seeManaging processor versions. us To request API access, fill out and submit theDocument AI limited access customer request form. The form requests information about you, your company, and your use case. Note that a Google Cloud Project ID is required for access. To create a new Google Cloud project, or identify your existing project's Project ID see the followinginstructions. After you submit the form, the Document AI team will review your request to ensure you meet the criteria for access. If approved, you will receive an email with instructions on how to access and use this feature. [\u2020]Identity Document Proofing works to extract and evaluate information from ID documents that contributes to identifying whether the input image represents an authentic ID.At Google Cloud, we prioritize helping customers safely develop and implement AI solutions, and Identity Proofing has been developed in accordance with Google's AI Principles.Based on Google's AI Principles and current product design, we strongly recommend using caution and carefully evaluating the potential benefits and risks of using Identity Document Proofing for the following: Decision-making without a human in the loop for predictions that can impact human rights. In sensitive domains including but not limited to employment, access to public services, healthcare, and safety-critical contexts. [\u2021]Always use Identity Proofing as part of your broader identity-detection process and workflow. It is important that you have a human reviewer in your workflow to verify"
  },
  {
    "source_url": "https://cloud.google.com/document-ai/docs/processors-list",
    "title": "Processor list",
    "chunk_id": "https://cloud.google.com/document-ai/docs/processors-list#chunk-12",
    "content": "whether the predicted signals are accurate. The Identity Proofing processor isn't meant to replace human review of IDs in a workflow, but rather to assist human reviewers in validating ID documents. The Identity Proofing processor shouldn't be used as an automated decision tool to determine whether an ID is valid. With human review, customers can achieve higher document processing accuracy and help businesses evaluate predictions using purpose-built tools to enable those reviews.Make sure that you review regulations in the region where you are implementing this technology, and research existing industry guidance to learn about policy guidelines and common fairness issues. Read about fairness in machine learning, including ways to mitigate bias in training datasets, evaluate your custom models for disparities in performance, and other considerations as you use your custom model.We encourage customers to keep fairness, interpretability, and privacy and security best practices in mind when implementing Identity Proofing. To learn more about how to implement responsible AI, read Google'srecommendations for Responsible AI practices.Refer to the blog postAutomate identity document processing with Document AI]for more information on use cases and a sample application code repository. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-13 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-0",
    "content": "Home Virtual Private Cloud Documentation Guides This page is an overview of Packet Mirroring. Packet Mirroring clones the traffic of specified instances in your Virtual Private Cloud (VPC) network and forwards it for examination. Packet Mirroring captures all traffic and packet data, including payloads and headers. The capture can be configured for both egress and ingress traffic, only ingress traffic, or only egress traffic. The mirroring happens on the virtual machine (VM) instances, not on the network. Consequently, Packet Mirroring consumes additional bandwidth on the VMs. Packet Mirroring is useful when you need to monitor and analyze your security status. It exports all traffic, not only the traffic between sampling periods. For example, you can use security software that analyzes mirrored traffic to detect all threats or anomalies. Additionally, you can inspect the full traffic flow to detect application performance issues. For more information, see the exampleuse cases. Packet Mirroring copies traffic frommirrored sourcesand sends it to acollector destination. To configure Packet Mirroring, you create apacket mirroring policythat specifies the source and destination. Mirrored sourcesare Compute Engine VM instances that you can select by specifying subnets, network tags, or instance names. If you specify a subnet, all existing and future instances in that subnet are mirrored. You can specify one or more source types\u2014if an instance matches at least one of them, it's mirrored.Packet Mirroring collects traffic from an instance's network interface in the network where the packet mirroring policy applies. In cases where an instance has multiple network interfaces, the other interfaces aren't mirrored unless another policy has been configured to do so. Mirrored"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-1",
    "content": "sourcesare Compute Engine VM instances that you can select by specifying subnets, network tags, or instance names. If you specify a subnet, all existing and future instances in that subnet are mirrored. You can specify one or more source types\u2014if an instance matches at least one of them, it's mirrored. Packet Mirroring collects traffic from an instance's network interface in the network where the packet mirroring policy applies. In cases where an instance has multiple network interfaces, the other interfaces aren't mirrored unless another policy has been configured to do so. Acollector destinationis an instance group that is behind an internal load balancer. Instances in the instance group are referred to ascollector instances.When you specify the collector destination, you enter the name of a forwarding rule that is associated with the internal passthrough Network Load Balancer. Google Cloud then forwards the mirrored traffic to the collector instances. An internal load balancer for Packet Mirroring is similar to other internal load balancers except that the forwarding rule must be configured for Packet Mirroring. Any non-mirrored traffic that is sent to the load balancer is dropped. Acollector destinationis an instance group that is behind an internal load balancer. Instances in the instance group are referred to ascollector instances. When you specify the collector destination, you enter the name of a forwarding rule that is associated with the internal passthrough Network Load Balancer. Google Cloud then forwards the mirrored traffic to the collector instances. An internal load balancer for Packet Mirroring is similar to other internal load balancers except that the forwarding rule must be configured for Packet Mirroring. Any non-mirrored traffic that is sent to the"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-2",
    "content": "load balancer is dropped. By default, Packet Mirroring collects all IPv4 traffic of mirrored instances. Instead of collecting all IPv4 traffic, you can use filters to expand the traffic that's collected to include all or some IPv6 traffic. You can also use filters to narrow the traffic that's mirrored, which can help you limit the bandwidth that's used by mirrored instances. You can configure filters to collect traffic based on protocol, CIDR ranges (IPv4, IPv6, or both), direction of traffic (ingress-only, egress-only, or both), or a combination. Multiple packet mirroring policies can apply to an instance. The priority of a packet mirroring policy is always1000and cannot be changed. Identical policies are not supported. Google Cloud can send traffic to any of the load balancers that have been configured with identical packet mirroring policies. To predictably and consistently send mirrored traffic to a single load balancer, create policies that have filters with non-overlapping address ranges. If ranges overlap, set unique filter protocols. Depending on each policy's filter, Google Cloud chooses a policy for each flow. If you have distinct policies, Google Cloud uses the corresponding policy that matches the mirrored traffic. For example, you might have one policy that has the filter198.51.100.3/24:TCPand another policy that has the filter2001:db8::/64:TCP:UDP. Because the policies are distinct, there's no ambiguity about which policy Google Cloud uses. However, if you have overlapping policies, Google Cloud evaluates their filters to choose which policy to use. For example, you might have two policies, one that has a filter for10.0.0.0/24:TCPand another for10.0.0.0/16:TCP. These policies overlap because their CIDR ranges overlap. When choosing a policy, Google Cloud"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-3",
    "content": "prioritizes policies by comparing their filter's CIDR range size. Google Cloud chooses a policy based on a filter: If policies have different but overlapping CIDR ranges and the same exact protocols, Google Cloud chooses the policy that uses the most specific CIDR range. Suppose the destination for a TCP packet leaving a mirrored instance is10.240.1.4, and there are two policies with the following filters:10.240.1.0/24:ALLand10.240.0.0/16:TCP. Because the most specific match for10.240.1.4is10.240.1.0/24:ALL, Google Cloud uses the policy that has the filter10.240.1.0/24:ALL. If policies have different but overlapping CIDR ranges and the same exact protocols, Google Cloud chooses the policy that uses the most specific CIDR range. Suppose the destination for a TCP packet leaving a mirrored instance is10.240.1.4, and there are two policies with the following filters:10.240.1.0/24:ALLand10.240.0.0/16:TCP. Because the most specific match for10.240.1.4is10.240.1.0/24:ALL, Google Cloud uses the policy that has the filter10.240.1.0/24:ALL. If policies specify the same exact CIDR range with overlapping protocols, Google Cloud chooses a policy with the most specific protocol. For example, the following filters have the same range but overlapping protocols:10.240.1.0/24:TCPand10.240.1.0/24:ALL. For matching TCP traffic, Google Cloud uses the10.240.1.0/24:TCPpolicy. The10.240.1.0/24:ALLpolicy applies to matching traffic for all other protocols. If policies specify the same exact CIDR range with overlapping protocols, Google Cloud chooses a policy with the most specific protocol. For example, the following filters have the same range but overlapping protocols:10.240.1.0/24:TCPand10.240.1.0/24:ALL. For matching TCP traffic, Google Cloud uses the10.240.1.0/24:TCPpolicy."
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-4",
    "content": "The10.240.1.0/24:ALLpolicy applies to matching traffic for all other protocols. If policies have the same exact CIDR range but distinct protocols, these policies don't overlap. Google Cloud uses the policy that corresponds to the mirrored traffic's protocol. For example, you might have a policy for2001:db8::/64:TCPand another for2001:db8::/64:UDP. Depending on the mirrored traffic's protocol, Google Cloud uses either the TCP or UDP policy. If policies have the same exact CIDR range but distinct protocols, these policies don't overlap. Google Cloud uses the policy that corresponds to the mirrored traffic's protocol. For example, you might have a policy for2001:db8::/64:TCPand another for2001:db8::/64:UDP. Depending on the mirrored traffic's protocol, Google Cloud uses either the TCP or UDP policy. If overlapping policies have the same exact filter, they are identical. In this case, Google Cloud might choose the same policy or a different policy each time that matching traffic is re-evaluated against these policies. We recommend that you avoid creating identical packet mirroring policies. If overlapping policies have the same exact filter, they are identical. In this case, Google Cloud might choose the same policy or a different policy each time that matching traffic is re-evaluated against these policies. We recommend that you avoid creating identical packet mirroring policies. VPC Flow Logs doesn't log mirrored packets. If a collector instance is on a subnet that has VPC Flow Logs enabled, traffic that is sent directly to the collector instance is logged, including traffic from mirrored instances. That is, if the original destination IPv4 or IPv6 address matches the IPv4 or IPv6 address of the collector instance, the flow is logged. For more information about VPC Flow"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-5",
    "content": "Logs, seeUsing VPC Flow Logs. The following list describes constraints or behaviors with Packet Mirroring that are important to understand before you use it: Each packet mirroring policy definesmirrored sourcesand acollector destination. You must adhere to the following rules:All mirrored sources must be in the same project, VPC network, and Google Cloud region.A collector destination must be in the same region as the mirrored sources. A collector destination can be located in either the same VPC network as the mirrored sources or a VPC network connected to the mirrored sources' network using VPC Network Peering.Each mirroring policy can only reference a single collector destination. However, a single collector destination can be referenced by multiple mirroring policies. Each packet mirroring policy definesmirrored sourcesand acollector destination. You must adhere to the following rules: All mirrored sources must be in the same project, VPC network, and Google Cloud region. A collector destination must be in the same region as the mirrored sources. A collector destination can be located in either the same VPC network as the mirrored sources or a VPC network connected to the mirrored sources' network using VPC Network Peering. Each mirroring policy can only reference a single collector destination. However, a single collector destination can be referenced by multiple mirroring policies. Alllayer 4 protocolsare supported by Packet Mirroring. Alllayer 4 protocolsare supported by Packet Mirroring. You cannot mirror and collect traffic on the same network interface of a VM instance because doing this would cause a mirroring loop. You cannot mirror and collect traffic on the same network interface of a VM instance because doing this would cause a mirroring loop. To mirror"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-6",
    "content": "traffic passing between Pods on the same Google Kubernetes Engine (GKE) node, you must enableIntranode visibilityfor the cluster. To mirror traffic passing between Pods on the same Google Kubernetes Engine (GKE) node, you must enableIntranode visibilityfor the cluster. To mirror IPv6 traffic, use filters to specify the IPv6 CIDR ranges of the IPv6 traffic that you want to mirror. You can mirror all IPv6 traffic by using a CIDR range filter of::/0. You can mirror all IPv4 and IPv6 traffic by using the following comma-separated CIDR range filter:0.0.0.0/0,::/0. To mirror IPv6 traffic, use filters to specify the IPv6 CIDR ranges of the IPv6 traffic that you want to mirror. You can mirror all IPv6 traffic by using a CIDR range filter of::/0. You can mirror all IPv4 and IPv6 traffic by using the following comma-separated CIDR range filter:0.0.0.0/0,::/0. Mirroring traffic consumes bandwidth on the mirrored instance. For example, if a mirrored instance experiences 1 Gbps of ingress traffic and 1 Gbps of egress traffic, the total traffic on the instances is 1 Gbps of ingress and 3 Gbps of egress (1 Gbps of normal egress traffic and 2 Gbps of mirrored egress traffic). To limit what traffic is collected, you can use filters. Mirroring traffic consumes bandwidth on the mirrored instance. For example, if a mirrored instance experiences 1 Gbps of ingress traffic and 1 Gbps of egress traffic, the total traffic on the instances is 1 Gbps of ingress and 3 Gbps of egress (1 Gbps of normal egress traffic and 2 Gbps of mirrored egress traffic). To limit what traffic is collected, you can use filters. The cost of Packet Mirroring varies depending on the amount of egress traffic traveling from a mirrored instance to an instance group and whether the traffic travels between zones. The cost"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-7",
    "content": "of Packet Mirroring varies depending on the amount of egress traffic traveling from a mirrored instance to an instance group and whether the traffic travels between zones. Packet Mirroring applies to both ingress and egress direction. If two VM instances that are being mirrored send traffic to each other, Google Cloud collects two versions of the same packet. You can alter this behaviour by specifying that only ingress or only egress packets are mirrored. Packet Mirroring applies to both ingress and egress direction. If two VM instances that are being mirrored send traffic to each other, Google Cloud collects two versions of the same packet. You can alter this behaviour by specifying that only ingress or only egress packets are mirrored. There is a maximum number of packet mirroring policies that you can create for a project. For more information, see the per-project quotas on thequotaspage. There is a maximum number of packet mirroring policies that you can create for a project. For more information, see the per-project quotas on thequotaspage. For each packet mirroring policy, the maximum number of mirrored sources that you can specify depends on the source type:5 subnets5 tags50 instances For each packet mirroring policy, the maximum number of mirrored sources that you can specify depends on the source type: 5 subnets 5 tags 50 instances The maximum number of packet mirroring filters is 30, which is the number of IPv4 and IPv6 address ranges multiplied by the number of protocols. For example, you can specify 30 ranges and 1 protocol, which would be 30 filters. However, you cannot specify 30 ranges and 2 protocols, which would be 60 filters and greater than the maximum. The maximum number of packet mirroring filters is 30, which is the number of IPv4 and IPv6 address"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-8",
    "content": "ranges multiplied by the number of protocols. For example, you can specify 30 ranges and 1 protocol, which would be 30 filters. However, you cannot specify 30 ranges and 2 protocols, which would be 60 filters and greater than the maximum. Mirrored traffic is encrypted only if the VM encrypts that traffic at the application layer. While VM-to-VM connections within VPC networks and peered VPC networks areencrypted, the encryption and decryption happens in the hypervisors. From the perspective of the VM, this traffic is not encrypted. Mirrored traffic is encrypted only if the VM encrypts that traffic at the application layer. While VM-to-VM connections within VPC networks and peered VPC networks areencrypted, the encryption and decryption happens in the hypervisors. From the perspective of the VM, this traffic is not encrypted. The following sections describe real-world scenarios that demonstrate why you might use Packet Mirroring. Security and network engineering teams must ensure that they are catching all anomalies and threats that might indicate security breaches and intrusions. They mirror all traffic so that they can complete a comprehensive inspection of suspicious flows. Because attacks can span multiple packets, security teams must be able to get all packets for each flow. For example, the following security tools require you to capture multiple packets: Intrusion detection system (IDS) tools require multiple packets of a single flow to match a signature so that the tools can detect persistent threats. Intrusion detection system (IDS) tools require multiple packets of a single flow to match a signature so that the tools can detect persistent threats. Deep Packet Inspection engines inspect packet payloads to detect protocol anomalies. Deep Packet Inspection engines"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-9",
    "content": "inspect packet payloads to detect protocol anomalies. Network forensics for PCI compliance and other regulatory use cases require that most packets be examined. Packet Mirroring provides a solution for capturing different attack vectors, such as infrequent communication or attempted but unsuccessful communication. Network forensics for PCI compliance and other regulatory use cases require that most packets be examined. Packet Mirroring provides a solution for capturing different attack vectors, such as infrequent communication or attempted but unsuccessful communication. Network engineers can use mirrored traffic to troubleshoot performance issues reported by application and database teams. To check for networking issues, network engineers can view what's going over the wire rather than relying on application logs. For example, network engineers can use data from Packet Mirroring to complete the following tasks: Analyze protocols and behaviors so that they can find and fix issues, such as packet loss or TCP resets. Analyze protocols and behaviors so that they can find and fix issues, such as packet loss or TCP resets. Analyze (in real time) traffic patterns from remote desktop, VoIP, and other interactive applications. Network engineers can search for issues that affect the application's user experience, such as multiple packet resends or more than expected reconnections. Analyze (in real time) traffic patterns from remote desktop, VoIP, and other interactive applications. Network engineers can search for issues that affect the application's user experience, such as multiple packet resends or more than expected reconnections. You can use Packet Mirroring in various setups. The following examples show the location of collector destinations and their policies for"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-10",
    "content": "different packet mirroring configurations, such as VPC Network Peering and Shared VPC. The following example shows a packet mirroring configuration where the mirrored source and collector destination are in the same VPC network. In the preceding diagram, the packet mirroring policy is configured to mirrormirrored-subnetand send mirrored traffic to the internal passthrough Network Load Balancer. Google Cloud mirrors the traffic on existing and future instances in the subnet. All traffic to and from the internet, on-premises hosts, or Google services is mirrored. You can build a centralized collector model, where instances in different VPC networks send mirrored traffic to a collector destination in a central VPC network. That way, you can use a single destination collector. In the following example, thecollector-load-balancerinternal passthrough Network Load Balancer is in theus-central1region in thenetwork-aVPC network inproject-a. This destination collector can be used by two packet mirroring policies: policy-1collects packets from mirrored sources in theus-central1region in thenetwork-aVPC network inproject-aand sends them to thecollector-load-balancerdestination. policy-1collects packets from mirrored sources in theus-central1region in thenetwork-aVPC network inproject-aand sends them to thecollector-load-balancerdestination. policy-2collects packets from mirrored sources in theus-central1region in thenetwork-bVPC network inproject-band sends them to the samecollector-load-balancerdestination. policy-2collects packets from mirrored sources in theus-central1region in thenetwork-bVPC network inproject-band sends them to the samecollector-load-balancerdestination. Two mirroring policies are required because mirrored sources exist in different VPC networks. In the"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-11",
    "content": "preceding diagram, the collector destination collects mirrored traffic from subnets in two different networks. All resources (the source and destination) must be in the same region. The setup innetwork-ais similar to the example where themirrored source and collector destination are in the same VPC network.policy-1is configured to collect traffic fromsubnet-aand send it tocollector-ilb. policy-2is configured inproject-abut specifiessubnet-bas a mirrored source. Becausenetwork-aandnetwork-bare peered, the destination collector can collect traffic fromsubnet-b. The networks are in different projects and might have different owners. It's possible for either owner to create the packet mirroring policy if they have the right permissions: If theowners ofproject-acreate the packet mirroring policy, they must have thecompute.packetMirroringAdminrole on the network, subnet, or instances to mirror inproject-b. If theowners ofproject-acreate the packet mirroring policy, they must have thecompute.packetMirroringAdminrole on the network, subnet, or instances to mirror inproject-b. If theowners ofproject-bcreate the packet mirroring policy, they must havecompute.packetMirroringUserrole inproject-a. If theowners ofproject-bcreate the packet mirroring policy, they must havecompute.packetMirroringUserrole inproject-a. For more information about enabling private connectivity across two VPC networks, seeVPC Network Peering. In the following Shared VPC scenarios, the mirrored instances for the collector destination are all in the same Shared VPC network. Even though the resources are all in the same network, they can be in different projects, such as the host project or several different service projects. The following examples show where packet mirroring policies must be created and who"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-12",
    "content": "can create them. If both the mirrored sources and collector destination are in the same project, either in a host project or service project, the setup is similar to having everything in thesame VPC network. The project owner can create all the resources and set the required permissions in that project. For more information, seeShared VPC overiew. In the following example, the collector destination is in a service project that uses a subnet in the host project. In this case, the policy is also in the service project. The policy could also be in the host project. In the preceding diagram, the service project contains the collector instances that use the collector subnet in the Shared VPC network. The packet mirroring policy was created in the service project and is configured to mirror instances that have a network interface insubnet-mirrored. Service or host project users can create the packet mirroring policy. To do so, users must have thecompute.packetMirroringUserrole in the service project where the collector destination is located. Users must also have thecompute.packetMirroringAdminrole on the mirrored sources. In the following example, the collector destination is in the host project and mirrored instances are in the service projects. This example might apply to scenarios where developers deploy applications in service projects and use the Shared VPC network. They don't have to manage the networking infrastructure or Packet Mirroring. Instead, a centralized networking or security team, who have control over the host project and Shared VPC network, are responsible for provisioning packet mirroring policies. In the preceding diagram, the packet mirroring policy is created in the host project, where the collector destination is located. The policy is configured to"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-13",
    "content": "mirror instances in the mirrored subnet. VM instances in service projects can use the mirrored subnet, and their traffic is mirrored. Service or host project users can create the packet mirroring policy. To do so, users in the service project must have thecompute.packetMirroringUserrole in the host project. Users in the host project require thecompute.packetMirroringAdminrole for mirrored sources in the service projects. You can include VM instances that have multiple network interfaces in a packet mirroring policy. Because a policy can mirror resources from a single network, you cannot create one policy to mirror traffic for all network interfaces of an instance. If you need to mirror more than one network interface of a multiple network interface instance, you must create one packet mirroring policy for each interface because each interface connects to a unique VPC network. You are charged for the amount of data processed by Packet Mirroring. For details, seePacket Mirroringpricing. You are also charged for all the prerequisite components and egress traffic that are related to Packet Mirroring. For example, the instances that collect traffic are charged at the regular rate. Also, if packet mirroring traffic travels between zones, you are charged for the egress traffic. For pricing details, see the relatedpricing page. To create and manage packet mirroring policies, seeUse Packet Mirroring. To view metrics and check your existing packet mirroring policies, seeMonitor Packet Mirroring. For information about internal passthrough Network Load Balancers, seeInternal passthrough Network Load Balancer overview. For a list of partner providers, seePacket Mirroring partner providers. Except as otherwise noted, the content of this page is licensed under theCreative Commons"
  },
  {
    "source_url": "https://cloud.google.com/vpc/docs/packet-mirroring",
    "title": "Packet Mirroring",
    "chunk_id": "https://cloud.google.com/vpc/docs/packet-mirroring#chunk-14",
    "content": "Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-30 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma",
    "title": "Use Gemma open modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma#chunk-0",
    "content": "Home Generative AI on Vertex AI Documentation Gemma is a set of lightweight, generative artificial intelligence (AI) open models. Gemma models are available to run in your applications and on your hardware, mobile devices, or hosted services. You can also customize these models using tuning techniques so that they excel at performing tasks that matter to you and your users. Gemma models are based onGeminimodels and are intended for the AI development community to extend and take further. Fine-tuning can help improve a model's performance in specific tasks. Because models in the Gemma model family are open weight, you can tune any of them using the AI framework of your choice and the Vertex AI SDK. You can open a notebook example to fine-tune the Gemma model using a link available on the Gemma model card in Model Garden. The following Gemma models are available to use with Vertex AI. To learn more about and test the Gemma models, see their Model Garden model cards. The following are some options for where you can use Gemma: Vertex AI offers a managed platform for rapidly building and scaling machine learning projects without needing in-house MLOps expertise. You can use Vertex AI as the downstream application that serves the Gemma models. For example, you might port weights from the Keras implementation of Gemma. Next, you can use Vertex AI to serve that version of Gemma to get predictions. We recommend using Vertex AI if you want end-to-end MLOps capabilities, value-added ML features, and a serverless experience for streamlined development. To get started with Gemma, see the following notebooks: Serve Gemma 3 in Vertex AI Serve Gemma 3 in Vertex AI Serve Gemma 2 in Vertex AI Serve Gemma 2 in Vertex AI Serve Gemma in Vertex AI Serve Gemma in Vertex AI Fine-tune Gemma 3"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma",
    "title": "Use Gemma open modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma#chunk-1",
    "content": "using PEFT and then deploy to Vertex AI from Vertex Fine-tune Gemma 3 using PEFT and then deploy to Vertex AI from Vertex Fine-tune Gemma 2 using PEFT and then deploy to Vertex AI from Vertex Fine-tune Gemma 2 using PEFT and then deploy to Vertex AI from Vertex Fine-tune Gemma using PEFT and then deploy to Vertex AI from Vertex Fine-tune Gemma using PEFT and then deploy to Vertex AI from Vertex Fine-tune Gemma using PEFT and then deploy to Vertex AI from Huggingface Fine-tune Gemma using PEFT and then deploy to Vertex AI from Huggingface Fine-tune Gemma using KerasNLP and then deploy to Vertex AI Fine-tune Gemma using KerasNLP and then deploy to Vertex AI Fine-tune Gemma with Ray on Vertex AI and then deploy to Vertex AI Fine-tune Gemma with Ray on Vertex AI and then deploy to Vertex AI Run local inference with ShieldGemma 2 with Hugging Face transformers Run local inference with ShieldGemma 2 with Hugging Face transformers You can use Gemma with other Google Cloud products, such as Google Kubernetes Engine and Dataflow. Google Kubernetes Engine (GKE) is the Google Cloud solution for managed Kubernetes that provides scalability, security, resilience, and cost effectiveness. We recommend this option if you have existing Kubernetes investments, your organization has in-house MLOps expertise, or if you need granular control over complex AI/ML workloads with unique security, data pipeline, and resource management requirements. To learn more, see the following tutorials in the GKE documentation: Serve Gemma with vLLM Serve Gemma with TGI Serve Gemma with Triton and TensorRT-LLM Serve Gemma with JetStream You can use Gemma models with Dataflow forsentiment analysis. Use Dataflow to run inference pipelines that use the Gemma models. To learn more, seeRun inference pipelines"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma",
    "title": "Use Gemma open modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma#chunk-2",
    "content": "with Gemma open models. You can use Gemma with Colaboratory to create your Gemma solution. In Colab, you can use Gemma with framework options such as PyTorch and JAX. To learn more, see: Get started with Gemma using Keras. Get started with Gemma using PyTorch. Basic tuning with Gemma using Keras. Distributed tuning with Gemma using Keras. Gemma models are available in several sizes so you can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them. Each model is available in a tuned and an untuned version: Pretrained- This version of the model wasn't trained on any specific tasks or instructions beyond the Gemma core data training set. We don't recommend using this model without performing some tuning. Pretrained- This version of the model wasn't trained on any specific tasks or instructions beyond the Gemma core data training set. We don't recommend using this model without performing some tuning. Instruction-tuned- This version of the model was trained with human language interactions so that it can participate in a conversation, similar to a basic chat bot. Instruction-tuned- This version of the model was trained with human language interactions so that it can participate in a conversation, similar to a basic chat bot. Mix fine-tuned- This version of the model is fine-tuned on a mixture of academic datasets and accepts natural language prompts. Mix fine-tuned- This version of the model is fine-tuned on a mixture of academic datasets and accepts natural language prompts. Lower parameter sizes means lower resource requirements and more deployment flexibility. Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma",
    "title": "Use Gemma open modelsStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma#chunk-3",
    "content": "Instruction-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Pretrained Mix fine-tuned Pretrained Mix fine-tuned Pretrained Mix fine-tuned Pretrained Mix fine-tuned Fine-tuned Pretrained Instruction-tuned Pretrained Instruction-tuned Pretrained Gemma has been tested using Google's purpose built v5e TPU hardware and NVIDIA's L4(G2 Standard), A100(A2 Standard), H100(A3 High) GPU hardware. SeeGemma documentation. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-0",
    "content": "Home BigQuery Documentation Guides BigQuery ML lets youcreate and run machine learning (ML) modelsby using GoogleSQL queries. BigQuery ML models are stored in BigQuery datasets, similar to tables and views. BigQuery ML also lets you accessVertex AI modelsandCloud AI APIsto perform artificial intelligence (AI) tasks like text generation or machine translation. Gemini for Google Cloud also provides AI-powered assistance for BigQuery tasks. To see a list of AI-powered features in BigQuery, seeGemini in BigQuery overview. Usually, performing ML or AI on large datasets requires extensive programming and knowledge of ML frameworks. These requirements restrict solution development to a very small set of people within each company, and they exclude data analysts who understand the data but have limited ML knowledge and programming expertise. However, with BigQuery ML, SQL practitioners can use existing SQL tools and skills to build and evaluate models, and to generate results from LLMs and Cloud AI APIs. You can work with BigQuery ML capabilities by using the following: The Google Cloud console The bq command-line tool The BigQuery REST API IntegratedColab Enterprise notebooks in BigQuery External tools such as a Jupyter notebook or business intelligence platform BigQuery ML offers several advantages over other approaches to using ML or AI with a cloud-based data warehouse: BigQuery ML democratizes the use of ML and AI by empowering data analysts, the primary data warehouse users, to build and run models using existing business intelligence tools and spreadsheets. Predictive analytics can guide business decision-making across the organization. You don't need to program an ML or AI solution using Python or Java. You train models and access AI resources by using SQL\u2014a language"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-1",
    "content": "that's familiar to data analysts. BigQuery ML increases the speed of model development and innovation by removing the need to move data from the data warehouse. Instead, BigQuery ML brings ML to the data, which offers the following advantages:Reduced complexity because fewer tools are required.Increased speed to production because moving and formatting large amounts of data for Python-based ML frameworks isn't required to train a model in BigQuery.For more information, watch the videoHow to accelerate machine learning development with BigQuery ML. BigQuery ML increases the speed of model development and innovation by removing the need to move data from the data warehouse. Instead, BigQuery ML brings ML to the data, which offers the following advantages: Reduced complexity because fewer tools are required. Increased speed to production because moving and formatting large amounts of data for Python-based ML frameworks isn't required to train a model in BigQuery. For more information, watch the videoHow to accelerate machine learning development with BigQuery ML. By using the default settings in theCREATE MODELstatements and the inference functions, you can create and use BigQuery ML models even without much ML knowledge. However, having basic knowledge about the ML development lifecycle, such as feature engineering and model training, helps you optimize both your data and your model to deliver better results. We recommend using the following resources to develop familiarity with ML techniques and processes: Machine Learning Crash Course Intro to Machine Learning Data Cleaning Feature Engineering Intermediate Machine Learning You can use BigQuery ML capabilities to perform a range of generative AI tasks. Useremote models, which are BigQuery ML models over Vertex AI models,"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-2",
    "content": "to perform the following tasks:Text generationby using Vertex AI text or multimodalmodels.Text or multimodal embeddingby using Vertex AI embedding models. Useremote models, which are BigQuery ML models over Vertex AI models, to perform the following tasks: Text generationby using Vertex AI text or multimodalmodels. Text or multimodal embeddingby using Vertex AI embedding models. Use BigQuery ML functions to perform the following tasks:Generate values of a specific typeby using functions over Vertex AI hosted models.Forecastingby using a function over BigQuery ML's built-inTimesFM time series model. Use BigQuery ML functions to perform the following tasks: Generate values of a specific typeby using functions over Vertex AI hosted models. Forecastingby using a function over BigQuery ML's built-inTimesFM time series model. Useremote modelsover Cloud AI APIs to perform the following tasks:Natural language processingby using theCloud Natural Language API.Machine translationby using theCloud Translation API.Document processingby using theDocument AI API.Audio transcriptionby using theSpeech-to-Text API.Computer vision Useremote modelsover Cloud AI APIs to perform the following tasks: Natural language processingby using theCloud Natural Language API. Machine translationby using theCloud Translation API. Document processingby using theDocument AI API. Audio transcriptionby using theSpeech-to-Text API. Computer vision Amodelin BigQuery ML represents what an ML system has learned from training data. The following sections describe the types of models that BigQuery ML supports. For more information about creating reservation assignments for the different types of models, seeAssign slots to BigQuery ML workloads. The following models are built in to BigQuery ML: Contribution"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-3",
    "content": "analysis(Preview) is for determining the effect of one or more dimensions on the value for a given metric. For example, seeing the effect of store location and sales date on store revenue. For more information, seeContribution analysis overview. Linear regressionis for predicting the value of a numerical metric for new data by using a model trained on similar remote data. Labels are real-valued, meaning they cannot be positive infinity or negative infinity or a NaN (Not a Number). Logistic regressionis for the classification of two or more possible values such as whether an input islow-value,medium-value, orhigh-value. Labels can have up to 50 unique values. K-means clusteringis for data segmentation. For example, this model identifies customer segments. K-means is an unsupervised learning technique, so model training doesn't require labels or split data for training or evaluation. Matrix factorizationis for creating product recommendation systems. You can create product recommendations using historical customer behavior, transactions, and product ratings, and then use those recommendations for personalized customer experiences. Principal component analysis (PCA)is the process of computing the principal components and using them to perform a change of basis on the data. It's commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. Time series is for performing time series forecasts. You can use this feature to create millions of time series models and use them for forecasting. TheARIMA_PLUSandARIMA_PLUS_XREGtime series models offer multiple tuning options, and automatically handle anomalies, seasonality, and"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-4",
    "content": "holidays.If you don't want to manage your own time series forecasting model, you can use theAI.FORECASTfunctionwith BigQuery ML's built-inTimesFM time series model(Preview) to perform forecasting. Time series is for performing time series forecasts. You can use this feature to create millions of time series models and use them for forecasting. TheARIMA_PLUSandARIMA_PLUS_XREGtime series models offer multiple tuning options, and automatically handle anomalies, seasonality, and holidays. If you don't want to manage your own time series forecasting model, you can use theAI.FORECASTfunctionwith BigQuery ML's built-inTimesFM time series model(Preview) to perform forecasting. You can perform adry runon theCREATE MODELstatements for internally trained models to get an estimate of how much data they will process if you run them. The following models are external to BigQuery ML and trained in Vertex AI: Deep neural network (DNN)is for creating TensorFlow-based deep neural networks for classification and regression models. Wide & Deepis useful for generic large-scale regression and classification problems with sparse inputs (categorical featureswith a large number of possible feature values), such as recommender systems, search, and ranking problems. Autoencoderis for creating TensorFlow-based models with the support of sparse data representations. You can use the models in BigQuery ML for tasks such as unsupervised anomaly detection and non-linear dimensionality reduction. Boosted Treeis for creating classification and regression models that are based onXGBoost. Random forestis for constructing multiple learning method decision trees for classification, regression, and other tasks at training time. AutoMLis a supervised ML service that builds and deploys classification and"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-5",
    "content": "regression models on tabular data at high speed and scale. You can't perform adry runon theCREATE MODELstatements for externally trained models to get an estimate of how much data they will process if you run them. You can createremote modelsin BigQuery that use models deployed toVertex AI. You reference the deployed model by specifying the model'sHTTPS endpointin the remote model'sCREATE MODELstatement. TheCREATE MODELstatements for remote models don't process any bytes and don't incur BigQuery charges. BigQuery ML lets you import custom models that are trained outside of BigQuery and then perform prediction within BigQuery. You can import the following models into BigQuery fromCloud Storage: Open Neural Network Exchange (ONNX)is an open standard format for representing ML models. Using ONNX, you can make models that are trained with popular ML frameworks like PyTorch and scikit-learn available in BigQuery ML. TensorFlowis a free, open source software library for ML and artificial intelligence. You can use TensorFlow across a range of tasks, but it has a particular focus on training and inference of deep neural networks. You can load previously trained TensorFlow models into BigQuery as BigQuery ML models and then perform prediction in BigQuery ML. TensorFlow Liteis a light version of TensorFlow for deployment on mobile devices, microcontrollers, and other edge devices. TensorFlow optimizes existing TensorFlow models for reduced model size and faster inference. XGBoostis an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements ML algorithms under thegradient boostingframework. TheCREATE MODELstatements for imported models don't process any bytes and don't incur BigQuery charges. In BigQuery ML, you can"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-6",
    "content": "use a model with data from multiple BigQuery Datasets for training and for prediction. Download the model selection decision tree. BigQuery ML integrates with Vertex AI, which is the end-to-end platform for AI and ML in Google Cloud. You can register your BigQuery ML models to Model Registry in order to deploy these models to endpoints for online prediction. For more information, see the following: To learn more about using your BigQuery ML models with Vertex AI, seeManage BigQuery ML models with Vertex AI. If you aren't familiar with Vertex AI and want to learn more about how it integrates with BigQuery ML, seeVertex AI for BigQuery users. Watch the videoHow to simplify AI models with Vertex AI and BigQuery ML. You can now use Colab Enterprise notebooks to perform ML workflows in BigQuery. Notebooks let you use SQL, Python, and other popular libraries and languages to accomplish your ML tasks. For more information, seeCreate notebooks. BigQuery ML is supported in the same regions as BigQuery. For more information, seeBigQuery ML locations. You are charged for the compute resources that you use to train models and to run queries against models. The type of model that you create affects where the model is trained and the pricing that applies to that operation. Queries against models always run in BigQuery and useBigQuery compute pricing. Becauseremote modelsmake calls to Vertex AI models, queries against remote models also incur charges from Vertex AI. You are charged for the storage used by trained models, usingBigQuery storage pricing. For more information, seeBigQuery ML pricing. In addition toBigQuery ML-specific limits, queries that use BigQuery ML functions andCREATE MODELstatements are subject to the quotas and limits on BigQueryquery jobs. BigQuery ML isn't"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/bqml-introduction",
    "title": "Introduction to AI and ML in BigQuery",
    "chunk_id": "https://cloud.google.com/bigquery/docs/bqml-introduction#chunk-7",
    "content": "available in theStandard edition. To get started using BigQuery ML, seeCreate machine learning models in BigQuery ML. To learn more about machine learning and BigQuery ML, see the following resources:Applying Machine Learning to your data with Google Cloudcourse at CourseraSmart analytics and data managementtraining programMachine learning crash courseMachine learning glossary Applying Machine Learning to your data with Google Cloudcourse at Coursera Smart analytics and data managementtraining program Machine learning crash course Machine learning glossary To learn about MLOps with Model Registry, seeManage BigQuery ML models in Vertex AI. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-05 UTC."
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-0",
    "content": "Home Vertex AI Documentation Vertex AI lets you perform machine learning with tabular data using simple processes and interfaces. You can create the following model types for your tabular data problems: Binary classificationmodels predict a binary outcome (one of two classes). Use this model type for yes or no questions. For example, you might want to build a binary classification model to predict whether a customer would buy a subscription. Generally, a binary classification problem requires less data than other model types. Multi-class classificationmodels predict one class from three or more discrete classes. Use this model type for categorization. For example, as a retailer, you might want to build a multi-class classification model to segment customers into different personas. Regressionmodels predict a continuous value. For example, as a retailer, you might want to build a regression model to predict how much a customer will spend next month. Forecastingmodels predict a sequence of values. For example, as a retailer, you might want to forecast daily demand of your products for the next 3 months so that you can appropriately stock product inventories in advance. For an introduction to machine learning with tabular data, seeIntroduction to Tabular Data. For further information about Vertex AI solutions, seeVertex AI solutions for classification and regressionandVertex AI solutions for forecasting. Google is committed to making progress in followingresponsible AI practices. To this end, our ML products, including AutoML, are designed around core principles such asfairnessandhuman-centered machine learning. For more information about best practices for mitigating bias when building your own ML system, seeInclusive ML guide - AutoML. Vertex AI offers the following"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-1",
    "content": "solutions for classification and regression: Tabular Workflow for End-to-End AutoML Tabular Workflow for TabNet Tabular Workflow for Wide & Deep Classification and regression with AutoML Tabular Workflow for End-to-End AutoML is a complete AutoML pipeline for classification and regression tasks. It is similar to theAutoML API, but allows you to choose what to control and what to automate. Instead of having controls for thewholepipeline, you have controls forevery stepin the pipeline. These pipeline controls include: Data splitting Feature engineering Architecture search Model training Model ensembling Model distillation Supportslarge datasetsthat are multiple TB in size and have up to 1000 columns.Allows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search.Allows you toimprove training speedby manually selecting the hardware used for training and architecture search.Allows you toreduce model size and improve latencywith distillation or by changing the ensemble size.Each AutoML component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures, and many more details.Each AutoML component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs, and more. Supportslarge datasetsthat are multiple TB in size and have up to 1000 columns. Allows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search. Allows you toimprove training speedby manually selecting the hardware used for training and architecture search. Allows you toreduce model size and improve latencywith distillation or by"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-2",
    "content": "changing the ensemble size. Each AutoML component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures, and many more details. Each AutoML component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs, and more. To learn more about Tabular Workflows, seeTabular Workflows on Vertex AI. To learn more about Tabular Workflow for End-to-End AutoML, seeTabular Workflow for End-to-End AutoML. Preview This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. Tabular Workflow for TabNet is a pipeline that you can use to train classification or regression models.TabNetusessequential attentionto choose which features to reason from at each decision step. This promotes interpretability and more efficient learning because the learning capacity is used for the most salient features. Automatically selects the appropriate hyperparameter search space based on the dataset size, prediction type, and training budget.Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.Provides inherent model interpretability. You can get insight into which features TabNet used to make its decision.Supports GPU training. Automatically selects the appropriate hyperparameter search space based on the dataset size, prediction type, and training budget. Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-3",
    "content": "online predictions right away. Provides inherent model interpretability. You can get insight into which features TabNet used to make its decision. Supports GPU training. To learn more about Tabular Workflows, seeTabular Workflows on Vertex AI. To learn more about Tabular Workflow for TabNet, seeTabular Workflow for TabNet. Preview This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. Tabular Workflow for Wide & Deep is a pipeline that you can use to train classification or regression models.Wide & Deepjointly trains wide linear models and deep neural networks. It combines the benefits of memorization and generalization. In some online experiments, the results showed that Wide & Deep significantly increased Google store application acquisitions compared with wide-only and deep-only models. Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away. Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away. To learn more about Tabular Workflows, seeTabular Workflows on Vertex AI. To learn more about Tabular Workflow for Wide & Deep, seeTabular Workflow for Wide & Deep. Vertex AI offers integrated, fully managed pipelines for end-to-end classification or regression tasks. Vertex AI searches for the optimal set of hyperparameters, trains multiple models with multiple sets of hyperparameters and then creates a single, final model from an ensemble of the top models. Vertex AI considersneural networksand"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-4",
    "content": "boosted trees for the model types. Easy to use: model type, model parameters, and hardware are chosen for you. For further information, seeClassification and Regression Overview. Vertex AI offers the following solutions for forecasting: Tabular Workflow for Forecasting Forecasting with AutoML Forecasting with BigQuery ML ARIMA_PLUS Forecasting with Prophet Preview This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of theService Specific Terms. Pre-GA features are available \"as is\" and might have limited support. For more information, see thelaunch stage descriptions. Tabular Workflow for Forecasting is the complete pipeline for forecasting tasks. It is similar to theAutoML API, but allows you to choose what to control and what to automate. Instead of having controls for thewholepipeline, you have controls forevery stepin the pipeline. These pipeline controls include: Data splitting Feature engineering Architecture search Model training Model ensembling Supportslarge datasetsthat are up to 1TB in size and have up to 200 columns.Allows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search.Allows you toimprove training speedby manually selecting the hardware used for training and architecture search.For some model training methods, allows you toreduce model size and improve latencyby changing the ensemble size.Each component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures and many more details.Each component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs and more. Supportslarge datasetsthat are up"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-5",
    "content": "to 1TB in size and have up to 200 columns. Allows you toimprove stability and lower training timeby limiting the search space of architecture types or skipping architecture search. Allows you toimprove training speedby manually selecting the hardware used for training and architecture search. For some model training methods, allows you toreduce model size and improve latencyby changing the ensemble size. Each component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures and many more details. Each component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs and more. To learn more about Tabular Workflows, seeTabular Workflows on Vertex AI. To learn more about Tabular Workflow for Forecasting, seeTabular Workflow for Forecasting. Vertex AI offers an integrated, fully managed pipeline for end-to-end forecasting tasks. Vertex AI searches for the optimal set of hyperparameters, trains multiple models with multiple sets of hyperparameters, and then creates a single, final model from an ensemble of the top models. You can choose betweenTime series Dense Encoder (TiDE),Temporal Fusion Transformer (TFT),AutoML (L2L), and Seq2Seq+ for your model training method. Vertex AI considers onlyneural networksfor the model type. Easy to use: model parameters and hardware are chosen for you. For further information, seeForecasting Overview. BigQuery ML ARIMA_PLUSis a univariate forecasting model. As a statistical model, it is faster to train than amodel based on neural networks. We recommend training a BigQuery ML ARIMA_PLUS model if you need to perform many quick iterations of model training or if you need an inexpensive baseline to"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-6",
    "content": "measure other models against. LikeProphet, BigQuery ML ARIMA_PLUS attempts to decompose each time series into trends, seasons, and holidays, producing a forecast using the aggregation of these models' predictions. One of the many differences, however, is that BQML ARIMA+ uses ARIMA to model the trend component, while Prophet attempts to fit a curve using a piecewise logistic or linear model. Google Cloud offers a pipeline for training a BigQuery ML ARIMA_PLUS model and a pipeline for getting batch predictions from a BigQuery ML ARIMA_PLUS model. Both pipelines are instances ofVertex AI PipelinesfromGoogle Cloud Pipeline Components(GCPC). Easy to use: model parameters and hardware are chosen for you. Fast: model training gives a low-cost baseline to compare other models against. For further information, seeForecasting with ARIMA+. Prophet is a forecasting model maintained by Meta. See theProphet paperfor algorithm details and thedocumentationfor more information about the library. LikeBigQuery ML ARIMA_PLUS, Prophet attempts to decompose each time series into trends, seasons, and holidays, producing a forecast using the aggregation of these models' predictions. An important difference, however, is that BQML ARIMA+ uses ARIMA to model the trend component, while Prophet attempts to fit a curve using a piecewise logistic or linear model. Google Cloud offers a pipeline for training a Prophet model and a pipeline for getting batch predictions from a Prophet model. Both pipelines are instances ofVertex AI PipelinesfromGoogle Cloud Pipeline Components(GCPC). Integration of Prophet with Vertex AI means that you can do the following: Use Vertex AIdata splittingandwindowing strategies. Read data from either BigQuery tables or CSVs stored in Cloud Storage. Vertex AI expects each"
  },
  {
    "source_url": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview",
    "title": "Tabular data overviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/vertex-ai/docs/tabular-data/overview#chunk-7",
    "content": "row to have the same format asVertex AI Forecasting. Although Prophet is a multivariate model, Vertex AI supports only a univariate version of it. Flexible: you can improve training speed by selecting the hardware used for training For further information, seeForecasting with Prophet. Learn aboutmachine learning with tabular data. Learn aboutclassification and regression with AutoML. Learn aboutforecasting with AutoML. Learn aboutforecasting with Prophet. Learn aboutforecasting with BigQuery ML ARIMA_PLUS. Learn aboutTabular Workflows. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-08 UTC."
  },
  {
    "source_url": "https://cloud.google.com/docs/data",
    "title": "Data analytics",
    "chunk_id": "https://cloud.google.com/docs/data#chunk-0",
    "content": "Home Documentation Unlock your data's potential. Transform data into actionable AI insights with data analytics. Get access to Gemini 2.0 Flash Thinking Free monthly usage of popular products, including AI APIs and BigQuery No automatic charges, no commitment Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Use a serverless, cost-effective, and multi-cloud data and AI platform designed to help you turn big data into valuable business insights powered by Gemini. Get business intelligence in real time, built on governed data, that offers repeatable analysis and inspires in-depth understanding of the data. Manage the end-to-end data lifecycle and make it easier to manage, discover, govern, and share data and AI assets. Ingest, transform, and load data from disparate data sources in a scalable and secure fashion, and build end-to-end orchestration for the enterprise. Migrate your lakehouse or warehouse to BigQuery with easy to use tools powered by Gemini that assist in every phase of migration. Empower your data journey, from robust batch processing using managed Apache Spark and Apache Hadoop, to dynamic real-time stream processing with serverless, scalable pipelines using Apache Beam. Ingest, process, and analyze event streams in real time, and generate actionable, real-time insights. Seamlessly integrate the power of generative AI and machine learning directly within your data to unlock deeper insights. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-09 UTC."
  },
  {
    "source_url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#chunk-0",
    "content": "Home Documentation Transfer Appliance Guides Transfer Appliance is a high-capacity storage device that enables you to transfer and securely ship your data to a Google upload facility, where we upload your data to Cloud Storage. For Transfer Appliance capacities and requirements, refer to theSpecifications page. Your data and network security is important. Transfer Appliance helps ensure that you're connecting a trusted device to your equipment and network, and secures your data from end to end so that it is read by people you trust. To ensure Transfer Appliance is trusted and safe to connect to your devices, Transfer Appliance offers the following features: Tamper resistant: Bad actors cannot easily open Transfer Appliance's physical case. We also apply tamper-evident tags to the shipping case, so that you can visually inspect each appliance's integrity prior to opening the package. Tamper resistant: Bad actors cannot easily open Transfer Appliance's physical case. We also apply tamper-evident tags to the shipping case, so that you can visually inspect each appliance's integrity prior to opening the package. Ruggedized: Transfer Appliance's shipping container is ruggedized, ensuring your data arrives safely. Ruggedized: Transfer Appliance's shipping container is ruggedized, ensuring your data arrives safely. Trusted Platform Module (TPM) chip: We validate the TPM's Platform Configuration Registers to ensure that the immutable root filesystem and software components haven't been tampered with. Trusted Platform Module (TPM) chip: We validate the TPM's Platform Configuration Registers to ensure that the immutable root filesystem and software components haven't been tampered with. Hardware attestation: We use a remote attestation process to validate the appliance before you"
  },
  {
    "source_url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#chunk-1",
    "content": "can connect it to your device and copy data to it. If anything is amiss, we work with you to quickly send you a new appliance. Hardware attestation: We use a remote attestation process to validate the appliance before you can connect it to your device and copy data to it. If anything is amiss, we work with you to quickly send you a new appliance. To ensure your data is safe during and after transit, Transfer Appliance uses the following features to protect you: AES 256 encryption: Your data is encrypted with industry-standard encryption to keep it safe. AES 256 encryption: Your data is encrypted with industry-standard encryption to keep it safe. Customer-managed encryption keys: We use encryption keys that you manage using Cloud Key Management Service (Cloud KMS), enabling you to control and secure your data prior to shipping an appliance back to us. Customer-managed encryption keys: We use encryption keys that you manage using Cloud Key Management Service (Cloud KMS), enabling you to control and secure your data prior to shipping an appliance back to us. NIST 800-88 compliant data erasure: We securely erase your data from Transfer Appliance after uploading your data to Cloud Storage. You can request a wipe certificate to verify that we've wiped your data. NIST 800-88 compliant data erasure: We securely erase your data from Transfer Appliance after uploading your data to Cloud Storage. You can request a wipe certificate to verify that we've wiped your data. For more information, refer toSecurity and encryption. To enable you move data quickly and efficiently, Transfer Appliance has the following performance features: All SSD drives: Increased reliability over hard disk drives to ensure your transfer is smooth. All SSD drives: Increased reliability over hard disk drives"
  },
  {
    "source_url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#chunk-2",
    "content": "to ensure your transfer is smooth. Multiple network connectivity options: Quickly move data from your devices to Transfer Appliance, using either a 10Gbps RJ45 interface or a 40Gbps QSFP+ interface. Multiple network connectivity options: Quickly move data from your devices to Transfer Appliance, using either a 10Gbps RJ45 interface or a 40Gbps QSFP+ interface. Scalability with multiple appliances: You can scale your transfers by ordering multiple appliance to increase your transfer speed. Scalability with multiple appliances: You can scale your transfers by ordering multiple appliance to increase your transfer speed. Globally distributed processing: Reduced shipping times to and from Google ensures your data transfer to Cloud Storage is quick. Globally distributed processing: Reduced shipping times to and from Google ensures your data transfer to Cloud Storage is quick. Minimal software: For Linux and Apple macOS systems, copy directly to Transfer Appliance by mounting the exposed NFS share on the appliance to your workstation, using common software already installed on the system. For Microsoft Windows systems, copy directly to Transfer Appliance from your workstation using SCP. Minimal software: For Linux and Apple macOS systems, copy directly to Transfer Appliance by mounting the exposed NFS share on the appliance to your workstation, using common software already installed on the system. For Microsoft Windows systems, copy directly to Transfer Appliance from your workstation using SCP. Enabling online mode allows you to perform online transfers by streaming data directly to your Cloud Storage bucket after copying it to your appliance. Online transfers offer the following benefits: Quickly transfer data to Cloud Storage with low latency: Online transfers are an"
  },
  {
    "source_url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#chunk-3",
    "content": "accelerated method of transferring your data to Cloud Storage, omitting the need to wait for your appliance to be shipped back to Google before the data is copied to your destination bucket. Quickly transfer data to Cloud Storage with low latency: Online transfers are an accelerated method of transferring your data to Cloud Storage, omitting the need to wait for your appliance to be shipped back to Google before the data is copied to your destination bucket. Connect to multiple appliances: Online mode allows parallel connectivity to multiple appliances. Connect to multiple appliances: Online mode allows parallel connectivity to multiple appliances. Cost-effective: Online capability is offered as a low-cost, fully-managed method for transferring your data. Cost-effective: Online capability is offered as a low-cost, fully-managed method for transferring your data. Secure connection: Your data is encrypted during online transfers, ensuring end-to-end security. After the transfer is complete, your data is removed from the appliance. Secure connection: Your data is encrypted during online transfers, ensuring end-to-end security. After the transfer is complete, your data is removed from the appliance. Easy to enable or disable: You can toggle between online and offline mode using simple commands. For more information on how to enable or disable online mode, refer to theOnline/offline transferpage. Easy to enable or disable: You can toggle between online and offline mode using simple commands. For more information on how to enable or disable online mode, refer to theOnline/offline transferpage. Transfer Appliance is a good fit for your data transfer needs if: You are an existing Google Cloud customer. Your data resides in locations that Transfer Appliance isavailable. It would"
  },
  {
    "source_url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#chunk-4",
    "content": "take more than one week to upload your data over the network. Other Google Cloud transfer options include: Storage Transfer Serviceto move data to a Cloud Storage bucket from other cloud storage providers or from youron-premisesstorage. Storage Transfer Serviceto move data to a Cloud Storage bucket from other cloud storage providers or from youron-premisesstorage. BigQuery Data Transfer Serviceto move data from software as a service (SaaS) applications to BigQuery. BigQuery Data Transfer Serviceto move data from software as a service (SaaS) applications to BigQuery. Transfer service for on-premises datato move data from your on-premises machines to Cloud Storage. Transfer service for on-premises datato move data from your on-premises machines to Cloud Storage. Transfer Appliance is available in the following locations: For a complete list of countries where Transfer Appliance is available, refer to the Order Appliance page on the Google Cloud console. If you don't find your country listed, reach out to Support at data-support@google.com. With a typical network bandwidth of 100 Mbps, 300 terabytes of data takes about 9 months to upload. However, with Transfer Appliance, you can receive the appliance and capture 300 terabytes of data in under 25 days. Your data can be accessed in Cloud Storage within another 25 days, all without consuming any outbound network bandwidth. If you need to transfer data from researchers, vendors, or other sites to Google Cloud, Transfer Appliance can move that data for you. Once transferred toCloud StorageorBigQuery, your data is accessible via our Dataflow processing service for machine learning projects. Google Cloud Machine Learning Engine is a managed service that enables you to easily build machine learning models, that work on any type"
  },
  {
    "source_url": "https://cloud.google.com/transfer-appliance/docs/4.0/overview",
    "title": "OverviewStay organized with collectionsSave and categorize content based on your preferences.",
    "chunk_id": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#chunk-5",
    "content": "of data, of any size. Transfer Appliance can assist you in taking advantage of hybrid architectures, supporting current operations with existing on-premises infrastructure while experimenting with the cloud. By transferring a copy of your data to Google Cloud, you can decommission duplicate datasets, test cloud infrastructure, and expose your data to machine learning and analysis. Offline data transfer is suited for moving large amounts of existing backup images and archives to Cloud Storage, which can be stored in ultra low-cost, highly-durable, and highly available storage classes such asArchive Storage. For structured and unstructured data sets, whether they are small and frequently accessed or huge and rarely referenced, Google offers solutions like Cloud Storage, BigQuery, and Dataproc to store and analyze that data. For customers in the EU, appliances are shipped from Belgium. When data capture is complete, you ship the appliance to Belgium for data upload. Your data is then uploaded to a Cloud Storage location in a region that you have specified. If you choose a destination region within the EU, your data never leaves the boundaries of the European Union during any part of the data transfer process. Request Transfer Appliance. Learn more about Transfer Appliance pricing. Review the procedure for using Transfer Appliance. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-14 UTC."
  },
  {
    "source_url": "https://cloud.google.com/code/docs/shell",
    "title": "Cloud Code for Cloud Shell documentation",
    "chunk_id": "https://cloud.google.com/code/docs/shell#chunk-0",
    "content": "Home Cloud Code Documentation Cloud Code for Cloud Shell Cloud Code for Cloud Shell provides IDE support for the full development cycle of Kubernetes and Cloud Run applications, from creating a cluster to running your finished application. Build and test a proof of concept with the free trial credits and free monthly usage of 20+ products. Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses, and more. Code with Gemini Code Assist Code with Gemini Code Assist Run a Kubernetes app with Cloud Code Run a Kubernetes app with Cloud Code Deploy a Cloud Run service with Cloud Code Deploy a Cloud Run service with Cloud Code Manage Cloud APIs and Libraries Manage Cloud APIs and Libraries Migrate applications to local IDEs Migrate applications to local IDEs Work with Google Cloud and Kubernetes YAML Work with Google Cloud and Kubernetes YAML Debug a Kubernetes application Debug a Kubernetes application Create and configure a GKE cluster Create and configure a GKE cluster Deploy a Cloud Run service Deploy a Cloud Run service Cloud Code features Cloud Code features Limitations and restrictions Limitations and restrictions Getting support Getting support Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-07 UTC."
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-0",
    "content": "Home BigQuery Documentation Guides This document describes how BigQuery processes queries, and it provides an overview of several features that are useful for understanding and analyzing your data. BigQuery is optimized to run analytic queries on large datasets, including terabytes of data in seconds and petabytes in minutes. Understanding its capabilities and how it processes queries can help you maximize your data analysis investments. To take a tour of BigQuery's data analytics features directly in the Google Cloud console, clickTake the tour. Take the tour BigQuery supports several data analysis workflows: Ad hoc analysis.BigQuery usesGoogleSQL, the SQL dialect in BigQuery, to support ad hoc analysis. You can run queries in the Google Cloud console or throughthird-party toolsthat integrate with BigQuery. Ad hoc analysis.BigQuery usesGoogleSQL, the SQL dialect in BigQuery, to support ad hoc analysis. You can run queries in the Google Cloud console or throughthird-party toolsthat integrate with BigQuery. Geospatial analysis.BigQuery uses geography data types and GoogleSQL geography functions to let you analyze and visualize geospatial data. For information about these data types and functions, seeIntroduction to geospatial analytics. Geospatial analysis.BigQuery uses geography data types and GoogleSQL geography functions to let you analyze and visualize geospatial data. For information about these data types and functions, seeIntroduction to geospatial analytics. Search for data.You canindex your datato perform flexible, optimizedsearcheson unstructured text or semi-structured JSON data. Search for data.You canindex your datato perform flexible, optimizedsearcheson unstructured text or semi-structured JSON data. Search for Google Cloud resources.Usenatural language"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-1",
    "content": "search(Preview) to discover Google Cloud resources from within BigQuery. Search for Google Cloud resources.Usenatural language search(Preview) to discover Google Cloud resources from within BigQuery. Machine learning.BigQuery MLuses GoogleSQL queries to let you create and execute machine learning (ML) models in BigQuery. Machine learning.BigQuery MLuses GoogleSQL queries to let you create and execute machine learning (ML) models in BigQuery. Business intelligence.BigQuery BI Engineis a fast, in-memory analysis service that lets you build rich, interactive dashboards and reports without compromising performance, scalability, security, or data freshness. Business intelligence.BigQuery BI Engineis a fast, in-memory analysis service that lets you build rich, interactive dashboards and reports without compromising performance, scalability, security, or data freshness. AI assistance.You can useGemini in BigQueryto prepare and explore your data, generate SQL queries and Python code, and visualize your results. AI assistance.You can useGemini in BigQueryto prepare and explore your data, generate SQL queries and Python code, and visualize your results. BigQuery can help you understand your data before you start writing SQL queries. Use the following features if you're unfamiliar with your data, don't know which questions to ask, or need help writing SQL: Table explorer.Visually explore the range and frequency of values in your table and interactively build queries. Table explorer.Visually explore the range and frequency of values in your table and interactively build queries. Data insights.Generate natural language questions about your data, along with the SQL queries to answer those questions. Data insights.Generate natural language questions about your data, along with the SQL"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-2",
    "content": "queries to answer those questions. Data profile scan.See statistical characteristics of your data, including average, unique, maximum, and minimum values. Data profile scan.See statistical characteristics of your data, including average, unique, maximum, and minimum values. Data canvas.Query your data using natural language, visualize results with charts, and ask follow-up questions. Data canvas.Query your data using natural language, visualize results with charts, and ask follow-up questions. The primary way to analyze data in BigQuery is torun a SQL query. TheGoogleSQL dialectsupportsSQL:2011and includes extensions that support geospatial analysis and ML. BigQuery lets you query the following types of data sources: Data stored in BigQuery.You canload data into BigQuery, modify existing data by usingdata manipulation language (DML) statements, orwrite query resultsto a table. You canquery historical datafrom a point in time within your time travel window.You can query data stored in single-region or multi-region locations, but you can't run a query against multiple locations even if one is a single-region location and the other is the multi-region location containing that single-region location. For more information, seeLocations, reservations, and jobs. Data stored in BigQuery.You canload data into BigQuery, modify existing data by usingdata manipulation language (DML) statements, orwrite query resultsto a table. You canquery historical datafrom a point in time within your time travel window. You can query data stored in single-region or multi-region locations, but you can't run a query against multiple locations even if one is a single-region location and the other is the multi-region location containing that single-region location. For more information,"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-3",
    "content": "seeLocations, reservations, and jobs. External data.You can query various external data sources such as Cloud Storage, or database services such as Spanner or Cloud SQL. For information about how to set up connections to external sources, seeIntroduction to external data sources External data.You can query various external data sources such as Cloud Storage, or database services such as Spanner or Cloud SQL. For information about how to set up connections to external sources, seeIntroduction to external data sources Multi-cloud data.You can query data that's stored in other public clouds such as AWS or Azure. For information on how to set up connections to Amazon Simple Storage Service (Amazon S3) or Azure Blob Storage, seeIntroduction to BigQuery Omni. Multi-cloud data.You can query data that's stored in other public clouds such as AWS or Azure. For information on how to set up connections to Amazon Simple Storage Service (Amazon S3) or Azure Blob Storage, seeIntroduction to BigQuery Omni. Public datasets.You can analyze any of the datasets that are available in thepublic dataset marketplace. Public datasets.You can analyze any of the datasets that are available in thepublic dataset marketplace. BigQuery sharing (formerly Analytics Hub).You can publish and subscribe to BigQuery datasets and Pub/Sub topics to share data across organizational boundaries. For more information, seeIntroduction to BigQuery sharing. BigQuery sharing (formerly Analytics Hub).You can publish and subscribe to BigQuery datasets and Pub/Sub topics to share data across organizational boundaries. For more information, seeIntroduction to BigQuery sharing. You canquery BigQuery databy using one of the following query job types: Interactive query jobs. By default, BigQuery runs queries as interactive"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-4",
    "content": "query jobs, which are intended to start executing as quickly as possible. Interactive query jobs. By default, BigQuery runs queries as interactive query jobs, which are intended to start executing as quickly as possible. Batch query jobs. Batch queries have lower priority than interactive queries. When a project or reservation is using all of its available compute resources, batch queries are more likely to be queued and remain in the queue. After a batch query starts running, the batch query runs the same as an interactive query. For more information, seequery queues. Batch query jobs. Batch queries have lower priority than interactive queries. When a project or reservation is using all of its available compute resources, batch queries are more likely to be queued and remain in the queue. After a batch query starts running, the batch query runs the same as an interactive query. For more information, seequery queues. Continuous query jobs(Preview). With these jobs, the query runs continuously, letting you analyze incoming data in BigQuery in real time and then write the results to a BigQuery table, or export the results to Bigtable or Pub/Sub. You can use this capability to perform time sensitive tasks, such as creating and immediately acting on insights, applying real time machine learning (ML) inference, and building event-driven data pipelines. Continuous query jobs(Preview). With these jobs, the query runs continuously, letting you analyze incoming data in BigQuery in real time and then write the results to a BigQuery table, or export the results to Bigtable or Pub/Sub. You can use this capability to perform time sensitive tasks, such as creating and immediately acting on insights, applying real time machine learning (ML) inference, and building event-driven data"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-5",
    "content": "pipelines. You can run query jobs by using the following methods: Compose and run a query in theGoogle Cloud console. Run thebq querycommand in thebq command-line tool. Programmatically call thejobs.queryorjobs.insertmethod in the BigQueryREST API. Use the BigQueryclient libraries. BigQuery lets yousave queriesandshare querieswith others. When you save a query, it can be private (visible only to you), shared at the project level (visible to specific principals), or public (anyone can view it). For more information, seeWork with saved queries. Several processes occur when BigQuery runs a query: Execution tree.When you run a query, BigQuery generates anexecution treethat breaks the query into stages. These stages contain steps that can run in parallel. Execution tree.When you run a query, BigQuery generates anexecution treethat breaks the query into stages. These stages contain steps that can run in parallel. Shuffle tier.Stages communicate with one another by using a fast, distributedshuffle tierthat stores intermediate data produced by the workers of a stage. When possible, the shuffle tier leverages technologies such as a petabit network and RAM to quickly move data to worker nodes. Shuffle tier.Stages communicate with one another by using a fast, distributedshuffle tierthat stores intermediate data produced by the workers of a stage. When possible, the shuffle tier leverages technologies such as a petabit network and RAM to quickly move data to worker nodes. Query plan.When BigQuery has all the information that it needs to run a query, it generates aquery plan. You canview the query planin the Google Cloud console and use it to troubleshoot oroptimize query performance. Query plan.When BigQuery has all the information that it needs to run a query, it generates aquery"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-6",
    "content": "plan. You canview the query planin the Google Cloud console and use it to troubleshoot oroptimize query performance. Query execution graph.You can review the query plan information in graphical format for any query, whether running or completed, and seeperformance insightsto help you optimize your queries. Query execution graph.You can review the query plan information in graphical format for any query, whether running or completed, and seeperformance insightsto help you optimize your queries. Query monitoring and dynamic planning.Besides the workers that perform the work of the query plan itself, additional workers monitor and direct the overall progress of work throughout the system. As the query progresses, BigQuery might dynamically adjust the query plan to adapt to the results of the various stages. Query monitoring and dynamic planning.Besides the workers that perform the work of the query plan itself, additional workers monitor and direct the overall progress of work throughout the system. As the query progresses, BigQuery might dynamically adjust the query plan to adapt to the results of the various stages. Query results.When a query is complete, BigQuery writes the results to persistent storage and returns them to the user. This design lets BigQuery servecached resultsthe next time that query is run. Query results.When a query is complete, BigQuery writes the results to persistent storage and returns them to the user. This design lets BigQuery servecached resultsthe next time that query is run. The performance of queries that are run repeatedly on the same data can vary because of the shared nature of the BigQuery environment, use of cached query results, or because BigQuery dynamically adjusts the query plan while the query runs. For a typical busy system"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-7",
    "content": "where many queries run concurrently, BigQuery uses several processes to smooth out variances in query performance: BigQuery runs many queries in parallel and canqueue queriesto run when resources are available. BigQuery runs many queries in parallel and canqueue queriesto run when resources are available. As queries start and finish, BigQuery redistributes resources fairly between new and running queries. This process ensures that query performance doesn't depend on the order in which queries are submitted but rather on the number of queries run at a given time. As queries start and finish, BigQuery redistributes resources fairly between new and running queries. This process ensures that query performance doesn't depend on the order in which queries are submitted but rather on the number of queries run at a given time. When you run a query, you canview the query planin the Google Cloud console. You can also request execution details by using theINFORMATION_SCHEMA.JOBS*viewsor thejobs.getREST API method. The query plan includes details about query stages and steps. These details can help you identify ways to improve query performance. For example, if you notice a stage that writes a lot more output than other stages, it might mean that you need to filter earlier in the query. For more information about the query plan and query optimization, see the following resources: To learn more about the query plan and see examples of how the plan information can help you to improve query performance, seeQuery plan and timeline. For more information about query optimization in general, seeIntroduction to optimizing query performance. Monitoring and logging are crucial for running reliable applications in the cloud. BigQuery workloads are no exception, especially if your workload has"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-8",
    "content": "high volumes or is mission critical. BigQuery provides various metrics, logs, and metadata views to help you monitor your BigQuery usage. For more information, see the following resources: To learn about monitoring options in BigQuery, seeIntroduction to BigQuery monitoring. To learn about audit logs and how to analyze query behavior, seeBigQuery audit logs. BigQuery offers two pricing models for analytics: On-demand pricing.You pay for the data scanned by your queries. You have a fixed,query-processing capacityfor each project, and your cost is based on the number of bytes processed. Capacity-based pricing.You purchase dedicated query-processing capacity. For information about the two pricing models and to learn more about making reservations for capacity-based pricing, seeIntroduction to reservations. BigQuery enforces project-level quotas on running queries. For information on query quotas, seeQuotas and limits. To control query costs, BigQuery provides several options, including custom quotas and billing alerts. For more information, seeCreating custom cost controls. BigQuery supports both descriptive and predictive analytics and helps you explore your data with AI powered tools, SQL, machine learning, notebooks, and other third-party integrations. BigQuery Studio helps you discover, analyze, and run inference on data in BigQuery with the following features: A robustSQL editorthat provides code completion and generation, query validation, and estimation of bytes processed. EmbeddedPython notebooksbuilt usingColab Enterprise. Notebooks provide one-click Python development runtimes, and built-in support forBigQuery DataFrames. APySpark editorthat lets you create stored Python procedures for Apache Spark. Asset management and version history for code assets such as"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-9",
    "content": "notebooks andsaved queries, built on top ofDataform. Assistive code development in the SQL editor and in notebooks, built on top ofGemini generative AI(Preview). Dataplexfeatures fordata discovery, anddata profilinganddata qualityscans. The ability to viewjob historyon a per-user or per-project basis. The ability to analyze saved query results by connecting to other tools such as Looker and Google Sheets, and to export saved query results for use in other applications. Compute Engine API Analytics Hub API Dataform API Vertex AI API BigQuery Connection API BigQuery Data Policy API BigQuery Reservation API Dataplex API BigQuery ML lets you use SQL in BigQuery to perform machine learning (ML) and predictive analytics. For more information, seeIntroduction to BigQuery ML. In addition to running queries in BigQuery, you can analyze your data with various analytics and business intelligence tools that integrate with BigQuery, such as the following: Looker.Looker is an enterprise platform for business intelligence, data applications, and embedded analytics. The Looker platform works with many datastores including BigQuery. For information on how to connect Looker to BigQuery, seeUsing Looker. Looker.Looker is an enterprise platform for business intelligence, data applications, and embedded analytics. The Looker platform works with many datastores including BigQuery. For information on how to connect Looker to BigQuery, seeUsing Looker. Looker Studio.After you run a query, you can launch Looker Studio directly from BigQuery in the Google Cloud console. Then, in Looker Studio you can create visualizations and explore the data that's returned from the query. For information about Looker Studio, seeLooker Studio overview. Looker Studio.After you run a query, you can launch Looker"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-10",
    "content": "Studio directly from BigQuery in the Google Cloud console. Then, in Looker Studio you can create visualizations and explore the data that's returned from the query. For information about Looker Studio, seeLooker Studio overview. Connected Sheets.You can also launch Connected Sheets directly from BigQuery in the console. Connected Sheets runs BigQuery queries on your behalf either upon your request or on a defined schedule. Results of those queries are saved in your spreadsheet for analysis and sharing. For information about Connected Sheets, seeUsing connected sheets. Connected Sheets.You can also launch Connected Sheets directly from BigQuery in the console. Connected Sheets runs BigQuery queries on your behalf either upon your request or on a defined schedule. Results of those queries are saved in your spreadsheet for analysis and sharing. For information about Connected Sheets, seeUsing connected sheets. Tableau.You canconnect to a dataset from Tableau. Use BigQuery to power your charts, dashboards, and other data visualizations. Tableau.You canconnect to a dataset from Tableau. Use BigQuery to power your charts, dashboards, and other data visualizations. Several third-party analytics tools work with BigQuery. For example, you can connectTableauto BigQuery data and use its visualization tools to analyze and share your analysis. For more information on considerations when using third-party tools, seeThird-party tool integration. ODBC and JDBC drivers are available and can be used to integrate your application with BigQuery. The intent of these drivers is to help users leverage the power of BigQuery with existing tooling and infrastructure. For information on latest release and known issues, seeODBC and JDBC drivers for BigQuery. The pandas libraries likepandas-gbqlet"
  },
  {
    "source_url": "https://cloud.google.com/bigquery/docs/query-overview",
    "title": "Overview of BigQuery analytics",
    "chunk_id": "https://cloud.google.com/bigquery/docs/query-overview#chunk-11",
    "content": "you interact with BigQuery data in Jupyter notebooks. For information about this library and how it compares with using the BigQueryPython client library, seeComparison withpandas-gbq. You can also use BigQuery with other notebooks and analysis tools. For more information, seeProgrammatic analysis tools. For a full list of BigQuery analytics and broader technology partners, see thePartnerslist on the BigQuery product page. For an introduction and overview of supported SQL statements, seeIntroduction to SQL in BigQuery. To learn about the GoogleSQL syntax used for querying data in BigQuery, seeQuery syntax in GoogleSQL. Learn how torun a queryin BigQuery. Learn more aboutoptimizing query performance. Learn about getting started withnotebooks. Learn how toschedule a recurring query. Except as otherwise noted, the content of this page is licensed under theCreative Commons Attribution 4.0 License, and code samples are licensed under theApache 2.0 License. For details, see theGoogle Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-05-21 UTC."
  }
]